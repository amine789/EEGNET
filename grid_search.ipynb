{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242be690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f03e8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(568, 9, 1280, 3) (568,)\n",
      "(447, 9, 1280, 3) (447,)\n",
      "(465, 9, 1280, 3) (465,)\n",
      "(337, 9, 1280, 3) (337,)\n",
      "(506, 9, 1280, 3) (506,)\n",
      "(569, 9, 1280, 3) (569,)\n",
      "(518, 9, 1280, 3) (518,)\n",
      "(835, 9, 1280, 3) (835,)\n",
      "(381, 9, 1280, 3) (381,)\n",
      "(480, 9, 1280, 3) (480,)\n",
      "(571, 9, 1280, 3) (571,)\n",
      "(509, 9, 1280, 3) (509,)\n",
      "(473, 9, 1280, 3) (473,)\n",
      "(513, 9, 1280, 3) (513,)\n",
      "(445, 9, 1280, 3) (445,)\n",
      "(380, 9, 1280, 3) (380,)\n",
      "(325, 9, 1280, 3) (325,)\n",
      "(349, 9, 1280, 3) (349,)\n",
      "(271, 9, 1280, 3) (271,)\n",
      "(294, 9, 1280, 3) (294,)\n",
      "(459, 9, 1280, 3) (459,)\n",
      "(416, 9, 1280, 3) (416,)\n",
      "(378, 9, 1280, 3) (378,)\n",
      "(285, 9, 1280, 3) (285,)\n",
      "(255, 9, 1280, 3) (255,)\n",
      "(357, 9, 1280, 3) (357,)\n",
      "(303, 9, 1280, 3) (303,)\n",
      "(568, 9, 1280, 3) (568,)\n",
      "(651, 9, 1280, 3) (651,)\n",
      "(551, 9, 1280, 3) (551,)\n",
      "(384, 9, 1280, 3) (384,)\n",
      "(499, 9, 1280, 3) (499,)\n",
      "(410, 9, 1280, 3) (410,)\n",
      "(648, 9, 1280, 3) (648,)\n",
      "(495, 9, 1280, 3) (495,)\n",
      "(615, 9, 1280, 3) (615,)\n",
      "(348, 9, 1280, 3) (348,)\n",
      "(550, 9, 1280, 3) (550,)\n",
      "(513, 9, 1280, 3) (513,)\n",
      "(473, 9, 1280, 3) (473,)\n",
      "(494, 9, 1280, 3) (494,)\n",
      "(571, 9, 1280, 3) (571,)\n",
      "(524, 9, 1280, 3) (524,)\n",
      "(365, 9, 1280, 3) (365,)\n",
      "(591, 9, 1280, 3) (591,)\n",
      "(511, 9, 1280, 3) (511,)\n",
      "(485, 9, 1280, 3) (485,)\n",
      "(480, 9, 1280, 3) (480,)\n",
      "(567, 9, 1280, 3) (567,)\n",
      "(418, 9, 1280, 3) (418,)\n",
      "(547, 9, 1280, 3) (547,)\n",
      "(425, 9, 1280, 3) (425,)\n",
      "(583, 9, 1280, 3) (583,)\n",
      "(393, 9, 1280, 3) (393,)\n",
      "(582, 9, 1280, 3) (582,)\n",
      "(484, 9, 1280, 3) (484,)\n",
      "(544, 9, 1280, 3) (544,)\n",
      "(349, 9, 1280, 3) (349,)\n",
      "(437, 9, 1280, 3) (437,)\n",
      "(327, 9, 1280, 3) (327,)\n",
      "(423, 9, 1280, 3) (423,)\n",
      "(376, 9, 1280, 3) (376,)\n"
     ]
    }
   ],
   "source": [
    "# Filter EEG and label files\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "data_dir = \"/home/glab/Desktop/gruendemann2/Amine/full_sleep_data/\"\n",
    "\n",
    "def numerical_sort(filename):\n",
    "    # Extract the number in the filename using a regular expression\n",
    "    numbers = re.findall(r'(\\d+)', filename)\n",
    "    return [int(num) for num in numbers]\n",
    "\n",
    "#all_files = os.listdir(data_dir)\n",
    "eeg_files = sorted(glob.glob(data_dir + '*eegeegmove4model_eeg.npy'), key=numerical_sort)\n",
    "\n",
    "label_files = sorted(glob.glob(data_dir + '*eegeegmove4model_label.npy'), key=numerical_sort)\n",
    "\n",
    "#print(eeg_files)\n",
    "#print(label_files)\n",
    "###Sanity check\n",
    "for file in range(len(eeg_files)):\n",
    "    eeg = np.load(os.path.join(data_dir, eeg_files[file]))\n",
    "    labels = np.load(os.path.join(data_dir, label_files[file]))\n",
    "    print(eeg.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2b0dd7",
   "metadata": {},
   "source": [
    "### Data Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cce1fe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "(568, 1280, 2) (568,)\n",
      "(447, 1280, 2) (447,)\n",
      "(465, 1280, 2) (465,)\n",
      "(337, 1280, 2) (337,)\n",
      "(506, 1280, 2) (506,)\n",
      "(569, 1280, 2) (569,)\n",
      "(518, 1280, 2) (518,)\n",
      "(835, 1280, 2) (835,)\n",
      "(381, 1280, 2) (381,)\n",
      "(480, 1280, 2) (480,)\n",
      "(571, 1280, 2) (571,)\n",
      "(509, 1280, 2) (509,)\n",
      "(473, 1280, 2) (473,)\n",
      "(513, 1280, 2) (513,)\n",
      "(445, 1280, 2) (445,)\n",
      "(380, 1280, 2) (380,)\n",
      "(325, 1280, 2) (325,)\n",
      "(349, 1280, 2) (349,)\n",
      "(271, 1280, 2) (271,)\n",
      "(294, 1280, 2) (294,)\n",
      "(459, 1280, 2) (459,)\n",
      "(416, 1280, 2) (416,)\n",
      "(378, 1280, 2) (378,)\n",
      "(285, 1280, 2) (285,)\n",
      "(255, 1280, 2) (255,)\n",
      "(357, 1280, 2) (357,)\n",
      "(303, 1280, 2) (303,)\n",
      "(568, 1280, 2) (568,)\n",
      "(651, 1280, 2) (651,)\n",
      "(551, 1280, 2) (551,)\n",
      "(384, 1280, 2) (384,)\n",
      "(499, 1280, 2) (499,)\n",
      "(410, 1280, 2) (410,)\n",
      "(648, 1280, 2) (648,)\n",
      "(495, 1280, 2) (495,)\n",
      "(615, 1280, 2) (615,)\n",
      "(348, 1280, 2) (348,)\n",
      "(550, 1280, 2) (550,)\n",
      "(513, 1280, 2) (513,)\n",
      "(473, 1280, 2) (473,)\n",
      "(494, 1280, 2) (494,)\n",
      "(571, 1280, 2) (571,)\n",
      "(524, 1280, 2) (524,)\n",
      "(365, 1280, 2) (365,)\n",
      "(591, 1280, 2) (591,)\n",
      "(511, 1280, 2) (511,)\n",
      "(485, 1280, 2) (485,)\n",
      "(480, 1280, 2) (480,)\n",
      "(567, 1280, 2) (567,)\n",
      "(418, 1280, 2) (418,)\n",
      "(547, 1280, 2) (547,)\n",
      "(425, 1280, 2) (425,)\n",
      "(583, 1280, 2) (583,)\n",
      "(393, 1280, 2) (393,)\n",
      "(582, 1280, 2) (582,)\n",
      "(484, 1280, 2) (484,)\n",
      "(544, 1280, 2) (544,)\n",
      "(349, 1280, 2) (349,)\n",
      "(437, 1280, 2) (437,)\n",
      "(327, 1280, 2) (327,)\n",
      "(423, 1280, 2) (423,)\n",
      "(376, 1280, 2) (376,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from utils import z_score_normalize# For classification report\n",
    "train_eeg_data = None\n",
    "train_labels = None\n",
    "val_eeg_data = None\n",
    "val_labels = None\n",
    "# Set device to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "test_index=22\n",
    "data_dir = \"/home/glab/Desktop/gruendemann2/Amine/full_sleep_data/\"\n",
    "\n",
    "# List all files in the directory\n",
    "\n",
    "for file in range(len(eeg_files)):\n",
    "        if 1 != 2:  # Exclude the current test index\n",
    "            # Load EEG data\n",
    "            eeg = np.load(os.path.join(data_dir, eeg_files[file]))\n",
    "            \n",
    "            # Exclude the first 8 blocks and keep only the last block (block 9)\n",
    "            eeg = eeg[:, -1, :, :]  # Shape: (n_samples, 1280, 3)\n",
    "            \n",
    "            # Exclude the third channel (keep only the first two channels)\n",
    "            eeg = eeg[:, :, :2]  # Shape: (n_samples, 1280, 2)\n",
    "            \n",
    "            \n",
    "            # Load labels\n",
    "            label = np.load(os.path.join(data_dir, label_files[file]))\n",
    "            label[label == 3] = 2\n",
    "            \n",
    "            \n",
    "            assert eeg.shape[0] == label.shape[0], f\"Mismatch in number of samples between EEG data and labels for file {eeg_files[file]} and {label_files[file]}: \" \\\n",
    "                                               f\"{eeg.shape[0]} samples in EEG, {label.shape[0]} samples in labels.\"\n",
    "\n",
    "            print(eeg.shape, label.shape)\n",
    "            for channel in range(eeg.shape[2]):  # Iterate over channels\n",
    "                eeg[:, :, channel] = z_score_normalize(eeg[:, :, channel])\n",
    "            # Stack EEG data under each other\n",
    "            if train_eeg_data is None:\n",
    "                train_eeg_data = eeg\n",
    "                \n",
    "            else:\n",
    "                train_eeg_data = np.vstack((train_eeg_data, eeg))\n",
    "\n",
    "            # Stack labels under each other\n",
    "            if train_labels is None:\n",
    "                train_labels = label\n",
    "            else:\n",
    "                train_labels = np.hstack((train_labels, label))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d7b882b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: [0 1 2]\n",
      "Class weights: [0.58754096 6.83961147 0.86821845]\n",
      "Class 0: 16379 occurrences\n",
      "Class 1: 1407 occurrences\n",
      "Class 2: 11084 occurrences\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "# Convert train_eeg_data and train_labels to PyTorch tensors\n",
    "# Ensure that train_eeg_data is already in the correct format (numpy or list) before converting\n",
    "train_eeg_data = torch.tensor(train_eeg_data, dtype=torch.float32).to(device)  # Convert to float32 for input data\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long).to(device)  # Convert to long (int64) for labels\n",
    "\n",
    "# Ensure both 'train_labels' and 'train_eeg_data' are numpy arrays to compute class weights\n",
    "train_labels_np = train_labels.cpu().numpy()  # Convert to numpy array for sklearn\n",
    "classes_np = np.unique(train_labels_np)  # Get unique classes from labels\n",
    "\n",
    "# Print the unique classes to make sure they are correctly identified\n",
    "print(f\"Unique classes: {classes_np}\")\n",
    "\n",
    "# Compute class weights using sklearn, with class_weight='balanced'\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes_np, y=train_labels_np)\n",
    "\n",
    "# Print the class weights to verify the computed values\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "label_counts = torch.bincount(train_labels)\n",
    "\n",
    "# Print the occurrences for each class\n",
    "for label, count in enumerate(label_counts):\n",
    "    print(f\"Class {label}: {count.item()} occurrences\")\n",
    "\n",
    "# Convert class_weights to a tensor and move to the correct device\n",
    "\n",
    "\n",
    "# You can then use this criterion for training:\n",
    "# outputs = model(inputs) \n",
    "# loss = criterion(outputs, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2270f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 22:20:13,381] A new study created in memory with name: no-name-1db7fda6-b9da-4e91-a325-713fbfaeaeb6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trial 1\n",
      "Training with F1=8, F2=16, D=2, dropout=0.634131042579391, LR=2.5471659095775814e-05, BS=128, WD=0.0003030088405181484\n",
      "Epoch 1/300 - Train Loss: 0.9987, Val Loss: 0.9704\n",
      "Epoch 2/300 - Train Loss: 0.8394, Val Loss: 0.8514\n",
      "Epoch 3/300 - Train Loss: 0.7257, Val Loss: 0.7243\n",
      "Epoch 4/300 - Train Loss: 0.6039, Val Loss: 0.5786\n",
      "Epoch 5/300 - Train Loss: 0.4864, Val Loss: 0.4522\n",
      "Epoch 6/300 - Train Loss: 0.3994, Val Loss: 0.3651\n",
      "Epoch 7/300 - Train Loss: 0.3362, Val Loss: 0.3167\n",
      "Epoch 8/300 - Train Loss: 0.2871, Val Loss: 0.2623\n",
      "Epoch 9/300 - Train Loss: 0.2558, Val Loss: 0.2385\n",
      "Epoch 10/300 - Train Loss: 0.2318, Val Loss: 0.2120\n",
      "Epoch 11/300 - Train Loss: 0.2145, Val Loss: 0.2019\n",
      "Epoch 12/300 - Train Loss: 0.2031, Val Loss: 0.1825\n",
      "Epoch 13/300 - Train Loss: 0.1884, Val Loss: 0.1775\n",
      "Epoch 14/300 - Train Loss: 0.1849, Val Loss: 0.1673\n",
      "Epoch 15/300 - Train Loss: 0.1774, Val Loss: 0.1699\n",
      "Epoch 16/300 - Train Loss: 0.1723, Val Loss: 0.1607\n",
      "Epoch 17/300 - Train Loss: 0.1721, Val Loss: 0.1609\n",
      "Epoch 18/300 - Train Loss: 0.1657, Val Loss: 0.1578\n",
      "Epoch 19/300 - Train Loss: 0.1654, Val Loss: 0.1513\n",
      "Epoch 20/300 - Train Loss: 0.1626, Val Loss: 0.1497\n",
      "Epoch 21/300 - Train Loss: 0.1588, Val Loss: 0.1440\n",
      "Epoch 22/300 - Train Loss: 0.1597, Val Loss: 0.1421\n",
      "Epoch 23/300 - Train Loss: 0.1561, Val Loss: 0.1431\n",
      "Epoch 24/300 - Train Loss: 0.1548, Val Loss: 0.1384\n",
      "Epoch 25/300 - Train Loss: 0.1522, Val Loss: 0.1363\n",
      "Epoch 26/300 - Train Loss: 0.1503, Val Loss: 0.1339\n",
      "Epoch 27/300 - Train Loss: 0.1486, Val Loss: 0.1326\n",
      "Epoch 28/300 - Train Loss: 0.1482, Val Loss: 0.1303\n",
      "Epoch 29/300 - Train Loss: 0.1456, Val Loss: 0.1276\n",
      "Epoch 30/300 - Train Loss: 0.1448, Val Loss: 0.1275\n",
      "Epoch 31/300 - Train Loss: 0.1460, Val Loss: 0.1325\n",
      "Epoch 32/300 - Train Loss: 0.1439, Val Loss: 0.1255\n",
      "Epoch 33/300 - Train Loss: 0.1413, Val Loss: 0.1254\n",
      "Epoch 34/300 - Train Loss: 0.1418, Val Loss: 0.1227\n",
      "Epoch 35/300 - Train Loss: 0.1404, Val Loss: 0.1171\n",
      "Epoch 36/300 - Train Loss: 0.1402, Val Loss: 0.1225\n",
      "Epoch 37/300 - Train Loss: 0.1383, Val Loss: 0.1190\n",
      "Epoch 38/300 - Train Loss: 0.1379, Val Loss: 0.1160\n",
      "Epoch 39/300 - Train Loss: 0.1372, Val Loss: 0.1161\n",
      "Epoch 40/300 - Train Loss: 0.1370, Val Loss: 0.1164\n",
      "Epoch 41/300 - Train Loss: 0.1342, Val Loss: 0.1129\n",
      "Epoch 42/300 - Train Loss: 0.1336, Val Loss: 0.1151\n",
      "Epoch 43/300 - Train Loss: 0.1337, Val Loss: 0.1152\n",
      "Epoch 44/300 - Train Loss: 0.1327, Val Loss: 0.1162\n",
      "Epoch 45/300 - Train Loss: 0.1311, Val Loss: 0.1094\n",
      "Epoch 46/300 - Train Loss: 0.1321, Val Loss: 0.1137\n",
      "Epoch 47/300 - Train Loss: 0.1312, Val Loss: 0.1096\n",
      "Epoch 48/300 - Train Loss: 0.1297, Val Loss: 0.1083\n",
      "Epoch 49/300 - Train Loss: 0.1293, Val Loss: 0.1092\n",
      "Epoch 50/300 - Train Loss: 0.1304, Val Loss: 0.1073\n",
      "Epoch 51/300 - Train Loss: 0.1281, Val Loss: 0.1104\n",
      "Epoch 52/300 - Train Loss: 0.1300, Val Loss: 0.1058\n",
      "Epoch 53/300 - Train Loss: 0.1273, Val Loss: 0.1092\n",
      "Epoch 54/300 - Train Loss: 0.1252, Val Loss: 0.1074\n",
      "Epoch 55/300 - Train Loss: 0.1276, Val Loss: 0.1059\n",
      "Epoch 56/300 - Train Loss: 0.1258, Val Loss: 0.1027\n",
      "Epoch 57/300 - Train Loss: 0.1235, Val Loss: 0.1053\n",
      "Epoch 58/300 - Train Loss: 0.1229, Val Loss: 0.1030\n",
      "Epoch 59/300 - Train Loss: 0.1212, Val Loss: 0.1037\n",
      "Epoch 60/300 - Train Loss: 0.1236, Val Loss: 0.1023\n",
      "Epoch 61/300 - Train Loss: 0.1231, Val Loss: 0.1004\n",
      "Epoch 62/300 - Train Loss: 0.1240, Val Loss: 0.1061\n",
      "Epoch 63/300 - Train Loss: 0.1214, Val Loss: 0.1012\n",
      "Epoch 64/300 - Train Loss: 0.1189, Val Loss: 0.0997\n",
      "Epoch 65/300 - Train Loss: 0.1199, Val Loss: 0.1015\n",
      "Epoch 66/300 - Train Loss: 0.1196, Val Loss: 0.1004\n",
      "Epoch 67/300 - Train Loss: 0.1174, Val Loss: 0.1008\n",
      "Epoch 68/300 - Train Loss: 0.1184, Val Loss: 0.1001\n",
      "Epoch 69/300 - Train Loss: 0.1188, Val Loss: 0.0988\n",
      "Epoch 70/300 - Train Loss: 0.1189, Val Loss: 0.1004\n",
      "Epoch 71/300 - Train Loss: 0.1181, Val Loss: 0.1006\n",
      "Epoch 72/300 - Train Loss: 0.1164, Val Loss: 0.0987\n",
      "Epoch 73/300 - Train Loss: 0.1169, Val Loss: 0.0988\n",
      "Epoch 74/300 - Train Loss: 0.1157, Val Loss: 0.0995\n",
      "Epoch 75/300 - Train Loss: 0.1159, Val Loss: 0.0989\n",
      "Epoch 76/300 - Train Loss: 0.1158, Val Loss: 0.1005\n",
      "Epoch 77/300 - Train Loss: 0.1134, Val Loss: 0.1001\n",
      "Epoch 78/300 - Train Loss: 0.1132, Val Loss: 0.0971\n",
      "Epoch 79/300 - Train Loss: 0.1152, Val Loss: 0.0995\n",
      "Epoch 80/300 - Train Loss: 0.1155, Val Loss: 0.0993\n",
      "Epoch 81/300 - Train Loss: 0.1121, Val Loss: 0.0986\n",
      "Epoch 82/300 - Train Loss: 0.1134, Val Loss: 0.0970\n",
      "Epoch 83/300 - Train Loss: 0.1123, Val Loss: 0.0983\n",
      "Epoch 84/300 - Train Loss: 0.1118, Val Loss: 0.0961\n",
      "Epoch 85/300 - Train Loss: 0.1109, Val Loss: 0.0966\n",
      "Epoch 86/300 - Train Loss: 0.1118, Val Loss: 0.0975\n",
      "Epoch 87/300 - Train Loss: 0.1122, Val Loss: 0.1003\n",
      "Epoch 88/300 - Train Loss: 0.1101, Val Loss: 0.0992\n",
      "Epoch 89/300 - Train Loss: 0.1140, Val Loss: 0.0974\n",
      "Epoch 90/300 - Train Loss: 0.1113, Val Loss: 0.0978\n",
      "Epoch 91/300 - Train Loss: 0.1111, Val Loss: 0.0963\n",
      "Epoch 92/300 - Train Loss: 0.1112, Val Loss: 0.0963\n",
      "Epoch 93/300 - Train Loss: 0.1109, Val Loss: 0.0980\n",
      "Epoch 94/300 - Train Loss: 0.1103, Val Loss: 0.0979\n",
      "Epoch 95/300 - Train Loss: 0.1128, Val Loss: 0.0955\n",
      "Epoch 96/300 - Train Loss: 0.1106, Val Loss: 0.0970\n",
      "Epoch 97/300 - Train Loss: 0.1104, Val Loss: 0.0947\n",
      "Epoch 98/300 - Train Loss: 0.1093, Val Loss: 0.0952\n",
      "Epoch 99/300 - Train Loss: 0.1105, Val Loss: 0.0934\n",
      "Epoch 100/300 - Train Loss: 0.1083, Val Loss: 0.0955\n",
      "Epoch 101/300 - Train Loss: 0.1117, Val Loss: 0.0930\n",
      "Epoch 102/300 - Train Loss: 0.1091, Val Loss: 0.0934\n",
      "Epoch 103/300 - Train Loss: 0.1080, Val Loss: 0.0953\n",
      "Epoch 104/300 - Train Loss: 0.1102, Val Loss: 0.0948\n",
      "Epoch 105/300 - Train Loss: 0.1103, Val Loss: 0.0977\n",
      "Epoch 106/300 - Train Loss: 0.1078, Val Loss: 0.0914\n",
      "Epoch 107/300 - Train Loss: 0.1086, Val Loss: 0.0936\n",
      "Epoch 108/300 - Train Loss: 0.1071, Val Loss: 0.0955\n",
      "Epoch 109/300 - Train Loss: 0.1071, Val Loss: 0.0944\n",
      "Epoch 110/300 - Train Loss: 0.1085, Val Loss: 0.0958\n",
      "Epoch 111/300 - Train Loss: 0.1083, Val Loss: 0.0919\n",
      "Epoch 112/300 - Train Loss: 0.1093, Val Loss: 0.0967\n",
      "Epoch 113/300 - Train Loss: 0.1090, Val Loss: 0.0935\n",
      "Epoch 114/300 - Train Loss: 0.1085, Val Loss: 0.0926\n",
      "Epoch 115/300 - Train Loss: 0.1078, Val Loss: 0.0950\n",
      "Epoch 116/300 - Train Loss: 0.1111, Val Loss: 0.0919\n",
      "Epoch 117/300 - Train Loss: 0.1079, Val Loss: 0.0906\n",
      "Epoch 118/300 - Train Loss: 0.1086, Val Loss: 0.0915\n",
      "Epoch 119/300 - Train Loss: 0.1085, Val Loss: 0.0951\n",
      "Epoch 120/300 - Train Loss: 0.1074, Val Loss: 0.0916\n",
      "Epoch 121/300 - Train Loss: 0.1059, Val Loss: 0.0932\n",
      "Epoch 122/300 - Train Loss: 0.1067, Val Loss: 0.0965\n",
      "Epoch 123/300 - Train Loss: 0.1080, Val Loss: 0.0896\n",
      "Epoch 124/300 - Train Loss: 0.1073, Val Loss: 0.0957\n",
      "Epoch 125/300 - Train Loss: 0.1071, Val Loss: 0.0916\n",
      "Epoch 126/300 - Train Loss: 0.1080, Val Loss: 0.0915\n",
      "Epoch 127/300 - Train Loss: 0.1076, Val Loss: 0.0917\n",
      "Epoch 128/300 - Train Loss: 0.1086, Val Loss: 0.0975\n",
      "Epoch 129/300 - Train Loss: 0.1080, Val Loss: 0.0938\n",
      "Epoch 130/300 - Train Loss: 0.1061, Val Loss: 0.0919\n",
      "Epoch 131/300 - Train Loss: 0.1057, Val Loss: 0.0932\n",
      "Epoch 132/300 - Train Loss: 0.1048, Val Loss: 0.0915\n",
      "Epoch 133/300 - Train Loss: 0.1065, Val Loss: 0.0938\n",
      "Epoch 134/300 - Train Loss: 0.1074, Val Loss: 0.0900\n",
      "Epoch 135/300 - Train Loss: 0.1074, Val Loss: 0.0903\n",
      "Epoch 136/300 - Train Loss: 0.1067, Val Loss: 0.0945\n",
      "Epoch 137/300 - Train Loss: 0.1053, Val Loss: 0.0911\n",
      "Epoch 138/300 - Train Loss: 0.1060, Val Loss: 0.0940\n",
      "Epoch 139/300 - Train Loss: 0.1037, Val Loss: 0.0942\n",
      "Epoch 140/300 - Train Loss: 0.1051, Val Loss: 0.0940\n",
      "Epoch 141/300 - Train Loss: 0.1073, Val Loss: 0.0925\n",
      "Epoch 142/300 - Train Loss: 0.1068, Val Loss: 0.0911\n",
      "Epoch 143/300 - Train Loss: 0.1076, Val Loss: 0.0900\n",
      "Epoch 144/300 - Train Loss: 0.1033, Val Loss: 0.0908\n",
      "Epoch 145/300 - Train Loss: 0.1050, Val Loss: 0.0916\n",
      "Epoch 146/300 - Train Loss: 0.1089, Val Loss: 0.0921\n",
      "Epoch 147/300 - Train Loss: 0.1053, Val Loss: 0.0936\n",
      "Epoch 148/300 - Train Loss: 0.1041, Val Loss: 0.0906\n",
      "Epoch 149/300 - Train Loss: 0.1036, Val Loss: 0.0961\n",
      "Epoch 150/300 - Train Loss: 0.1055, Val Loss: 0.0908\n",
      "Epoch 151/300 - Train Loss: 0.1051, Val Loss: 0.0928\n",
      "Epoch 152/300 - Train Loss: 0.1041, Val Loss: 0.0907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 22:22:41,567] Trial 0 finished with value: 0.947401072682117 and parameters: {'F1': 8, 'F2': 16, 'D': 2, 'dropout': 0.634131042579391, 'learning_rate': 2.5471659095775814e-05, 'batch_size': 128, 'weight_decay': 0.0003030088405181484}. Best is trial 0 with value: 0.947401072682117.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/300 - Train Loss: 0.1044, Val Loss: 0.0940\n",
      "Early stopping at epoch 153\n",
      "Macro F1 Score: 0.9474, Macro Precision: 0.9336, Macro Recall: 0.9638\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.84      0.95      0.89        61\n",
      "           2       0.99      0.95      0.97       593\n",
      "\n",
      "    accuracy                           0.97      1443\n",
      "   macro avg       0.93      0.96      0.95      1443\n",
      "weighted avg       0.97      0.97      0.97      1443\n",
      "\n",
      "\n",
      "Trial 2\n",
      "Training with F1=8, F2=8, D=4, dropout=0.18463296029798004, LR=1.359807186908951e-05, BS=64, WD=0.00014318798607877405\n",
      "Epoch 1/300 - Train Loss: 1.0069, Val Loss: 0.9172\n",
      "Epoch 2/300 - Train Loss: 0.7949, Val Loss: 0.6739\n",
      "Epoch 3/300 - Train Loss: 0.5673, Val Loss: 0.4810\n",
      "Epoch 4/300 - Train Loss: 0.4412, Val Loss: 0.3941\n",
      "Epoch 5/300 - Train Loss: 0.3764, Val Loss: 0.3445\n",
      "Epoch 6/300 - Train Loss: 0.3317, Val Loss: 0.3019\n",
      "Epoch 7/300 - Train Loss: 0.2994, Val Loss: 0.2652\n",
      "Epoch 8/300 - Train Loss: 0.2716, Val Loss: 0.2448\n",
      "Epoch 9/300 - Train Loss: 0.2505, Val Loss: 0.2296\n",
      "Epoch 10/300 - Train Loss: 0.2358, Val Loss: 0.2154\n",
      "Epoch 11/300 - Train Loss: 0.2209, Val Loss: 0.2007\n",
      "Epoch 12/300 - Train Loss: 0.2096, Val Loss: 0.1862\n",
      "Epoch 13/300 - Train Loss: 0.1993, Val Loss: 0.1763\n",
      "Epoch 14/300 - Train Loss: 0.1879, Val Loss: 0.1705\n",
      "Epoch 15/300 - Train Loss: 0.1788, Val Loss: 0.1621\n",
      "Epoch 16/300 - Train Loss: 0.1709, Val Loss: 0.1546\n",
      "Epoch 17/300 - Train Loss: 0.1653, Val Loss: 0.1433\n",
      "Epoch 18/300 - Train Loss: 0.1589, Val Loss: 0.1397\n",
      "Epoch 19/300 - Train Loss: 0.1501, Val Loss: 0.1330\n",
      "Epoch 20/300 - Train Loss: 0.1469, Val Loss: 0.1257\n",
      "Epoch 21/300 - Train Loss: 0.1408, Val Loss: 0.1229\n",
      "Epoch 22/300 - Train Loss: 0.1363, Val Loss: 0.1184\n",
      "Epoch 23/300 - Train Loss: 0.1325, Val Loss: 0.1161\n",
      "Epoch 24/300 - Train Loss: 0.1285, Val Loss: 0.1119\n",
      "Epoch 25/300 - Train Loss: 0.1233, Val Loss: 0.1099\n",
      "Epoch 26/300 - Train Loss: 0.1253, Val Loss: 0.1073\n",
      "Epoch 27/300 - Train Loss: 0.1204, Val Loss: 0.1042\n",
      "Epoch 28/300 - Train Loss: 0.1178, Val Loss: 0.1044\n",
      "Epoch 29/300 - Train Loss: 0.1171, Val Loss: 0.1031\n",
      "Epoch 30/300 - Train Loss: 0.1164, Val Loss: 0.1002\n",
      "Epoch 31/300 - Train Loss: 0.1152, Val Loss: 0.1041\n",
      "Epoch 32/300 - Train Loss: 0.1121, Val Loss: 0.1004\n",
      "Epoch 33/300 - Train Loss: 0.1135, Val Loss: 0.0972\n",
      "Epoch 34/300 - Train Loss: 0.1117, Val Loss: 0.0965\n",
      "Epoch 35/300 - Train Loss: 0.1102, Val Loss: 0.0997\n",
      "Epoch 36/300 - Train Loss: 0.1101, Val Loss: 0.0939\n",
      "Epoch 37/300 - Train Loss: 0.1091, Val Loss: 0.0952\n",
      "Epoch 38/300 - Train Loss: 0.1089, Val Loss: 0.0955\n",
      "Epoch 39/300 - Train Loss: 0.1046, Val Loss: 0.0923\n",
      "Epoch 40/300 - Train Loss: 0.1058, Val Loss: 0.0923\n",
      "Epoch 41/300 - Train Loss: 0.1059, Val Loss: 0.0927\n",
      "Epoch 42/300 - Train Loss: 0.1042, Val Loss: 0.0894\n",
      "Epoch 43/300 - Train Loss: 0.1042, Val Loss: 0.0911\n",
      "Epoch 44/300 - Train Loss: 0.1038, Val Loss: 0.0915\n",
      "Epoch 45/300 - Train Loss: 0.1011, Val Loss: 0.0897\n",
      "Epoch 46/300 - Train Loss: 0.1022, Val Loss: 0.0879\n",
      "Epoch 47/300 - Train Loss: 0.1034, Val Loss: 0.0887\n",
      "Epoch 48/300 - Train Loss: 0.0996, Val Loss: 0.0922\n",
      "Epoch 49/300 - Train Loss: 0.1025, Val Loss: 0.0900\n",
      "Epoch 50/300 - Train Loss: 0.1006, Val Loss: 0.0897\n",
      "Epoch 51/300 - Train Loss: 0.0997, Val Loss: 0.0880\n",
      "Epoch 52/300 - Train Loss: 0.1007, Val Loss: 0.0874\n",
      "Epoch 53/300 - Train Loss: 0.1003, Val Loss: 0.0882\n",
      "Epoch 54/300 - Train Loss: 0.0981, Val Loss: 0.0866\n",
      "Epoch 55/300 - Train Loss: 0.1002, Val Loss: 0.0859\n",
      "Epoch 56/300 - Train Loss: 0.0974, Val Loss: 0.0871\n",
      "Epoch 57/300 - Train Loss: 0.0969, Val Loss: 0.0865\n",
      "Epoch 58/300 - Train Loss: 0.0981, Val Loss: 0.0868\n",
      "Epoch 59/300 - Train Loss: 0.0977, Val Loss: 0.0861\n",
      "Epoch 60/300 - Train Loss: 0.0955, Val Loss: 0.0860\n",
      "Epoch 61/300 - Train Loss: 0.0958, Val Loss: 0.0872\n",
      "Epoch 62/300 - Train Loss: 0.0959, Val Loss: 0.0851\n",
      "Epoch 63/300 - Train Loss: 0.0958, Val Loss: 0.0855\n",
      "Epoch 64/300 - Train Loss: 0.0969, Val Loss: 0.0860\n",
      "Epoch 65/300 - Train Loss: 0.0952, Val Loss: 0.0858\n",
      "Epoch 66/300 - Train Loss: 0.0950, Val Loss: 0.0856\n",
      "Epoch 67/300 - Train Loss: 0.0938, Val Loss: 0.0860\n",
      "Epoch 68/300 - Train Loss: 0.0933, Val Loss: 0.0847\n",
      "Epoch 69/300 - Train Loss: 0.0931, Val Loss: 0.0846\n",
      "Epoch 70/300 - Train Loss: 0.0934, Val Loss: 0.0846\n",
      "Epoch 71/300 - Train Loss: 0.0942, Val Loss: 0.0834\n",
      "Epoch 72/300 - Train Loss: 0.0938, Val Loss: 0.0848\n",
      "Epoch 73/300 - Train Loss: 0.0927, Val Loss: 0.0854\n",
      "Epoch 74/300 - Train Loss: 0.0921, Val Loss: 0.0852\n",
      "Epoch 75/300 - Train Loss: 0.0930, Val Loss: 0.0828\n",
      "Epoch 76/300 - Train Loss: 0.0922, Val Loss: 0.0835\n",
      "Epoch 77/300 - Train Loss: 0.0930, Val Loss: 0.0837\n",
      "Epoch 78/300 - Train Loss: 0.0929, Val Loss: 0.0843\n",
      "Epoch 79/300 - Train Loss: 0.0918, Val Loss: 0.0834\n",
      "Epoch 80/300 - Train Loss: 0.0922, Val Loss: 0.0824\n",
      "Epoch 81/300 - Train Loss: 0.0916, Val Loss: 0.0838\n",
      "Epoch 82/300 - Train Loss: 0.0904, Val Loss: 0.0843\n",
      "Epoch 83/300 - Train Loss: 0.0900, Val Loss: 0.0827\n",
      "Epoch 84/300 - Train Loss: 0.0934, Val Loss: 0.0824\n",
      "Epoch 85/300 - Train Loss: 0.0903, Val Loss: 0.0809\n",
      "Epoch 86/300 - Train Loss: 0.0897, Val Loss: 0.0830\n",
      "Epoch 87/300 - Train Loss: 0.0903, Val Loss: 0.0827\n",
      "Epoch 88/300 - Train Loss: 0.0907, Val Loss: 0.0824\n",
      "Epoch 89/300 - Train Loss: 0.0908, Val Loss: 0.0834\n",
      "Epoch 90/300 - Train Loss: 0.0904, Val Loss: 0.0811\n",
      "Epoch 91/300 - Train Loss: 0.0908, Val Loss: 0.0802\n",
      "Epoch 92/300 - Train Loss: 0.0910, Val Loss: 0.0825\n",
      "Epoch 93/300 - Train Loss: 0.0874, Val Loss: 0.0823\n",
      "Epoch 94/300 - Train Loss: 0.0882, Val Loss: 0.0831\n",
      "Epoch 95/300 - Train Loss: 0.0874, Val Loss: 0.0822\n",
      "Epoch 96/300 - Train Loss: 0.0891, Val Loss: 0.0817\n",
      "Epoch 97/300 - Train Loss: 0.0883, Val Loss: 0.0809\n",
      "Epoch 98/300 - Train Loss: 0.0882, Val Loss: 0.0820\n",
      "Epoch 99/300 - Train Loss: 0.0885, Val Loss: 0.0817\n",
      "Epoch 100/300 - Train Loss: 0.0898, Val Loss: 0.0802\n",
      "Epoch 101/300 - Train Loss: 0.0902, Val Loss: 0.0805\n",
      "Epoch 102/300 - Train Loss: 0.0883, Val Loss: 0.0820\n",
      "Epoch 103/300 - Train Loss: 0.0881, Val Loss: 0.0818\n",
      "Epoch 104/300 - Train Loss: 0.0889, Val Loss: 0.0806\n",
      "Epoch 105/300 - Train Loss: 0.0874, Val Loss: 0.0814\n",
      "Epoch 106/300 - Train Loss: 0.0874, Val Loss: 0.0793\n",
      "Epoch 107/300 - Train Loss: 0.0861, Val Loss: 0.0812\n",
      "Epoch 108/300 - Train Loss: 0.0883, Val Loss: 0.0808\n",
      "Epoch 109/300 - Train Loss: 0.0876, Val Loss: 0.0798\n",
      "Epoch 110/300 - Train Loss: 0.0866, Val Loss: 0.0792\n",
      "Epoch 111/300 - Train Loss: 0.0863, Val Loss: 0.0796\n",
      "Epoch 112/300 - Train Loss: 0.0867, Val Loss: 0.0786\n",
      "Epoch 113/300 - Train Loss: 0.0862, Val Loss: 0.0814\n",
      "Epoch 114/300 - Train Loss: 0.0865, Val Loss: 0.0791\n",
      "Epoch 115/300 - Train Loss: 0.0882, Val Loss: 0.0793\n",
      "Epoch 116/300 - Train Loss: 0.0877, Val Loss: 0.0801\n",
      "Epoch 117/300 - Train Loss: 0.0852, Val Loss: 0.0802\n",
      "Epoch 118/300 - Train Loss: 0.0868, Val Loss: 0.0794\n",
      "Epoch 119/300 - Train Loss: 0.0873, Val Loss: 0.0796\n",
      "Epoch 120/300 - Train Loss: 0.0850, Val Loss: 0.0780\n",
      "Epoch 121/300 - Train Loss: 0.0852, Val Loss: 0.0788\n",
      "Epoch 122/300 - Train Loss: 0.0852, Val Loss: 0.0792\n",
      "Epoch 123/300 - Train Loss: 0.0842, Val Loss: 0.0806\n",
      "Epoch 124/300 - Train Loss: 0.0843, Val Loss: 0.0802\n",
      "Epoch 125/300 - Train Loss: 0.0851, Val Loss: 0.0795\n",
      "Epoch 126/300 - Train Loss: 0.0859, Val Loss: 0.0811\n",
      "Epoch 127/300 - Train Loss: 0.0852, Val Loss: 0.0797\n",
      "Epoch 128/300 - Train Loss: 0.0862, Val Loss: 0.0802\n",
      "Epoch 129/300 - Train Loss: 0.0858, Val Loss: 0.0774\n",
      "Epoch 130/300 - Train Loss: 0.0844, Val Loss: 0.0786\n",
      "Epoch 131/300 - Train Loss: 0.0862, Val Loss: 0.0780\n",
      "Epoch 132/300 - Train Loss: 0.0846, Val Loss: 0.0794\n",
      "Epoch 133/300 - Train Loss: 0.0849, Val Loss: 0.0784\n",
      "Epoch 134/300 - Train Loss: 0.0844, Val Loss: 0.0782\n",
      "Epoch 135/300 - Train Loss: 0.0857, Val Loss: 0.0779\n",
      "Epoch 136/300 - Train Loss: 0.0840, Val Loss: 0.0802\n",
      "Epoch 137/300 - Train Loss: 0.0840, Val Loss: 0.0780\n",
      "Epoch 138/300 - Train Loss: 0.0837, Val Loss: 0.0791\n",
      "Epoch 139/300 - Train Loss: 0.0840, Val Loss: 0.0788\n",
      "Epoch 140/300 - Train Loss: 0.0834, Val Loss: 0.0792\n",
      "Epoch 141/300 - Train Loss: 0.0832, Val Loss: 0.0780\n",
      "Epoch 142/300 - Train Loss: 0.0844, Val Loss: 0.0799\n",
      "Epoch 143/300 - Train Loss: 0.0844, Val Loss: 0.0787\n",
      "Epoch 144/300 - Train Loss: 0.0847, Val Loss: 0.0794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0853, Val Loss: 0.0772\n",
      "Epoch 146/300 - Train Loss: 0.0829, Val Loss: 0.0789\n",
      "Epoch 147/300 - Train Loss: 0.0837, Val Loss: 0.0775\n",
      "Epoch 148/300 - Train Loss: 0.0836, Val Loss: 0.0797\n",
      "Epoch 149/300 - Train Loss: 0.0822, Val Loss: 0.0773\n",
      "Epoch 150/300 - Train Loss: 0.0821, Val Loss: 0.0779\n",
      "Epoch 151/300 - Train Loss: 0.0831, Val Loss: 0.0765\n",
      "Epoch 152/300 - Train Loss: 0.0818, Val Loss: 0.0798\n",
      "Epoch 153/300 - Train Loss: 0.0830, Val Loss: 0.0763\n",
      "Epoch 154/300 - Train Loss: 0.0833, Val Loss: 0.0780\n",
      "Epoch 155/300 - Train Loss: 0.0828, Val Loss: 0.0781\n",
      "Epoch 156/300 - Train Loss: 0.0829, Val Loss: 0.0773\n",
      "Epoch 157/300 - Train Loss: 0.0818, Val Loss: 0.0768\n",
      "Epoch 158/300 - Train Loss: 0.0829, Val Loss: 0.0759\n",
      "Epoch 159/300 - Train Loss: 0.0821, Val Loss: 0.0765\n",
      "Epoch 160/300 - Train Loss: 0.0830, Val Loss: 0.0790\n",
      "Epoch 161/300 - Train Loss: 0.0810, Val Loss: 0.0774\n",
      "Epoch 162/300 - Train Loss: 0.0826, Val Loss: 0.0768\n",
      "Epoch 163/300 - Train Loss: 0.0809, Val Loss: 0.0770\n",
      "Epoch 164/300 - Train Loss: 0.0831, Val Loss: 0.0780\n",
      "Epoch 165/300 - Train Loss: 0.0806, Val Loss: 0.0766\n",
      "Epoch 166/300 - Train Loss: 0.0776, Val Loss: 0.0771\n",
      "Epoch 167/300 - Train Loss: 0.0809, Val Loss: 0.0779\n",
      "Epoch 168/300 - Train Loss: 0.0801, Val Loss: 0.0771\n",
      "Epoch 169/300 - Train Loss: 0.0804, Val Loss: 0.0761\n",
      "Epoch 170/300 - Train Loss: 0.0809, Val Loss: 0.0749\n",
      "Epoch 171/300 - Train Loss: 0.0792, Val Loss: 0.0753\n",
      "Epoch 172/300 - Train Loss: 0.0797, Val Loss: 0.0761\n",
      "Epoch 173/300 - Train Loss: 0.0798, Val Loss: 0.0759\n",
      "Epoch 174/300 - Train Loss: 0.0789, Val Loss: 0.0765\n",
      "Epoch 175/300 - Train Loss: 0.0810, Val Loss: 0.0757\n",
      "Epoch 176/300 - Train Loss: 0.0790, Val Loss: 0.0769\n",
      "Epoch 177/300 - Train Loss: 0.0805, Val Loss: 0.0762\n",
      "Epoch 178/300 - Train Loss: 0.0800, Val Loss: 0.0768\n",
      "Epoch 179/300 - Train Loss: 0.0795, Val Loss: 0.0746\n",
      "Epoch 180/300 - Train Loss: 0.0806, Val Loss: 0.0766\n",
      "Epoch 181/300 - Train Loss: 0.0804, Val Loss: 0.0768\n",
      "Epoch 182/300 - Train Loss: 0.0791, Val Loss: 0.0773\n",
      "Epoch 183/300 - Train Loss: 0.0806, Val Loss: 0.0744\n",
      "Epoch 184/300 - Train Loss: 0.0787, Val Loss: 0.0753\n",
      "Epoch 185/300 - Train Loss: 0.0795, Val Loss: 0.0749\n",
      "Epoch 186/300 - Train Loss: 0.0788, Val Loss: 0.0756\n",
      "Epoch 187/300 - Train Loss: 0.0794, Val Loss: 0.0748\n",
      "Epoch 188/300 - Train Loss: 0.0804, Val Loss: 0.0761\n",
      "Epoch 189/300 - Train Loss: 0.0793, Val Loss: 0.0744\n",
      "Epoch 190/300 - Train Loss: 0.0778, Val Loss: 0.0746\n",
      "Epoch 191/300 - Train Loss: 0.0794, Val Loss: 0.0741\n",
      "Epoch 192/300 - Train Loss: 0.0788, Val Loss: 0.0751\n",
      "Epoch 193/300 - Train Loss: 0.0783, Val Loss: 0.0747\n",
      "Epoch 194/300 - Train Loss: 0.0796, Val Loss: 0.0753\n",
      "Epoch 195/300 - Train Loss: 0.0788, Val Loss: 0.0746\n",
      "Epoch 196/300 - Train Loss: 0.0796, Val Loss: 0.0755\n",
      "Epoch 197/300 - Train Loss: 0.0786, Val Loss: 0.0766\n",
      "Epoch 198/300 - Train Loss: 0.0787, Val Loss: 0.0744\n",
      "Epoch 199/300 - Train Loss: 0.0769, Val Loss: 0.0744\n",
      "Epoch 200/300 - Train Loss: 0.0795, Val Loss: 0.0743\n",
      "Epoch 201/300 - Train Loss: 0.0791, Val Loss: 0.0754\n",
      "Epoch 202/300 - Train Loss: 0.0802, Val Loss: 0.0739\n",
      "Epoch 203/300 - Train Loss: 0.0784, Val Loss: 0.0734\n",
      "Epoch 204/300 - Train Loss: 0.0763, Val Loss: 0.0750\n",
      "Epoch 205/300 - Train Loss: 0.0797, Val Loss: 0.0747\n",
      "Epoch 206/300 - Train Loss: 0.0783, Val Loss: 0.0729\n",
      "Epoch 207/300 - Train Loss: 0.0766, Val Loss: 0.0741\n",
      "Epoch 208/300 - Train Loss: 0.0784, Val Loss: 0.0726\n",
      "Epoch 209/300 - Train Loss: 0.0783, Val Loss: 0.0731\n",
      "Epoch 210/300 - Train Loss: 0.0788, Val Loss: 0.0743\n",
      "Epoch 211/300 - Train Loss: 0.0780, Val Loss: 0.0738\n",
      "Epoch 212/300 - Train Loss: 0.0781, Val Loss: 0.0738\n",
      "Epoch 213/300 - Train Loss: 0.0767, Val Loss: 0.0732\n",
      "Epoch 214/300 - Train Loss: 0.0783, Val Loss: 0.0732\n",
      "Epoch 215/300 - Train Loss: 0.0766, Val Loss: 0.0745\n",
      "Epoch 216/300 - Train Loss: 0.0767, Val Loss: 0.0727\n",
      "Epoch 217/300 - Train Loss: 0.0782, Val Loss: 0.0768\n",
      "Epoch 218/300 - Train Loss: 0.0769, Val Loss: 0.0738\n",
      "Epoch 219/300 - Train Loss: 0.0773, Val Loss: 0.0747\n",
      "Epoch 220/300 - Train Loss: 0.0783, Val Loss: 0.0739\n",
      "Epoch 221/300 - Train Loss: 0.0778, Val Loss: 0.0739\n",
      "Epoch 222/300 - Train Loss: 0.0779, Val Loss: 0.0739\n",
      "Epoch 223/300 - Train Loss: 0.0768, Val Loss: 0.0732\n",
      "Epoch 224/300 - Train Loss: 0.0775, Val Loss: 0.0725\n",
      "Epoch 225/300 - Train Loss: 0.0764, Val Loss: 0.0736\n",
      "Epoch 226/300 - Train Loss: 0.0757, Val Loss: 0.0737\n",
      "Epoch 227/300 - Train Loss: 0.0777, Val Loss: 0.0741\n",
      "Epoch 228/300 - Train Loss: 0.0779, Val Loss: 0.0749\n",
      "Epoch 229/300 - Train Loss: 0.0769, Val Loss: 0.0732\n",
      "Epoch 230/300 - Train Loss: 0.0764, Val Loss: 0.0734\n",
      "Epoch 231/300 - Train Loss: 0.0770, Val Loss: 0.0731\n",
      "Epoch 232/300 - Train Loss: 0.0779, Val Loss: 0.0723\n",
      "Epoch 233/300 - Train Loss: 0.0752, Val Loss: 0.0736\n",
      "Epoch 234/300 - Train Loss: 0.0760, Val Loss: 0.0726\n",
      "Epoch 235/300 - Train Loss: 0.0768, Val Loss: 0.0735\n",
      "Epoch 236/300 - Train Loss: 0.0759, Val Loss: 0.0741\n",
      "Epoch 237/300 - Train Loss: 0.0772, Val Loss: 0.0737\n",
      "Epoch 238/300 - Train Loss: 0.0766, Val Loss: 0.0746\n",
      "Epoch 239/300 - Train Loss: 0.0774, Val Loss: 0.0731\n",
      "Epoch 240/300 - Train Loss: 0.0775, Val Loss: 0.0735\n",
      "Epoch 241/300 - Train Loss: 0.0760, Val Loss: 0.0738\n",
      "Epoch 242/300 - Train Loss: 0.0763, Val Loss: 0.0731\n",
      "Epoch 243/300 - Train Loss: 0.0756, Val Loss: 0.0728\n",
      "Epoch 244/300 - Train Loss: 0.0754, Val Loss: 0.0743\n",
      "Epoch 245/300 - Train Loss: 0.0757, Val Loss: 0.0728\n",
      "Epoch 246/300 - Train Loss: 0.0756, Val Loss: 0.0722\n",
      "Epoch 247/300 - Train Loss: 0.0763, Val Loss: 0.0737\n",
      "Epoch 248/300 - Train Loss: 0.0764, Val Loss: 0.0729\n",
      "Epoch 249/300 - Train Loss: 0.0776, Val Loss: 0.0730\n",
      "Epoch 250/300 - Train Loss: 0.0750, Val Loss: 0.0729\n",
      "Epoch 251/300 - Train Loss: 0.0766, Val Loss: 0.0730\n",
      "Epoch 252/300 - Train Loss: 0.0766, Val Loss: 0.0731\n",
      "Epoch 253/300 - Train Loss: 0.0763, Val Loss: 0.0744\n",
      "Epoch 254/300 - Train Loss: 0.0764, Val Loss: 0.0733\n",
      "Epoch 255/300 - Train Loss: 0.0771, Val Loss: 0.0738\n",
      "Epoch 256/300 - Train Loss: 0.0748, Val Loss: 0.0732\n",
      "Epoch 257/300 - Train Loss: 0.0754, Val Loss: 0.0736\n",
      "Epoch 258/300 - Train Loss: 0.0752, Val Loss: 0.0724\n",
      "Epoch 259/300 - Train Loss: 0.0765, Val Loss: 0.0734\n",
      "Epoch 260/300 - Train Loss: 0.0768, Val Loss: 0.0715\n",
      "Epoch 261/300 - Train Loss: 0.0770, Val Loss: 0.0728\n",
      "Epoch 262/300 - Train Loss: 0.0766, Val Loss: 0.0737\n",
      "Epoch 263/300 - Train Loss: 0.0742, Val Loss: 0.0731\n",
      "Epoch 264/300 - Train Loss: 0.0757, Val Loss: 0.0735\n",
      "Epoch 265/300 - Train Loss: 0.0751, Val Loss: 0.0735\n",
      "Epoch 266/300 - Train Loss: 0.0758, Val Loss: 0.0726\n",
      "Epoch 267/300 - Train Loss: 0.0749, Val Loss: 0.0711\n",
      "Epoch 268/300 - Train Loss: 0.0770, Val Loss: 0.0723\n",
      "Epoch 269/300 - Train Loss: 0.0766, Val Loss: 0.0750\n",
      "Epoch 270/300 - Train Loss: 0.0749, Val Loss: 0.0765\n",
      "Epoch 271/300 - Train Loss: 0.0763, Val Loss: 0.0735\n",
      "Epoch 272/300 - Train Loss: 0.0765, Val Loss: 0.0719\n",
      "Epoch 273/300 - Train Loss: 0.0762, Val Loss: 0.0716\n",
      "Epoch 274/300 - Train Loss: 0.0768, Val Loss: 0.0720\n",
      "Epoch 275/300 - Train Loss: 0.0744, Val Loss: 0.0732\n",
      "Epoch 276/300 - Train Loss: 0.0765, Val Loss: 0.0710\n",
      "Epoch 277/300 - Train Loss: 0.0773, Val Loss: 0.0716\n",
      "Epoch 278/300 - Train Loss: 0.0755, Val Loss: 0.0733\n",
      "Epoch 279/300 - Train Loss: 0.0768, Val Loss: 0.0739\n",
      "Epoch 280/300 - Train Loss: 0.0761, Val Loss: 0.0717\n",
      "Epoch 281/300 - Train Loss: 0.0743, Val Loss: 0.0731\n",
      "Epoch 282/300 - Train Loss: 0.0750, Val Loss: 0.0728\n",
      "Epoch 283/300 - Train Loss: 0.0750, Val Loss: 0.0722\n",
      "Epoch 284/300 - Train Loss: 0.0765, Val Loss: 0.0737\n",
      "Epoch 285/300 - Train Loss: 0.0763, Val Loss: 0.0733\n",
      "Epoch 286/300 - Train Loss: 0.0750, Val Loss: 0.0715\n",
      "Epoch 287/300 - Train Loss: 0.0757, Val Loss: 0.0712\n",
      "Epoch 288/300 - Train Loss: 0.0763, Val Loss: 0.0724\n",
      "Epoch 289/300 - Train Loss: 0.0748, Val Loss: 0.0718\n",
      "Epoch 290/300 - Train Loss: 0.0739, Val Loss: 0.0748\n",
      "Epoch 291/300 - Train Loss: 0.0757, Val Loss: 0.0718\n",
      "Epoch 292/300 - Train Loss: 0.0766, Val Loss: 0.0721\n",
      "Epoch 293/300 - Train Loss: 0.0762, Val Loss: 0.0729\n",
      "Epoch 294/300 - Train Loss: 0.0765, Val Loss: 0.0728\n",
      "Epoch 295/300 - Train Loss: 0.0758, Val Loss: 0.0735\n",
      "Epoch 296/300 - Train Loss: 0.0767, Val Loss: 0.0714\n",
      "Epoch 297/300 - Train Loss: 0.0757, Val Loss: 0.0730\n",
      "Epoch 298/300 - Train Loss: 0.0771, Val Loss: 0.0715\n",
      "Epoch 299/300 - Train Loss: 0.0758, Val Loss: 0.0766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 22:29:25,049] Trial 1 finished with value: 0.9667219239125732 and parameters: {'F1': 8, 'F2': 8, 'D': 4, 'dropout': 0.18463296029798004, 'learning_rate': 1.359807186908951e-05, 'batch_size': 64, 'weight_decay': 0.00014318798607877405}. Best is trial 1 with value: 0.9667219239125732.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/300 - Train Loss: 0.0741, Val Loss: 0.0727\n",
      "Macro F1 Score: 0.9667, Macro Precision: 0.9529, Macro Recall: 0.9824\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.88      0.98      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 3\n",
      "Training with F1=8, F2=16, D=8, dropout=0.41918139531955056, LR=6.349577740603107e-05, BS=64, WD=3.618910158093432e-05\n",
      "Epoch 1/300 - Train Loss: 0.5653, Val Loss: 0.2845\n",
      "Epoch 2/300 - Train Loss: 0.2511, Val Loss: 0.1952\n",
      "Epoch 3/300 - Train Loss: 0.1918, Val Loss: 0.1469\n",
      "Epoch 4/300 - Train Loss: 0.1446, Val Loss: 0.1113\n",
      "Epoch 5/300 - Train Loss: 0.1207, Val Loss: 0.1003\n",
      "Epoch 6/300 - Train Loss: 0.1123, Val Loss: 0.0925\n",
      "Epoch 7/300 - Train Loss: 0.1046, Val Loss: 0.0872\n",
      "Epoch 8/300 - Train Loss: 0.1026, Val Loss: 0.0876\n",
      "Epoch 9/300 - Train Loss: 0.1000, Val Loss: 0.0851\n",
      "Epoch 10/300 - Train Loss: 0.0986, Val Loss: 0.0817\n",
      "Epoch 11/300 - Train Loss: 0.0992, Val Loss: 0.0805\n",
      "Epoch 12/300 - Train Loss: 0.0943, Val Loss: 0.0802\n",
      "Epoch 13/300 - Train Loss: 0.0940, Val Loss: 0.0814\n",
      "Epoch 14/300 - Train Loss: 0.0933, Val Loss: 0.0824\n",
      "Epoch 15/300 - Train Loss: 0.0939, Val Loss: 0.0770\n",
      "Epoch 16/300 - Train Loss: 0.0937, Val Loss: 0.0786\n",
      "Epoch 17/300 - Train Loss: 0.0923, Val Loss: 0.0821\n",
      "Epoch 18/300 - Train Loss: 0.0911, Val Loss: 0.0765\n",
      "Epoch 19/300 - Train Loss: 0.0898, Val Loss: 0.0759\n",
      "Epoch 20/300 - Train Loss: 0.0908, Val Loss: 0.0783\n",
      "Epoch 21/300 - Train Loss: 0.0896, Val Loss: 0.0768\n",
      "Epoch 22/300 - Train Loss: 0.0887, Val Loss: 0.0761\n",
      "Epoch 23/300 - Train Loss: 0.0877, Val Loss: 0.0737\n",
      "Epoch 24/300 - Train Loss: 0.0864, Val Loss: 0.0743\n",
      "Epoch 25/300 - Train Loss: 0.0865, Val Loss: 0.0800\n",
      "Epoch 26/300 - Train Loss: 0.0852, Val Loss: 0.0771\n",
      "Epoch 27/300 - Train Loss: 0.0852, Val Loss: 0.0746\n",
      "Epoch 28/300 - Train Loss: 0.0834, Val Loss: 0.0745\n",
      "Epoch 29/300 - Train Loss: 0.0857, Val Loss: 0.0748\n",
      "Epoch 30/300 - Train Loss: 0.0855, Val Loss: 0.0738\n",
      "Epoch 31/300 - Train Loss: 0.0817, Val Loss: 0.0729\n",
      "Epoch 32/300 - Train Loss: 0.0831, Val Loss: 0.0732\n",
      "Epoch 33/300 - Train Loss: 0.0834, Val Loss: 0.0744\n",
      "Epoch 34/300 - Train Loss: 0.0829, Val Loss: 0.0753\n",
      "Epoch 35/300 - Train Loss: 0.0833, Val Loss: 0.0735\n",
      "Epoch 36/300 - Train Loss: 0.0821, Val Loss: 0.0738\n",
      "Epoch 37/300 - Train Loss: 0.0832, Val Loss: 0.0722\n",
      "Epoch 38/300 - Train Loss: 0.0824, Val Loss: 0.0739\n",
      "Epoch 39/300 - Train Loss: 0.0837, Val Loss: 0.0733\n",
      "Epoch 40/300 - Train Loss: 0.0798, Val Loss: 0.0744\n",
      "Epoch 41/300 - Train Loss: 0.0825, Val Loss: 0.0743\n",
      "Epoch 42/300 - Train Loss: 0.0787, Val Loss: 0.0735\n",
      "Epoch 43/300 - Train Loss: 0.0811, Val Loss: 0.0734\n",
      "Epoch 44/300 - Train Loss: 0.0805, Val Loss: 0.0759\n",
      "Epoch 45/300 - Train Loss: 0.0790, Val Loss: 0.0741\n",
      "Epoch 46/300 - Train Loss: 0.0783, Val Loss: 0.0728\n",
      "Epoch 47/300 - Train Loss: 0.0793, Val Loss: 0.0721\n",
      "Epoch 48/300 - Train Loss: 0.0807, Val Loss: 0.0725\n",
      "Epoch 49/300 - Train Loss: 0.0799, Val Loss: 0.0741\n",
      "Epoch 50/300 - Train Loss: 0.0789, Val Loss: 0.0724\n",
      "Epoch 51/300 - Train Loss: 0.0794, Val Loss: 0.0721\n",
      "Epoch 52/300 - Train Loss: 0.0799, Val Loss: 0.0733\n",
      "Epoch 53/300 - Train Loss: 0.0790, Val Loss: 0.0717\n",
      "Epoch 54/300 - Train Loss: 0.0781, Val Loss: 0.0753\n",
      "Epoch 55/300 - Train Loss: 0.0792, Val Loss: 0.0737\n",
      "Epoch 56/300 - Train Loss: 0.0800, Val Loss: 0.0734\n",
      "Epoch 57/300 - Train Loss: 0.0778, Val Loss: 0.0735\n",
      "Epoch 58/300 - Train Loss: 0.0779, Val Loss: 0.0748\n",
      "Epoch 59/300 - Train Loss: 0.0776, Val Loss: 0.0738\n",
      "Epoch 60/300 - Train Loss: 0.0780, Val Loss: 0.0723\n",
      "Epoch 61/300 - Train Loss: 0.0768, Val Loss: 0.0725\n",
      "Epoch 62/300 - Train Loss: 0.0789, Val Loss: 0.0751\n",
      "Epoch 63/300 - Train Loss: 0.0784, Val Loss: 0.0739\n",
      "Epoch 64/300 - Train Loss: 0.0770, Val Loss: 0.0712\n",
      "Epoch 65/300 - Train Loss: 0.0778, Val Loss: 0.0710\n",
      "Epoch 66/300 - Train Loss: 0.0780, Val Loss: 0.0719\n",
      "Epoch 67/300 - Train Loss: 0.0778, Val Loss: 0.0736\n",
      "Epoch 68/300 - Train Loss: 0.0778, Val Loss: 0.0736\n",
      "Epoch 69/300 - Train Loss: 0.0780, Val Loss: 0.0742\n",
      "Epoch 70/300 - Train Loss: 0.0778, Val Loss: 0.0753\n",
      "Epoch 71/300 - Train Loss: 0.0757, Val Loss: 0.0761\n",
      "Epoch 72/300 - Train Loss: 0.0756, Val Loss: 0.0727\n",
      "Epoch 73/300 - Train Loss: 0.0760, Val Loss: 0.0736\n",
      "Epoch 74/300 - Train Loss: 0.0772, Val Loss: 0.0728\n",
      "Epoch 75/300 - Train Loss: 0.0760, Val Loss: 0.0725\n",
      "Epoch 76/300 - Train Loss: 0.0757, Val Loss: 0.0725\n",
      "Epoch 77/300 - Train Loss: 0.0760, Val Loss: 0.0743\n",
      "Epoch 78/300 - Train Loss: 0.0749, Val Loss: 0.0741\n",
      "Epoch 79/300 - Train Loss: 0.0760, Val Loss: 0.0730\n",
      "Epoch 80/300 - Train Loss: 0.0748, Val Loss: 0.0748\n",
      "Epoch 81/300 - Train Loss: 0.0747, Val Loss: 0.0749\n",
      "Epoch 82/300 - Train Loss: 0.0747, Val Loss: 0.0733\n",
      "Epoch 83/300 - Train Loss: 0.0750, Val Loss: 0.0736\n",
      "Epoch 84/300 - Train Loss: 0.0760, Val Loss: 0.0736\n",
      "Epoch 85/300 - Train Loss: 0.0760, Val Loss: 0.0724\n",
      "Epoch 86/300 - Train Loss: 0.0742, Val Loss: 0.0731\n",
      "Epoch 87/300 - Train Loss: 0.0762, Val Loss: 0.0726\n",
      "Epoch 88/300 - Train Loss: 0.0763, Val Loss: 0.0729\n",
      "Epoch 89/300 - Train Loss: 0.0737, Val Loss: 0.0736\n",
      "Epoch 90/300 - Train Loss: 0.0735, Val Loss: 0.0740\n",
      "Epoch 91/300 - Train Loss: 0.0751, Val Loss: 0.0715\n",
      "Epoch 92/300 - Train Loss: 0.0735, Val Loss: 0.0730\n",
      "Epoch 93/300 - Train Loss: 0.0742, Val Loss: 0.0721\n",
      "Epoch 94/300 - Train Loss: 0.0742, Val Loss: 0.0739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 22:32:31,989] Trial 2 finished with value: 0.9648778589302093 and parameters: {'F1': 8, 'F2': 16, 'D': 8, 'dropout': 0.41918139531955056, 'learning_rate': 6.349577740603107e-05, 'batch_size': 64, 'weight_decay': 3.618910158093432e-05}. Best is trial 1 with value: 0.9667219239125732.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/300 - Train Loss: 0.0763, Val Loss: 0.0728\n",
      "Early stopping at epoch 95\n",
      "Macro F1 Score: 0.9649, Macro Precision: 0.9549, Macro Recall: 0.9759\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 4\n",
      "Training with F1=4, F2=8, D=8, dropout=0.1121872496700353, LR=0.0008786980779890255, BS=256, WD=0.004816546132911367\n",
      "Epoch 1/300 - Train Loss: 0.3489, Val Loss: 0.2142\n",
      "Epoch 2/300 - Train Loss: 0.1395, Val Loss: 0.0892\n",
      "Epoch 3/300 - Train Loss: 0.1010, Val Loss: 0.0842\n",
      "Epoch 4/300 - Train Loss: 0.0973, Val Loss: 0.0892\n",
      "Epoch 5/300 - Train Loss: 0.0942, Val Loss: 0.0831\n",
      "Epoch 6/300 - Train Loss: 0.0938, Val Loss: 0.0802\n",
      "Epoch 7/300 - Train Loss: 0.0929, Val Loss: 0.0829\n",
      "Epoch 8/300 - Train Loss: 0.0910, Val Loss: 0.0813\n",
      "Epoch 9/300 - Train Loss: 0.0916, Val Loss: 0.0806\n",
      "Epoch 10/300 - Train Loss: 0.0902, Val Loss: 0.0813\n",
      "Epoch 11/300 - Train Loss: 0.0915, Val Loss: 0.0811\n",
      "Epoch 12/300 - Train Loss: 0.0908, Val Loss: 0.0803\n",
      "Epoch 13/300 - Train Loss: 0.0901, Val Loss: 0.0931\n",
      "Epoch 14/300 - Train Loss: 0.0901, Val Loss: 0.0945\n",
      "Epoch 15/300 - Train Loss: 0.0897, Val Loss: 0.0803\n",
      "Epoch 16/300 - Train Loss: 0.0909, Val Loss: 0.0827\n",
      "Epoch 17/300 - Train Loss: 0.0899, Val Loss: 0.0876\n",
      "Epoch 18/300 - Train Loss: 0.0930, Val Loss: 0.0847\n",
      "Epoch 19/300 - Train Loss: 0.0922, Val Loss: 0.1243\n",
      "Epoch 20/300 - Train Loss: 0.0907, Val Loss: 0.0789\n",
      "Epoch 21/300 - Train Loss: 0.0928, Val Loss: 0.0937\n",
      "Epoch 22/300 - Train Loss: 0.0915, Val Loss: 0.0864\n",
      "Epoch 23/300 - Train Loss: 0.0922, Val Loss: 0.1158\n",
      "Epoch 24/300 - Train Loss: 0.0935, Val Loss: 0.0970\n",
      "Epoch 25/300 - Train Loss: 0.0957, Val Loss: 0.0782\n",
      "Epoch 26/300 - Train Loss: 0.0954, Val Loss: 0.0948\n",
      "Epoch 27/300 - Train Loss: 0.0911, Val Loss: 0.0841\n",
      "Epoch 28/300 - Train Loss: 0.0933, Val Loss: 0.0859\n",
      "Epoch 29/300 - Train Loss: 0.0922, Val Loss: 0.1033\n",
      "Epoch 30/300 - Train Loss: 0.0945, Val Loss: 0.0786\n",
      "Epoch 31/300 - Train Loss: 0.0942, Val Loss: 0.1047\n",
      "Epoch 32/300 - Train Loss: 0.0951, Val Loss: 0.0807\n",
      "Epoch 33/300 - Train Loss: 0.0936, Val Loss: 0.0847\n",
      "Epoch 34/300 - Train Loss: 0.0912, Val Loss: 0.0907\n",
      "Epoch 35/300 - Train Loss: 0.0935, Val Loss: 0.0903\n",
      "Epoch 36/300 - Train Loss: 0.0936, Val Loss: 0.0863\n",
      "Epoch 37/300 - Train Loss: 0.0938, Val Loss: 0.0871\n",
      "Epoch 38/300 - Train Loss: 0.0948, Val Loss: 0.1083\n",
      "Epoch 39/300 - Train Loss: 0.0943, Val Loss: 0.1225\n",
      "Epoch 40/300 - Train Loss: 0.0952, Val Loss: 0.0927\n",
      "Epoch 41/300 - Train Loss: 0.0935, Val Loss: 0.0822\n",
      "Epoch 42/300 - Train Loss: 0.0937, Val Loss: 0.0971\n",
      "Epoch 43/300 - Train Loss: 0.0945, Val Loss: 0.0859\n",
      "Epoch 44/300 - Train Loss: 0.0937, Val Loss: 0.0955\n",
      "Epoch 45/300 - Train Loss: 0.0965, Val Loss: 0.1536\n",
      "Epoch 46/300 - Train Loss: 0.0935, Val Loss: 0.0909\n",
      "Epoch 47/300 - Train Loss: 0.0920, Val Loss: 0.0830\n",
      "Epoch 48/300 - Train Loss: 0.0937, Val Loss: 0.1221\n",
      "Epoch 49/300 - Train Loss: 0.0931, Val Loss: 0.1287\n",
      "Epoch 50/300 - Train Loss: 0.0917, Val Loss: 0.0825\n",
      "Epoch 51/300 - Train Loss: 0.0941, Val Loss: 0.0831\n",
      "Epoch 52/300 - Train Loss: 0.0940, Val Loss: 0.0870\n",
      "Epoch 53/300 - Train Loss: 0.0926, Val Loss: 0.0852\n",
      "Epoch 54/300 - Train Loss: 0.0932, Val Loss: 0.0847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 22:33:32,597] Trial 3 finished with value: 0.9489783452682178 and parameters: {'F1': 4, 'F2': 8, 'D': 8, 'dropout': 0.1121872496700353, 'learning_rate': 0.0008786980779890255, 'batch_size': 256, 'weight_decay': 0.004816546132911367}. Best is trial 1 with value: 0.9667219239125732.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300 - Train Loss: 0.0953, Val Loss: 0.0893\n",
      "Early stopping at epoch 55\n",
      "Macro F1 Score: 0.9490, Macro Precision: 0.9575, Macro Recall: 0.9411\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.91      0.87      0.89        61\n",
      "           2       0.99      0.96      0.97       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.94      0.95      1443\n",
      "weighted avg       0.98      0.98      0.97      1443\n",
      "\n",
      "\n",
      "Trial 5\n",
      "Training with F1=16, F2=8, D=8, dropout=0.38416334500242744, LR=1.0024457868454064e-05, BS=256, WD=0.006811747381591768\n",
      "Epoch 1/300 - Train Loss: 1.1313, Val Loss: 1.0813\n",
      "Epoch 2/300 - Train Loss: 1.0706, Val Loss: 1.0224\n",
      "Epoch 3/300 - Train Loss: 0.9995, Val Loss: 0.9505\n",
      "Epoch 4/300 - Train Loss: 0.9231, Val Loss: 0.8711\n",
      "Epoch 5/300 - Train Loss: 0.8431, Val Loss: 0.7871\n",
      "Epoch 6/300 - Train Loss: 0.7606, Val Loss: 0.7115\n",
      "Epoch 7/300 - Train Loss: 0.6917, Val Loss: 0.6460\n",
      "Epoch 8/300 - Train Loss: 0.6285, Val Loss: 0.5906\n",
      "Epoch 9/300 - Train Loss: 0.5748, Val Loss: 0.5439\n",
      "Epoch 10/300 - Train Loss: 0.5297, Val Loss: 0.5017\n",
      "Epoch 11/300 - Train Loss: 0.4907, Val Loss: 0.4689\n",
      "Epoch 12/300 - Train Loss: 0.4611, Val Loss: 0.4391\n",
      "Epoch 13/300 - Train Loss: 0.4329, Val Loss: 0.4135\n",
      "Epoch 14/300 - Train Loss: 0.4064, Val Loss: 0.3911\n",
      "Epoch 15/300 - Train Loss: 0.3833, Val Loss: 0.3780\n",
      "Epoch 16/300 - Train Loss: 0.3649, Val Loss: 0.3519\n",
      "Epoch 17/300 - Train Loss: 0.3481, Val Loss: 0.3384\n",
      "Epoch 18/300 - Train Loss: 0.3352, Val Loss: 0.3223\n",
      "Epoch 19/300 - Train Loss: 0.3220, Val Loss: 0.3141\n",
      "Epoch 20/300 - Train Loss: 0.3096, Val Loss: 0.2962\n",
      "Epoch 21/300 - Train Loss: 0.2994, Val Loss: 0.2846\n",
      "Epoch 22/300 - Train Loss: 0.2895, Val Loss: 0.2773\n",
      "Epoch 23/300 - Train Loss: 0.2820, Val Loss: 0.2631\n",
      "Epoch 24/300 - Train Loss: 0.2711, Val Loss: 0.2627\n",
      "Epoch 25/300 - Train Loss: 0.2636, Val Loss: 0.2506\n",
      "Epoch 26/300 - Train Loss: 0.2565, Val Loss: 0.2458\n",
      "Epoch 27/300 - Train Loss: 0.2504, Val Loss: 0.2337\n",
      "Epoch 28/300 - Train Loss: 0.2444, Val Loss: 0.2303\n",
      "Epoch 29/300 - Train Loss: 0.2365, Val Loss: 0.2214\n",
      "Epoch 30/300 - Train Loss: 0.2290, Val Loss: 0.2146\n",
      "Epoch 31/300 - Train Loss: 0.2226, Val Loss: 0.2134\n",
      "Epoch 32/300 - Train Loss: 0.2201, Val Loss: 0.2011\n",
      "Epoch 33/300 - Train Loss: 0.2111, Val Loss: 0.1968\n",
      "Epoch 34/300 - Train Loss: 0.2055, Val Loss: 0.1915\n",
      "Epoch 35/300 - Train Loss: 0.2019, Val Loss: 0.1825\n",
      "Epoch 36/300 - Train Loss: 0.1962, Val Loss: 0.1835\n",
      "Epoch 37/300 - Train Loss: 0.1926, Val Loss: 0.1762\n",
      "Epoch 38/300 - Train Loss: 0.1885, Val Loss: 0.1749\n",
      "Epoch 39/300 - Train Loss: 0.1834, Val Loss: 0.1672\n",
      "Epoch 40/300 - Train Loss: 0.1787, Val Loss: 0.1635\n",
      "Epoch 41/300 - Train Loss: 0.1736, Val Loss: 0.1646\n",
      "Epoch 42/300 - Train Loss: 0.1711, Val Loss: 0.1557\n",
      "Epoch 43/300 - Train Loss: 0.1677, Val Loss: 0.1509\n",
      "Epoch 44/300 - Train Loss: 0.1649, Val Loss: 0.1472\n",
      "Epoch 45/300 - Train Loss: 0.1633, Val Loss: 0.1452\n",
      "Epoch 46/300 - Train Loss: 0.1598, Val Loss: 0.1461\n",
      "Epoch 47/300 - Train Loss: 0.1559, Val Loss: 0.1382\n",
      "Epoch 48/300 - Train Loss: 0.1530, Val Loss: 0.1384\n",
      "Epoch 49/300 - Train Loss: 0.1502, Val Loss: 0.1337\n",
      "Epoch 50/300 - Train Loss: 0.1477, Val Loss: 0.1323\n",
      "Epoch 51/300 - Train Loss: 0.1458, Val Loss: 0.1297\n",
      "Epoch 52/300 - Train Loss: 0.1444, Val Loss: 0.1286\n",
      "Epoch 53/300 - Train Loss: 0.1414, Val Loss: 0.1233\n",
      "Epoch 54/300 - Train Loss: 0.1396, Val Loss: 0.1252\n",
      "Epoch 55/300 - Train Loss: 0.1379, Val Loss: 0.1218\n",
      "Epoch 56/300 - Train Loss: 0.1382, Val Loss: 0.1199\n",
      "Epoch 57/300 - Train Loss: 0.1337, Val Loss: 0.1220\n",
      "Epoch 58/300 - Train Loss: 0.1334, Val Loss: 0.1165\n",
      "Epoch 59/300 - Train Loss: 0.1320, Val Loss: 0.1163\n",
      "Epoch 60/300 - Train Loss: 0.1299, Val Loss: 0.1146\n",
      "Epoch 61/300 - Train Loss: 0.1292, Val Loss: 0.1141\n",
      "Epoch 62/300 - Train Loss: 0.1284, Val Loss: 0.1137\n",
      "Epoch 63/300 - Train Loss: 0.1278, Val Loss: 0.1128\n",
      "Epoch 64/300 - Train Loss: 0.1246, Val Loss: 0.1102\n",
      "Epoch 65/300 - Train Loss: 0.1242, Val Loss: 0.1083\n",
      "Epoch 66/300 - Train Loss: 0.1243, Val Loss: 0.1060\n",
      "Epoch 67/300 - Train Loss: 0.1226, Val Loss: 0.1086\n",
      "Epoch 68/300 - Train Loss: 0.1215, Val Loss: 0.1066\n",
      "Epoch 69/300 - Train Loss: 0.1199, Val Loss: 0.1059\n",
      "Epoch 70/300 - Train Loss: 0.1200, Val Loss: 0.1050\n",
      "Epoch 71/300 - Train Loss: 0.1223, Val Loss: 0.1035\n",
      "Epoch 72/300 - Train Loss: 0.1183, Val Loss: 0.1026\n",
      "Epoch 73/300 - Train Loss: 0.1164, Val Loss: 0.1019\n",
      "Epoch 74/300 - Train Loss: 0.1173, Val Loss: 0.1028\n",
      "Epoch 75/300 - Train Loss: 0.1160, Val Loss: 0.1026\n",
      "Epoch 76/300 - Train Loss: 0.1160, Val Loss: 0.0999\n",
      "Epoch 77/300 - Train Loss: 0.1137, Val Loss: 0.0991\n",
      "Epoch 78/300 - Train Loss: 0.1123, Val Loss: 0.0982\n",
      "Epoch 79/300 - Train Loss: 0.1141, Val Loss: 0.0986\n",
      "Epoch 80/300 - Train Loss: 0.1129, Val Loss: 0.0972\n",
      "Epoch 81/300 - Train Loss: 0.1140, Val Loss: 0.0967\n",
      "Epoch 82/300 - Train Loss: 0.1125, Val Loss: 0.0973\n",
      "Epoch 83/300 - Train Loss: 0.1120, Val Loss: 0.0958\n",
      "Epoch 84/300 - Train Loss: 0.1134, Val Loss: 0.0948\n",
      "Epoch 85/300 - Train Loss: 0.1108, Val Loss: 0.0957\n",
      "Epoch 86/300 - Train Loss: 0.1109, Val Loss: 0.0947\n",
      "Epoch 87/300 - Train Loss: 0.1106, Val Loss: 0.0952\n",
      "Epoch 88/300 - Train Loss: 0.1109, Val Loss: 0.0969\n",
      "Epoch 89/300 - Train Loss: 0.1111, Val Loss: 0.0927\n",
      "Epoch 90/300 - Train Loss: 0.1094, Val Loss: 0.0939\n",
      "Epoch 91/300 - Train Loss: 0.1085, Val Loss: 0.0929\n",
      "Epoch 92/300 - Train Loss: 0.1080, Val Loss: 0.0935\n",
      "Epoch 93/300 - Train Loss: 0.1076, Val Loss: 0.0930\n",
      "Epoch 94/300 - Train Loss: 0.1071, Val Loss: 0.0922\n",
      "Epoch 95/300 - Train Loss: 0.1068, Val Loss: 0.0918\n",
      "Epoch 96/300 - Train Loss: 0.1086, Val Loss: 0.0924\n",
      "Epoch 97/300 - Train Loss: 0.1068, Val Loss: 0.0898\n",
      "Epoch 98/300 - Train Loss: 0.1086, Val Loss: 0.0908\n",
      "Epoch 99/300 - Train Loss: 0.1069, Val Loss: 0.0924\n",
      "Epoch 100/300 - Train Loss: 0.1050, Val Loss: 0.0896\n",
      "Epoch 101/300 - Train Loss: 0.1066, Val Loss: 0.0888\n",
      "Epoch 102/300 - Train Loss: 0.1040, Val Loss: 0.0895\n",
      "Epoch 103/300 - Train Loss: 0.1048, Val Loss: 0.0891\n",
      "Epoch 104/300 - Train Loss: 0.1050, Val Loss: 0.0878\n",
      "Epoch 105/300 - Train Loss: 0.1032, Val Loss: 0.0886\n",
      "Epoch 106/300 - Train Loss: 0.1059, Val Loss: 0.0881\n",
      "Epoch 107/300 - Train Loss: 0.1044, Val Loss: 0.0878\n",
      "Epoch 108/300 - Train Loss: 0.1052, Val Loss: 0.0891\n",
      "Epoch 109/300 - Train Loss: 0.1025, Val Loss: 0.0894\n",
      "Epoch 110/300 - Train Loss: 0.1031, Val Loss: 0.0878\n",
      "Epoch 111/300 - Train Loss: 0.1012, Val Loss: 0.0860\n",
      "Epoch 112/300 - Train Loss: 0.1047, Val Loss: 0.0866\n",
      "Epoch 113/300 - Train Loss: 0.1023, Val Loss: 0.0872\n",
      "Epoch 114/300 - Train Loss: 0.1022, Val Loss: 0.0869\n",
      "Epoch 115/300 - Train Loss: 0.1012, Val Loss: 0.0855\n",
      "Epoch 116/300 - Train Loss: 0.0995, Val Loss: 0.0866\n",
      "Epoch 117/300 - Train Loss: 0.1021, Val Loss: 0.0864\n",
      "Epoch 118/300 - Train Loss: 0.1001, Val Loss: 0.0862\n",
      "Epoch 119/300 - Train Loss: 0.0998, Val Loss: 0.0854\n",
      "Epoch 120/300 - Train Loss: 0.1001, Val Loss: 0.0841\n",
      "Epoch 121/300 - Train Loss: 0.1002, Val Loss: 0.0834\n",
      "Epoch 122/300 - Train Loss: 0.0995, Val Loss: 0.0846\n",
      "Epoch 123/300 - Train Loss: 0.0993, Val Loss: 0.0855\n",
      "Epoch 124/300 - Train Loss: 0.0984, Val Loss: 0.0849\n",
      "Epoch 125/300 - Train Loss: 0.1003, Val Loss: 0.0849\n",
      "Epoch 126/300 - Train Loss: 0.1003, Val Loss: 0.0839\n",
      "Epoch 127/300 - Train Loss: 0.0994, Val Loss: 0.0841\n",
      "Epoch 128/300 - Train Loss: 0.0994, Val Loss: 0.0848\n",
      "Epoch 129/300 - Train Loss: 0.0977, Val Loss: 0.0832\n",
      "Epoch 130/300 - Train Loss: 0.0991, Val Loss: 0.0823\n",
      "Epoch 131/300 - Train Loss: 0.0998, Val Loss: 0.0833\n",
      "Epoch 132/300 - Train Loss: 0.0978, Val Loss: 0.0831\n",
      "Epoch 133/300 - Train Loss: 0.0996, Val Loss: 0.0823\n",
      "Epoch 134/300 - Train Loss: 0.0987, Val Loss: 0.0828\n",
      "Epoch 135/300 - Train Loss: 0.0977, Val Loss: 0.0827\n",
      "Epoch 136/300 - Train Loss: 0.0968, Val Loss: 0.0829\n",
      "Epoch 137/300 - Train Loss: 0.0971, Val Loss: 0.0819\n",
      "Epoch 138/300 - Train Loss: 0.0965, Val Loss: 0.0827\n",
      "Epoch 139/300 - Train Loss: 0.0979, Val Loss: 0.0811\n",
      "Epoch 140/300 - Train Loss: 0.0960, Val Loss: 0.0811\n",
      "Epoch 141/300 - Train Loss: 0.0960, Val Loss: 0.0812\n",
      "Epoch 142/300 - Train Loss: 0.0967, Val Loss: 0.0806\n",
      "Epoch 143/300 - Train Loss: 0.0958, Val Loss: 0.0810\n",
      "Epoch 144/300 - Train Loss: 0.0973, Val Loss: 0.0818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0987, Val Loss: 0.0811\n",
      "Epoch 146/300 - Train Loss: 0.0966, Val Loss: 0.0802\n",
      "Epoch 147/300 - Train Loss: 0.0968, Val Loss: 0.0804\n",
      "Epoch 148/300 - Train Loss: 0.0960, Val Loss: 0.0798\n",
      "Epoch 149/300 - Train Loss: 0.0976, Val Loss: 0.0815\n",
      "Epoch 150/300 - Train Loss: 0.0965, Val Loss: 0.0807\n",
      "Epoch 151/300 - Train Loss: 0.0954, Val Loss: 0.0810\n",
      "Epoch 152/300 - Train Loss: 0.0941, Val Loss: 0.0805\n",
      "Epoch 153/300 - Train Loss: 0.0979, Val Loss: 0.0801\n",
      "Epoch 154/300 - Train Loss: 0.0970, Val Loss: 0.0808\n",
      "Epoch 155/300 - Train Loss: 0.0959, Val Loss: 0.0808\n",
      "Epoch 156/300 - Train Loss: 0.0948, Val Loss: 0.0788\n",
      "Epoch 157/300 - Train Loss: 0.0942, Val Loss: 0.0803\n",
      "Epoch 158/300 - Train Loss: 0.0933, Val Loss: 0.0797\n",
      "Epoch 159/300 - Train Loss: 0.0937, Val Loss: 0.0802\n",
      "Epoch 160/300 - Train Loss: 0.0944, Val Loss: 0.0794\n",
      "Epoch 161/300 - Train Loss: 0.0940, Val Loss: 0.0793\n",
      "Epoch 162/300 - Train Loss: 0.0946, Val Loss: 0.0795\n",
      "Epoch 163/300 - Train Loss: 0.0947, Val Loss: 0.0789\n",
      "Epoch 164/300 - Train Loss: 0.0962, Val Loss: 0.0800\n",
      "Epoch 165/300 - Train Loss: 0.0960, Val Loss: 0.0802\n",
      "Epoch 166/300 - Train Loss: 0.0930, Val Loss: 0.0787\n",
      "Epoch 167/300 - Train Loss: 0.0942, Val Loss: 0.0794\n",
      "Epoch 168/300 - Train Loss: 0.0948, Val Loss: 0.0799\n",
      "Epoch 169/300 - Train Loss: 0.0953, Val Loss: 0.0781\n",
      "Epoch 170/300 - Train Loss: 0.0930, Val Loss: 0.0778\n",
      "Epoch 171/300 - Train Loss: 0.0940, Val Loss: 0.0780\n",
      "Epoch 172/300 - Train Loss: 0.0946, Val Loss: 0.0774\n",
      "Epoch 173/300 - Train Loss: 0.0937, Val Loss: 0.0795\n",
      "Epoch 174/300 - Train Loss: 0.0942, Val Loss: 0.0794\n",
      "Epoch 175/300 - Train Loss: 0.0941, Val Loss: 0.0780\n",
      "Epoch 176/300 - Train Loss: 0.0913, Val Loss: 0.0784\n",
      "Epoch 177/300 - Train Loss: 0.0927, Val Loss: 0.0779\n",
      "Epoch 178/300 - Train Loss: 0.0919, Val Loss: 0.0776\n",
      "Epoch 179/300 - Train Loss: 0.0926, Val Loss: 0.0769\n",
      "Epoch 180/300 - Train Loss: 0.0928, Val Loss: 0.0778\n",
      "Epoch 181/300 - Train Loss: 0.0915, Val Loss: 0.0764\n",
      "Epoch 182/300 - Train Loss: 0.0907, Val Loss: 0.0783\n",
      "Epoch 183/300 - Train Loss: 0.0925, Val Loss: 0.0763\n",
      "Epoch 184/300 - Train Loss: 0.0906, Val Loss: 0.0786\n",
      "Epoch 185/300 - Train Loss: 0.0926, Val Loss: 0.0772\n",
      "Epoch 186/300 - Train Loss: 0.0927, Val Loss: 0.0776\n",
      "Epoch 187/300 - Train Loss: 0.0904, Val Loss: 0.0773\n",
      "Epoch 188/300 - Train Loss: 0.0924, Val Loss: 0.0765\n",
      "Epoch 189/300 - Train Loss: 0.0931, Val Loss: 0.0762\n",
      "Epoch 190/300 - Train Loss: 0.0916, Val Loss: 0.0765\n",
      "Epoch 191/300 - Train Loss: 0.0928, Val Loss: 0.0769\n",
      "Epoch 192/300 - Train Loss: 0.0900, Val Loss: 0.0754\n",
      "Epoch 193/300 - Train Loss: 0.0916, Val Loss: 0.0766\n",
      "Epoch 194/300 - Train Loss: 0.0936, Val Loss: 0.0765\n",
      "Epoch 195/300 - Train Loss: 0.0910, Val Loss: 0.0769\n",
      "Epoch 196/300 - Train Loss: 0.0930, Val Loss: 0.0768\n",
      "Epoch 197/300 - Train Loss: 0.0910, Val Loss: 0.0776\n",
      "Epoch 198/300 - Train Loss: 0.0918, Val Loss: 0.0766\n",
      "Epoch 199/300 - Train Loss: 0.0899, Val Loss: 0.0756\n",
      "Epoch 200/300 - Train Loss: 0.0906, Val Loss: 0.0763\n",
      "Epoch 201/300 - Train Loss: 0.0918, Val Loss: 0.0755\n",
      "Epoch 202/300 - Train Loss: 0.0912, Val Loss: 0.0758\n",
      "Epoch 203/300 - Train Loss: 0.0901, Val Loss: 0.0745\n",
      "Epoch 204/300 - Train Loss: 0.0902, Val Loss: 0.0755\n",
      "Epoch 205/300 - Train Loss: 0.0898, Val Loss: 0.0763\n",
      "Epoch 206/300 - Train Loss: 0.0895, Val Loss: 0.0754\n",
      "Epoch 207/300 - Train Loss: 0.0896, Val Loss: 0.0750\n",
      "Epoch 208/300 - Train Loss: 0.0910, Val Loss: 0.0750\n",
      "Epoch 209/300 - Train Loss: 0.0933, Val Loss: 0.0737\n",
      "Epoch 210/300 - Train Loss: 0.0906, Val Loss: 0.0754\n",
      "Epoch 211/300 - Train Loss: 0.0884, Val Loss: 0.0749\n",
      "Epoch 212/300 - Train Loss: 0.0904, Val Loss: 0.0747\n",
      "Epoch 213/300 - Train Loss: 0.0900, Val Loss: 0.0750\n",
      "Epoch 214/300 - Train Loss: 0.0908, Val Loss: 0.0760\n",
      "Epoch 215/300 - Train Loss: 0.0896, Val Loss: 0.0744\n",
      "Epoch 216/300 - Train Loss: 0.0902, Val Loss: 0.0741\n",
      "Epoch 217/300 - Train Loss: 0.0906, Val Loss: 0.0740\n",
      "Epoch 218/300 - Train Loss: 0.0913, Val Loss: 0.0745\n",
      "Epoch 219/300 - Train Loss: 0.0903, Val Loss: 0.0741\n",
      "Epoch 220/300 - Train Loss: 0.0915, Val Loss: 0.0743\n",
      "Epoch 221/300 - Train Loss: 0.0896, Val Loss: 0.0738\n",
      "Epoch 222/300 - Train Loss: 0.0895, Val Loss: 0.0744\n",
      "Epoch 223/300 - Train Loss: 0.0896, Val Loss: 0.0742\n",
      "Epoch 224/300 - Train Loss: 0.0882, Val Loss: 0.0750\n",
      "Epoch 225/300 - Train Loss: 0.0879, Val Loss: 0.0753\n",
      "Epoch 226/300 - Train Loss: 0.0911, Val Loss: 0.0743\n",
      "Epoch 227/300 - Train Loss: 0.0902, Val Loss: 0.0737\n",
      "Epoch 228/300 - Train Loss: 0.0894, Val Loss: 0.0739\n",
      "Epoch 229/300 - Train Loss: 0.0886, Val Loss: 0.0740\n",
      "Epoch 230/300 - Train Loss: 0.0904, Val Loss: 0.0742\n",
      "Epoch 231/300 - Train Loss: 0.0890, Val Loss: 0.0750\n",
      "Epoch 232/300 - Train Loss: 0.0920, Val Loss: 0.0732\n",
      "Epoch 233/300 - Train Loss: 0.0886, Val Loss: 0.0746\n",
      "Epoch 234/300 - Train Loss: 0.0886, Val Loss: 0.0747\n",
      "Epoch 235/300 - Train Loss: 0.0895, Val Loss: 0.0737\n",
      "Epoch 236/300 - Train Loss: 0.0931, Val Loss: 0.0736\n",
      "Epoch 237/300 - Train Loss: 0.0901, Val Loss: 0.0744\n",
      "Epoch 238/300 - Train Loss: 0.0891, Val Loss: 0.0744\n",
      "Epoch 239/300 - Train Loss: 0.0881, Val Loss: 0.0726\n",
      "Epoch 240/300 - Train Loss: 0.0898, Val Loss: 0.0743\n",
      "Epoch 241/300 - Train Loss: 0.0893, Val Loss: 0.0730\n",
      "Epoch 242/300 - Train Loss: 0.0876, Val Loss: 0.0732\n",
      "Epoch 243/300 - Train Loss: 0.0884, Val Loss: 0.0738\n",
      "Epoch 244/300 - Train Loss: 0.0908, Val Loss: 0.0732\n",
      "Epoch 245/300 - Train Loss: 0.0882, Val Loss: 0.0746\n",
      "Epoch 246/300 - Train Loss: 0.0877, Val Loss: 0.0739\n",
      "Epoch 247/300 - Train Loss: 0.0882, Val Loss: 0.0739\n",
      "Epoch 248/300 - Train Loss: 0.0881, Val Loss: 0.0729\n",
      "Epoch 249/300 - Train Loss: 0.0891, Val Loss: 0.0738\n",
      "Epoch 250/300 - Train Loss: 0.0889, Val Loss: 0.0724\n",
      "Epoch 251/300 - Train Loss: 0.0886, Val Loss: 0.0746\n",
      "Epoch 252/300 - Train Loss: 0.0879, Val Loss: 0.0744\n",
      "Epoch 253/300 - Train Loss: 0.0887, Val Loss: 0.0732\n",
      "Epoch 254/300 - Train Loss: 0.0877, Val Loss: 0.0721\n",
      "Epoch 255/300 - Train Loss: 0.0880, Val Loss: 0.0732\n",
      "Epoch 256/300 - Train Loss: 0.0880, Val Loss: 0.0729\n",
      "Epoch 257/300 - Train Loss: 0.0877, Val Loss: 0.0722\n",
      "Epoch 258/300 - Train Loss: 0.0884, Val Loss: 0.0725\n",
      "Epoch 259/300 - Train Loss: 0.0868, Val Loss: 0.0725\n",
      "Epoch 260/300 - Train Loss: 0.0869, Val Loss: 0.0722\n",
      "Epoch 261/300 - Train Loss: 0.0876, Val Loss: 0.0728\n",
      "Epoch 262/300 - Train Loss: 0.0871, Val Loss: 0.0720\n",
      "Epoch 263/300 - Train Loss: 0.0884, Val Loss: 0.0725\n",
      "Epoch 264/300 - Train Loss: 0.0863, Val Loss: 0.0725\n",
      "Epoch 265/300 - Train Loss: 0.0868, Val Loss: 0.0733\n",
      "Epoch 266/300 - Train Loss: 0.0899, Val Loss: 0.0712\n",
      "Epoch 267/300 - Train Loss: 0.0872, Val Loss: 0.0733\n",
      "Epoch 268/300 - Train Loss: 0.0905, Val Loss: 0.0728\n",
      "Epoch 269/300 - Train Loss: 0.0865, Val Loss: 0.0720\n",
      "Epoch 270/300 - Train Loss: 0.0895, Val Loss: 0.0722\n",
      "Epoch 271/300 - Train Loss: 0.0881, Val Loss: 0.0728\n",
      "Epoch 272/300 - Train Loss: 0.0870, Val Loss: 0.0730\n",
      "Epoch 273/300 - Train Loss: 0.0888, Val Loss: 0.0717\n",
      "Epoch 274/300 - Train Loss: 0.0867, Val Loss: 0.0715\n",
      "Epoch 275/300 - Train Loss: 0.0870, Val Loss: 0.0716\n",
      "Epoch 276/300 - Train Loss: 0.0870, Val Loss: 0.0720\n",
      "Epoch 277/300 - Train Loss: 0.0868, Val Loss: 0.0715\n",
      "Epoch 278/300 - Train Loss: 0.0871, Val Loss: 0.0722\n",
      "Epoch 279/300 - Train Loss: 0.0871, Val Loss: 0.0716\n",
      "Epoch 280/300 - Train Loss: 0.0880, Val Loss: 0.0722\n",
      "Epoch 281/300 - Train Loss: 0.0877, Val Loss: 0.0719\n",
      "Epoch 282/300 - Train Loss: 0.0861, Val Loss: 0.0726\n",
      "Epoch 283/300 - Train Loss: 0.0878, Val Loss: 0.0716\n",
      "Epoch 284/300 - Train Loss: 0.0863, Val Loss: 0.0716\n",
      "Epoch 285/300 - Train Loss: 0.0882, Val Loss: 0.0714\n",
      "Epoch 286/300 - Train Loss: 0.0866, Val Loss: 0.0713\n",
      "Epoch 287/300 - Train Loss: 0.0879, Val Loss: 0.0724\n",
      "Epoch 288/300 - Train Loss: 0.0865, Val Loss: 0.0724\n",
      "Epoch 289/300 - Train Loss: 0.0868, Val Loss: 0.0733\n",
      "Epoch 290/300 - Train Loss: 0.0894, Val Loss: 0.0720\n",
      "Epoch 291/300 - Train Loss: 0.0869, Val Loss: 0.0720\n",
      "Epoch 292/300 - Train Loss: 0.0871, Val Loss: 0.0719\n",
      "Epoch 293/300 - Train Loss: 0.0856, Val Loss: 0.0712\n",
      "Epoch 294/300 - Train Loss: 0.0887, Val Loss: 0.0707\n",
      "Epoch 295/300 - Train Loss: 0.0859, Val Loss: 0.0712\n",
      "Epoch 296/300 - Train Loss: 0.0877, Val Loss: 0.0713\n",
      "Epoch 297/300 - Train Loss: 0.0856, Val Loss: 0.0725\n",
      "Epoch 298/300 - Train Loss: 0.0863, Val Loss: 0.0725\n",
      "Epoch 299/300 - Train Loss: 0.0895, Val Loss: 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 22:48:21,250] Trial 4 finished with value: 0.961329841987132 and parameters: {'F1': 16, 'F2': 8, 'D': 8, 'dropout': 0.38416334500242744, 'learning_rate': 1.0024457868454064e-05, 'batch_size': 256, 'weight_decay': 0.006811747381591768}. Best is trial 1 with value: 0.9667219239125732.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/300 - Train Loss: 0.0871, Val Loss: 0.0711\n",
      "Macro F1 Score: 0.9613, Macro Precision: 0.9541, Macro Recall: 0.9694\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 6\n",
      "Training with F1=8, F2=16, D=2, dropout=0.22066116417382065, LR=1.6413338509932647e-05, BS=256, WD=0.0010233218939908906\n",
      "Epoch 1/300 - Train Loss: 1.0523, Val Loss: 0.9998\n",
      "Epoch 2/300 - Train Loss: 0.9453, Val Loss: 0.9120\n",
      "Epoch 3/300 - Train Loss: 0.8527, Val Loss: 0.8246\n",
      "Epoch 4/300 - Train Loss: 0.7598, Val Loss: 0.7297\n",
      "Epoch 5/300 - Train Loss: 0.6698, Val Loss: 0.6400\n",
      "Epoch 6/300 - Train Loss: 0.5835, Val Loss: 0.5559\n",
      "Epoch 7/300 - Train Loss: 0.5078, Val Loss: 0.4791\n",
      "Epoch 8/300 - Train Loss: 0.4446, Val Loss: 0.4296\n",
      "Epoch 9/300 - Train Loss: 0.4014, Val Loss: 0.3896\n",
      "Epoch 10/300 - Train Loss: 0.3688, Val Loss: 0.3561\n",
      "Epoch 11/300 - Train Loss: 0.3429, Val Loss: 0.3335\n",
      "Epoch 12/300 - Train Loss: 0.3230, Val Loss: 0.3182\n",
      "Epoch 13/300 - Train Loss: 0.3067, Val Loss: 0.3031\n",
      "Epoch 14/300 - Train Loss: 0.2923, Val Loss: 0.2894\n",
      "Epoch 15/300 - Train Loss: 0.2816, Val Loss: 0.2771\n",
      "Epoch 16/300 - Train Loss: 0.2711, Val Loss: 0.2659\n",
      "Epoch 17/300 - Train Loss: 0.2615, Val Loss: 0.2571\n",
      "Epoch 18/300 - Train Loss: 0.2549, Val Loss: 0.2463\n",
      "Epoch 19/300 - Train Loss: 0.2486, Val Loss: 0.2383\n",
      "Epoch 20/300 - Train Loss: 0.2389, Val Loss: 0.2321\n",
      "Epoch 21/300 - Train Loss: 0.2332, Val Loss: 0.2241\n",
      "Epoch 22/300 - Train Loss: 0.2267, Val Loss: 0.2130\n",
      "Epoch 23/300 - Train Loss: 0.2187, Val Loss: 0.2057\n",
      "Epoch 24/300 - Train Loss: 0.2144, Val Loss: 0.2031\n",
      "Epoch 25/300 - Train Loss: 0.2096, Val Loss: 0.1969\n",
      "Epoch 26/300 - Train Loss: 0.2005, Val Loss: 0.1871\n",
      "Epoch 27/300 - Train Loss: 0.1969, Val Loss: 0.1814\n",
      "Epoch 28/300 - Train Loss: 0.1927, Val Loss: 0.1771\n",
      "Epoch 29/300 - Train Loss: 0.1872, Val Loss: 0.1707\n",
      "Epoch 30/300 - Train Loss: 0.1817, Val Loss: 0.1677\n",
      "Epoch 31/300 - Train Loss: 0.1795, Val Loss: 0.1627\n",
      "Epoch 32/300 - Train Loss: 0.1741, Val Loss: 0.1563\n",
      "Epoch 33/300 - Train Loss: 0.1708, Val Loss: 0.1569\n",
      "Epoch 34/300 - Train Loss: 0.1680, Val Loss: 0.1500\n",
      "Epoch 35/300 - Train Loss: 0.1647, Val Loss: 0.1463\n",
      "Epoch 36/300 - Train Loss: 0.1638, Val Loss: 0.1448\n",
      "Epoch 37/300 - Train Loss: 0.1590, Val Loss: 0.1431\n",
      "Epoch 38/300 - Train Loss: 0.1563, Val Loss: 0.1375\n",
      "Epoch 39/300 - Train Loss: 0.1529, Val Loss: 0.1349\n",
      "Epoch 40/300 - Train Loss: 0.1499, Val Loss: 0.1315\n",
      "Epoch 41/300 - Train Loss: 0.1493, Val Loss: 0.1291\n",
      "Epoch 42/300 - Train Loss: 0.1451, Val Loss: 0.1274\n",
      "Epoch 43/300 - Train Loss: 0.1450, Val Loss: 0.1266\n",
      "Epoch 44/300 - Train Loss: 0.1415, Val Loss: 0.1227\n",
      "Epoch 45/300 - Train Loss: 0.1404, Val Loss: 0.1212\n",
      "Epoch 46/300 - Train Loss: 0.1380, Val Loss: 0.1176\n",
      "Epoch 47/300 - Train Loss: 0.1355, Val Loss: 0.1185\n",
      "Epoch 48/300 - Train Loss: 0.1343, Val Loss: 0.1156\n",
      "Epoch 49/300 - Train Loss: 0.1317, Val Loss: 0.1142\n",
      "Epoch 50/300 - Train Loss: 0.1290, Val Loss: 0.1119\n",
      "Epoch 51/300 - Train Loss: 0.1270, Val Loss: 0.1107\n",
      "Epoch 52/300 - Train Loss: 0.1276, Val Loss: 0.1105\n",
      "Epoch 53/300 - Train Loss: 0.1242, Val Loss: 0.1080\n",
      "Epoch 54/300 - Train Loss: 0.1233, Val Loss: 0.1063\n",
      "Epoch 55/300 - Train Loss: 0.1221, Val Loss: 0.1071\n",
      "Epoch 56/300 - Train Loss: 0.1209, Val Loss: 0.1047\n",
      "Epoch 57/300 - Train Loss: 0.1198, Val Loss: 0.1035\n",
      "Epoch 58/300 - Train Loss: 0.1178, Val Loss: 0.1032\n",
      "Epoch 59/300 - Train Loss: 0.1182, Val Loss: 0.1028\n",
      "Epoch 60/300 - Train Loss: 0.1151, Val Loss: 0.1015\n",
      "Epoch 61/300 - Train Loss: 0.1153, Val Loss: 0.1016\n",
      "Epoch 62/300 - Train Loss: 0.1140, Val Loss: 0.0993\n",
      "Epoch 63/300 - Train Loss: 0.1125, Val Loss: 0.0998\n",
      "Epoch 64/300 - Train Loss: 0.1109, Val Loss: 0.0990\n",
      "Epoch 65/300 - Train Loss: 0.1128, Val Loss: 0.0991\n",
      "Epoch 66/300 - Train Loss: 0.1105, Val Loss: 0.0980\n",
      "Epoch 67/300 - Train Loss: 0.1109, Val Loss: 0.0966\n",
      "Epoch 68/300 - Train Loss: 0.1082, Val Loss: 0.0972\n",
      "Epoch 69/300 - Train Loss: 0.1093, Val Loss: 0.0967\n",
      "Epoch 70/300 - Train Loss: 0.1089, Val Loss: 0.0951\n",
      "Epoch 71/300 - Train Loss: 0.1064, Val Loss: 0.0951\n",
      "Epoch 72/300 - Train Loss: 0.1081, Val Loss: 0.0939\n",
      "Epoch 73/300 - Train Loss: 0.1066, Val Loss: 0.0949\n",
      "Epoch 74/300 - Train Loss: 0.1071, Val Loss: 0.0937\n",
      "Epoch 75/300 - Train Loss: 0.1061, Val Loss: 0.0932\n",
      "Epoch 76/300 - Train Loss: 0.1050, Val Loss: 0.0917\n",
      "Epoch 77/300 - Train Loss: 0.1062, Val Loss: 0.0925\n",
      "Epoch 78/300 - Train Loss: 0.1040, Val Loss: 0.0925\n",
      "Epoch 79/300 - Train Loss: 0.1035, Val Loss: 0.0921\n",
      "Epoch 80/300 - Train Loss: 0.1045, Val Loss: 0.0916\n",
      "Epoch 81/300 - Train Loss: 0.1034, Val Loss: 0.0923\n",
      "Epoch 82/300 - Train Loss: 0.1033, Val Loss: 0.0918\n",
      "Epoch 83/300 - Train Loss: 0.1019, Val Loss: 0.0901\n",
      "Epoch 84/300 - Train Loss: 0.1022, Val Loss: 0.0904\n",
      "Epoch 85/300 - Train Loss: 0.1015, Val Loss: 0.0910\n",
      "Epoch 86/300 - Train Loss: 0.1008, Val Loss: 0.0894\n",
      "Epoch 87/300 - Train Loss: 0.1019, Val Loss: 0.0907\n",
      "Epoch 88/300 - Train Loss: 0.1002, Val Loss: 0.0903\n",
      "Epoch 89/300 - Train Loss: 0.0990, Val Loss: 0.0909\n",
      "Epoch 90/300 - Train Loss: 0.0998, Val Loss: 0.0893\n",
      "Epoch 91/300 - Train Loss: 0.1000, Val Loss: 0.0882\n",
      "Epoch 92/300 - Train Loss: 0.0984, Val Loss: 0.0881\n",
      "Epoch 93/300 - Train Loss: 0.0992, Val Loss: 0.0887\n",
      "Epoch 94/300 - Train Loss: 0.1004, Val Loss: 0.0876\n",
      "Epoch 95/300 - Train Loss: 0.0998, Val Loss: 0.0879\n",
      "Epoch 96/300 - Train Loss: 0.0964, Val Loss: 0.0871\n",
      "Epoch 97/300 - Train Loss: 0.0997, Val Loss: 0.0869\n",
      "Epoch 98/300 - Train Loss: 0.0975, Val Loss: 0.0897\n",
      "Epoch 99/300 - Train Loss: 0.0978, Val Loss: 0.0870\n",
      "Epoch 100/300 - Train Loss: 0.0976, Val Loss: 0.0863\n",
      "Epoch 101/300 - Train Loss: 0.0975, Val Loss: 0.0861\n",
      "Epoch 102/300 - Train Loss: 0.0963, Val Loss: 0.0868\n",
      "Epoch 103/300 - Train Loss: 0.0957, Val Loss: 0.0865\n",
      "Epoch 104/300 - Train Loss: 0.0968, Val Loss: 0.0865\n",
      "Epoch 105/300 - Train Loss: 0.0958, Val Loss: 0.0864\n",
      "Epoch 106/300 - Train Loss: 0.0972, Val Loss: 0.0867\n",
      "Epoch 107/300 - Train Loss: 0.0959, Val Loss: 0.0847\n",
      "Epoch 108/300 - Train Loss: 0.0950, Val Loss: 0.0856\n",
      "Epoch 109/300 - Train Loss: 0.0967, Val Loss: 0.0843\n",
      "Epoch 110/300 - Train Loss: 0.0981, Val Loss: 0.0852\n",
      "Epoch 111/300 - Train Loss: 0.0936, Val Loss: 0.0837\n",
      "Epoch 112/300 - Train Loss: 0.0945, Val Loss: 0.0859\n",
      "Epoch 113/300 - Train Loss: 0.0945, Val Loss: 0.0864\n",
      "Epoch 114/300 - Train Loss: 0.0949, Val Loss: 0.0853\n",
      "Epoch 115/300 - Train Loss: 0.0955, Val Loss: 0.0840\n",
      "Epoch 116/300 - Train Loss: 0.0927, Val Loss: 0.0829\n",
      "Epoch 117/300 - Train Loss: 0.0960, Val Loss: 0.0850\n",
      "Epoch 118/300 - Train Loss: 0.0932, Val Loss: 0.0852\n",
      "Epoch 119/300 - Train Loss: 0.0933, Val Loss: 0.0843\n",
      "Epoch 120/300 - Train Loss: 0.0940, Val Loss: 0.0849\n",
      "Epoch 121/300 - Train Loss: 0.0932, Val Loss: 0.0827\n",
      "Epoch 122/300 - Train Loss: 0.0924, Val Loss: 0.0847\n",
      "Epoch 123/300 - Train Loss: 0.0928, Val Loss: 0.0818\n",
      "Epoch 124/300 - Train Loss: 0.0956, Val Loss: 0.0825\n",
      "Epoch 125/300 - Train Loss: 0.0921, Val Loss: 0.0825\n",
      "Epoch 126/300 - Train Loss: 0.0931, Val Loss: 0.0831\n",
      "Epoch 127/300 - Train Loss: 0.0928, Val Loss: 0.0821\n",
      "Epoch 128/300 - Train Loss: 0.0918, Val Loss: 0.0831\n",
      "Epoch 129/300 - Train Loss: 0.0906, Val Loss: 0.0834\n",
      "Epoch 130/300 - Train Loss: 0.0921, Val Loss: 0.0836\n",
      "Epoch 131/300 - Train Loss: 0.0926, Val Loss: 0.0811\n",
      "Epoch 132/300 - Train Loss: 0.0938, Val Loss: 0.0824\n",
      "Epoch 133/300 - Train Loss: 0.0926, Val Loss: 0.0838\n",
      "Epoch 134/300 - Train Loss: 0.0926, Val Loss: 0.0813\n",
      "Epoch 135/300 - Train Loss: 0.0933, Val Loss: 0.0846\n",
      "Epoch 136/300 - Train Loss: 0.0911, Val Loss: 0.0814\n",
      "Epoch 137/300 - Train Loss: 0.0910, Val Loss: 0.0833\n",
      "Epoch 138/300 - Train Loss: 0.0921, Val Loss: 0.0824\n",
      "Epoch 139/300 - Train Loss: 0.0904, Val Loss: 0.0810\n",
      "Epoch 140/300 - Train Loss: 0.0918, Val Loss: 0.0818\n",
      "Epoch 141/300 - Train Loss: 0.0899, Val Loss: 0.0826\n",
      "Epoch 142/300 - Train Loss: 0.0915, Val Loss: 0.0808\n",
      "Epoch 143/300 - Train Loss: 0.0900, Val Loss: 0.0815\n",
      "Epoch 144/300 - Train Loss: 0.0904, Val Loss: 0.0820\n",
      "Epoch 145/300 - Train Loss: 0.0901, Val Loss: 0.0819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/300 - Train Loss: 0.0921, Val Loss: 0.0816\n",
      "Epoch 147/300 - Train Loss: 0.0899, Val Loss: 0.0807\n",
      "Epoch 148/300 - Train Loss: 0.0908, Val Loss: 0.0800\n",
      "Epoch 149/300 - Train Loss: 0.0899, Val Loss: 0.0809\n",
      "Epoch 150/300 - Train Loss: 0.0906, Val Loss: 0.0800\n",
      "Epoch 151/300 - Train Loss: 0.0883, Val Loss: 0.0809\n",
      "Epoch 152/300 - Train Loss: 0.0923, Val Loss: 0.0807\n",
      "Epoch 153/300 - Train Loss: 0.0896, Val Loss: 0.0796\n",
      "Epoch 154/300 - Train Loss: 0.0910, Val Loss: 0.0803\n",
      "Epoch 155/300 - Train Loss: 0.0888, Val Loss: 0.0809\n",
      "Epoch 156/300 - Train Loss: 0.0889, Val Loss: 0.0827\n",
      "Epoch 157/300 - Train Loss: 0.0893, Val Loss: 0.0796\n",
      "Epoch 158/300 - Train Loss: 0.0907, Val Loss: 0.0811\n",
      "Epoch 159/300 - Train Loss: 0.0893, Val Loss: 0.0810\n",
      "Epoch 160/300 - Train Loss: 0.0896, Val Loss: 0.0815\n",
      "Epoch 161/300 - Train Loss: 0.0886, Val Loss: 0.0813\n",
      "Epoch 162/300 - Train Loss: 0.0885, Val Loss: 0.0797\n",
      "Epoch 163/300 - Train Loss: 0.0908, Val Loss: 0.0803\n",
      "Epoch 164/300 - Train Loss: 0.0883, Val Loss: 0.0803\n",
      "Epoch 165/300 - Train Loss: 0.0917, Val Loss: 0.0795\n",
      "Epoch 166/300 - Train Loss: 0.0882, Val Loss: 0.0810\n",
      "Epoch 167/300 - Train Loss: 0.0885, Val Loss: 0.0799\n",
      "Epoch 168/300 - Train Loss: 0.0884, Val Loss: 0.0800\n",
      "Epoch 169/300 - Train Loss: 0.0891, Val Loss: 0.0794\n",
      "Epoch 170/300 - Train Loss: 0.0884, Val Loss: 0.0802\n",
      "Epoch 171/300 - Train Loss: 0.0869, Val Loss: 0.0798\n",
      "Epoch 172/300 - Train Loss: 0.0880, Val Loss: 0.0807\n",
      "Epoch 173/300 - Train Loss: 0.0867, Val Loss: 0.0792\n",
      "Epoch 174/300 - Train Loss: 0.0870, Val Loss: 0.0799\n",
      "Epoch 175/300 - Train Loss: 0.0862, Val Loss: 0.0797\n",
      "Epoch 176/300 - Train Loss: 0.0886, Val Loss: 0.0804\n",
      "Epoch 177/300 - Train Loss: 0.0879, Val Loss: 0.0788\n",
      "Epoch 178/300 - Train Loss: 0.0856, Val Loss: 0.0796\n",
      "Epoch 179/300 - Train Loss: 0.0883, Val Loss: 0.0788\n",
      "Epoch 180/300 - Train Loss: 0.0887, Val Loss: 0.0798\n",
      "Epoch 181/300 - Train Loss: 0.0869, Val Loss: 0.0813\n",
      "Epoch 182/300 - Train Loss: 0.0870, Val Loss: 0.0782\n",
      "Epoch 183/300 - Train Loss: 0.0867, Val Loss: 0.0782\n",
      "Epoch 184/300 - Train Loss: 0.0875, Val Loss: 0.0771\n",
      "Epoch 185/300 - Train Loss: 0.0875, Val Loss: 0.0788\n",
      "Epoch 186/300 - Train Loss: 0.0868, Val Loss: 0.0786\n",
      "Epoch 187/300 - Train Loss: 0.0864, Val Loss: 0.0786\n",
      "Epoch 188/300 - Train Loss: 0.0873, Val Loss: 0.0784\n",
      "Epoch 189/300 - Train Loss: 0.0863, Val Loss: 0.0790\n",
      "Epoch 190/300 - Train Loss: 0.0892, Val Loss: 0.0772\n",
      "Epoch 191/300 - Train Loss: 0.0873, Val Loss: 0.0782\n",
      "Epoch 192/300 - Train Loss: 0.0861, Val Loss: 0.0785\n",
      "Epoch 193/300 - Train Loss: 0.0865, Val Loss: 0.0773\n",
      "Epoch 194/300 - Train Loss: 0.0870, Val Loss: 0.0788\n",
      "Epoch 195/300 - Train Loss: 0.0887, Val Loss: 0.0784\n",
      "Epoch 196/300 - Train Loss: 0.0852, Val Loss: 0.0790\n",
      "Epoch 197/300 - Train Loss: 0.0860, Val Loss: 0.0798\n",
      "Epoch 198/300 - Train Loss: 0.0861, Val Loss: 0.0786\n",
      "Epoch 199/300 - Train Loss: 0.0863, Val Loss: 0.0781\n",
      "Epoch 200/300 - Train Loss: 0.0863, Val Loss: 0.0780\n",
      "Epoch 201/300 - Train Loss: 0.0853, Val Loss: 0.0790\n",
      "Epoch 202/300 - Train Loss: 0.0863, Val Loss: 0.0781\n",
      "Epoch 203/300 - Train Loss: 0.0879, Val Loss: 0.0801\n",
      "Epoch 204/300 - Train Loss: 0.0859, Val Loss: 0.0771\n",
      "Epoch 205/300 - Train Loss: 0.0876, Val Loss: 0.0798\n",
      "Epoch 206/300 - Train Loss: 0.0846, Val Loss: 0.0793\n",
      "Epoch 207/300 - Train Loss: 0.0847, Val Loss: 0.0791\n",
      "Epoch 208/300 - Train Loss: 0.0858, Val Loss: 0.0779\n",
      "Epoch 209/300 - Train Loss: 0.0855, Val Loss: 0.0772\n",
      "Epoch 210/300 - Train Loss: 0.0853, Val Loss: 0.0777\n",
      "Epoch 211/300 - Train Loss: 0.0847, Val Loss: 0.0777\n",
      "Epoch 212/300 - Train Loss: 0.0848, Val Loss: 0.0774\n",
      "Epoch 213/300 - Train Loss: 0.0837, Val Loss: 0.0769\n",
      "Epoch 214/300 - Train Loss: 0.0846, Val Loss: 0.0785\n",
      "Epoch 215/300 - Train Loss: 0.0843, Val Loss: 0.0768\n",
      "Epoch 216/300 - Train Loss: 0.0847, Val Loss: 0.0785\n",
      "Epoch 217/300 - Train Loss: 0.0840, Val Loss: 0.0794\n",
      "Epoch 218/300 - Train Loss: 0.0840, Val Loss: 0.0783\n",
      "Epoch 219/300 - Train Loss: 0.0837, Val Loss: 0.0780\n",
      "Epoch 220/300 - Train Loss: 0.0856, Val Loss: 0.0777\n",
      "Epoch 221/300 - Train Loss: 0.0837, Val Loss: 0.0776\n",
      "Epoch 222/300 - Train Loss: 0.0829, Val Loss: 0.0777\n",
      "Epoch 223/300 - Train Loss: 0.0846, Val Loss: 0.0765\n",
      "Epoch 224/300 - Train Loss: 0.0864, Val Loss: 0.0768\n",
      "Epoch 225/300 - Train Loss: 0.0865, Val Loss: 0.0763\n",
      "Epoch 226/300 - Train Loss: 0.0854, Val Loss: 0.0779\n",
      "Epoch 227/300 - Train Loss: 0.0848, Val Loss: 0.0785\n",
      "Epoch 228/300 - Train Loss: 0.0835, Val Loss: 0.0772\n",
      "Epoch 229/300 - Train Loss: 0.0843, Val Loss: 0.0778\n",
      "Epoch 230/300 - Train Loss: 0.0872, Val Loss: 0.0767\n",
      "Epoch 231/300 - Train Loss: 0.0869, Val Loss: 0.0787\n",
      "Epoch 232/300 - Train Loss: 0.0828, Val Loss: 0.0772\n",
      "Epoch 233/300 - Train Loss: 0.0844, Val Loss: 0.0769\n",
      "Epoch 234/300 - Train Loss: 0.0835, Val Loss: 0.0769\n",
      "Epoch 235/300 - Train Loss: 0.0834, Val Loss: 0.0774\n",
      "Epoch 236/300 - Train Loss: 0.0829, Val Loss: 0.0766\n",
      "Epoch 237/300 - Train Loss: 0.0853, Val Loss: 0.0786\n",
      "Epoch 238/300 - Train Loss: 0.0857, Val Loss: 0.0772\n",
      "Epoch 239/300 - Train Loss: 0.0844, Val Loss: 0.0760\n",
      "Epoch 240/300 - Train Loss: 0.0835, Val Loss: 0.0763\n",
      "Epoch 241/300 - Train Loss: 0.0836, Val Loss: 0.0775\n",
      "Epoch 242/300 - Train Loss: 0.0831, Val Loss: 0.0774\n",
      "Epoch 243/300 - Train Loss: 0.0819, Val Loss: 0.0762\n",
      "Epoch 244/300 - Train Loss: 0.0832, Val Loss: 0.0770\n",
      "Epoch 245/300 - Train Loss: 0.0843, Val Loss: 0.0756\n",
      "Epoch 246/300 - Train Loss: 0.0828, Val Loss: 0.0772\n",
      "Epoch 247/300 - Train Loss: 0.0822, Val Loss: 0.0765\n",
      "Epoch 248/300 - Train Loss: 0.0844, Val Loss: 0.0766\n",
      "Epoch 249/300 - Train Loss: 0.0829, Val Loss: 0.0757\n",
      "Epoch 250/300 - Train Loss: 0.0832, Val Loss: 0.0774\n",
      "Epoch 251/300 - Train Loss: 0.0837, Val Loss: 0.0758\n",
      "Epoch 252/300 - Train Loss: 0.0835, Val Loss: 0.0777\n",
      "Epoch 253/300 - Train Loss: 0.0823, Val Loss: 0.0752\n",
      "Epoch 254/300 - Train Loss: 0.0820, Val Loss: 0.0764\n",
      "Epoch 255/300 - Train Loss: 0.0821, Val Loss: 0.0764\n",
      "Epoch 256/300 - Train Loss: 0.0817, Val Loss: 0.0766\n",
      "Epoch 257/300 - Train Loss: 0.0836, Val Loss: 0.0766\n",
      "Epoch 258/300 - Train Loss: 0.0823, Val Loss: 0.0779\n",
      "Epoch 259/300 - Train Loss: 0.0844, Val Loss: 0.0763\n",
      "Epoch 260/300 - Train Loss: 0.0824, Val Loss: 0.0758\n",
      "Epoch 261/300 - Train Loss: 0.0831, Val Loss: 0.0760\n",
      "Epoch 262/300 - Train Loss: 0.0839, Val Loss: 0.0764\n",
      "Epoch 263/300 - Train Loss: 0.0828, Val Loss: 0.0757\n",
      "Epoch 264/300 - Train Loss: 0.0812, Val Loss: 0.0766\n",
      "Epoch 265/300 - Train Loss: 0.0822, Val Loss: 0.0775\n",
      "Epoch 266/300 - Train Loss: 0.0822, Val Loss: 0.0760\n",
      "Epoch 267/300 - Train Loss: 0.0835, Val Loss: 0.0764\n",
      "Epoch 268/300 - Train Loss: 0.0832, Val Loss: 0.0750\n",
      "Epoch 269/300 - Train Loss: 0.0836, Val Loss: 0.0760\n",
      "Epoch 270/300 - Train Loss: 0.0822, Val Loss: 0.0752\n",
      "Epoch 271/300 - Train Loss: 0.0832, Val Loss: 0.0779\n",
      "Epoch 272/300 - Train Loss: 0.0821, Val Loss: 0.0768\n",
      "Epoch 273/300 - Train Loss: 0.0820, Val Loss: 0.0748\n",
      "Epoch 274/300 - Train Loss: 0.0810, Val Loss: 0.0762\n",
      "Epoch 275/300 - Train Loss: 0.0821, Val Loss: 0.0753\n",
      "Epoch 276/300 - Train Loss: 0.0819, Val Loss: 0.0754\n",
      "Epoch 277/300 - Train Loss: 0.0819, Val Loss: 0.0753\n",
      "Epoch 278/300 - Train Loss: 0.0828, Val Loss: 0.0758\n",
      "Epoch 279/300 - Train Loss: 0.0807, Val Loss: 0.0763\n",
      "Epoch 280/300 - Train Loss: 0.0817, Val Loss: 0.0763\n",
      "Epoch 281/300 - Train Loss: 0.0829, Val Loss: 0.0744\n",
      "Epoch 282/300 - Train Loss: 0.0824, Val Loss: 0.0755\n",
      "Epoch 283/300 - Train Loss: 0.0829, Val Loss: 0.0758\n",
      "Epoch 284/300 - Train Loss: 0.0818, Val Loss: 0.0768\n",
      "Epoch 285/300 - Train Loss: 0.0824, Val Loss: 0.0751\n",
      "Epoch 286/300 - Train Loss: 0.0816, Val Loss: 0.0767\n",
      "Epoch 287/300 - Train Loss: 0.0827, Val Loss: 0.0752\n",
      "Epoch 288/300 - Train Loss: 0.0803, Val Loss: 0.0756\n",
      "Epoch 289/300 - Train Loss: 0.0829, Val Loss: 0.0756\n",
      "Epoch 290/300 - Train Loss: 0.0800, Val Loss: 0.0749\n",
      "Epoch 291/300 - Train Loss: 0.0814, Val Loss: 0.0746\n",
      "Epoch 292/300 - Train Loss: 0.0824, Val Loss: 0.0762\n",
      "Epoch 293/300 - Train Loss: 0.0805, Val Loss: 0.0755\n",
      "Epoch 294/300 - Train Loss: 0.0844, Val Loss: 0.0772\n",
      "Epoch 295/300 - Train Loss: 0.0803, Val Loss: 0.0756\n",
      "Epoch 296/300 - Train Loss: 0.0814, Val Loss: 0.0756\n",
      "Epoch 297/300 - Train Loss: 0.0796, Val Loss: 0.0755\n",
      "Epoch 298/300 - Train Loss: 0.0805, Val Loss: 0.0749\n",
      "Epoch 299/300 - Train Loss: 0.0830, Val Loss: 0.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 22:53:02,939] Trial 5 finished with value: 0.9576000278193634 and parameters: {'F1': 8, 'F2': 16, 'D': 2, 'dropout': 0.22066116417382065, 'learning_rate': 1.6413338509932647e-05, 'batch_size': 256, 'weight_decay': 0.0010233218939908906}. Best is trial 1 with value: 0.9667219239125732.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/300 - Train Loss: 0.0801, Val Loss: 0.0746\n",
      "Macro F1 Score: 0.9576, Macro Precision: 0.9462, Macro Recall: 0.9704\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.87      0.95      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 7\n",
      "Training with F1=8, F2=16, D=4, dropout=0.19223815962822166, LR=0.00036545579910001716, BS=32, WD=0.0018908988645410757\n",
      "Epoch 1/300 - Train Loss: 0.1997, Val Loss: 0.0893\n",
      "Epoch 2/300 - Train Loss: 0.1058, Val Loss: 0.0862\n",
      "Epoch 3/300 - Train Loss: 0.0992, Val Loss: 0.0771\n",
      "Epoch 4/300 - Train Loss: 0.0957, Val Loss: 0.0723\n",
      "Epoch 5/300 - Train Loss: 0.0980, Val Loss: 0.0759\n",
      "Epoch 6/300 - Train Loss: 0.0938, Val Loss: 0.0811\n",
      "Epoch 7/300 - Train Loss: 0.0923, Val Loss: 0.0774\n",
      "Epoch 8/300 - Train Loss: 0.0930, Val Loss: 0.0715\n",
      "Epoch 9/300 - Train Loss: 0.0902, Val Loss: 0.0726\n",
      "Epoch 10/300 - Train Loss: 0.0908, Val Loss: 0.0694\n",
      "Epoch 11/300 - Train Loss: 0.0934, Val Loss: 0.0731\n",
      "Epoch 12/300 - Train Loss: 0.0897, Val Loss: 0.0738\n",
      "Epoch 13/300 - Train Loss: 0.0925, Val Loss: 0.0741\n",
      "Epoch 14/300 - Train Loss: 0.0927, Val Loss: 0.0721\n",
      "Epoch 15/300 - Train Loss: 0.0916, Val Loss: 0.0740\n",
      "Epoch 16/300 - Train Loss: 0.0932, Val Loss: 0.0743\n",
      "Epoch 17/300 - Train Loss: 0.0937, Val Loss: 0.0860\n",
      "Epoch 18/300 - Train Loss: 0.0906, Val Loss: 0.0748\n",
      "Epoch 19/300 - Train Loss: 0.0922, Val Loss: 0.0716\n",
      "Epoch 20/300 - Train Loss: 0.0911, Val Loss: 0.0878\n",
      "Epoch 21/300 - Train Loss: 0.0904, Val Loss: 0.0723\n",
      "Epoch 22/300 - Train Loss: 0.0900, Val Loss: 0.0669\n",
      "Epoch 23/300 - Train Loss: 0.0910, Val Loss: 0.0923\n",
      "Epoch 24/300 - Train Loss: 0.0922, Val Loss: 0.0713\n",
      "Epoch 25/300 - Train Loss: 0.0926, Val Loss: 0.0819\n",
      "Epoch 26/300 - Train Loss: 0.0930, Val Loss: 0.0798\n",
      "Epoch 27/300 - Train Loss: 0.0922, Val Loss: 0.0744\n",
      "Epoch 28/300 - Train Loss: 0.0942, Val Loss: 0.0758\n",
      "Epoch 29/300 - Train Loss: 0.0924, Val Loss: 0.0807\n",
      "Epoch 30/300 - Train Loss: 0.0943, Val Loss: 0.0722\n",
      "Epoch 31/300 - Train Loss: 0.0925, Val Loss: 0.0728\n",
      "Epoch 32/300 - Train Loss: 0.0893, Val Loss: 0.0798\n",
      "Epoch 33/300 - Train Loss: 0.0923, Val Loss: 0.0779\n",
      "Epoch 34/300 - Train Loss: 0.0926, Val Loss: 0.0688\n",
      "Epoch 35/300 - Train Loss: 0.0938, Val Loss: 0.0688\n",
      "Epoch 36/300 - Train Loss: 0.0923, Val Loss: 0.0689\n",
      "Epoch 37/300 - Train Loss: 0.0959, Val Loss: 0.0671\n",
      "Epoch 38/300 - Train Loss: 0.0920, Val Loss: 0.0718\n",
      "Epoch 39/300 - Train Loss: 0.0896, Val Loss: 0.0730\n",
      "Epoch 40/300 - Train Loss: 0.0931, Val Loss: 0.0698\n",
      "Epoch 41/300 - Train Loss: 0.0926, Val Loss: 0.0773\n",
      "Epoch 42/300 - Train Loss: 0.0914, Val Loss: 0.0735\n",
      "Epoch 43/300 - Train Loss: 0.0932, Val Loss: 0.0707\n",
      "Epoch 44/300 - Train Loss: 0.0934, Val Loss: 0.0744\n",
      "Epoch 45/300 - Train Loss: 0.0915, Val Loss: 0.0725\n",
      "Epoch 46/300 - Train Loss: 0.0908, Val Loss: 0.0751\n",
      "Epoch 47/300 - Train Loss: 0.0942, Val Loss: 0.0744\n",
      "Epoch 48/300 - Train Loss: 0.0937, Val Loss: 0.0761\n",
      "Epoch 49/300 - Train Loss: 0.0925, Val Loss: 0.0712\n",
      "Epoch 50/300 - Train Loss: 0.0947, Val Loss: 0.0890\n",
      "Epoch 51/300 - Train Loss: 0.0923, Val Loss: 0.0742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 22:54:47,514] Trial 6 finished with value: 0.9667097169888752 and parameters: {'F1': 8, 'F2': 16, 'D': 4, 'dropout': 0.19223815962822166, 'learning_rate': 0.00036545579910001716, 'batch_size': 32, 'weight_decay': 0.0018908988645410757}. Best is trial 1 with value: 0.9667219239125732.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300 - Train Loss: 0.0915, Val Loss: 0.0748\n",
      "Early stopping at epoch 52\n",
      "Macro F1 Score: 0.9667, Macro Precision: 0.9633, Macro Recall: 0.9704\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 8\n",
      "Training with F1=16, F2=16, D=2, dropout=0.324448297300447, LR=6.6602338779345e-05, BS=32, WD=1.3652228489997789e-05\n",
      "Epoch 1/300 - Train Loss: 0.4342, Val Loss: 0.2043\n",
      "Epoch 2/300 - Train Loss: 0.1919, Val Loss: 0.1283\n",
      "Epoch 3/300 - Train Loss: 0.1442, Val Loss: 0.1067\n",
      "Epoch 4/300 - Train Loss: 0.1229, Val Loss: 0.1002\n",
      "Epoch 5/300 - Train Loss: 0.1159, Val Loss: 0.0896\n",
      "Epoch 6/300 - Train Loss: 0.1088, Val Loss: 0.0905\n",
      "Epoch 7/300 - Train Loss: 0.1046, Val Loss: 0.0843\n",
      "Epoch 8/300 - Train Loss: 0.1039, Val Loss: 0.0847\n",
      "Epoch 9/300 - Train Loss: 0.1009, Val Loss: 0.0721\n",
      "Epoch 10/300 - Train Loss: 0.0986, Val Loss: 0.0784\n",
      "Epoch 11/300 - Train Loss: 0.0970, Val Loss: 0.0826\n",
      "Epoch 12/300 - Train Loss: 0.0980, Val Loss: 0.0821\n",
      "Epoch 13/300 - Train Loss: 0.0951, Val Loss: 0.0795\n",
      "Epoch 14/300 - Train Loss: 0.0948, Val Loss: 0.0735\n",
      "Epoch 15/300 - Train Loss: 0.0944, Val Loss: 0.0800\n",
      "Epoch 16/300 - Train Loss: 0.0941, Val Loss: 0.0759\n",
      "Epoch 17/300 - Train Loss: 0.0915, Val Loss: 0.0745\n",
      "Epoch 18/300 - Train Loss: 0.0920, Val Loss: 0.0765\n",
      "Epoch 19/300 - Train Loss: 0.0918, Val Loss: 0.0723\n",
      "Epoch 20/300 - Train Loss: 0.0905, Val Loss: 0.0760\n",
      "Epoch 21/300 - Train Loss: 0.0936, Val Loss: 0.0788\n",
      "Epoch 22/300 - Train Loss: 0.0902, Val Loss: 0.0692\n",
      "Epoch 23/300 - Train Loss: 0.0885, Val Loss: 0.0751\n",
      "Epoch 24/300 - Train Loss: 0.0887, Val Loss: 0.0712\n",
      "Epoch 25/300 - Train Loss: 0.0912, Val Loss: 0.0750\n",
      "Epoch 26/300 - Train Loss: 0.0882, Val Loss: 0.0765\n",
      "Epoch 27/300 - Train Loss: 0.0889, Val Loss: 0.0727\n",
      "Epoch 28/300 - Train Loss: 0.0880, Val Loss: 0.0729\n",
      "Epoch 29/300 - Train Loss: 0.0903, Val Loss: 0.0697\n",
      "Epoch 30/300 - Train Loss: 0.0913, Val Loss: 0.0718\n",
      "Epoch 31/300 - Train Loss: 0.0858, Val Loss: 0.0739\n",
      "Epoch 32/300 - Train Loss: 0.0881, Val Loss: 0.0730\n",
      "Epoch 33/300 - Train Loss: 0.0880, Val Loss: 0.0721\n",
      "Epoch 34/300 - Train Loss: 0.0862, Val Loss: 0.0718\n",
      "Epoch 35/300 - Train Loss: 0.0841, Val Loss: 0.0712\n",
      "Epoch 36/300 - Train Loss: 0.0862, Val Loss: 0.0725\n",
      "Epoch 37/300 - Train Loss: 0.0876, Val Loss: 0.0704\n",
      "Epoch 38/300 - Train Loss: 0.0878, Val Loss: 0.0747\n",
      "Epoch 39/300 - Train Loss: 0.0858, Val Loss: 0.0726\n",
      "Epoch 40/300 - Train Loss: 0.0870, Val Loss: 0.0800\n",
      "Epoch 41/300 - Train Loss: 0.0836, Val Loss: 0.0728\n",
      "Epoch 42/300 - Train Loss: 0.0868, Val Loss: 0.0707\n",
      "Epoch 43/300 - Train Loss: 0.0835, Val Loss: 0.0727\n",
      "Epoch 44/300 - Train Loss: 0.0850, Val Loss: 0.0677\n",
      "Epoch 45/300 - Train Loss: 0.0858, Val Loss: 0.0742\n",
      "Epoch 46/300 - Train Loss: 0.0821, Val Loss: 0.0758\n",
      "Epoch 47/300 - Train Loss: 0.0843, Val Loss: 0.0770\n",
      "Epoch 48/300 - Train Loss: 0.0854, Val Loss: 0.0715\n",
      "Epoch 49/300 - Train Loss: 0.0864, Val Loss: 0.0778\n",
      "Epoch 50/300 - Train Loss: 0.0840, Val Loss: 0.0741\n",
      "Epoch 51/300 - Train Loss: 0.0855, Val Loss: 0.0721\n",
      "Epoch 52/300 - Train Loss: 0.0849, Val Loss: 0.0754\n",
      "Epoch 53/300 - Train Loss: 0.0837, Val Loss: 0.0751\n",
      "Epoch 54/300 - Train Loss: 0.0852, Val Loss: 0.0717\n",
      "Epoch 55/300 - Train Loss: 0.0819, Val Loss: 0.0706\n",
      "Epoch 56/300 - Train Loss: 0.0823, Val Loss: 0.0695\n",
      "Epoch 57/300 - Train Loss: 0.0843, Val Loss: 0.0733\n",
      "Epoch 58/300 - Train Loss: 0.0835, Val Loss: 0.0737\n",
      "Epoch 59/300 - Train Loss: 0.0817, Val Loss: 0.0741\n",
      "Epoch 60/300 - Train Loss: 0.0834, Val Loss: 0.0726\n",
      "Epoch 61/300 - Train Loss: 0.0817, Val Loss: 0.0711\n",
      "Epoch 62/300 - Train Loss: 0.0804, Val Loss: 0.0700\n",
      "Epoch 63/300 - Train Loss: 0.0833, Val Loss: 0.0707\n",
      "Epoch 64/300 - Train Loss: 0.0825, Val Loss: 0.0731\n",
      "Epoch 65/300 - Train Loss: 0.0809, Val Loss: 0.0706\n",
      "Epoch 66/300 - Train Loss: 0.0810, Val Loss: 0.0726\n",
      "Epoch 67/300 - Train Loss: 0.0833, Val Loss: 0.0828\n",
      "Epoch 68/300 - Train Loss: 0.0807, Val Loss: 0.0718\n",
      "Epoch 69/300 - Train Loss: 0.0817, Val Loss: 0.0734\n",
      "Epoch 70/300 - Train Loss: 0.0793, Val Loss: 0.0713\n",
      "Epoch 71/300 - Train Loss: 0.0803, Val Loss: 0.0698\n",
      "Epoch 72/300 - Train Loss: 0.0787, Val Loss: 0.0701\n",
      "Epoch 73/300 - Train Loss: 0.0815, Val Loss: 0.0754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 22:57:22,126] Trial 7 finished with value: 0.9680494923625419 and parameters: {'F1': 16, 'F2': 16, 'D': 2, 'dropout': 0.324448297300447, 'learning_rate': 6.6602338779345e-05, 'batch_size': 32, 'weight_decay': 1.3652228489997789e-05}. Best is trial 7 with value: 0.9680494923625419.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300 - Train Loss: 0.0780, Val Loss: 0.0714\n",
      "Early stopping at epoch 74\n",
      "Macro F1 Score: 0.9680, Macro Precision: 0.9598, Macro Recall: 0.9770\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 9\n",
      "Training with F1=4, F2=16, D=2, dropout=0.5341620756047222, LR=0.0001240611954319157, BS=64, WD=1.3284498087486863e-05\n",
      "Epoch 1/300 - Train Loss: 0.5977, Val Loss: 0.3055\n",
      "Epoch 2/300 - Train Loss: 0.2726, Val Loss: 0.1831\n",
      "Epoch 3/300 - Train Loss: 0.1887, Val Loss: 0.1495\n",
      "Epoch 4/300 - Train Loss: 0.1652, Val Loss: 0.1311\n",
      "Epoch 5/300 - Train Loss: 0.1515, Val Loss: 0.1190\n",
      "Epoch 6/300 - Train Loss: 0.1480, Val Loss: 0.1146\n",
      "Epoch 7/300 - Train Loss: 0.1415, Val Loss: 0.1106\n",
      "Epoch 8/300 - Train Loss: 0.1350, Val Loss: 0.1102\n",
      "Epoch 9/300 - Train Loss: 0.1338, Val Loss: 0.1085\n",
      "Epoch 10/300 - Train Loss: 0.1324, Val Loss: 0.1050\n",
      "Epoch 11/300 - Train Loss: 0.1323, Val Loss: 0.1062\n",
      "Epoch 12/300 - Train Loss: 0.1313, Val Loss: 0.1063\n",
      "Epoch 13/300 - Train Loss: 0.1319, Val Loss: 0.1023\n",
      "Epoch 14/300 - Train Loss: 0.1302, Val Loss: 0.0985\n",
      "Epoch 15/300 - Train Loss: 0.1297, Val Loss: 0.1010\n",
      "Epoch 16/300 - Train Loss: 0.1264, Val Loss: 0.0996\n",
      "Epoch 17/300 - Train Loss: 0.1273, Val Loss: 0.0976\n",
      "Epoch 18/300 - Train Loss: 0.1250, Val Loss: 0.0988\n",
      "Epoch 19/300 - Train Loss: 0.1258, Val Loss: 0.0986\n",
      "Epoch 20/300 - Train Loss: 0.1272, Val Loss: 0.0972\n",
      "Epoch 21/300 - Train Loss: 0.1240, Val Loss: 0.0963\n",
      "Epoch 22/300 - Train Loss: 0.1264, Val Loss: 0.0973\n",
      "Epoch 23/300 - Train Loss: 0.1224, Val Loss: 0.0942\n",
      "Epoch 24/300 - Train Loss: 0.1213, Val Loss: 0.0963\n",
      "Epoch 25/300 - Train Loss: 0.1239, Val Loss: 0.0946\n",
      "Epoch 26/300 - Train Loss: 0.1228, Val Loss: 0.0986\n",
      "Epoch 27/300 - Train Loss: 0.1218, Val Loss: 0.0928\n",
      "Epoch 28/300 - Train Loss: 0.1199, Val Loss: 0.0932\n",
      "Epoch 29/300 - Train Loss: 0.1194, Val Loss: 0.0953\n",
      "Epoch 30/300 - Train Loss: 0.1195, Val Loss: 0.0965\n",
      "Epoch 31/300 - Train Loss: 0.1185, Val Loss: 0.0931\n",
      "Epoch 32/300 - Train Loss: 0.1214, Val Loss: 0.0923\n",
      "Epoch 33/300 - Train Loss: 0.1208, Val Loss: 0.0919\n",
      "Epoch 34/300 - Train Loss: 0.1176, Val Loss: 0.0919\n",
      "Epoch 35/300 - Train Loss: 0.1199, Val Loss: 0.0920\n",
      "Epoch 36/300 - Train Loss: 0.1175, Val Loss: 0.0944\n",
      "Epoch 37/300 - Train Loss: 0.1189, Val Loss: 0.0939\n",
      "Epoch 38/300 - Train Loss: 0.1181, Val Loss: 0.0916\n",
      "Epoch 39/300 - Train Loss: 0.1181, Val Loss: 0.0892\n",
      "Epoch 40/300 - Train Loss: 0.1176, Val Loss: 0.0916\n",
      "Epoch 41/300 - Train Loss: 0.1162, Val Loss: 0.0899\n",
      "Epoch 42/300 - Train Loss: 0.1155, Val Loss: 0.0899\n",
      "Epoch 43/300 - Train Loss: 0.1174, Val Loss: 0.0918\n",
      "Epoch 44/300 - Train Loss: 0.1169, Val Loss: 0.0923\n",
      "Epoch 45/300 - Train Loss: 0.1161, Val Loss: 0.0942\n",
      "Epoch 46/300 - Train Loss: 0.1147, Val Loss: 0.0879\n",
      "Epoch 47/300 - Train Loss: 0.1152, Val Loss: 0.0950\n",
      "Epoch 48/300 - Train Loss: 0.1198, Val Loss: 0.0921\n",
      "Epoch 49/300 - Train Loss: 0.1164, Val Loss: 0.0910\n",
      "Epoch 50/300 - Train Loss: 0.1160, Val Loss: 0.0914\n",
      "Epoch 51/300 - Train Loss: 0.1163, Val Loss: 0.0909\n",
      "Epoch 52/300 - Train Loss: 0.1159, Val Loss: 0.0884\n",
      "Epoch 53/300 - Train Loss: 0.1148, Val Loss: 0.0918\n",
      "Epoch 54/300 - Train Loss: 0.1153, Val Loss: 0.0891\n",
      "Epoch 55/300 - Train Loss: 0.1159, Val Loss: 0.0934\n",
      "Epoch 56/300 - Train Loss: 0.1148, Val Loss: 0.0911\n",
      "Epoch 57/300 - Train Loss: 0.1148, Val Loss: 0.0861\n",
      "Epoch 58/300 - Train Loss: 0.1149, Val Loss: 0.0926\n",
      "Epoch 59/300 - Train Loss: 0.1139, Val Loss: 0.0905\n",
      "Epoch 60/300 - Train Loss: 0.1134, Val Loss: 0.0954\n",
      "Epoch 61/300 - Train Loss: 0.1109, Val Loss: 0.0872\n",
      "Epoch 62/300 - Train Loss: 0.1114, Val Loss: 0.0871\n",
      "Epoch 63/300 - Train Loss: 0.1116, Val Loss: 0.0892\n",
      "Epoch 64/300 - Train Loss: 0.1104, Val Loss: 0.0855\n",
      "Epoch 65/300 - Train Loss: 0.1081, Val Loss: 0.0975\n",
      "Epoch 66/300 - Train Loss: 0.1064, Val Loss: 0.0906\n",
      "Epoch 67/300 - Train Loss: 0.1050, Val Loss: 0.0984\n",
      "Epoch 68/300 - Train Loss: 0.1098, Val Loss: 0.0948\n",
      "Epoch 69/300 - Train Loss: 0.1063, Val Loss: 0.0917\n",
      "Epoch 70/300 - Train Loss: 0.1032, Val Loss: 0.0866\n",
      "Epoch 71/300 - Train Loss: 0.1038, Val Loss: 0.0899\n",
      "Epoch 72/300 - Train Loss: 0.1026, Val Loss: 0.0884\n",
      "Epoch 73/300 - Train Loss: 0.1053, Val Loss: 0.0869\n",
      "Epoch 74/300 - Train Loss: 0.1038, Val Loss: 0.0866\n",
      "Epoch 75/300 - Train Loss: 0.1047, Val Loss: 0.0878\n",
      "Epoch 76/300 - Train Loss: 0.1031, Val Loss: 0.0848\n",
      "Epoch 77/300 - Train Loss: 0.1021, Val Loss: 0.0901\n",
      "Epoch 78/300 - Train Loss: 0.1019, Val Loss: 0.0831\n",
      "Epoch 79/300 - Train Loss: 0.1040, Val Loss: 0.0941\n",
      "Epoch 80/300 - Train Loss: 0.1015, Val Loss: 0.0850\n",
      "Epoch 81/300 - Train Loss: 0.1012, Val Loss: 0.0890\n",
      "Epoch 82/300 - Train Loss: 0.1054, Val Loss: 0.0859\n",
      "Epoch 83/300 - Train Loss: 0.1028, Val Loss: 0.0918\n",
      "Epoch 84/300 - Train Loss: 0.1052, Val Loss: 0.0900\n",
      "Epoch 85/300 - Train Loss: 0.1027, Val Loss: 0.0837\n",
      "Epoch 86/300 - Train Loss: 0.1033, Val Loss: 0.0873\n",
      "Epoch 87/300 - Train Loss: 0.1021, Val Loss: 0.0857\n",
      "Epoch 88/300 - Train Loss: 0.1033, Val Loss: 0.0845\n",
      "Epoch 89/300 - Train Loss: 0.1012, Val Loss: 0.0894\n",
      "Epoch 90/300 - Train Loss: 0.1034, Val Loss: 0.0887\n",
      "Epoch 91/300 - Train Loss: 0.1019, Val Loss: 0.0840\n",
      "Epoch 92/300 - Train Loss: 0.1001, Val Loss: 0.0881\n",
      "Epoch 93/300 - Train Loss: 0.0993, Val Loss: 0.0847\n",
      "Epoch 94/300 - Train Loss: 0.0990, Val Loss: 0.0869\n",
      "Epoch 95/300 - Train Loss: 0.1020, Val Loss: 0.0864\n",
      "Epoch 96/300 - Train Loss: 0.1015, Val Loss: 0.0882\n",
      "Epoch 97/300 - Train Loss: 0.0995, Val Loss: 0.0918\n",
      "Epoch 98/300 - Train Loss: 0.1031, Val Loss: 0.0864\n",
      "Epoch 99/300 - Train Loss: 0.0999, Val Loss: 0.0848\n",
      "Epoch 100/300 - Train Loss: 0.1018, Val Loss: 0.0879\n",
      "Epoch 101/300 - Train Loss: 0.1018, Val Loss: 0.0851\n",
      "Epoch 102/300 - Train Loss: 0.1026, Val Loss: 0.0858\n",
      "Epoch 103/300 - Train Loss: 0.1001, Val Loss: 0.0882\n",
      "Epoch 104/300 - Train Loss: 0.0984, Val Loss: 0.0876\n",
      "Epoch 105/300 - Train Loss: 0.0998, Val Loss: 0.0915\n",
      "Epoch 106/300 - Train Loss: 0.0994, Val Loss: 0.0973\n",
      "Epoch 107/300 - Train Loss: 0.0997, Val Loss: 0.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:01:55,347] Trial 8 finished with value: 0.9465543487370378 and parameters: {'F1': 4, 'F2': 16, 'D': 2, 'dropout': 0.5341620756047222, 'learning_rate': 0.0001240611954319157, 'batch_size': 64, 'weight_decay': 1.3284498087486863e-05}. Best is trial 7 with value: 0.9680494923625419.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/300 - Train Loss: 0.1028, Val Loss: 0.0858\n",
      "Early stopping at epoch 108\n",
      "Macro F1 Score: 0.9466, Macro Precision: 0.9231, Macro Recall: 0.9764\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.80      0.98      0.88        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.97      1443\n",
      "   macro avg       0.92      0.98      0.95      1443\n",
      "weighted avg       0.98      0.97      0.97      1443\n",
      "\n",
      "\n",
      "Trial 10\n",
      "Training with F1=4, F2=16, D=2, dropout=0.12473993665157797, LR=0.0008288975753081865, BS=256, WD=0.0001262443753987097\n",
      "Epoch 1/300 - Train Loss: 0.3390, Val Loss: 0.1650\n",
      "Epoch 2/300 - Train Loss: 0.1323, Val Loss: 0.0965\n",
      "Epoch 3/300 - Train Loss: 0.1114, Val Loss: 0.0884\n",
      "Epoch 4/300 - Train Loss: 0.1040, Val Loss: 0.0830\n",
      "Epoch 5/300 - Train Loss: 0.0961, Val Loss: 0.0842\n",
      "Epoch 6/300 - Train Loss: 0.0953, Val Loss: 0.0807\n",
      "Epoch 7/300 - Train Loss: 0.0901, Val Loss: 0.0819\n",
      "Epoch 8/300 - Train Loss: 0.0879, Val Loss: 0.0781\n",
      "Epoch 9/300 - Train Loss: 0.0873, Val Loss: 0.0786\n",
      "Epoch 10/300 - Train Loss: 0.0882, Val Loss: 0.0765\n",
      "Epoch 11/300 - Train Loss: 0.0852, Val Loss: 0.0792\n",
      "Epoch 12/300 - Train Loss: 0.0839, Val Loss: 0.0745\n",
      "Epoch 13/300 - Train Loss: 0.0874, Val Loss: 0.0783\n",
      "Epoch 14/300 - Train Loss: 0.0833, Val Loss: 0.0747\n",
      "Epoch 15/300 - Train Loss: 0.0833, Val Loss: 0.0756\n",
      "Epoch 16/300 - Train Loss: 0.0821, Val Loss: 0.0773\n",
      "Epoch 17/300 - Train Loss: 0.0824, Val Loss: 0.0771\n",
      "Epoch 18/300 - Train Loss: 0.0810, Val Loss: 0.0754\n",
      "Epoch 19/300 - Train Loss: 0.0804, Val Loss: 0.0793\n",
      "Epoch 20/300 - Train Loss: 0.0798, Val Loss: 0.0721\n",
      "Epoch 21/300 - Train Loss: 0.0799, Val Loss: 0.0732\n",
      "Epoch 22/300 - Train Loss: 0.0809, Val Loss: 0.0791\n",
      "Epoch 23/300 - Train Loss: 0.0789, Val Loss: 0.0737\n",
      "Epoch 24/300 - Train Loss: 0.0791, Val Loss: 0.0790\n",
      "Epoch 25/300 - Train Loss: 0.0783, Val Loss: 0.0763\n",
      "Epoch 26/300 - Train Loss: 0.0778, Val Loss: 0.0726\n",
      "Epoch 27/300 - Train Loss: 0.0781, Val Loss: 0.0823\n",
      "Epoch 28/300 - Train Loss: 0.0770, Val Loss: 0.0766\n",
      "Epoch 29/300 - Train Loss: 0.0757, Val Loss: 0.0756\n",
      "Epoch 30/300 - Train Loss: 0.0763, Val Loss: 0.0732\n",
      "Epoch 31/300 - Train Loss: 0.0745, Val Loss: 0.0736\n",
      "Epoch 32/300 - Train Loss: 0.0761, Val Loss: 0.0765\n",
      "Epoch 33/300 - Train Loss: 0.0754, Val Loss: 0.0804\n",
      "Epoch 34/300 - Train Loss: 0.0751, Val Loss: 0.0725\n",
      "Epoch 35/300 - Train Loss: 0.0746, Val Loss: 0.0730\n",
      "Epoch 36/300 - Train Loss: 0.0745, Val Loss: 0.0737\n",
      "Epoch 37/300 - Train Loss: 0.0753, Val Loss: 0.0706\n",
      "Epoch 38/300 - Train Loss: 0.0728, Val Loss: 0.0720\n",
      "Epoch 39/300 - Train Loss: 0.0737, Val Loss: 0.0713\n",
      "Epoch 40/300 - Train Loss: 0.0752, Val Loss: 0.0762\n",
      "Epoch 41/300 - Train Loss: 0.0729, Val Loss: 0.0750\n",
      "Epoch 42/300 - Train Loss: 0.0728, Val Loss: 0.0702\n",
      "Epoch 43/300 - Train Loss: 0.0725, Val Loss: 0.0732\n",
      "Epoch 44/300 - Train Loss: 0.0726, Val Loss: 0.0804\n",
      "Epoch 45/300 - Train Loss: 0.0752, Val Loss: 0.0749\n",
      "Epoch 46/300 - Train Loss: 0.0721, Val Loss: 0.0738\n",
      "Epoch 47/300 - Train Loss: 0.0724, Val Loss: 0.0760\n",
      "Epoch 48/300 - Train Loss: 0.0699, Val Loss: 0.0788\n",
      "Epoch 49/300 - Train Loss: 0.0720, Val Loss: 0.0712\n",
      "Epoch 50/300 - Train Loss: 0.0701, Val Loss: 0.0740\n",
      "Epoch 51/300 - Train Loss: 0.0702, Val Loss: 0.0725\n",
      "Epoch 52/300 - Train Loss: 0.0735, Val Loss: 0.0754\n",
      "Epoch 53/300 - Train Loss: 0.0703, Val Loss: 0.0775\n",
      "Epoch 54/300 - Train Loss: 0.0716, Val Loss: 0.0730\n",
      "Epoch 55/300 - Train Loss: 0.0705, Val Loss: 0.0792\n",
      "Epoch 56/300 - Train Loss: 0.0706, Val Loss: 0.0749\n",
      "Epoch 57/300 - Train Loss: 0.0711, Val Loss: 0.0732\n",
      "Epoch 58/300 - Train Loss: 0.0708, Val Loss: 0.0777\n",
      "Epoch 59/300 - Train Loss: 0.0699, Val Loss: 0.0735\n",
      "Epoch 60/300 - Train Loss: 0.0694, Val Loss: 0.0737\n",
      "Epoch 61/300 - Train Loss: 0.0691, Val Loss: 0.0747\n",
      "Epoch 62/300 - Train Loss: 0.0683, Val Loss: 0.0806\n",
      "Epoch 63/300 - Train Loss: 0.0679, Val Loss: 0.0737\n",
      "Epoch 64/300 - Train Loss: 0.0678, Val Loss: 0.0780\n",
      "Epoch 65/300 - Train Loss: 0.0689, Val Loss: 0.0794\n",
      "Epoch 66/300 - Train Loss: 0.0677, Val Loss: 0.0726\n",
      "Epoch 67/300 - Train Loss: 0.0681, Val Loss: 0.0743\n",
      "Epoch 68/300 - Train Loss: 0.0668, Val Loss: 0.0714\n",
      "Epoch 69/300 - Train Loss: 0.0671, Val Loss: 0.0772\n",
      "Epoch 70/300 - Train Loss: 0.0676, Val Loss: 0.0783\n",
      "Epoch 71/300 - Train Loss: 0.0667, Val Loss: 0.0761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:04:38,382] Trial 9 finished with value: 0.9577214132493137 and parameters: {'F1': 4, 'F2': 16, 'D': 2, 'dropout': 0.12473993665157797, 'learning_rate': 0.0008288975753081865, 'batch_size': 256, 'weight_decay': 0.0001262443753987097}. Best is trial 7 with value: 0.9680494923625419.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300 - Train Loss: 0.0661, Val Loss: 0.0750\n",
      "Early stopping at epoch 72\n",
      "Macro F1 Score: 0.9577, Macro Precision: 0.9609, Macro Recall: 0.9547\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.90      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.95      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 11\n",
      "Training with F1=16, F2=32, D=2, dropout=0.33235498036200944, LR=8.251194245886343e-05, BS=32, WD=1.1478780420286013e-05\n",
      "Epoch 1/300 - Train Loss: 0.3762, Val Loss: 0.1874\n",
      "Epoch 2/300 - Train Loss: 0.1966, Val Loss: 0.1332\n",
      "Epoch 3/300 - Train Loss: 0.1443, Val Loss: 0.1054\n",
      "Epoch 4/300 - Train Loss: 0.1225, Val Loss: 0.0898\n",
      "Epoch 5/300 - Train Loss: 0.1126, Val Loss: 0.0824\n",
      "Epoch 6/300 - Train Loss: 0.1082, Val Loss: 0.0849\n",
      "Epoch 7/300 - Train Loss: 0.1032, Val Loss: 0.0842\n",
      "Epoch 8/300 - Train Loss: 0.1004, Val Loss: 0.0794\n",
      "Epoch 9/300 - Train Loss: 0.0982, Val Loss: 0.0752\n",
      "Epoch 10/300 - Train Loss: 0.0953, Val Loss: 0.0814\n",
      "Epoch 11/300 - Train Loss: 0.0943, Val Loss: 0.0772\n",
      "Epoch 12/300 - Train Loss: 0.0925, Val Loss: 0.0743\n",
      "Epoch 13/300 - Train Loss: 0.0921, Val Loss: 0.0814\n",
      "Epoch 14/300 - Train Loss: 0.0894, Val Loss: 0.0784\n",
      "Epoch 15/300 - Train Loss: 0.0911, Val Loss: 0.0726\n",
      "Epoch 16/300 - Train Loss: 0.0911, Val Loss: 0.0781\n",
      "Epoch 17/300 - Train Loss: 0.0901, Val Loss: 0.0734\n",
      "Epoch 18/300 - Train Loss: 0.0891, Val Loss: 0.0776\n",
      "Epoch 19/300 - Train Loss: 0.0882, Val Loss: 0.0799\n",
      "Epoch 20/300 - Train Loss: 0.0880, Val Loss: 0.0768\n",
      "Epoch 21/300 - Train Loss: 0.0853, Val Loss: 0.0737\n",
      "Epoch 22/300 - Train Loss: 0.0840, Val Loss: 0.0774\n",
      "Epoch 23/300 - Train Loss: 0.0870, Val Loss: 0.0795\n",
      "Epoch 24/300 - Train Loss: 0.0863, Val Loss: 0.0735\n",
      "Epoch 25/300 - Train Loss: 0.0838, Val Loss: 0.0789\n",
      "Epoch 26/300 - Train Loss: 0.0830, Val Loss: 0.0763\n",
      "Epoch 27/300 - Train Loss: 0.0846, Val Loss: 0.0740\n",
      "Epoch 28/300 - Train Loss: 0.0835, Val Loss: 0.0749\n",
      "Epoch 29/300 - Train Loss: 0.0832, Val Loss: 0.0722\n",
      "Epoch 30/300 - Train Loss: 0.0814, Val Loss: 0.0776\n",
      "Epoch 31/300 - Train Loss: 0.0825, Val Loss: 0.0784\n",
      "Epoch 32/300 - Train Loss: 0.0828, Val Loss: 0.0775\n",
      "Epoch 33/300 - Train Loss: 0.0843, Val Loss: 0.0728\n",
      "Epoch 34/300 - Train Loss: 0.0820, Val Loss: 0.0782\n",
      "Epoch 35/300 - Train Loss: 0.0815, Val Loss: 0.0729\n",
      "Epoch 36/300 - Train Loss: 0.0829, Val Loss: 0.0796\n",
      "Epoch 37/300 - Train Loss: 0.0831, Val Loss: 0.0756\n",
      "Epoch 38/300 - Train Loss: 0.0827, Val Loss: 0.0773\n",
      "Epoch 39/300 - Train Loss: 0.0792, Val Loss: 0.0764\n",
      "Epoch 40/300 - Train Loss: 0.0811, Val Loss: 0.0765\n",
      "Epoch 41/300 - Train Loss: 0.0793, Val Loss: 0.0772\n",
      "Epoch 42/300 - Train Loss: 0.0812, Val Loss: 0.0787\n",
      "Epoch 43/300 - Train Loss: 0.0805, Val Loss: 0.0744\n",
      "Epoch 44/300 - Train Loss: 0.0811, Val Loss: 0.0787\n",
      "Epoch 45/300 - Train Loss: 0.0787, Val Loss: 0.0733\n",
      "Epoch 46/300 - Train Loss: 0.0839, Val Loss: 0.0774\n",
      "Epoch 47/300 - Train Loss: 0.0783, Val Loss: 0.0724\n",
      "Epoch 48/300 - Train Loss: 0.0787, Val Loss: 0.0742\n",
      "Epoch 49/300 - Train Loss: 0.0772, Val Loss: 0.0780\n",
      "Epoch 50/300 - Train Loss: 0.0754, Val Loss: 0.0784\n",
      "Epoch 51/300 - Train Loss: 0.0778, Val Loss: 0.0789\n",
      "Epoch 52/300 - Train Loss: 0.0776, Val Loss: 0.0797\n",
      "Epoch 53/300 - Train Loss: 0.0776, Val Loss: 0.0734\n",
      "Epoch 54/300 - Train Loss: 0.0772, Val Loss: 0.0708\n",
      "Epoch 55/300 - Train Loss: 0.0772, Val Loss: 0.0749\n",
      "Epoch 56/300 - Train Loss: 0.0790, Val Loss: 0.0741\n",
      "Epoch 57/300 - Train Loss: 0.0769, Val Loss: 0.0782\n",
      "Epoch 58/300 - Train Loss: 0.0764, Val Loss: 0.0739\n",
      "Epoch 59/300 - Train Loss: 0.0756, Val Loss: 0.0770\n",
      "Epoch 60/300 - Train Loss: 0.0756, Val Loss: 0.0750\n",
      "Epoch 61/300 - Train Loss: 0.0766, Val Loss: 0.0735\n",
      "Epoch 62/300 - Train Loss: 0.0745, Val Loss: 0.0734\n",
      "Epoch 63/300 - Train Loss: 0.0738, Val Loss: 0.0720\n",
      "Epoch 64/300 - Train Loss: 0.0741, Val Loss: 0.0751\n",
      "Epoch 65/300 - Train Loss: 0.0731, Val Loss: 0.0734\n",
      "Epoch 66/300 - Train Loss: 0.0739, Val Loss: 0.0744\n",
      "Epoch 67/300 - Train Loss: 0.0771, Val Loss: 0.0734\n",
      "Epoch 68/300 - Train Loss: 0.0739, Val Loss: 0.0761\n",
      "Epoch 69/300 - Train Loss: 0.0754, Val Loss: 0.0785\n",
      "Epoch 70/300 - Train Loss: 0.0734, Val Loss: 0.0756\n",
      "Epoch 71/300 - Train Loss: 0.0719, Val Loss: 0.0762\n",
      "Epoch 72/300 - Train Loss: 0.0762, Val Loss: 0.0713\n",
      "Epoch 73/300 - Train Loss: 0.0748, Val Loss: 0.0763\n",
      "Epoch 74/300 - Train Loss: 0.0732, Val Loss: 0.0717\n",
      "Epoch 75/300 - Train Loss: 0.0752, Val Loss: 0.0751\n",
      "Epoch 76/300 - Train Loss: 0.0720, Val Loss: 0.0791\n",
      "Epoch 77/300 - Train Loss: 0.0729, Val Loss: 0.0723\n",
      "Epoch 78/300 - Train Loss: 0.0774, Val Loss: 0.0804\n",
      "Epoch 79/300 - Train Loss: 0.0717, Val Loss: 0.0727\n",
      "Epoch 80/300 - Train Loss: 0.0719, Val Loss: 0.0748\n",
      "Epoch 81/300 - Train Loss: 0.0700, Val Loss: 0.0738\n",
      "Epoch 82/300 - Train Loss: 0.0743, Val Loss: 0.0775\n",
      "Epoch 83/300 - Train Loss: 0.0722, Val Loss: 0.0766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:07:37,032] Trial 10 finished with value: 0.9693616971084471 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.33235498036200944, 'learning_rate': 8.251194245886343e-05, 'batch_size': 32, 'weight_decay': 1.1478780420286013e-05}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300 - Train Loss: 0.0718, Val Loss: 0.0767\n",
      "Early stopping at epoch 84\n",
      "Macro F1 Score: 0.9694, Macro Precision: 0.9571, Macro Recall: 0.9830\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 12\n",
      "Training with F1=16, F2=32, D=2, dropout=0.3328417208451954, LR=7.792226533137181e-05, BS=32, WD=1.0853034287879244e-05\n",
      "Epoch 1/300 - Train Loss: 0.3538, Val Loss: 0.1518\n",
      "Epoch 2/300 - Train Loss: 0.1496, Val Loss: 0.1087\n",
      "Epoch 3/300 - Train Loss: 0.1236, Val Loss: 0.0914\n",
      "Epoch 4/300 - Train Loss: 0.1114, Val Loss: 0.0893\n",
      "Epoch 5/300 - Train Loss: 0.1080, Val Loss: 0.0953\n",
      "Epoch 6/300 - Train Loss: 0.1047, Val Loss: 0.0867\n",
      "Epoch 7/300 - Train Loss: 0.1009, Val Loss: 0.0981\n",
      "Epoch 8/300 - Train Loss: 0.0999, Val Loss: 0.0766\n",
      "Epoch 9/300 - Train Loss: 0.0967, Val Loss: 0.0754\n",
      "Epoch 10/300 - Train Loss: 0.0948, Val Loss: 0.0818\n",
      "Epoch 11/300 - Train Loss: 0.0946, Val Loss: 0.0842\n",
      "Epoch 12/300 - Train Loss: 0.0968, Val Loss: 0.0759\n",
      "Epoch 13/300 - Train Loss: 0.0918, Val Loss: 0.0726\n",
      "Epoch 14/300 - Train Loss: 0.0924, Val Loss: 0.0779\n",
      "Epoch 15/300 - Train Loss: 0.0931, Val Loss: 0.0775\n",
      "Epoch 16/300 - Train Loss: 0.0919, Val Loss: 0.0798\n",
      "Epoch 17/300 - Train Loss: 0.0906, Val Loss: 0.0767\n",
      "Epoch 18/300 - Train Loss: 0.0897, Val Loss: 0.0751\n",
      "Epoch 19/300 - Train Loss: 0.0862, Val Loss: 0.0742\n",
      "Epoch 20/300 - Train Loss: 0.0905, Val Loss: 0.0777\n",
      "Epoch 21/300 - Train Loss: 0.0895, Val Loss: 0.0780\n",
      "Epoch 22/300 - Train Loss: 0.0881, Val Loss: 0.0803\n",
      "Epoch 23/300 - Train Loss: 0.0842, Val Loss: 0.0780\n",
      "Epoch 24/300 - Train Loss: 0.0892, Val Loss: 0.0739\n",
      "Epoch 25/300 - Train Loss: 0.0851, Val Loss: 0.0725\n",
      "Epoch 26/300 - Train Loss: 0.0861, Val Loss: 0.0839\n",
      "Epoch 27/300 - Train Loss: 0.0863, Val Loss: 0.0877\n",
      "Epoch 28/300 - Train Loss: 0.0864, Val Loss: 0.0798\n",
      "Epoch 29/300 - Train Loss: 0.0856, Val Loss: 0.0799\n",
      "Epoch 30/300 - Train Loss: 0.0834, Val Loss: 0.0745\n",
      "Epoch 31/300 - Train Loss: 0.0835, Val Loss: 0.0777\n",
      "Epoch 32/300 - Train Loss: 0.0831, Val Loss: 0.0730\n",
      "Epoch 33/300 - Train Loss: 0.0853, Val Loss: 0.0718\n",
      "Epoch 34/300 - Train Loss: 0.0831, Val Loss: 0.0728\n",
      "Epoch 35/300 - Train Loss: 0.0834, Val Loss: 0.0864\n",
      "Epoch 36/300 - Train Loss: 0.0830, Val Loss: 0.0734\n",
      "Epoch 37/300 - Train Loss: 0.0823, Val Loss: 0.0763\n",
      "Epoch 38/300 - Train Loss: 0.0825, Val Loss: 0.0753\n",
      "Epoch 39/300 - Train Loss: 0.0828, Val Loss: 0.0801\n",
      "Epoch 40/300 - Train Loss: 0.0808, Val Loss: 0.0717\n",
      "Epoch 41/300 - Train Loss: 0.0810, Val Loss: 0.0786\n",
      "Epoch 42/300 - Train Loss: 0.0819, Val Loss: 0.0747\n",
      "Epoch 43/300 - Train Loss: 0.0786, Val Loss: 0.0742\n",
      "Epoch 44/300 - Train Loss: 0.0808, Val Loss: 0.0788\n",
      "Epoch 45/300 - Train Loss: 0.0825, Val Loss: 0.0737\n",
      "Epoch 46/300 - Train Loss: 0.0795, Val Loss: 0.0783\n",
      "Epoch 47/300 - Train Loss: 0.0832, Val Loss: 0.0721\n",
      "Epoch 48/300 - Train Loss: 0.0800, Val Loss: 0.0757\n",
      "Epoch 49/300 - Train Loss: 0.0777, Val Loss: 0.0741\n",
      "Epoch 50/300 - Train Loss: 0.0811, Val Loss: 0.0782\n",
      "Epoch 51/300 - Train Loss: 0.0782, Val Loss: 0.0777\n",
      "Epoch 52/300 - Train Loss: 0.0773, Val Loss: 0.0724\n",
      "Epoch 53/300 - Train Loss: 0.0781, Val Loss: 0.0776\n",
      "Epoch 54/300 - Train Loss: 0.0779, Val Loss: 0.0738\n",
      "Epoch 55/300 - Train Loss: 0.0761, Val Loss: 0.0740\n",
      "Epoch 56/300 - Train Loss: 0.0768, Val Loss: 0.0699\n",
      "Epoch 57/300 - Train Loss: 0.0785, Val Loss: 0.0754\n",
      "Epoch 58/300 - Train Loss: 0.0788, Val Loss: 0.0724\n",
      "Epoch 59/300 - Train Loss: 0.0768, Val Loss: 0.0728\n",
      "Epoch 60/300 - Train Loss: 0.0762, Val Loss: 0.0763\n",
      "Epoch 61/300 - Train Loss: 0.0787, Val Loss: 0.0739\n",
      "Epoch 62/300 - Train Loss: 0.0783, Val Loss: 0.0745\n",
      "Epoch 63/300 - Train Loss: 0.0783, Val Loss: 0.0788\n",
      "Epoch 64/300 - Train Loss: 0.0765, Val Loss: 0.0714\n",
      "Epoch 65/300 - Train Loss: 0.0772, Val Loss: 0.0743\n",
      "Epoch 66/300 - Train Loss: 0.0786, Val Loss: 0.0738\n",
      "Epoch 67/300 - Train Loss: 0.0748, Val Loss: 0.0715\n",
      "Epoch 68/300 - Train Loss: 0.0768, Val Loss: 0.0768\n",
      "Epoch 69/300 - Train Loss: 0.0761, Val Loss: 0.0728\n",
      "Epoch 70/300 - Train Loss: 0.0754, Val Loss: 0.0745\n",
      "Epoch 71/300 - Train Loss: 0.0739, Val Loss: 0.0712\n",
      "Epoch 72/300 - Train Loss: 0.0751, Val Loss: 0.0739\n",
      "Epoch 73/300 - Train Loss: 0.0767, Val Loss: 0.0718\n",
      "Epoch 74/300 - Train Loss: 0.0740, Val Loss: 0.0769\n",
      "Epoch 75/300 - Train Loss: 0.0753, Val Loss: 0.0757\n",
      "Epoch 76/300 - Train Loss: 0.0750, Val Loss: 0.0747\n",
      "Epoch 77/300 - Train Loss: 0.0740, Val Loss: 0.0734\n",
      "Epoch 78/300 - Train Loss: 0.0741, Val Loss: 0.0744\n",
      "Epoch 79/300 - Train Loss: 0.0730, Val Loss: 0.0696\n",
      "Epoch 80/300 - Train Loss: 0.0742, Val Loss: 0.0719\n",
      "Epoch 81/300 - Train Loss: 0.0743, Val Loss: 0.0748\n",
      "Epoch 82/300 - Train Loss: 0.0759, Val Loss: 0.0691\n",
      "Epoch 83/300 - Train Loss: 0.0726, Val Loss: 0.0711\n",
      "Epoch 84/300 - Train Loss: 0.0730, Val Loss: 0.0765\n",
      "Epoch 85/300 - Train Loss: 0.0721, Val Loss: 0.0721\n",
      "Epoch 86/300 - Train Loss: 0.0734, Val Loss: 0.0747\n",
      "Epoch 87/300 - Train Loss: 0.0759, Val Loss: 0.0789\n",
      "Epoch 88/300 - Train Loss: 0.0733, Val Loss: 0.0773\n",
      "Epoch 89/300 - Train Loss: 0.0711, Val Loss: 0.0777\n",
      "Epoch 90/300 - Train Loss: 0.0721, Val Loss: 0.0704\n",
      "Epoch 91/300 - Train Loss: 0.0696, Val Loss: 0.0741\n",
      "Epoch 92/300 - Train Loss: 0.0733, Val Loss: 0.0783\n",
      "Epoch 93/300 - Train Loss: 0.0717, Val Loss: 0.0704\n",
      "Epoch 94/300 - Train Loss: 0.0712, Val Loss: 0.0763\n",
      "Epoch 95/300 - Train Loss: 0.0722, Val Loss: 0.0750\n",
      "Epoch 96/300 - Train Loss: 0.0722, Val Loss: 0.0733\n",
      "Epoch 97/300 - Train Loss: 0.0698, Val Loss: 0.0729\n",
      "Epoch 98/300 - Train Loss: 0.0694, Val Loss: 0.0763\n",
      "Epoch 99/300 - Train Loss: 0.0722, Val Loss: 0.0721\n",
      "Epoch 100/300 - Train Loss: 0.0712, Val Loss: 0.0727\n",
      "Epoch 101/300 - Train Loss: 0.0685, Val Loss: 0.0739\n",
      "Epoch 102/300 - Train Loss: 0.0698, Val Loss: 0.0731\n",
      "Epoch 103/300 - Train Loss: 0.0682, Val Loss: 0.0760\n",
      "Epoch 104/300 - Train Loss: 0.0700, Val Loss: 0.0738\n",
      "Epoch 105/300 - Train Loss: 0.0715, Val Loss: 0.0737\n",
      "Epoch 106/300 - Train Loss: 0.0699, Val Loss: 0.0757\n",
      "Epoch 107/300 - Train Loss: 0.0697, Val Loss: 0.0719\n",
      "Epoch 108/300 - Train Loss: 0.0690, Val Loss: 0.0744\n",
      "Epoch 109/300 - Train Loss: 0.0676, Val Loss: 0.0729\n",
      "Epoch 110/300 - Train Loss: 0.0686, Val Loss: 0.0772\n",
      "Epoch 111/300 - Train Loss: 0.0673, Val Loss: 0.0813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:11:36,523] Trial 11 finished with value: 0.9658693916631039 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.3328417208451954, 'learning_rate': 7.792226533137181e-05, 'batch_size': 32, 'weight_decay': 1.0853034287879244e-05}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/300 - Train Loss: 0.0673, Val Loss: 0.0707\n",
      "Early stopping at epoch 112\n",
      "Macro F1 Score: 0.9659, Macro Precision: 0.9558, Macro Recall: 0.9770\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 13\n",
      "Training with F1=32, F2=32, D=2, dropout=0.3083130475867101, LR=0.0001576267906705285, BS=32, WD=3.830389757037512e-05\n",
      "Epoch 1/300 - Train Loss: 0.2478, Val Loss: 0.1135\n",
      "Epoch 2/300 - Train Loss: 0.1183, Val Loss: 0.0864\n",
      "Epoch 3/300 - Train Loss: 0.1044, Val Loss: 0.0796\n",
      "Epoch 4/300 - Train Loss: 0.0983, Val Loss: 0.0752\n",
      "Epoch 5/300 - Train Loss: 0.0942, Val Loss: 0.0715\n",
      "Epoch 6/300 - Train Loss: 0.0920, Val Loss: 0.0752\n",
      "Epoch 7/300 - Train Loss: 0.0894, Val Loss: 0.0782\n",
      "Epoch 8/300 - Train Loss: 0.0892, Val Loss: 0.0739\n",
      "Epoch 9/300 - Train Loss: 0.0878, Val Loss: 0.0763\n",
      "Epoch 10/300 - Train Loss: 0.0885, Val Loss: 0.0778\n",
      "Epoch 11/300 - Train Loss: 0.0846, Val Loss: 0.0693\n",
      "Epoch 12/300 - Train Loss: 0.0857, Val Loss: 0.0754\n",
      "Epoch 13/300 - Train Loss: 0.0833, Val Loss: 0.0713\n",
      "Epoch 14/300 - Train Loss: 0.0822, Val Loss: 0.0684\n",
      "Epoch 15/300 - Train Loss: 0.0815, Val Loss: 0.0759\n",
      "Epoch 16/300 - Train Loss: 0.0809, Val Loss: 0.0711\n",
      "Epoch 17/300 - Train Loss: 0.0811, Val Loss: 0.0732\n",
      "Epoch 18/300 - Train Loss: 0.0798, Val Loss: 0.0793\n",
      "Epoch 19/300 - Train Loss: 0.0794, Val Loss: 0.0734\n",
      "Epoch 20/300 - Train Loss: 0.0758, Val Loss: 0.0757\n",
      "Epoch 21/300 - Train Loss: 0.0801, Val Loss: 0.0679\n",
      "Epoch 22/300 - Train Loss: 0.0784, Val Loss: 0.0728\n",
      "Epoch 23/300 - Train Loss: 0.0772, Val Loss: 0.0855\n",
      "Epoch 24/300 - Train Loss: 0.0788, Val Loss: 0.0686\n",
      "Epoch 25/300 - Train Loss: 0.0771, Val Loss: 0.0762\n",
      "Epoch 26/300 - Train Loss: 0.0776, Val Loss: 0.0684\n",
      "Epoch 27/300 - Train Loss: 0.0762, Val Loss: 0.0703\n",
      "Epoch 28/300 - Train Loss: 0.0743, Val Loss: 0.0765\n",
      "Epoch 29/300 - Train Loss: 0.0754, Val Loss: 0.0695\n",
      "Epoch 30/300 - Train Loss: 0.0716, Val Loss: 0.0762\n",
      "Epoch 31/300 - Train Loss: 0.0746, Val Loss: 0.0748\n",
      "Epoch 32/300 - Train Loss: 0.0740, Val Loss: 0.0722\n",
      "Epoch 33/300 - Train Loss: 0.0712, Val Loss: 0.0735\n",
      "Epoch 34/300 - Train Loss: 0.0726, Val Loss: 0.0719\n",
      "Epoch 35/300 - Train Loss: 0.0726, Val Loss: 0.0724\n",
      "Epoch 36/300 - Train Loss: 0.0715, Val Loss: 0.0753\n",
      "Epoch 37/300 - Train Loss: 0.0712, Val Loss: 0.0735\n",
      "Epoch 38/300 - Train Loss: 0.0710, Val Loss: 0.0716\n",
      "Epoch 39/300 - Train Loss: 0.0677, Val Loss: 0.0697\n",
      "Epoch 40/300 - Train Loss: 0.0669, Val Loss: 0.0726\n",
      "Epoch 41/300 - Train Loss: 0.0694, Val Loss: 0.0736\n",
      "Epoch 42/300 - Train Loss: 0.0684, Val Loss: 0.0709\n",
      "Epoch 43/300 - Train Loss: 0.0683, Val Loss: 0.0750\n",
      "Epoch 44/300 - Train Loss: 0.0702, Val Loss: 0.0664\n",
      "Epoch 45/300 - Train Loss: 0.0678, Val Loss: 0.0772\n",
      "Epoch 46/300 - Train Loss: 0.0692, Val Loss: 0.0721\n",
      "Epoch 47/300 - Train Loss: 0.0680, Val Loss: 0.0780\n",
      "Epoch 48/300 - Train Loss: 0.0673, Val Loss: 0.0737\n",
      "Epoch 49/300 - Train Loss: 0.0656, Val Loss: 0.0731\n",
      "Epoch 50/300 - Train Loss: 0.0668, Val Loss: 0.0740\n",
      "Epoch 51/300 - Train Loss: 0.0696, Val Loss: 0.0756\n",
      "Epoch 52/300 - Train Loss: 0.0639, Val Loss: 0.0739\n",
      "Epoch 53/300 - Train Loss: 0.0659, Val Loss: 0.0720\n",
      "Epoch 54/300 - Train Loss: 0.0641, Val Loss: 0.0743\n",
      "Epoch 55/300 - Train Loss: 0.0668, Val Loss: 0.0735\n",
      "Epoch 56/300 - Train Loss: 0.0678, Val Loss: 0.0786\n",
      "Epoch 57/300 - Train Loss: 0.0657, Val Loss: 0.0757\n",
      "Epoch 58/300 - Train Loss: 0.0665, Val Loss: 0.0765\n",
      "Epoch 59/300 - Train Loss: 0.0635, Val Loss: 0.0753\n",
      "Epoch 60/300 - Train Loss: 0.0662, Val Loss: 0.0718\n",
      "Epoch 61/300 - Train Loss: 0.0659, Val Loss: 0.0731\n",
      "Epoch 62/300 - Train Loss: 0.0645, Val Loss: 0.0738\n",
      "Epoch 63/300 - Train Loss: 0.0624, Val Loss: 0.0784\n",
      "Epoch 64/300 - Train Loss: 0.0618, Val Loss: 0.0732\n",
      "Epoch 65/300 - Train Loss: 0.0619, Val Loss: 0.0845\n",
      "Epoch 66/300 - Train Loss: 0.0624, Val Loss: 0.0726\n",
      "Epoch 67/300 - Train Loss: 0.0624, Val Loss: 0.0745\n",
      "Epoch 68/300 - Train Loss: 0.0609, Val Loss: 0.0730\n",
      "Epoch 69/300 - Train Loss: 0.0620, Val Loss: 0.0788\n",
      "Epoch 70/300 - Train Loss: 0.0625, Val Loss: 0.0762\n",
      "Epoch 71/300 - Train Loss: 0.0587, Val Loss: 0.0733\n",
      "Epoch 72/300 - Train Loss: 0.0602, Val Loss: 0.0729\n",
      "Epoch 73/300 - Train Loss: 0.0608, Val Loss: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:14:58,838] Trial 12 finished with value: 0.9662227149482154 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.3083130475867101, 'learning_rate': 0.0001576267906705285, 'batch_size': 32, 'weight_decay': 3.830389757037512e-05}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300 - Train Loss: 0.0620, Val Loss: 0.0724\n",
      "Early stopping at epoch 74\n",
      "Macro F1 Score: 0.9662, Macro Precision: 0.9626, Macro Recall: 0.9701\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 14\n",
      "Training with F1=16, F2=32, D=2, dropout=0.47051011195053694, LR=4.134588386523421e-05, BS=32, WD=4.060040672171418e-05\n",
      "Epoch 1/300 - Train Loss: 0.5622, Val Loss: 0.3141\n",
      "Epoch 2/300 - Train Loss: 0.2527, Val Loss: 0.1820\n",
      "Epoch 3/300 - Train Loss: 0.1862, Val Loss: 0.1468\n",
      "Epoch 4/300 - Train Loss: 0.1584, Val Loss: 0.1226\n",
      "Epoch 5/300 - Train Loss: 0.1380, Val Loss: 0.1051\n",
      "Epoch 6/300 - Train Loss: 0.1301, Val Loss: 0.1055\n",
      "Epoch 7/300 - Train Loss: 0.1236, Val Loss: 0.0929\n",
      "Epoch 8/300 - Train Loss: 0.1160, Val Loss: 0.0974\n",
      "Epoch 9/300 - Train Loss: 0.1197, Val Loss: 0.0948\n",
      "Epoch 10/300 - Train Loss: 0.1135, Val Loss: 0.0890\n",
      "Epoch 11/300 - Train Loss: 0.1142, Val Loss: 0.0933\n",
      "Epoch 12/300 - Train Loss: 0.1067, Val Loss: 0.0902\n",
      "Epoch 13/300 - Train Loss: 0.1104, Val Loss: 0.0903\n",
      "Epoch 14/300 - Train Loss: 0.1083, Val Loss: 0.0919\n",
      "Epoch 15/300 - Train Loss: 0.1052, Val Loss: 0.0950\n",
      "Epoch 16/300 - Train Loss: 0.1018, Val Loss: 0.0871\n",
      "Epoch 17/300 - Train Loss: 0.1044, Val Loss: 0.0800\n",
      "Epoch 18/300 - Train Loss: 0.1046, Val Loss: 0.0822\n",
      "Epoch 19/300 - Train Loss: 0.1028, Val Loss: 0.0803\n",
      "Epoch 20/300 - Train Loss: 0.1002, Val Loss: 0.0846\n",
      "Epoch 21/300 - Train Loss: 0.1025, Val Loss: 0.0822\n",
      "Epoch 22/300 - Train Loss: 0.1027, Val Loss: 0.0887\n",
      "Epoch 23/300 - Train Loss: 0.0985, Val Loss: 0.0794\n",
      "Epoch 24/300 - Train Loss: 0.1007, Val Loss: 0.0804\n",
      "Epoch 25/300 - Train Loss: 0.0972, Val Loss: 0.0810\n",
      "Epoch 26/300 - Train Loss: 0.0964, Val Loss: 0.0887\n",
      "Epoch 27/300 - Train Loss: 0.0983, Val Loss: 0.0780\n",
      "Epoch 28/300 - Train Loss: 0.0971, Val Loss: 0.0807\n",
      "Epoch 29/300 - Train Loss: 0.0961, Val Loss: 0.0815\n",
      "Epoch 30/300 - Train Loss: 0.0972, Val Loss: 0.0783\n",
      "Epoch 31/300 - Train Loss: 0.0929, Val Loss: 0.0854\n",
      "Epoch 32/300 - Train Loss: 0.0969, Val Loss: 0.0774\n",
      "Epoch 33/300 - Train Loss: 0.0973, Val Loss: 0.0749\n",
      "Epoch 34/300 - Train Loss: 0.0932, Val Loss: 0.0777\n",
      "Epoch 35/300 - Train Loss: 0.0948, Val Loss: 0.0788\n",
      "Epoch 36/300 - Train Loss: 0.0940, Val Loss: 0.0788\n",
      "Epoch 37/300 - Train Loss: 0.0944, Val Loss: 0.0814\n",
      "Epoch 38/300 - Train Loss: 0.0939, Val Loss: 0.0761\n",
      "Epoch 39/300 - Train Loss: 0.0940, Val Loss: 0.0821\n",
      "Epoch 40/300 - Train Loss: 0.0926, Val Loss: 0.0753\n",
      "Epoch 41/300 - Train Loss: 0.0907, Val Loss: 0.0843\n",
      "Epoch 42/300 - Train Loss: 0.0923, Val Loss: 0.0801\n",
      "Epoch 43/300 - Train Loss: 0.0936, Val Loss: 0.0791\n",
      "Epoch 44/300 - Train Loss: 0.0939, Val Loss: 0.0769\n",
      "Epoch 45/300 - Train Loss: 0.0893, Val Loss: 0.0763\n",
      "Epoch 46/300 - Train Loss: 0.0915, Val Loss: 0.0826\n",
      "Epoch 47/300 - Train Loss: 0.0911, Val Loss: 0.0749\n",
      "Epoch 48/300 - Train Loss: 0.0906, Val Loss: 0.0766\n",
      "Epoch 49/300 - Train Loss: 0.0895, Val Loss: 0.0758\n",
      "Epoch 50/300 - Train Loss: 0.0917, Val Loss: 0.0726\n",
      "Epoch 51/300 - Train Loss: 0.0924, Val Loss: 0.0832\n",
      "Epoch 52/300 - Train Loss: 0.0898, Val Loss: 0.0798\n",
      "Epoch 53/300 - Train Loss: 0.0876, Val Loss: 0.0740\n",
      "Epoch 54/300 - Train Loss: 0.0911, Val Loss: 0.0773\n",
      "Epoch 55/300 - Train Loss: 0.0895, Val Loss: 0.0761\n",
      "Epoch 56/300 - Train Loss: 0.0896, Val Loss: 0.0773\n",
      "Epoch 57/300 - Train Loss: 0.0908, Val Loss: 0.0766\n",
      "Epoch 58/300 - Train Loss: 0.0871, Val Loss: 0.0726\n",
      "Epoch 59/300 - Train Loss: 0.0883, Val Loss: 0.0732\n",
      "Epoch 60/300 - Train Loss: 0.0891, Val Loss: 0.0730\n",
      "Epoch 61/300 - Train Loss: 0.0872, Val Loss: 0.0726\n",
      "Epoch 62/300 - Train Loss: 0.0879, Val Loss: 0.0751\n",
      "Epoch 63/300 - Train Loss: 0.0900, Val Loss: 0.0768\n",
      "Epoch 64/300 - Train Loss: 0.0916, Val Loss: 0.0797\n",
      "Epoch 65/300 - Train Loss: 0.0879, Val Loss: 0.0763\n",
      "Epoch 66/300 - Train Loss: 0.0872, Val Loss: 0.0763\n",
      "Epoch 67/300 - Train Loss: 0.0881, Val Loss: 0.0731\n",
      "Epoch 68/300 - Train Loss: 0.0886, Val Loss: 0.0730\n",
      "Epoch 69/300 - Train Loss: 0.0888, Val Loss: 0.0726\n",
      "Epoch 70/300 - Train Loss: 0.0883, Val Loss: 0.0727\n",
      "Epoch 71/300 - Train Loss: 0.0869, Val Loss: 0.0754\n",
      "Epoch 72/300 - Train Loss: 0.0875, Val Loss: 0.0721\n",
      "Epoch 73/300 - Train Loss: 0.0854, Val Loss: 0.0716\n",
      "Epoch 74/300 - Train Loss: 0.0869, Val Loss: 0.0853\n",
      "Epoch 75/300 - Train Loss: 0.0870, Val Loss: 0.0714\n",
      "Epoch 76/300 - Train Loss: 0.0849, Val Loss: 0.0777\n",
      "Epoch 77/300 - Train Loss: 0.0877, Val Loss: 0.0789\n",
      "Epoch 78/300 - Train Loss: 0.0867, Val Loss: 0.0786\n",
      "Epoch 79/300 - Train Loss: 0.0845, Val Loss: 0.0798\n",
      "Epoch 80/300 - Train Loss: 0.0862, Val Loss: 0.0699\n",
      "Epoch 81/300 - Train Loss: 0.0856, Val Loss: 0.0748\n",
      "Epoch 82/300 - Train Loss: 0.0856, Val Loss: 0.0739\n",
      "Epoch 83/300 - Train Loss: 0.0857, Val Loss: 0.0723\n",
      "Epoch 84/300 - Train Loss: 0.0863, Val Loss: 0.0740\n",
      "Epoch 85/300 - Train Loss: 0.0862, Val Loss: 0.0751\n",
      "Epoch 86/300 - Train Loss: 0.0855, Val Loss: 0.0733\n",
      "Epoch 87/300 - Train Loss: 0.0858, Val Loss: 0.0749\n",
      "Epoch 88/300 - Train Loss: 0.0859, Val Loss: 0.0746\n",
      "Epoch 89/300 - Train Loss: 0.0873, Val Loss: 0.0734\n",
      "Epoch 90/300 - Train Loss: 0.0915, Val Loss: 0.0719\n",
      "Epoch 91/300 - Train Loss: 0.0854, Val Loss: 0.0745\n",
      "Epoch 92/300 - Train Loss: 0.0879, Val Loss: 0.0721\n",
      "Epoch 93/300 - Train Loss: 0.0847, Val Loss: 0.0710\n",
      "Epoch 94/300 - Train Loss: 0.0841, Val Loss: 0.0735\n",
      "Epoch 95/300 - Train Loss: 0.0857, Val Loss: 0.0728\n",
      "Epoch 96/300 - Train Loss: 0.0845, Val Loss: 0.0759\n",
      "Epoch 97/300 - Train Loss: 0.0871, Val Loss: 0.0779\n",
      "Epoch 98/300 - Train Loss: 0.0830, Val Loss: 0.0735\n",
      "Epoch 99/300 - Train Loss: 0.0844, Val Loss: 0.0742\n",
      "Epoch 100/300 - Train Loss: 0.0840, Val Loss: 0.0777\n",
      "Epoch 101/300 - Train Loss: 0.0805, Val Loss: 0.0734\n",
      "Epoch 102/300 - Train Loss: 0.0830, Val Loss: 0.0726\n",
      "Epoch 103/300 - Train Loss: 0.0819, Val Loss: 0.0729\n",
      "Epoch 104/300 - Train Loss: 0.0826, Val Loss: 0.0782\n",
      "Epoch 105/300 - Train Loss: 0.0834, Val Loss: 0.0712\n",
      "Epoch 106/300 - Train Loss: 0.0854, Val Loss: 0.0764\n",
      "Epoch 107/300 - Train Loss: 0.0832, Val Loss: 0.0749\n",
      "Epoch 108/300 - Train Loss: 0.0842, Val Loss: 0.0714\n",
      "Epoch 109/300 - Train Loss: 0.0825, Val Loss: 0.0776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:18:49,655] Trial 13 finished with value: 0.9663791644537142 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.47051011195053694, 'learning_rate': 4.134588386523421e-05, 'batch_size': 32, 'weight_decay': 4.060040672171418e-05}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/300 - Train Loss: 0.0828, Val Loss: 0.0704\n",
      "Early stopping at epoch 110\n",
      "Macro F1 Score: 0.9664, Macro Precision: 0.9548, Macro Recall: 0.9795\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 15\n",
      "Training with F1=16, F2=32, D=2, dropout=0.28236094393646827, LR=0.0001932619305457479, BS=32, WD=2.5180273711005862e-05\n",
      "Epoch 1/300 - Train Loss: 0.2400, Val Loss: 0.1028\n",
      "Epoch 2/300 - Train Loss: 0.1158, Val Loss: 0.0886\n",
      "Epoch 3/300 - Train Loss: 0.1033, Val Loss: 0.0762\n",
      "Epoch 4/300 - Train Loss: 0.0988, Val Loss: 0.0787\n",
      "Epoch 5/300 - Train Loss: 0.0950, Val Loss: 0.0750\n",
      "Epoch 6/300 - Train Loss: 0.0924, Val Loss: 0.0789\n",
      "Epoch 7/300 - Train Loss: 0.0888, Val Loss: 0.0753\n",
      "Epoch 8/300 - Train Loss: 0.0906, Val Loss: 0.0728\n",
      "Epoch 9/300 - Train Loss: 0.0868, Val Loss: 0.0728\n",
      "Epoch 10/300 - Train Loss: 0.0836, Val Loss: 0.0739\n",
      "Epoch 11/300 - Train Loss: 0.0869, Val Loss: 0.0717\n",
      "Epoch 12/300 - Train Loss: 0.0854, Val Loss: 0.0725\n",
      "Epoch 13/300 - Train Loss: 0.0865, Val Loss: 0.0744\n",
      "Epoch 14/300 - Train Loss: 0.0866, Val Loss: 0.0733\n",
      "Epoch 15/300 - Train Loss: 0.0850, Val Loss: 0.0716\n",
      "Epoch 16/300 - Train Loss: 0.0865, Val Loss: 0.0735\n",
      "Epoch 17/300 - Train Loss: 0.0819, Val Loss: 0.0740\n",
      "Epoch 18/300 - Train Loss: 0.0817, Val Loss: 0.0777\n",
      "Epoch 19/300 - Train Loss: 0.0817, Val Loss: 0.0724\n",
      "Epoch 20/300 - Train Loss: 0.0810, Val Loss: 0.0743\n",
      "Epoch 21/300 - Train Loss: 0.0800, Val Loss: 0.0699\n",
      "Epoch 22/300 - Train Loss: 0.0777, Val Loss: 0.0728\n",
      "Epoch 23/300 - Train Loss: 0.0787, Val Loss: 0.0758\n",
      "Epoch 24/300 - Train Loss: 0.0779, Val Loss: 0.0743\n",
      "Epoch 25/300 - Train Loss: 0.0778, Val Loss: 0.0716\n",
      "Epoch 26/300 - Train Loss: 0.0788, Val Loss: 0.0794\n",
      "Epoch 27/300 - Train Loss: 0.0781, Val Loss: 0.0786\n",
      "Epoch 28/300 - Train Loss: 0.0808, Val Loss: 0.0742\n",
      "Epoch 29/300 - Train Loss: 0.0766, Val Loss: 0.0729\n",
      "Epoch 30/300 - Train Loss: 0.0752, Val Loss: 0.0709\n",
      "Epoch 31/300 - Train Loss: 0.0760, Val Loss: 0.0750\n",
      "Epoch 32/300 - Train Loss: 0.0754, Val Loss: 0.0811\n",
      "Epoch 33/300 - Train Loss: 0.0769, Val Loss: 0.0741\n",
      "Epoch 34/300 - Train Loss: 0.0741, Val Loss: 0.0696\n",
      "Epoch 35/300 - Train Loss: 0.0747, Val Loss: 0.0732\n",
      "Epoch 36/300 - Train Loss: 0.0741, Val Loss: 0.0747\n",
      "Epoch 37/300 - Train Loss: 0.0748, Val Loss: 0.0740\n",
      "Epoch 38/300 - Train Loss: 0.0725, Val Loss: 0.0771\n",
      "Epoch 39/300 - Train Loss: 0.0727, Val Loss: 0.0744\n",
      "Epoch 40/300 - Train Loss: 0.0718, Val Loss: 0.0814\n",
      "Epoch 41/300 - Train Loss: 0.0733, Val Loss: 0.0738\n",
      "Epoch 42/300 - Train Loss: 0.0736, Val Loss: 0.0707\n",
      "Epoch 43/300 - Train Loss: 0.0708, Val Loss: 0.0722\n",
      "Epoch 44/300 - Train Loss: 0.0731, Val Loss: 0.0710\n",
      "Epoch 45/300 - Train Loss: 0.0718, Val Loss: 0.0734\n",
      "Epoch 46/300 - Train Loss: 0.0719, Val Loss: 0.0706\n",
      "Epoch 47/300 - Train Loss: 0.0734, Val Loss: 0.0727\n",
      "Epoch 48/300 - Train Loss: 0.0702, Val Loss: 0.0720\n",
      "Epoch 49/300 - Train Loss: 0.0713, Val Loss: 0.0741\n",
      "Epoch 50/300 - Train Loss: 0.0702, Val Loss: 0.0737\n",
      "Epoch 51/300 - Train Loss: 0.0683, Val Loss: 0.0720\n",
      "Epoch 52/300 - Train Loss: 0.0682, Val Loss: 0.0763\n",
      "Epoch 53/300 - Train Loss: 0.0678, Val Loss: 0.0743\n",
      "Epoch 54/300 - Train Loss: 0.0690, Val Loss: 0.0721\n",
      "Epoch 55/300 - Train Loss: 0.0706, Val Loss: 0.0764\n",
      "Epoch 56/300 - Train Loss: 0.0684, Val Loss: 0.0774\n",
      "Epoch 57/300 - Train Loss: 0.0685, Val Loss: 0.0724\n",
      "Epoch 58/300 - Train Loss: 0.0656, Val Loss: 0.0700\n",
      "Epoch 59/300 - Train Loss: 0.0719, Val Loss: 0.0797\n",
      "Epoch 60/300 - Train Loss: 0.0668, Val Loss: 0.0725\n",
      "Epoch 61/300 - Train Loss: 0.0671, Val Loss: 0.0721\n",
      "Epoch 62/300 - Train Loss: 0.0670, Val Loss: 0.0709\n",
      "Epoch 63/300 - Train Loss: 0.0691, Val Loss: 0.0713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:21:04,210] Trial 14 finished with value: 0.9601954085222246 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.28236094393646827, 'learning_rate': 0.0001932619305457479, 'batch_size': 32, 'weight_decay': 2.5180273711005862e-05}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300 - Train Loss: 0.0643, Val Loss: 0.0730\n",
      "Early stopping at epoch 64\n",
      "Macro F1 Score: 0.9602, Macro Precision: 0.9504, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.88      0.95      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 16\n",
      "Training with F1=16, F2=32, D=4, dropout=0.499077744559542, LR=4.784925912685647e-05, BS=128, WD=6.915360303202149e-05\n",
      "Epoch 1/300 - Train Loss: 0.7084, Val Loss: 0.4426\n",
      "Epoch 2/300 - Train Loss: 0.3557, Val Loss: 0.3049\n",
      "Epoch 3/300 - Train Loss: 0.2668, Val Loss: 0.2505\n",
      "Epoch 4/300 - Train Loss: 0.2278, Val Loss: 0.2058\n",
      "Epoch 5/300 - Train Loss: 0.2037, Val Loss: 0.1820\n",
      "Epoch 6/300 - Train Loss: 0.1808, Val Loss: 0.1549\n",
      "Epoch 7/300 - Train Loss: 0.1623, Val Loss: 0.1433\n",
      "Epoch 8/300 - Train Loss: 0.1470, Val Loss: 0.1337\n",
      "Epoch 9/300 - Train Loss: 0.1340, Val Loss: 0.1207\n",
      "Epoch 10/300 - Train Loss: 0.1263, Val Loss: 0.1130\n",
      "Epoch 11/300 - Train Loss: 0.1184, Val Loss: 0.1108\n",
      "Epoch 12/300 - Train Loss: 0.1155, Val Loss: 0.1069\n",
      "Epoch 13/300 - Train Loss: 0.1101, Val Loss: 0.1031\n",
      "Epoch 14/300 - Train Loss: 0.1077, Val Loss: 0.1046\n",
      "Epoch 15/300 - Train Loss: 0.1045, Val Loss: 0.1004\n",
      "Epoch 16/300 - Train Loss: 0.1036, Val Loss: 0.0957\n",
      "Epoch 17/300 - Train Loss: 0.1010, Val Loss: 0.0997\n",
      "Epoch 18/300 - Train Loss: 0.0998, Val Loss: 0.0960\n",
      "Epoch 19/300 - Train Loss: 0.0994, Val Loss: 0.0945\n",
      "Epoch 20/300 - Train Loss: 0.0985, Val Loss: 0.0945\n",
      "Epoch 21/300 - Train Loss: 0.0975, Val Loss: 0.0915\n",
      "Epoch 22/300 - Train Loss: 0.0958, Val Loss: 0.0912\n",
      "Epoch 23/300 - Train Loss: 0.0955, Val Loss: 0.0891\n",
      "Epoch 24/300 - Train Loss: 0.0944, Val Loss: 0.0879\n",
      "Epoch 25/300 - Train Loss: 0.0932, Val Loss: 0.0927\n",
      "Epoch 26/300 - Train Loss: 0.0924, Val Loss: 0.0912\n",
      "Epoch 27/300 - Train Loss: 0.0925, Val Loss: 0.0918\n",
      "Epoch 28/300 - Train Loss: 0.0937, Val Loss: 0.0971\n",
      "Epoch 29/300 - Train Loss: 0.0904, Val Loss: 0.0892\n",
      "Epoch 30/300 - Train Loss: 0.0911, Val Loss: 0.0889\n",
      "Epoch 31/300 - Train Loss: 0.0909, Val Loss: 0.0869\n",
      "Epoch 32/300 - Train Loss: 0.0902, Val Loss: 0.0882\n",
      "Epoch 33/300 - Train Loss: 0.0892, Val Loss: 0.0870\n",
      "Epoch 34/300 - Train Loss: 0.0896, Val Loss: 0.0888\n",
      "Epoch 35/300 - Train Loss: 0.0893, Val Loss: 0.0874\n",
      "Epoch 36/300 - Train Loss: 0.0879, Val Loss: 0.0858\n",
      "Epoch 37/300 - Train Loss: 0.0879, Val Loss: 0.0838\n",
      "Epoch 38/300 - Train Loss: 0.0884, Val Loss: 0.0844\n",
      "Epoch 39/300 - Train Loss: 0.0887, Val Loss: 0.0882\n",
      "Epoch 40/300 - Train Loss: 0.0889, Val Loss: 0.0877\n",
      "Epoch 41/300 - Train Loss: 0.0863, Val Loss: 0.0834\n",
      "Epoch 42/300 - Train Loss: 0.0862, Val Loss: 0.0890\n",
      "Epoch 43/300 - Train Loss: 0.0866, Val Loss: 0.0854\n",
      "Epoch 44/300 - Train Loss: 0.0874, Val Loss: 0.0826\n",
      "Epoch 45/300 - Train Loss: 0.0849, Val Loss: 0.0850\n",
      "Epoch 46/300 - Train Loss: 0.0865, Val Loss: 0.0828\n",
      "Epoch 47/300 - Train Loss: 0.0849, Val Loss: 0.0825\n",
      "Epoch 48/300 - Train Loss: 0.0865, Val Loss: 0.0856\n",
      "Epoch 49/300 - Train Loss: 0.0844, Val Loss: 0.0833\n",
      "Epoch 50/300 - Train Loss: 0.0848, Val Loss: 0.0823\n",
      "Epoch 51/300 - Train Loss: 0.0840, Val Loss: 0.0848\n",
      "Epoch 52/300 - Train Loss: 0.0836, Val Loss: 0.0877\n",
      "Epoch 53/300 - Train Loss: 0.0846, Val Loss: 0.0847\n",
      "Epoch 54/300 - Train Loss: 0.0844, Val Loss: 0.0842\n",
      "Epoch 55/300 - Train Loss: 0.0824, Val Loss: 0.0833\n",
      "Epoch 56/300 - Train Loss: 0.0837, Val Loss: 0.0843\n",
      "Epoch 57/300 - Train Loss: 0.0842, Val Loss: 0.0827\n",
      "Epoch 58/300 - Train Loss: 0.0842, Val Loss: 0.0813\n",
      "Epoch 59/300 - Train Loss: 0.0821, Val Loss: 0.0847\n",
      "Epoch 60/300 - Train Loss: 0.0828, Val Loss: 0.0833\n",
      "Epoch 61/300 - Train Loss: 0.0828, Val Loss: 0.0832\n",
      "Epoch 62/300 - Train Loss: 0.0830, Val Loss: 0.0831\n",
      "Epoch 63/300 - Train Loss: 0.0833, Val Loss: 0.0835\n",
      "Epoch 64/300 - Train Loss: 0.0829, Val Loss: 0.0847\n",
      "Epoch 65/300 - Train Loss: 0.0821, Val Loss: 0.0837\n",
      "Epoch 66/300 - Train Loss: 0.0818, Val Loss: 0.0873\n",
      "Epoch 67/300 - Train Loss: 0.0834, Val Loss: 0.0815\n",
      "Epoch 68/300 - Train Loss: 0.0811, Val Loss: 0.0843\n",
      "Epoch 69/300 - Train Loss: 0.0820, Val Loss: 0.0843\n",
      "Epoch 70/300 - Train Loss: 0.0809, Val Loss: 0.0807\n",
      "Epoch 71/300 - Train Loss: 0.0804, Val Loss: 0.0822\n",
      "Epoch 72/300 - Train Loss: 0.0799, Val Loss: 0.0820\n",
      "Epoch 73/300 - Train Loss: 0.0807, Val Loss: 0.0817\n",
      "Epoch 74/300 - Train Loss: 0.0825, Val Loss: 0.0820\n",
      "Epoch 75/300 - Train Loss: 0.0801, Val Loss: 0.0838\n",
      "Epoch 76/300 - Train Loss: 0.0806, Val Loss: 0.0859\n",
      "Epoch 77/300 - Train Loss: 0.0802, Val Loss: 0.0839\n",
      "Epoch 78/300 - Train Loss: 0.0787, Val Loss: 0.0854\n",
      "Epoch 79/300 - Train Loss: 0.0806, Val Loss: 0.0831\n",
      "Epoch 80/300 - Train Loss: 0.0794, Val Loss: 0.0795\n",
      "Epoch 81/300 - Train Loss: 0.0803, Val Loss: 0.0805\n",
      "Epoch 82/300 - Train Loss: 0.0793, Val Loss: 0.0817\n",
      "Epoch 83/300 - Train Loss: 0.0794, Val Loss: 0.0800\n",
      "Epoch 84/300 - Train Loss: 0.0799, Val Loss: 0.0822\n",
      "Epoch 85/300 - Train Loss: 0.0788, Val Loss: 0.0830\n",
      "Epoch 86/300 - Train Loss: 0.0785, Val Loss: 0.0842\n",
      "Epoch 87/300 - Train Loss: 0.0813, Val Loss: 0.0826\n",
      "Epoch 88/300 - Train Loss: 0.0781, Val Loss: 0.0823\n",
      "Epoch 89/300 - Train Loss: 0.0780, Val Loss: 0.0819\n",
      "Epoch 90/300 - Train Loss: 0.0780, Val Loss: 0.0819\n",
      "Epoch 91/300 - Train Loss: 0.0790, Val Loss: 0.0802\n",
      "Epoch 92/300 - Train Loss: 0.0794, Val Loss: 0.0801\n",
      "Epoch 93/300 - Train Loss: 0.0787, Val Loss: 0.0811\n",
      "Epoch 94/300 - Train Loss: 0.0789, Val Loss: 0.0822\n",
      "Epoch 95/300 - Train Loss: 0.0777, Val Loss: 0.0825\n",
      "Epoch 96/300 - Train Loss: 0.0776, Val Loss: 0.0801\n",
      "Epoch 97/300 - Train Loss: 0.0783, Val Loss: 0.0804\n",
      "Epoch 98/300 - Train Loss: 0.0771, Val Loss: 0.0820\n",
      "Epoch 99/300 - Train Loss: 0.0782, Val Loss: 0.0801\n",
      "Epoch 100/300 - Train Loss: 0.0773, Val Loss: 0.0809\n",
      "Epoch 101/300 - Train Loss: 0.0762, Val Loss: 0.0802\n",
      "Epoch 102/300 - Train Loss: 0.0776, Val Loss: 0.0803\n",
      "Epoch 103/300 - Train Loss: 0.0776, Val Loss: 0.0789\n",
      "Epoch 104/300 - Train Loss: 0.0774, Val Loss: 0.0808\n",
      "Epoch 105/300 - Train Loss: 0.0772, Val Loss: 0.0798\n",
      "Epoch 106/300 - Train Loss: 0.0772, Val Loss: 0.0806\n",
      "Epoch 107/300 - Train Loss: 0.0782, Val Loss: 0.0787\n",
      "Epoch 108/300 - Train Loss: 0.0776, Val Loss: 0.0803\n",
      "Epoch 109/300 - Train Loss: 0.0774, Val Loss: 0.0799\n",
      "Epoch 110/300 - Train Loss: 0.0757, Val Loss: 0.0816\n",
      "Epoch 111/300 - Train Loss: 0.0768, Val Loss: 0.0803\n",
      "Epoch 112/300 - Train Loss: 0.0767, Val Loss: 0.0779\n",
      "Epoch 113/300 - Train Loss: 0.0778, Val Loss: 0.0785\n",
      "Epoch 114/300 - Train Loss: 0.0765, Val Loss: 0.0775\n",
      "Epoch 115/300 - Train Loss: 0.0752, Val Loss: 0.0802\n",
      "Epoch 116/300 - Train Loss: 0.0774, Val Loss: 0.0781\n",
      "Epoch 117/300 - Train Loss: 0.0762, Val Loss: 0.0818\n",
      "Epoch 118/300 - Train Loss: 0.0771, Val Loss: 0.0809\n",
      "Epoch 119/300 - Train Loss: 0.0749, Val Loss: 0.0800\n",
      "Epoch 120/300 - Train Loss: 0.0754, Val Loss: 0.0815\n",
      "Epoch 121/300 - Train Loss: 0.0770, Val Loss: 0.0770\n",
      "Epoch 122/300 - Train Loss: 0.0764, Val Loss: 0.0803\n",
      "Epoch 123/300 - Train Loss: 0.0743, Val Loss: 0.0806\n",
      "Epoch 124/300 - Train Loss: 0.0751, Val Loss: 0.0804\n",
      "Epoch 125/300 - Train Loss: 0.0742, Val Loss: 0.0823\n",
      "Epoch 126/300 - Train Loss: 0.0748, Val Loss: 0.0799\n",
      "Epoch 127/300 - Train Loss: 0.0749, Val Loss: 0.0796\n",
      "Epoch 128/300 - Train Loss: 0.0743, Val Loss: 0.0792\n",
      "Epoch 129/300 - Train Loss: 0.0722, Val Loss: 0.0801\n",
      "Epoch 130/300 - Train Loss: 0.0740, Val Loss: 0.0812\n",
      "Epoch 131/300 - Train Loss: 0.0746, Val Loss: 0.0813\n",
      "Epoch 132/300 - Train Loss: 0.0727, Val Loss: 0.0805\n",
      "Epoch 133/300 - Train Loss: 0.0733, Val Loss: 0.0800\n",
      "Epoch 134/300 - Train Loss: 0.0748, Val Loss: 0.0794\n",
      "Epoch 135/300 - Train Loss: 0.0732, Val Loss: 0.0783\n",
      "Epoch 136/300 - Train Loss: 0.0754, Val Loss: 0.0766\n",
      "Epoch 137/300 - Train Loss: 0.0747, Val Loss: 0.0795\n",
      "Epoch 138/300 - Train Loss: 0.0738, Val Loss: 0.0779\n",
      "Epoch 139/300 - Train Loss: 0.0744, Val Loss: 0.0798\n",
      "Epoch 140/300 - Train Loss: 0.0747, Val Loss: 0.0800\n",
      "Epoch 141/300 - Train Loss: 0.0739, Val Loss: 0.0808\n",
      "Epoch 142/300 - Train Loss: 0.0744, Val Loss: 0.0761\n",
      "Epoch 143/300 - Train Loss: 0.0748, Val Loss: 0.0787\n",
      "Epoch 144/300 - Train Loss: 0.0744, Val Loss: 0.0785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0745, Val Loss: 0.0778\n",
      "Epoch 146/300 - Train Loss: 0.0735, Val Loss: 0.0793\n",
      "Epoch 147/300 - Train Loss: 0.0729, Val Loss: 0.0784\n",
      "Epoch 148/300 - Train Loss: 0.0729, Val Loss: 0.0800\n",
      "Epoch 149/300 - Train Loss: 0.0720, Val Loss: 0.0778\n",
      "Epoch 150/300 - Train Loss: 0.0737, Val Loss: 0.0780\n",
      "Epoch 151/300 - Train Loss: 0.0733, Val Loss: 0.0763\n",
      "Epoch 152/300 - Train Loss: 0.0739, Val Loss: 0.0811\n",
      "Epoch 153/300 - Train Loss: 0.0728, Val Loss: 0.0789\n",
      "Epoch 154/300 - Train Loss: 0.0711, Val Loss: 0.0778\n",
      "Epoch 155/300 - Train Loss: 0.0731, Val Loss: 0.0793\n",
      "Epoch 156/300 - Train Loss: 0.0722, Val Loss: 0.0792\n",
      "Epoch 157/300 - Train Loss: 0.0728, Val Loss: 0.0767\n",
      "Epoch 158/300 - Train Loss: 0.0723, Val Loss: 0.0777\n",
      "Epoch 159/300 - Train Loss: 0.0714, Val Loss: 0.0777\n",
      "Epoch 160/300 - Train Loss: 0.0741, Val Loss: 0.0776\n",
      "Epoch 161/300 - Train Loss: 0.0724, Val Loss: 0.0770\n",
      "Epoch 162/300 - Train Loss: 0.0731, Val Loss: 0.0765\n",
      "Epoch 163/300 - Train Loss: 0.0709, Val Loss: 0.0772\n",
      "Epoch 164/300 - Train Loss: 0.0728, Val Loss: 0.0783\n",
      "Epoch 165/300 - Train Loss: 0.0714, Val Loss: 0.0784\n",
      "Epoch 166/300 - Train Loss: 0.0712, Val Loss: 0.0757\n",
      "Epoch 167/300 - Train Loss: 0.0738, Val Loss: 0.0781\n",
      "Epoch 168/300 - Train Loss: 0.0735, Val Loss: 0.0788\n",
      "Epoch 169/300 - Train Loss: 0.0734, Val Loss: 0.0774\n",
      "Epoch 170/300 - Train Loss: 0.0703, Val Loss: 0.0771\n",
      "Epoch 171/300 - Train Loss: 0.0706, Val Loss: 0.0788\n",
      "Epoch 172/300 - Train Loss: 0.0706, Val Loss: 0.0765\n",
      "Epoch 173/300 - Train Loss: 0.0718, Val Loss: 0.0771\n",
      "Epoch 174/300 - Train Loss: 0.0713, Val Loss: 0.0769\n",
      "Epoch 175/300 - Train Loss: 0.0714, Val Loss: 0.0783\n",
      "Epoch 176/300 - Train Loss: 0.0730, Val Loss: 0.0772\n",
      "Epoch 177/300 - Train Loss: 0.0710, Val Loss: 0.0792\n",
      "Epoch 178/300 - Train Loss: 0.0716, Val Loss: 0.0786\n",
      "Epoch 179/300 - Train Loss: 0.0703, Val Loss: 0.0779\n",
      "Epoch 180/300 - Train Loss: 0.0701, Val Loss: 0.0776\n",
      "Epoch 181/300 - Train Loss: 0.0700, Val Loss: 0.0763\n",
      "Epoch 182/300 - Train Loss: 0.0731, Val Loss: 0.0772\n",
      "Epoch 183/300 - Train Loss: 0.0703, Val Loss: 0.0802\n",
      "Epoch 184/300 - Train Loss: 0.0709, Val Loss: 0.0764\n",
      "Epoch 185/300 - Train Loss: 0.0721, Val Loss: 0.0764\n",
      "Epoch 186/300 - Train Loss: 0.0715, Val Loss: 0.0765\n",
      "Epoch 187/300 - Train Loss: 0.0693, Val Loss: 0.0756\n",
      "Epoch 188/300 - Train Loss: 0.0710, Val Loss: 0.0764\n",
      "Epoch 189/300 - Train Loss: 0.0728, Val Loss: 0.0765\n",
      "Epoch 190/300 - Train Loss: 0.0702, Val Loss: 0.0763\n",
      "Epoch 191/300 - Train Loss: 0.0686, Val Loss: 0.0762\n",
      "Epoch 192/300 - Train Loss: 0.0714, Val Loss: 0.0786\n",
      "Epoch 193/300 - Train Loss: 0.0700, Val Loss: 0.0753\n",
      "Epoch 194/300 - Train Loss: 0.0692, Val Loss: 0.0751\n",
      "Epoch 195/300 - Train Loss: 0.0703, Val Loss: 0.0778\n",
      "Epoch 196/300 - Train Loss: 0.0695, Val Loss: 0.0775\n",
      "Epoch 197/300 - Train Loss: 0.0699, Val Loss: 0.0785\n",
      "Epoch 198/300 - Train Loss: 0.0705, Val Loss: 0.0760\n",
      "Epoch 199/300 - Train Loss: 0.0694, Val Loss: 0.0769\n",
      "Epoch 200/300 - Train Loss: 0.0700, Val Loss: 0.0765\n",
      "Epoch 201/300 - Train Loss: 0.0700, Val Loss: 0.0761\n",
      "Epoch 202/300 - Train Loss: 0.0690, Val Loss: 0.0768\n",
      "Epoch 203/300 - Train Loss: 0.0697, Val Loss: 0.0759\n",
      "Epoch 204/300 - Train Loss: 0.0689, Val Loss: 0.0762\n",
      "Epoch 205/300 - Train Loss: 0.0676, Val Loss: 0.0757\n",
      "Epoch 206/300 - Train Loss: 0.0683, Val Loss: 0.0757\n",
      "Epoch 207/300 - Train Loss: 0.0715, Val Loss: 0.0771\n",
      "Epoch 208/300 - Train Loss: 0.0690, Val Loss: 0.0753\n",
      "Epoch 209/300 - Train Loss: 0.0698, Val Loss: 0.0761\n",
      "Epoch 210/300 - Train Loss: 0.0716, Val Loss: 0.0745\n",
      "Epoch 211/300 - Train Loss: 0.0703, Val Loss: 0.0742\n",
      "Epoch 212/300 - Train Loss: 0.0689, Val Loss: 0.0731\n",
      "Epoch 213/300 - Train Loss: 0.0682, Val Loss: 0.0754\n",
      "Epoch 214/300 - Train Loss: 0.0692, Val Loss: 0.0762\n",
      "Epoch 215/300 - Train Loss: 0.0694, Val Loss: 0.0756\n",
      "Epoch 216/300 - Train Loss: 0.0681, Val Loss: 0.0735\n",
      "Epoch 217/300 - Train Loss: 0.0671, Val Loss: 0.0754\n",
      "Epoch 218/300 - Train Loss: 0.0689, Val Loss: 0.0748\n",
      "Epoch 219/300 - Train Loss: 0.0675, Val Loss: 0.0731\n",
      "Epoch 220/300 - Train Loss: 0.0686, Val Loss: 0.0752\n",
      "Epoch 221/300 - Train Loss: 0.0696, Val Loss: 0.0728\n",
      "Epoch 222/300 - Train Loss: 0.0686, Val Loss: 0.0736\n",
      "Epoch 223/300 - Train Loss: 0.0680, Val Loss: 0.0736\n",
      "Epoch 224/300 - Train Loss: 0.0684, Val Loss: 0.0747\n",
      "Epoch 225/300 - Train Loss: 0.0693, Val Loss: 0.0756\n",
      "Epoch 226/300 - Train Loss: 0.0691, Val Loss: 0.0762\n",
      "Epoch 227/300 - Train Loss: 0.0698, Val Loss: 0.0744\n",
      "Epoch 228/300 - Train Loss: 0.0678, Val Loss: 0.0739\n",
      "Epoch 229/300 - Train Loss: 0.0672, Val Loss: 0.0758\n",
      "Epoch 230/300 - Train Loss: 0.0669, Val Loss: 0.0740\n",
      "Epoch 231/300 - Train Loss: 0.0680, Val Loss: 0.0748\n",
      "Epoch 232/300 - Train Loss: 0.0682, Val Loss: 0.0754\n",
      "Epoch 233/300 - Train Loss: 0.0666, Val Loss: 0.0751\n",
      "Epoch 234/300 - Train Loss: 0.0669, Val Loss: 0.0750\n",
      "Epoch 235/300 - Train Loss: 0.0674, Val Loss: 0.0752\n",
      "Epoch 236/300 - Train Loss: 0.0676, Val Loss: 0.0748\n",
      "Epoch 237/300 - Train Loss: 0.0691, Val Loss: 0.0778\n",
      "Epoch 238/300 - Train Loss: 0.0671, Val Loss: 0.0752\n",
      "Epoch 239/300 - Train Loss: 0.0665, Val Loss: 0.0766\n",
      "Epoch 240/300 - Train Loss: 0.0681, Val Loss: 0.0755\n",
      "Epoch 241/300 - Train Loss: 0.0688, Val Loss: 0.0744\n",
      "Epoch 242/300 - Train Loss: 0.0694, Val Loss: 0.0759\n",
      "Epoch 243/300 - Train Loss: 0.0644, Val Loss: 0.0759\n",
      "Epoch 244/300 - Train Loss: 0.0691, Val Loss: 0.0767\n",
      "Epoch 245/300 - Train Loss: 0.0678, Val Loss: 0.0736\n",
      "Epoch 246/300 - Train Loss: 0.0664, Val Loss: 0.0742\n",
      "Epoch 247/300 - Train Loss: 0.0670, Val Loss: 0.0757\n",
      "Epoch 248/300 - Train Loss: 0.0671, Val Loss: 0.0756\n",
      "Epoch 249/300 - Train Loss: 0.0676, Val Loss: 0.0753\n",
      "Epoch 250/300 - Train Loss: 0.0672, Val Loss: 0.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:29:13,021] Trial 15 finished with value: 0.9648738783215703 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.499077744559542, 'learning_rate': 4.784925912685647e-05, 'batch_size': 128, 'weight_decay': 6.915360303202149e-05}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 251/300 - Train Loss: 0.0673, Val Loss: 0.0735\n",
      "Early stopping at epoch 251\n",
      "Macro F1 Score: 0.9649, Macro Precision: 0.9551, Macro Recall: 0.9757\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 17\n",
      "Training with F1=32, F2=32, D=2, dropout=0.38054111864764467, LR=0.0002457718293164558, BS=32, WD=1.0506047996907796e-05\n",
      "Epoch 1/300 - Train Loss: 0.1945, Val Loss: 0.0962\n",
      "Epoch 2/300 - Train Loss: 0.1052, Val Loss: 0.0781\n",
      "Epoch 3/300 - Train Loss: 0.0970, Val Loss: 0.0717\n",
      "Epoch 4/300 - Train Loss: 0.0947, Val Loss: 0.0744\n",
      "Epoch 5/300 - Train Loss: 0.0933, Val Loss: 0.0754\n",
      "Epoch 6/300 - Train Loss: 0.0916, Val Loss: 0.0730\n",
      "Epoch 7/300 - Train Loss: 0.0903, Val Loss: 0.0759\n",
      "Epoch 8/300 - Train Loss: 0.0876, Val Loss: 0.0667\n",
      "Epoch 9/300 - Train Loss: 0.0888, Val Loss: 0.0716\n",
      "Epoch 10/300 - Train Loss: 0.0908, Val Loss: 0.0712\n",
      "Epoch 11/300 - Train Loss: 0.0852, Val Loss: 0.0681\n",
      "Epoch 12/300 - Train Loss: 0.0843, Val Loss: 0.0698\n",
      "Epoch 13/300 - Train Loss: 0.0853, Val Loss: 0.0704\n",
      "Epoch 14/300 - Train Loss: 0.0885, Val Loss: 0.0688\n",
      "Epoch 15/300 - Train Loss: 0.0824, Val Loss: 0.0672\n",
      "Epoch 16/300 - Train Loss: 0.0832, Val Loss: 0.0724\n",
      "Epoch 17/300 - Train Loss: 0.0819, Val Loss: 0.0693\n",
      "Epoch 18/300 - Train Loss: 0.0791, Val Loss: 0.0701\n",
      "Epoch 19/300 - Train Loss: 0.0802, Val Loss: 0.0710\n",
      "Epoch 20/300 - Train Loss: 0.0802, Val Loss: 0.0712\n",
      "Epoch 21/300 - Train Loss: 0.0798, Val Loss: 0.0684\n",
      "Epoch 22/300 - Train Loss: 0.0796, Val Loss: 0.0683\n",
      "Epoch 23/300 - Train Loss: 0.0791, Val Loss: 0.0669\n",
      "Epoch 24/300 - Train Loss: 0.0761, Val Loss: 0.0690\n",
      "Epoch 25/300 - Train Loss: 0.0766, Val Loss: 0.0654\n",
      "Epoch 26/300 - Train Loss: 0.0769, Val Loss: 0.0695\n",
      "Epoch 27/300 - Train Loss: 0.0774, Val Loss: 0.0721\n",
      "Epoch 28/300 - Train Loss: 0.0758, Val Loss: 0.0644\n",
      "Epoch 29/300 - Train Loss: 0.0777, Val Loss: 0.0789\n",
      "Epoch 30/300 - Train Loss: 0.0737, Val Loss: 0.0637\n",
      "Epoch 31/300 - Train Loss: 0.0729, Val Loss: 0.0669\n",
      "Epoch 32/300 - Train Loss: 0.0735, Val Loss: 0.0683\n",
      "Epoch 33/300 - Train Loss: 0.0748, Val Loss: 0.0703\n",
      "Epoch 34/300 - Train Loss: 0.0750, Val Loss: 0.0706\n",
      "Epoch 35/300 - Train Loss: 0.0738, Val Loss: 0.0671\n",
      "Epoch 36/300 - Train Loss: 0.0714, Val Loss: 0.0682\n",
      "Epoch 37/300 - Train Loss: 0.0733, Val Loss: 0.0676\n",
      "Epoch 38/300 - Train Loss: 0.0722, Val Loss: 0.0673\n",
      "Epoch 39/300 - Train Loss: 0.0700, Val Loss: 0.0681\n",
      "Epoch 40/300 - Train Loss: 0.0721, Val Loss: 0.0690\n",
      "Epoch 41/300 - Train Loss: 0.0705, Val Loss: 0.0680\n",
      "Epoch 42/300 - Train Loss: 0.0700, Val Loss: 0.0669\n",
      "Epoch 43/300 - Train Loss: 0.0717, Val Loss: 0.0729\n",
      "Epoch 44/300 - Train Loss: 0.0743, Val Loss: 0.0659\n",
      "Epoch 45/300 - Train Loss: 0.0710, Val Loss: 0.0676\n",
      "Epoch 46/300 - Train Loss: 0.0687, Val Loss: 0.0687\n",
      "Epoch 47/300 - Train Loss: 0.0687, Val Loss: 0.0681\n",
      "Epoch 48/300 - Train Loss: 0.0671, Val Loss: 0.0661\n",
      "Epoch 49/300 - Train Loss: 0.0708, Val Loss: 0.0678\n",
      "Epoch 50/300 - Train Loss: 0.0685, Val Loss: 0.0718\n",
      "Epoch 51/300 - Train Loss: 0.0696, Val Loss: 0.0696\n",
      "Epoch 52/300 - Train Loss: 0.0675, Val Loss: 0.0703\n",
      "Epoch 53/300 - Train Loss: 0.0684, Val Loss: 0.0719\n",
      "Epoch 54/300 - Train Loss: 0.0668, Val Loss: 0.0719\n",
      "Epoch 55/300 - Train Loss: 0.0652, Val Loss: 0.0668\n",
      "Epoch 56/300 - Train Loss: 0.0666, Val Loss: 0.0733\n",
      "Epoch 57/300 - Train Loss: 0.0671, Val Loss: 0.0783\n",
      "Epoch 58/300 - Train Loss: 0.0652, Val Loss: 0.0736\n",
      "Epoch 59/300 - Train Loss: 0.0689, Val Loss: 0.0656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:31:57,011] Trial 16 finished with value: 0.9646256857292879 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.38054111864764467, 'learning_rate': 0.0002457718293164558, 'batch_size': 32, 'weight_decay': 1.0506047996907796e-05}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300 - Train Loss: 0.0642, Val Loss: 0.0689\n",
      "Early stopping at epoch 60\n",
      "Macro F1 Score: 0.9646, Macro Precision: 0.9627, Macro Recall: 0.9666\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.93      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 18\n",
      "Training with F1=16, F2=16, D=2, dropout=0.6024932299548313, LR=2.9527278818546302e-05, BS=32, WD=0.0006228106838368567\n",
      "Epoch 1/300 - Train Loss: 0.8231, Val Loss: 0.6086\n",
      "Epoch 2/300 - Train Loss: 0.4443, Val Loss: 0.3488\n",
      "Epoch 3/300 - Train Loss: 0.3117, Val Loss: 0.2735\n",
      "Epoch 4/300 - Train Loss: 0.2639, Val Loss: 0.2187\n",
      "Epoch 5/300 - Train Loss: 0.2390, Val Loss: 0.1981\n",
      "Epoch 6/300 - Train Loss: 0.2234, Val Loss: 0.1770\n",
      "Epoch 7/300 - Train Loss: 0.2002, Val Loss: 0.1515\n",
      "Epoch 8/300 - Train Loss: 0.1837, Val Loss: 0.1368\n",
      "Epoch 9/300 - Train Loss: 0.1680, Val Loss: 0.1755\n",
      "Epoch 10/300 - Train Loss: 0.1557, Val Loss: 0.1235\n",
      "Epoch 11/300 - Train Loss: 0.1461, Val Loss: 0.1137\n",
      "Epoch 12/300 - Train Loss: 0.1439, Val Loss: 0.1041\n",
      "Epoch 13/300 - Train Loss: 0.1366, Val Loss: 0.1067\n",
      "Epoch 14/300 - Train Loss: 0.1329, Val Loss: 0.0944\n",
      "Epoch 15/300 - Train Loss: 0.1334, Val Loss: 0.1014\n",
      "Epoch 16/300 - Train Loss: 0.1289, Val Loss: 0.0934\n",
      "Epoch 17/300 - Train Loss: 0.1242, Val Loss: 0.0992\n",
      "Epoch 18/300 - Train Loss: 0.1240, Val Loss: 0.0976\n",
      "Epoch 19/300 - Train Loss: 0.1214, Val Loss: 0.0937\n",
      "Epoch 20/300 - Train Loss: 0.1222, Val Loss: 0.0904\n",
      "Epoch 21/300 - Train Loss: 0.1203, Val Loss: 0.0961\n",
      "Epoch 22/300 - Train Loss: 0.1203, Val Loss: 0.0905\n",
      "Epoch 23/300 - Train Loss: 0.1192, Val Loss: 0.0952\n",
      "Epoch 24/300 - Train Loss: 0.1195, Val Loss: 0.0843\n",
      "Epoch 25/300 - Train Loss: 0.1209, Val Loss: 0.0934\n",
      "Epoch 26/300 - Train Loss: 0.1169, Val Loss: 0.0925\n",
      "Epoch 27/300 - Train Loss: 0.1163, Val Loss: 0.0878\n",
      "Epoch 28/300 - Train Loss: 0.1177, Val Loss: 0.0872\n",
      "Epoch 29/300 - Train Loss: 0.1149, Val Loss: 0.0900\n",
      "Epoch 30/300 - Train Loss: 0.1146, Val Loss: 0.0837\n",
      "Epoch 31/300 - Train Loss: 0.1121, Val Loss: 0.0867\n",
      "Epoch 32/300 - Train Loss: 0.1141, Val Loss: 0.0790\n",
      "Epoch 33/300 - Train Loss: 0.1126, Val Loss: 0.0852\n",
      "Epoch 34/300 - Train Loss: 0.1122, Val Loss: 0.0829\n",
      "Epoch 35/300 - Train Loss: 0.1125, Val Loss: 0.0825\n",
      "Epoch 36/300 - Train Loss: 0.1095, Val Loss: 0.0840\n",
      "Epoch 37/300 - Train Loss: 0.1149, Val Loss: 0.0825\n",
      "Epoch 38/300 - Train Loss: 0.1126, Val Loss: 0.0770\n",
      "Epoch 39/300 - Train Loss: 0.1082, Val Loss: 0.0823\n",
      "Epoch 40/300 - Train Loss: 0.1099, Val Loss: 0.0849\n",
      "Epoch 41/300 - Train Loss: 0.1121, Val Loss: 0.0803\n",
      "Epoch 42/300 - Train Loss: 0.1099, Val Loss: 0.0790\n",
      "Epoch 43/300 - Train Loss: 0.1089, Val Loss: 0.0777\n",
      "Epoch 44/300 - Train Loss: 0.1061, Val Loss: 0.0779\n",
      "Epoch 45/300 - Train Loss: 0.1075, Val Loss: 0.0788\n",
      "Epoch 46/300 - Train Loss: 0.1092, Val Loss: 0.0763\n",
      "Epoch 47/300 - Train Loss: 0.1067, Val Loss: 0.0790\n",
      "Epoch 48/300 - Train Loss: 0.1076, Val Loss: 0.0773\n",
      "Epoch 49/300 - Train Loss: 0.1090, Val Loss: 0.0788\n",
      "Epoch 50/300 - Train Loss: 0.1065, Val Loss: 0.0814\n",
      "Epoch 51/300 - Train Loss: 0.1055, Val Loss: 0.0781\n",
      "Epoch 52/300 - Train Loss: 0.1059, Val Loss: 0.0796\n",
      "Epoch 53/300 - Train Loss: 0.1070, Val Loss: 0.0823\n",
      "Epoch 54/300 - Train Loss: 0.1057, Val Loss: 0.0773\n",
      "Epoch 55/300 - Train Loss: 0.1077, Val Loss: 0.0802\n",
      "Epoch 56/300 - Train Loss: 0.1074, Val Loss: 0.0767\n",
      "Epoch 57/300 - Train Loss: 0.1061, Val Loss: 0.0781\n",
      "Epoch 58/300 - Train Loss: 0.1065, Val Loss: 0.0749\n",
      "Epoch 59/300 - Train Loss: 0.1071, Val Loss: 0.0782\n",
      "Epoch 60/300 - Train Loss: 0.1085, Val Loss: 0.0795\n",
      "Epoch 61/300 - Train Loss: 0.1039, Val Loss: 0.0830\n",
      "Epoch 62/300 - Train Loss: 0.1043, Val Loss: 0.0815\n",
      "Epoch 63/300 - Train Loss: 0.1038, Val Loss: 0.0780\n",
      "Epoch 64/300 - Train Loss: 0.1092, Val Loss: 0.0783\n",
      "Epoch 65/300 - Train Loss: 0.1038, Val Loss: 0.0797\n",
      "Epoch 66/300 - Train Loss: 0.1032, Val Loss: 0.0772\n",
      "Epoch 67/300 - Train Loss: 0.1029, Val Loss: 0.0806\n",
      "Epoch 68/300 - Train Loss: 0.1029, Val Loss: 0.0824\n",
      "Epoch 69/300 - Train Loss: 0.1016, Val Loss: 0.0822\n",
      "Epoch 70/300 - Train Loss: 0.1038, Val Loss: 0.0797\n",
      "Epoch 71/300 - Train Loss: 0.1036, Val Loss: 0.0804\n",
      "Epoch 72/300 - Train Loss: 0.1016, Val Loss: 0.0822\n",
      "Epoch 73/300 - Train Loss: 0.1051, Val Loss: 0.0787\n",
      "Epoch 74/300 - Train Loss: 0.1031, Val Loss: 0.0784\n",
      "Epoch 75/300 - Train Loss: 0.1041, Val Loss: 0.0788\n",
      "Epoch 76/300 - Train Loss: 0.1019, Val Loss: 0.0799\n",
      "Epoch 77/300 - Train Loss: 0.1050, Val Loss: 0.0753\n",
      "Epoch 78/300 - Train Loss: 0.1016, Val Loss: 0.0769\n",
      "Epoch 79/300 - Train Loss: 0.1019, Val Loss: 0.0752\n",
      "Epoch 80/300 - Train Loss: 0.1045, Val Loss: 0.0791\n",
      "Epoch 81/300 - Train Loss: 0.1052, Val Loss: 0.0772\n",
      "Epoch 82/300 - Train Loss: 0.1033, Val Loss: 0.0768\n",
      "Epoch 83/300 - Train Loss: 0.1062, Val Loss: 0.0831\n",
      "Epoch 84/300 - Train Loss: 0.1010, Val Loss: 0.0770\n",
      "Epoch 85/300 - Train Loss: 0.1024, Val Loss: 0.0787\n",
      "Epoch 86/300 - Train Loss: 0.1038, Val Loss: 0.0749\n",
      "Epoch 87/300 - Train Loss: 0.1021, Val Loss: 0.0793\n",
      "Epoch 88/300 - Train Loss: 0.1034, Val Loss: 0.0756\n",
      "Epoch 89/300 - Train Loss: 0.1004, Val Loss: 0.0781\n",
      "Epoch 90/300 - Train Loss: 0.1019, Val Loss: 0.0720\n",
      "Epoch 91/300 - Train Loss: 0.1012, Val Loss: 0.0778\n",
      "Epoch 92/300 - Train Loss: 0.0990, Val Loss: 0.0774\n",
      "Epoch 93/300 - Train Loss: 0.1003, Val Loss: 0.0861\n",
      "Epoch 94/300 - Train Loss: 0.1018, Val Loss: 0.0752\n",
      "Epoch 95/300 - Train Loss: 0.0989, Val Loss: 0.0781\n",
      "Epoch 96/300 - Train Loss: 0.1001, Val Loss: 0.0754\n",
      "Epoch 97/300 - Train Loss: 0.1007, Val Loss: 0.0764\n",
      "Epoch 98/300 - Train Loss: 0.1005, Val Loss: 0.0766\n",
      "Epoch 99/300 - Train Loss: 0.1010, Val Loss: 0.0745\n",
      "Epoch 100/300 - Train Loss: 0.1024, Val Loss: 0.0759\n",
      "Epoch 101/300 - Train Loss: 0.1023, Val Loss: 0.0761\n",
      "Epoch 102/300 - Train Loss: 0.0988, Val Loss: 0.0801\n",
      "Epoch 103/300 - Train Loss: 0.1009, Val Loss: 0.0760\n",
      "Epoch 104/300 - Train Loss: 0.0991, Val Loss: 0.0760\n",
      "Epoch 105/300 - Train Loss: 0.0997, Val Loss: 0.0754\n",
      "Epoch 106/300 - Train Loss: 0.1004, Val Loss: 0.0768\n",
      "Epoch 107/300 - Train Loss: 0.0968, Val Loss: 0.0739\n",
      "Epoch 108/300 - Train Loss: 0.0971, Val Loss: 0.0775\n",
      "Epoch 109/300 - Train Loss: 0.0985, Val Loss: 0.0709\n",
      "Epoch 110/300 - Train Loss: 0.0970, Val Loss: 0.0741\n",
      "Epoch 111/300 - Train Loss: 0.0992, Val Loss: 0.0814\n",
      "Epoch 112/300 - Train Loss: 0.1003, Val Loss: 0.0797\n",
      "Epoch 113/300 - Train Loss: 0.0996, Val Loss: 0.0745\n",
      "Epoch 114/300 - Train Loss: 0.0999, Val Loss: 0.0733\n",
      "Epoch 115/300 - Train Loss: 0.0989, Val Loss: 0.0787\n",
      "Epoch 116/300 - Train Loss: 0.0992, Val Loss: 0.0802\n",
      "Epoch 117/300 - Train Loss: 0.0975, Val Loss: 0.0722\n",
      "Epoch 118/300 - Train Loss: 0.0966, Val Loss: 0.0752\n",
      "Epoch 119/300 - Train Loss: 0.1004, Val Loss: 0.0765\n",
      "Epoch 120/300 - Train Loss: 0.1004, Val Loss: 0.0729\n",
      "Epoch 121/300 - Train Loss: 0.0998, Val Loss: 0.0722\n",
      "Epoch 122/300 - Train Loss: 0.1012, Val Loss: 0.0744\n",
      "Epoch 123/300 - Train Loss: 0.0998, Val Loss: 0.0789\n",
      "Epoch 124/300 - Train Loss: 0.0989, Val Loss: 0.0737\n",
      "Epoch 125/300 - Train Loss: 0.0991, Val Loss: 0.0713\n",
      "Epoch 126/300 - Train Loss: 0.1001, Val Loss: 0.0736\n",
      "Epoch 127/300 - Train Loss: 0.0978, Val Loss: 0.0760\n",
      "Epoch 128/300 - Train Loss: 0.0992, Val Loss: 0.0720\n",
      "Epoch 129/300 - Train Loss: 0.1009, Val Loss: 0.0734\n",
      "Epoch 130/300 - Train Loss: 0.0954, Val Loss: 0.0777\n",
      "Epoch 131/300 - Train Loss: 0.0963, Val Loss: 0.0721\n",
      "Epoch 132/300 - Train Loss: 0.1002, Val Loss: 0.0744\n",
      "Epoch 133/300 - Train Loss: 0.0978, Val Loss: 0.0724\n",
      "Epoch 134/300 - Train Loss: 0.0958, Val Loss: 0.0735\n",
      "Epoch 135/300 - Train Loss: 0.0989, Val Loss: 0.0770\n",
      "Epoch 136/300 - Train Loss: 0.0981, Val Loss: 0.0733\n",
      "Epoch 137/300 - Train Loss: 0.0986, Val Loss: 0.0746\n",
      "Epoch 138/300 - Train Loss: 0.0979, Val Loss: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:36:45,788] Trial 17 finished with value: 0.9652245696823423 and parameters: {'F1': 16, 'F2': 16, 'D': 2, 'dropout': 0.6024932299548313, 'learning_rate': 2.9527278818546302e-05, 'batch_size': 32, 'weight_decay': 0.0006228106838368567}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/300 - Train Loss: 0.1007, Val Loss: 0.0791\n",
      "Early stopping at epoch 139\n",
      "Macro F1 Score: 0.9652, Macro Precision: 0.9619, Macro Recall: 0.9688\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 19\n",
      "Training with F1=16, F2=32, D=8, dropout=0.25776653492170887, LR=9.707765444466385e-05, BS=32, WD=9.689907765276655e-05\n",
      "Epoch 1/300 - Train Loss: 0.2633, Val Loss: 0.1179\n",
      "Epoch 2/300 - Train Loss: 0.1139, Val Loss: 0.0941\n",
      "Epoch 3/300 - Train Loss: 0.1043, Val Loss: 0.0805\n",
      "Epoch 4/300 - Train Loss: 0.0997, Val Loss: 0.0776\n",
      "Epoch 5/300 - Train Loss: 0.0945, Val Loss: 0.0770\n",
      "Epoch 6/300 - Train Loss: 0.0923, Val Loss: 0.0774\n",
      "Epoch 7/300 - Train Loss: 0.0926, Val Loss: 0.0759\n",
      "Epoch 8/300 - Train Loss: 0.0914, Val Loss: 0.0770\n",
      "Epoch 9/300 - Train Loss: 0.0884, Val Loss: 0.0733\n",
      "Epoch 10/300 - Train Loss: 0.0867, Val Loss: 0.0746\n",
      "Epoch 11/300 - Train Loss: 0.0848, Val Loss: 0.0764\n",
      "Epoch 12/300 - Train Loss: 0.0817, Val Loss: 0.0742\n",
      "Epoch 13/300 - Train Loss: 0.0834, Val Loss: 0.0786\n",
      "Epoch 14/300 - Train Loss: 0.0807, Val Loss: 0.0789\n",
      "Epoch 15/300 - Train Loss: 0.0810, Val Loss: 0.0757\n",
      "Epoch 16/300 - Train Loss: 0.0833, Val Loss: 0.0753\n",
      "Epoch 17/300 - Train Loss: 0.0807, Val Loss: 0.0752\n",
      "Epoch 18/300 - Train Loss: 0.0817, Val Loss: 0.0810\n",
      "Epoch 19/300 - Train Loss: 0.0777, Val Loss: 0.0701\n",
      "Epoch 20/300 - Train Loss: 0.0754, Val Loss: 0.0723\n",
      "Epoch 21/300 - Train Loss: 0.0767, Val Loss: 0.0743\n",
      "Epoch 22/300 - Train Loss: 0.0756, Val Loss: 0.0766\n",
      "Epoch 23/300 - Train Loss: 0.0771, Val Loss: 0.0708\n",
      "Epoch 24/300 - Train Loss: 0.0760, Val Loss: 0.0692\n",
      "Epoch 25/300 - Train Loss: 0.0756, Val Loss: 0.0686\n",
      "Epoch 26/300 - Train Loss: 0.0751, Val Loss: 0.0709\n",
      "Epoch 27/300 - Train Loss: 0.0752, Val Loss: 0.0766\n",
      "Epoch 28/300 - Train Loss: 0.0747, Val Loss: 0.0698\n",
      "Epoch 29/300 - Train Loss: 0.0729, Val Loss: 0.0743\n",
      "Epoch 30/300 - Train Loss: 0.0710, Val Loss: 0.0737\n",
      "Epoch 31/300 - Train Loss: 0.0716, Val Loss: 0.0734\n",
      "Epoch 32/300 - Train Loss: 0.0730, Val Loss: 0.0743\n",
      "Epoch 33/300 - Train Loss: 0.0699, Val Loss: 0.0760\n",
      "Epoch 34/300 - Train Loss: 0.0701, Val Loss: 0.0709\n",
      "Epoch 35/300 - Train Loss: 0.0679, Val Loss: 0.0685\n",
      "Epoch 36/300 - Train Loss: 0.0698, Val Loss: 0.0678\n",
      "Epoch 37/300 - Train Loss: 0.0705, Val Loss: 0.0771\n",
      "Epoch 38/300 - Train Loss: 0.0690, Val Loss: 0.0684\n",
      "Epoch 39/300 - Train Loss: 0.0690, Val Loss: 0.0718\n",
      "Epoch 40/300 - Train Loss: 0.0686, Val Loss: 0.0749\n",
      "Epoch 41/300 - Train Loss: 0.0655, Val Loss: 0.0778\n",
      "Epoch 42/300 - Train Loss: 0.0683, Val Loss: 0.0734\n",
      "Epoch 43/300 - Train Loss: 0.0686, Val Loss: 0.0722\n",
      "Epoch 44/300 - Train Loss: 0.0636, Val Loss: 0.0662\n",
      "Epoch 45/300 - Train Loss: 0.0685, Val Loss: 0.0705\n",
      "Epoch 46/300 - Train Loss: 0.0648, Val Loss: 0.0721\n",
      "Epoch 47/300 - Train Loss: 0.0647, Val Loss: 0.0700\n",
      "Epoch 48/300 - Train Loss: 0.0642, Val Loss: 0.0702\n",
      "Epoch 49/300 - Train Loss: 0.0642, Val Loss: 0.0699\n",
      "Epoch 50/300 - Train Loss: 0.0655, Val Loss: 0.0704\n",
      "Epoch 51/300 - Train Loss: 0.0637, Val Loss: 0.0700\n",
      "Epoch 52/300 - Train Loss: 0.0647, Val Loss: 0.0750\n",
      "Epoch 53/300 - Train Loss: 0.0627, Val Loss: 0.0743\n",
      "Epoch 54/300 - Train Loss: 0.0648, Val Loss: 0.0752\n",
      "Epoch 55/300 - Train Loss: 0.0637, Val Loss: 0.0701\n",
      "Epoch 56/300 - Train Loss: 0.0651, Val Loss: 0.0703\n",
      "Epoch 57/300 - Train Loss: 0.0620, Val Loss: 0.0753\n",
      "Epoch 58/300 - Train Loss: 0.0623, Val Loss: 0.0716\n",
      "Epoch 59/300 - Train Loss: 0.0611, Val Loss: 0.0742\n",
      "Epoch 60/300 - Train Loss: 0.0612, Val Loss: 0.0719\n",
      "Epoch 61/300 - Train Loss: 0.0609, Val Loss: 0.0817\n",
      "Epoch 62/300 - Train Loss: 0.0627, Val Loss: 0.0702\n",
      "Epoch 63/300 - Train Loss: 0.0611, Val Loss: 0.0795\n",
      "Epoch 64/300 - Train Loss: 0.0600, Val Loss: 0.0708\n",
      "Epoch 65/300 - Train Loss: 0.0601, Val Loss: 0.0716\n",
      "Epoch 66/300 - Train Loss: 0.0588, Val Loss: 0.0746\n",
      "Epoch 67/300 - Train Loss: 0.0596, Val Loss: 0.0705\n",
      "Epoch 68/300 - Train Loss: 0.0586, Val Loss: 0.0708\n",
      "Epoch 69/300 - Train Loss: 0.0596, Val Loss: 0.0747\n",
      "Epoch 70/300 - Train Loss: 0.0610, Val Loss: 0.0788\n",
      "Epoch 71/300 - Train Loss: 0.0590, Val Loss: 0.0715\n",
      "Epoch 72/300 - Train Loss: 0.0581, Val Loss: 0.0715\n",
      "Epoch 73/300 - Train Loss: 0.0574, Val Loss: 0.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:41:30,424] Trial 18 finished with value: 0.9636623917999275 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.25776653492170887, 'learning_rate': 9.707765444466385e-05, 'batch_size': 32, 'weight_decay': 9.689907765276655e-05}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300 - Train Loss: 0.0568, Val Loss: 0.0765\n",
      "Early stopping at epoch 74\n",
      "Macro F1 Score: 0.9637, Macro Precision: 0.9579, Macro Recall: 0.9698\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 20\n",
      "Training with F1=16, F2=8, D=4, dropout=0.4301999218082358, LR=0.00032807263074253096, BS=128, WD=2.0991357472945348e-05\n",
      "Epoch 1/300 - Train Loss: 0.3770, Val Loss: 0.1559\n",
      "Epoch 2/300 - Train Loss: 0.1372, Val Loss: 0.1083\n",
      "Epoch 3/300 - Train Loss: 0.1137, Val Loss: 0.0933\n",
      "Epoch 4/300 - Train Loss: 0.1022, Val Loss: 0.0878\n",
      "Epoch 5/300 - Train Loss: 0.0983, Val Loss: 0.0867\n",
      "Epoch 6/300 - Train Loss: 0.0968, Val Loss: 0.0863\n",
      "Epoch 7/300 - Train Loss: 0.0936, Val Loss: 0.0820\n",
      "Epoch 8/300 - Train Loss: 0.0935, Val Loss: 0.0805\n",
      "Epoch 9/300 - Train Loss: 0.0902, Val Loss: 0.0800\n",
      "Epoch 10/300 - Train Loss: 0.0889, Val Loss: 0.0862\n",
      "Epoch 11/300 - Train Loss: 0.0898, Val Loss: 0.0771\n",
      "Epoch 12/300 - Train Loss: 0.0869, Val Loss: 0.0756\n",
      "Epoch 13/300 - Train Loss: 0.0890, Val Loss: 0.0745\n",
      "Epoch 14/300 - Train Loss: 0.0866, Val Loss: 0.0756\n",
      "Epoch 15/300 - Train Loss: 0.0845, Val Loss: 0.0743\n",
      "Epoch 16/300 - Train Loss: 0.0825, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.0827, Val Loss: 0.0734\n",
      "Epoch 18/300 - Train Loss: 0.0819, Val Loss: 0.0715\n",
      "Epoch 19/300 - Train Loss: 0.0823, Val Loss: 0.0745\n",
      "Epoch 20/300 - Train Loss: 0.0815, Val Loss: 0.0758\n",
      "Epoch 21/300 - Train Loss: 0.0804, Val Loss: 0.0728\n",
      "Epoch 22/300 - Train Loss: 0.0805, Val Loss: 0.0756\n",
      "Epoch 23/300 - Train Loss: 0.0819, Val Loss: 0.0748\n",
      "Epoch 24/300 - Train Loss: 0.0801, Val Loss: 0.0720\n",
      "Epoch 25/300 - Train Loss: 0.0817, Val Loss: 0.0737\n",
      "Epoch 26/300 - Train Loss: 0.0810, Val Loss: 0.0740\n",
      "Epoch 27/300 - Train Loss: 0.0788, Val Loss: 0.0699\n",
      "Epoch 28/300 - Train Loss: 0.0798, Val Loss: 0.0704\n",
      "Epoch 29/300 - Train Loss: 0.0783, Val Loss: 0.0727\n",
      "Epoch 30/300 - Train Loss: 0.0790, Val Loss: 0.0725\n",
      "Epoch 31/300 - Train Loss: 0.0783, Val Loss: 0.0741\n",
      "Epoch 32/300 - Train Loss: 0.0774, Val Loss: 0.0729\n",
      "Epoch 33/300 - Train Loss: 0.0769, Val Loss: 0.0737\n",
      "Epoch 34/300 - Train Loss: 0.0777, Val Loss: 0.0769\n",
      "Epoch 35/300 - Train Loss: 0.0768, Val Loss: 0.0703\n",
      "Epoch 36/300 - Train Loss: 0.0775, Val Loss: 0.0731\n",
      "Epoch 37/300 - Train Loss: 0.0764, Val Loss: 0.0712\n",
      "Epoch 38/300 - Train Loss: 0.0786, Val Loss: 0.0732\n",
      "Epoch 39/300 - Train Loss: 0.0773, Val Loss: 0.0735\n",
      "Epoch 40/300 - Train Loss: 0.0778, Val Loss: 0.0715\n",
      "Epoch 41/300 - Train Loss: 0.0757, Val Loss: 0.0702\n",
      "Epoch 42/300 - Train Loss: 0.0770, Val Loss: 0.0753\n",
      "Epoch 43/300 - Train Loss: 0.0751, Val Loss: 0.0726\n",
      "Epoch 44/300 - Train Loss: 0.0757, Val Loss: 0.0697\n",
      "Epoch 45/300 - Train Loss: 0.0771, Val Loss: 0.0724\n",
      "Epoch 46/300 - Train Loss: 0.0754, Val Loss: 0.0698\n",
      "Epoch 47/300 - Train Loss: 0.0749, Val Loss: 0.0697\n",
      "Epoch 48/300 - Train Loss: 0.0745, Val Loss: 0.0733\n",
      "Epoch 49/300 - Train Loss: 0.0747, Val Loss: 0.0700\n",
      "Epoch 50/300 - Train Loss: 0.0768, Val Loss: 0.0709\n",
      "Epoch 51/300 - Train Loss: 0.0753, Val Loss: 0.0718\n",
      "Epoch 52/300 - Train Loss: 0.0751, Val Loss: 0.0720\n",
      "Epoch 53/300 - Train Loss: 0.0757, Val Loss: 0.0728\n",
      "Epoch 54/300 - Train Loss: 0.0741, Val Loss: 0.0714\n",
      "Epoch 55/300 - Train Loss: 0.0745, Val Loss: 0.0710\n",
      "Epoch 56/300 - Train Loss: 0.0753, Val Loss: 0.0704\n",
      "Epoch 57/300 - Train Loss: 0.0754, Val Loss: 0.0708\n",
      "Epoch 58/300 - Train Loss: 0.0737, Val Loss: 0.0717\n",
      "Epoch 59/300 - Train Loss: 0.0730, Val Loss: 0.0705\n",
      "Epoch 60/300 - Train Loss: 0.0750, Val Loss: 0.0719\n",
      "Epoch 61/300 - Train Loss: 0.0734, Val Loss: 0.0707\n",
      "Epoch 62/300 - Train Loss: 0.0740, Val Loss: 0.0693\n",
      "Epoch 63/300 - Train Loss: 0.0749, Val Loss: 0.0690\n",
      "Epoch 64/300 - Train Loss: 0.0740, Val Loss: 0.0711\n",
      "Epoch 65/300 - Train Loss: 0.0756, Val Loss: 0.0718\n",
      "Epoch 66/300 - Train Loss: 0.0721, Val Loss: 0.0725\n",
      "Epoch 67/300 - Train Loss: 0.0727, Val Loss: 0.0702\n",
      "Epoch 68/300 - Train Loss: 0.0730, Val Loss: 0.0686\n",
      "Epoch 69/300 - Train Loss: 0.0733, Val Loss: 0.0702\n",
      "Epoch 70/300 - Train Loss: 0.0737, Val Loss: 0.0704\n",
      "Epoch 71/300 - Train Loss: 0.0723, Val Loss: 0.0704\n",
      "Epoch 72/300 - Train Loss: 0.0726, Val Loss: 0.0745\n",
      "Epoch 73/300 - Train Loss: 0.0723, Val Loss: 0.0696\n",
      "Epoch 74/300 - Train Loss: 0.0720, Val Loss: 0.0696\n",
      "Epoch 75/300 - Train Loss: 0.0735, Val Loss: 0.0701\n",
      "Epoch 76/300 - Train Loss: 0.0717, Val Loss: 0.0695\n",
      "Epoch 77/300 - Train Loss: 0.0716, Val Loss: 0.0707\n",
      "Epoch 78/300 - Train Loss: 0.0717, Val Loss: 0.0728\n",
      "Epoch 79/300 - Train Loss: 0.0714, Val Loss: 0.0697\n",
      "Epoch 80/300 - Train Loss: 0.0713, Val Loss: 0.0739\n",
      "Epoch 81/300 - Train Loss: 0.0717, Val Loss: 0.0731\n",
      "Epoch 82/300 - Train Loss: 0.0742, Val Loss: 0.0703\n",
      "Epoch 83/300 - Train Loss: 0.0723, Val Loss: 0.0711\n",
      "Epoch 84/300 - Train Loss: 0.0719, Val Loss: 0.0707\n",
      "Epoch 85/300 - Train Loss: 0.0726, Val Loss: 0.0690\n",
      "Epoch 86/300 - Train Loss: 0.0706, Val Loss: 0.0712\n",
      "Epoch 87/300 - Train Loss: 0.0723, Val Loss: 0.0696\n",
      "Epoch 88/300 - Train Loss: 0.0715, Val Loss: 0.0689\n",
      "Epoch 89/300 - Train Loss: 0.0715, Val Loss: 0.0719\n",
      "Epoch 90/300 - Train Loss: 0.0720, Val Loss: 0.0716\n",
      "Epoch 91/300 - Train Loss: 0.0721, Val Loss: 0.0752\n",
      "Epoch 92/300 - Train Loss: 0.0722, Val Loss: 0.0700\n",
      "Epoch 93/300 - Train Loss: 0.0698, Val Loss: 0.0713\n",
      "Epoch 94/300 - Train Loss: 0.0722, Val Loss: 0.0675\n",
      "Epoch 95/300 - Train Loss: 0.0719, Val Loss: 0.0717\n",
      "Epoch 96/300 - Train Loss: 0.0698, Val Loss: 0.0692\n",
      "Epoch 97/300 - Train Loss: 0.0711, Val Loss: 0.0692\n",
      "Epoch 98/300 - Train Loss: 0.0705, Val Loss: 0.0690\n",
      "Epoch 99/300 - Train Loss: 0.0712, Val Loss: 0.0694\n",
      "Epoch 100/300 - Train Loss: 0.0719, Val Loss: 0.0718\n",
      "Epoch 101/300 - Train Loss: 0.0706, Val Loss: 0.0693\n",
      "Epoch 102/300 - Train Loss: 0.0696, Val Loss: 0.0699\n",
      "Epoch 103/300 - Train Loss: 0.0694, Val Loss: 0.0694\n",
      "Epoch 104/300 - Train Loss: 0.0710, Val Loss: 0.0718\n",
      "Epoch 105/300 - Train Loss: 0.0717, Val Loss: 0.0698\n",
      "Epoch 106/300 - Train Loss: 0.0708, Val Loss: 0.0716\n",
      "Epoch 107/300 - Train Loss: 0.0712, Val Loss: 0.0708\n",
      "Epoch 108/300 - Train Loss: 0.0710, Val Loss: 0.0741\n",
      "Epoch 109/300 - Train Loss: 0.0713, Val Loss: 0.0683\n",
      "Epoch 110/300 - Train Loss: 0.0697, Val Loss: 0.0716\n",
      "Epoch 111/300 - Train Loss: 0.0708, Val Loss: 0.0685\n",
      "Epoch 112/300 - Train Loss: 0.0708, Val Loss: 0.0684\n",
      "Epoch 113/300 - Train Loss: 0.0702, Val Loss: 0.0683\n",
      "Epoch 114/300 - Train Loss: 0.0708, Val Loss: 0.0692\n",
      "Epoch 115/300 - Train Loss: 0.0681, Val Loss: 0.0710\n",
      "Epoch 116/300 - Train Loss: 0.0690, Val Loss: 0.0697\n",
      "Epoch 117/300 - Train Loss: 0.0689, Val Loss: 0.0720\n",
      "Epoch 118/300 - Train Loss: 0.0698, Val Loss: 0.0712\n",
      "Epoch 119/300 - Train Loss: 0.0704, Val Loss: 0.0682\n",
      "Epoch 120/300 - Train Loss: 0.0687, Val Loss: 0.0736\n",
      "Epoch 121/300 - Train Loss: 0.0693, Val Loss: 0.0672\n",
      "Epoch 122/300 - Train Loss: 0.0690, Val Loss: 0.0701\n",
      "Epoch 123/300 - Train Loss: 0.0681, Val Loss: 0.0710\n",
      "Epoch 124/300 - Train Loss: 0.0670, Val Loss: 0.0718\n",
      "Epoch 125/300 - Train Loss: 0.0688, Val Loss: 0.0670\n",
      "Epoch 126/300 - Train Loss: 0.0689, Val Loss: 0.0675\n",
      "Epoch 127/300 - Train Loss: 0.0696, Val Loss: 0.0682\n",
      "Epoch 128/300 - Train Loss: 0.0680, Val Loss: 0.0693\n",
      "Epoch 129/300 - Train Loss: 0.0694, Val Loss: 0.0718\n",
      "Epoch 130/300 - Train Loss: 0.0702, Val Loss: 0.0710\n",
      "Epoch 131/300 - Train Loss: 0.0688, Val Loss: 0.0720\n",
      "Epoch 132/300 - Train Loss: 0.0696, Val Loss: 0.0706\n",
      "Epoch 133/300 - Train Loss: 0.0683, Val Loss: 0.0690\n",
      "Epoch 134/300 - Train Loss: 0.0700, Val Loss: 0.0693\n",
      "Epoch 135/300 - Train Loss: 0.0690, Val Loss: 0.0711\n",
      "Epoch 136/300 - Train Loss: 0.0705, Val Loss: 0.0768\n",
      "Epoch 137/300 - Train Loss: 0.0695, Val Loss: 0.0717\n",
      "Epoch 138/300 - Train Loss: 0.0679, Val Loss: 0.0689\n",
      "Epoch 139/300 - Train Loss: 0.0677, Val Loss: 0.0693\n",
      "Epoch 140/300 - Train Loss: 0.0664, Val Loss: 0.0704\n",
      "Epoch 141/300 - Train Loss: 0.0682, Val Loss: 0.0695\n",
      "Epoch 142/300 - Train Loss: 0.0680, Val Loss: 0.0704\n",
      "Epoch 143/300 - Train Loss: 0.0679, Val Loss: 0.0697\n",
      "Epoch 144/300 - Train Loss: 0.0681, Val Loss: 0.0732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0695, Val Loss: 0.0718\n",
      "Epoch 146/300 - Train Loss: 0.0684, Val Loss: 0.0733\n",
      "Epoch 147/300 - Train Loss: 0.0697, Val Loss: 0.0698\n",
      "Epoch 148/300 - Train Loss: 0.0664, Val Loss: 0.0704\n",
      "Epoch 149/300 - Train Loss: 0.0662, Val Loss: 0.0703\n",
      "Epoch 150/300 - Train Loss: 0.0670, Val Loss: 0.0695\n",
      "Epoch 151/300 - Train Loss: 0.0695, Val Loss: 0.0698\n",
      "Epoch 152/300 - Train Loss: 0.0680, Val Loss: 0.0703\n",
      "Epoch 153/300 - Train Loss: 0.0686, Val Loss: 0.0689\n",
      "Epoch 154/300 - Train Loss: 0.0690, Val Loss: 0.0712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:46:09,809] Trial 19 finished with value: 0.9650764242744697 and parameters: {'F1': 16, 'F2': 8, 'D': 4, 'dropout': 0.4301999218082358, 'learning_rate': 0.00032807263074253096, 'batch_size': 128, 'weight_decay': 2.0991357472945348e-05}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 155/300 - Train Loss: 0.0676, Val Loss: 0.0703\n",
      "Early stopping at epoch 155\n",
      "Macro F1 Score: 0.9651, Macro Precision: 0.9590, Macro Recall: 0.9715\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 21\n",
      "Training with F1=32, F2=32, D=2, dropout=0.34027163306294245, LR=5.786154693998908e-05, BS=32, WD=0.00024833251941456434\n",
      "Epoch 1/300 - Train Loss: 0.3855, Val Loss: 0.1805\n",
      "Epoch 2/300 - Train Loss: 0.1551, Val Loss: 0.1178\n",
      "Epoch 3/300 - Train Loss: 0.1246, Val Loss: 0.1099\n",
      "Epoch 4/300 - Train Loss: 0.1123, Val Loss: 0.1041\n",
      "Epoch 5/300 - Train Loss: 0.1095, Val Loss: 0.0916\n",
      "Epoch 6/300 - Train Loss: 0.1053, Val Loss: 0.0867\n",
      "Epoch 7/300 - Train Loss: 0.1027, Val Loss: 0.0871\n",
      "Epoch 8/300 - Train Loss: 0.0999, Val Loss: 0.0865\n",
      "Epoch 9/300 - Train Loss: 0.0976, Val Loss: 0.0827\n",
      "Epoch 10/300 - Train Loss: 0.0984, Val Loss: 0.0787\n",
      "Epoch 11/300 - Train Loss: 0.0958, Val Loss: 0.0853\n",
      "Epoch 12/300 - Train Loss: 0.0936, Val Loss: 0.0807\n",
      "Epoch 13/300 - Train Loss: 0.0942, Val Loss: 0.0848\n",
      "Epoch 14/300 - Train Loss: 0.0925, Val Loss: 0.0755\n",
      "Epoch 15/300 - Train Loss: 0.0895, Val Loss: 0.0753\n",
      "Epoch 16/300 - Train Loss: 0.0906, Val Loss: 0.0844\n",
      "Epoch 17/300 - Train Loss: 0.0892, Val Loss: 0.0828\n",
      "Epoch 18/300 - Train Loss: 0.0887, Val Loss: 0.0776\n",
      "Epoch 19/300 - Train Loss: 0.0884, Val Loss: 0.0754\n",
      "Epoch 20/300 - Train Loss: 0.0869, Val Loss: 0.0742\n",
      "Epoch 21/300 - Train Loss: 0.0866, Val Loss: 0.0804\n",
      "Epoch 22/300 - Train Loss: 0.0874, Val Loss: 0.0754\n",
      "Epoch 23/300 - Train Loss: 0.0860, Val Loss: 0.0750\n",
      "Epoch 24/300 - Train Loss: 0.0851, Val Loss: 0.0771\n",
      "Epoch 25/300 - Train Loss: 0.0827, Val Loss: 0.0731\n",
      "Epoch 26/300 - Train Loss: 0.0837, Val Loss: 0.0735\n",
      "Epoch 27/300 - Train Loss: 0.0836, Val Loss: 0.0748\n",
      "Epoch 28/300 - Train Loss: 0.0854, Val Loss: 0.0738\n",
      "Epoch 29/300 - Train Loss: 0.0855, Val Loss: 0.0739\n",
      "Epoch 30/300 - Train Loss: 0.0833, Val Loss: 0.0727\n",
      "Epoch 31/300 - Train Loss: 0.0819, Val Loss: 0.0779\n",
      "Epoch 32/300 - Train Loss: 0.0842, Val Loss: 0.0742\n",
      "Epoch 33/300 - Train Loss: 0.0822, Val Loss: 0.0726\n",
      "Epoch 34/300 - Train Loss: 0.0808, Val Loss: 0.0704\n",
      "Epoch 35/300 - Train Loss: 0.0814, Val Loss: 0.0736\n",
      "Epoch 36/300 - Train Loss: 0.0830, Val Loss: 0.0747\n",
      "Epoch 37/300 - Train Loss: 0.0832, Val Loss: 0.0744\n",
      "Epoch 38/300 - Train Loss: 0.0791, Val Loss: 0.0739\n",
      "Epoch 39/300 - Train Loss: 0.0839, Val Loss: 0.0725\n",
      "Epoch 40/300 - Train Loss: 0.0802, Val Loss: 0.0751\n",
      "Epoch 41/300 - Train Loss: 0.0801, Val Loss: 0.0756\n",
      "Epoch 42/300 - Train Loss: 0.0808, Val Loss: 0.0746\n",
      "Epoch 43/300 - Train Loss: 0.0799, Val Loss: 0.0712\n",
      "Epoch 44/300 - Train Loss: 0.0794, Val Loss: 0.0763\n",
      "Epoch 45/300 - Train Loss: 0.0807, Val Loss: 0.0745\n",
      "Epoch 46/300 - Train Loss: 0.0807, Val Loss: 0.0735\n",
      "Epoch 47/300 - Train Loss: 0.0773, Val Loss: 0.0726\n",
      "Epoch 48/300 - Train Loss: 0.0781, Val Loss: 0.0718\n",
      "Epoch 49/300 - Train Loss: 0.0796, Val Loss: 0.0750\n",
      "Epoch 50/300 - Train Loss: 0.0769, Val Loss: 0.0727\n",
      "Epoch 51/300 - Train Loss: 0.0781, Val Loss: 0.0734\n",
      "Epoch 52/300 - Train Loss: 0.0755, Val Loss: 0.0760\n",
      "Epoch 53/300 - Train Loss: 0.0777, Val Loss: 0.0729\n",
      "Epoch 54/300 - Train Loss: 0.0775, Val Loss: 0.0728\n",
      "Epoch 55/300 - Train Loss: 0.0789, Val Loss: 0.0701\n",
      "Epoch 56/300 - Train Loss: 0.0776, Val Loss: 0.0729\n",
      "Epoch 57/300 - Train Loss: 0.0775, Val Loss: 0.0748\n",
      "Epoch 58/300 - Train Loss: 0.0759, Val Loss: 0.0743\n",
      "Epoch 59/300 - Train Loss: 0.0761, Val Loss: 0.0735\n",
      "Epoch 60/300 - Train Loss: 0.0755, Val Loss: 0.0740\n",
      "Epoch 61/300 - Train Loss: 0.0760, Val Loss: 0.0743\n",
      "Epoch 62/300 - Train Loss: 0.0757, Val Loss: 0.0737\n",
      "Epoch 63/300 - Train Loss: 0.0754, Val Loss: 0.0727\n",
      "Epoch 64/300 - Train Loss: 0.0762, Val Loss: 0.0730\n",
      "Epoch 65/300 - Train Loss: 0.0756, Val Loss: 0.0755\n",
      "Epoch 66/300 - Train Loss: 0.0766, Val Loss: 0.0745\n",
      "Epoch 67/300 - Train Loss: 0.0739, Val Loss: 0.0715\n",
      "Epoch 68/300 - Train Loss: 0.0723, Val Loss: 0.0710\n",
      "Epoch 69/300 - Train Loss: 0.0738, Val Loss: 0.0720\n",
      "Epoch 70/300 - Train Loss: 0.0733, Val Loss: 0.0724\n",
      "Epoch 71/300 - Train Loss: 0.0748, Val Loss: 0.0716\n",
      "Epoch 72/300 - Train Loss: 0.0736, Val Loss: 0.0698\n",
      "Epoch 73/300 - Train Loss: 0.0739, Val Loss: 0.0733\n",
      "Epoch 74/300 - Train Loss: 0.0738, Val Loss: 0.0703\n",
      "Epoch 75/300 - Train Loss: 0.0729, Val Loss: 0.0695\n",
      "Epoch 76/300 - Train Loss: 0.0718, Val Loss: 0.0692\n",
      "Epoch 77/300 - Train Loss: 0.0708, Val Loss: 0.0693\n",
      "Epoch 78/300 - Train Loss: 0.0729, Val Loss: 0.0719\n",
      "Epoch 79/300 - Train Loss: 0.0719, Val Loss: 0.0716\n",
      "Epoch 80/300 - Train Loss: 0.0729, Val Loss: 0.0694\n",
      "Epoch 81/300 - Train Loss: 0.0714, Val Loss: 0.0701\n",
      "Epoch 82/300 - Train Loss: 0.0716, Val Loss: 0.0700\n",
      "Epoch 83/300 - Train Loss: 0.0700, Val Loss: 0.0682\n",
      "Epoch 84/300 - Train Loss: 0.0735, Val Loss: 0.0739\n",
      "Epoch 85/300 - Train Loss: 0.0704, Val Loss: 0.0720\n",
      "Epoch 86/300 - Train Loss: 0.0717, Val Loss: 0.0735\n",
      "Epoch 87/300 - Train Loss: 0.0716, Val Loss: 0.0726\n",
      "Epoch 88/300 - Train Loss: 0.0729, Val Loss: 0.0695\n",
      "Epoch 89/300 - Train Loss: 0.0707, Val Loss: 0.0693\n",
      "Epoch 90/300 - Train Loss: 0.0723, Val Loss: 0.0718\n",
      "Epoch 91/300 - Train Loss: 0.0715, Val Loss: 0.0673\n",
      "Epoch 92/300 - Train Loss: 0.0705, Val Loss: 0.0737\n",
      "Epoch 93/300 - Train Loss: 0.0724, Val Loss: 0.0719\n",
      "Epoch 94/300 - Train Loss: 0.0711, Val Loss: 0.0722\n",
      "Epoch 95/300 - Train Loss: 0.0699, Val Loss: 0.0705\n",
      "Epoch 96/300 - Train Loss: 0.0679, Val Loss: 0.0677\n",
      "Epoch 97/300 - Train Loss: 0.0696, Val Loss: 0.0739\n",
      "Epoch 98/300 - Train Loss: 0.0704, Val Loss: 0.0701\n",
      "Epoch 99/300 - Train Loss: 0.0704, Val Loss: 0.0711\n",
      "Epoch 100/300 - Train Loss: 0.0714, Val Loss: 0.0733\n",
      "Epoch 101/300 - Train Loss: 0.0727, Val Loss: 0.0697\n",
      "Epoch 102/300 - Train Loss: 0.0718, Val Loss: 0.0733\n",
      "Epoch 103/300 - Train Loss: 0.0712, Val Loss: 0.0700\n",
      "Epoch 104/300 - Train Loss: 0.0713, Val Loss: 0.0733\n",
      "Epoch 105/300 - Train Loss: 0.0676, Val Loss: 0.0735\n",
      "Epoch 106/300 - Train Loss: 0.0693, Val Loss: 0.0712\n",
      "Epoch 107/300 - Train Loss: 0.0726, Val Loss: 0.0691\n",
      "Epoch 108/300 - Train Loss: 0.0693, Val Loss: 0.0707\n",
      "Epoch 109/300 - Train Loss: 0.0687, Val Loss: 0.0707\n",
      "Epoch 110/300 - Train Loss: 0.0678, Val Loss: 0.0680\n",
      "Epoch 111/300 - Train Loss: 0.0680, Val Loss: 0.0675\n",
      "Epoch 112/300 - Train Loss: 0.0722, Val Loss: 0.0782\n",
      "Epoch 113/300 - Train Loss: 0.0698, Val Loss: 0.0734\n",
      "Epoch 114/300 - Train Loss: 0.0677, Val Loss: 0.0735\n",
      "Epoch 115/300 - Train Loss: 0.0650, Val Loss: 0.0689\n",
      "Epoch 116/300 - Train Loss: 0.0673, Val Loss: 0.0685\n",
      "Epoch 117/300 - Train Loss: 0.0668, Val Loss: 0.0680\n",
      "Epoch 118/300 - Train Loss: 0.0679, Val Loss: 0.0694\n",
      "Epoch 119/300 - Train Loss: 0.0700, Val Loss: 0.0763\n",
      "Epoch 120/300 - Train Loss: 0.0669, Val Loss: 0.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:51:40,820] Trial 20 finished with value: 0.9617519982271419 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.34027163306294245, 'learning_rate': 5.786154693998908e-05, 'batch_size': 32, 'weight_decay': 0.00024833251941456434}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 121/300 - Train Loss: 0.0686, Val Loss: 0.0697\n",
      "Early stopping at epoch 121\n",
      "Macro F1 Score: 0.9618, Macro Precision: 0.9499, Macro Recall: 0.9750\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 22\n",
      "Training with F1=8, F2=8, D=4, dropout=0.1758130806314788, LR=1.1471400194518907e-05, BS=64, WD=0.00015531401052541069\n",
      "Epoch 1/300 - Train Loss: 1.0560, Val Loss: 0.9688\n",
      "Epoch 2/300 - Train Loss: 0.8640, Val Loss: 0.7595\n",
      "Epoch 3/300 - Train Loss: 0.6679, Val Loss: 0.5998\n",
      "Epoch 4/300 - Train Loss: 0.5379, Val Loss: 0.4915\n",
      "Epoch 5/300 - Train Loss: 0.4626, Val Loss: 0.4277\n",
      "Epoch 6/300 - Train Loss: 0.4134, Val Loss: 0.3891\n",
      "Epoch 7/300 - Train Loss: 0.3751, Val Loss: 0.3522\n",
      "Epoch 8/300 - Train Loss: 0.3491, Val Loss: 0.3239\n",
      "Epoch 9/300 - Train Loss: 0.3240, Val Loss: 0.3014\n",
      "Epoch 10/300 - Train Loss: 0.3029, Val Loss: 0.2808\n",
      "Epoch 11/300 - Train Loss: 0.2857, Val Loss: 0.2638\n",
      "Epoch 12/300 - Train Loss: 0.2687, Val Loss: 0.2500\n",
      "Epoch 13/300 - Train Loss: 0.2556, Val Loss: 0.2388\n",
      "Epoch 14/300 - Train Loss: 0.2440, Val Loss: 0.2223\n",
      "Epoch 15/300 - Train Loss: 0.2325, Val Loss: 0.2144\n",
      "Epoch 16/300 - Train Loss: 0.2244, Val Loss: 0.2045\n",
      "Epoch 17/300 - Train Loss: 0.2163, Val Loss: 0.1947\n",
      "Epoch 18/300 - Train Loss: 0.2110, Val Loss: 0.1893\n",
      "Epoch 19/300 - Train Loss: 0.2016, Val Loss: 0.1810\n",
      "Epoch 20/300 - Train Loss: 0.1947, Val Loss: 0.1743\n",
      "Epoch 21/300 - Train Loss: 0.1864, Val Loss: 0.1656\n",
      "Epoch 22/300 - Train Loss: 0.1817, Val Loss: 0.1570\n",
      "Epoch 23/300 - Train Loss: 0.1729, Val Loss: 0.1489\n",
      "Epoch 24/300 - Train Loss: 0.1667, Val Loss: 0.1411\n",
      "Epoch 25/300 - Train Loss: 0.1601, Val Loss: 0.1368\n",
      "Epoch 26/300 - Train Loss: 0.1522, Val Loss: 0.1289\n",
      "Epoch 27/300 - Train Loss: 0.1460, Val Loss: 0.1267\n",
      "Epoch 28/300 - Train Loss: 0.1397, Val Loss: 0.1192\n",
      "Epoch 29/300 - Train Loss: 0.1358, Val Loss: 0.1164\n",
      "Epoch 30/300 - Train Loss: 0.1302, Val Loss: 0.1118\n",
      "Epoch 31/300 - Train Loss: 0.1272, Val Loss: 0.1099\n",
      "Epoch 32/300 - Train Loss: 0.1224, Val Loss: 0.1061\n",
      "Epoch 33/300 - Train Loss: 0.1199, Val Loss: 0.1047\n",
      "Epoch 34/300 - Train Loss: 0.1171, Val Loss: 0.1035\n",
      "Epoch 35/300 - Train Loss: 0.1143, Val Loss: 0.0997\n",
      "Epoch 36/300 - Train Loss: 0.1132, Val Loss: 0.1001\n",
      "Epoch 37/300 - Train Loss: 0.1105, Val Loss: 0.0983\n",
      "Epoch 38/300 - Train Loss: 0.1081, Val Loss: 0.0948\n",
      "Epoch 39/300 - Train Loss: 0.1080, Val Loss: 0.0950\n",
      "Epoch 40/300 - Train Loss: 0.1060, Val Loss: 0.0952\n",
      "Epoch 41/300 - Train Loss: 0.1064, Val Loss: 0.0933\n",
      "Epoch 42/300 - Train Loss: 0.1033, Val Loss: 0.0934\n",
      "Epoch 43/300 - Train Loss: 0.1030, Val Loss: 0.0926\n",
      "Epoch 44/300 - Train Loss: 0.1026, Val Loss: 0.0931\n",
      "Epoch 45/300 - Train Loss: 0.1013, Val Loss: 0.0928\n",
      "Epoch 46/300 - Train Loss: 0.1015, Val Loss: 0.0902\n",
      "Epoch 47/300 - Train Loss: 0.0986, Val Loss: 0.0909\n",
      "Epoch 48/300 - Train Loss: 0.1010, Val Loss: 0.0900\n",
      "Epoch 49/300 - Train Loss: 0.0989, Val Loss: 0.0892\n",
      "Epoch 50/300 - Train Loss: 0.0996, Val Loss: 0.0911\n",
      "Epoch 51/300 - Train Loss: 0.0998, Val Loss: 0.0918\n",
      "Epoch 52/300 - Train Loss: 0.0999, Val Loss: 0.0915\n",
      "Epoch 53/300 - Train Loss: 0.0957, Val Loss: 0.0909\n",
      "Epoch 54/300 - Train Loss: 0.0968, Val Loss: 0.0890\n",
      "Epoch 55/300 - Train Loss: 0.0955, Val Loss: 0.0899\n",
      "Epoch 56/300 - Train Loss: 0.0961, Val Loss: 0.0904\n",
      "Epoch 57/300 - Train Loss: 0.0968, Val Loss: 0.0870\n",
      "Epoch 58/300 - Train Loss: 0.0975, Val Loss: 0.0905\n",
      "Epoch 59/300 - Train Loss: 0.0944, Val Loss: 0.0875\n",
      "Epoch 60/300 - Train Loss: 0.0930, Val Loss: 0.0875\n",
      "Epoch 61/300 - Train Loss: 0.0940, Val Loss: 0.0870\n",
      "Epoch 62/300 - Train Loss: 0.0943, Val Loss: 0.0866\n",
      "Epoch 63/300 - Train Loss: 0.0954, Val Loss: 0.0874\n",
      "Epoch 64/300 - Train Loss: 0.0935, Val Loss: 0.0853\n",
      "Epoch 65/300 - Train Loss: 0.0946, Val Loss: 0.0855\n",
      "Epoch 66/300 - Train Loss: 0.0930, Val Loss: 0.0864\n",
      "Epoch 67/300 - Train Loss: 0.0948, Val Loss: 0.0863\n",
      "Epoch 68/300 - Train Loss: 0.0922, Val Loss: 0.0865\n",
      "Epoch 69/300 - Train Loss: 0.0929, Val Loss: 0.0861\n",
      "Epoch 70/300 - Train Loss: 0.0937, Val Loss: 0.0858\n",
      "Epoch 71/300 - Train Loss: 0.0918, Val Loss: 0.0912\n",
      "Epoch 72/300 - Train Loss: 0.0920, Val Loss: 0.0853\n",
      "Epoch 73/300 - Train Loss: 0.0904, Val Loss: 0.0870\n",
      "Epoch 74/300 - Train Loss: 0.0921, Val Loss: 0.0856\n",
      "Epoch 75/300 - Train Loss: 0.0917, Val Loss: 0.0838\n",
      "Epoch 76/300 - Train Loss: 0.0902, Val Loss: 0.0845\n",
      "Epoch 77/300 - Train Loss: 0.0907, Val Loss: 0.0844\n",
      "Epoch 78/300 - Train Loss: 0.0913, Val Loss: 0.0851\n",
      "Epoch 79/300 - Train Loss: 0.0893, Val Loss: 0.0848\n",
      "Epoch 80/300 - Train Loss: 0.0908, Val Loss: 0.0855\n",
      "Epoch 81/300 - Train Loss: 0.0892, Val Loss: 0.0855\n",
      "Epoch 82/300 - Train Loss: 0.0876, Val Loss: 0.0857\n",
      "Epoch 83/300 - Train Loss: 0.0895, Val Loss: 0.0846\n",
      "Epoch 84/300 - Train Loss: 0.0896, Val Loss: 0.0853\n",
      "Epoch 85/300 - Train Loss: 0.0900, Val Loss: 0.0856\n",
      "Epoch 86/300 - Train Loss: 0.0881, Val Loss: 0.0842\n",
      "Epoch 87/300 - Train Loss: 0.0880, Val Loss: 0.0857\n",
      "Epoch 88/300 - Train Loss: 0.0896, Val Loss: 0.0854\n",
      "Epoch 89/300 - Train Loss: 0.0889, Val Loss: 0.0846\n",
      "Epoch 90/300 - Train Loss: 0.0884, Val Loss: 0.0850\n",
      "Epoch 91/300 - Train Loss: 0.0891, Val Loss: 0.0854\n",
      "Epoch 92/300 - Train Loss: 0.0868, Val Loss: 0.0835\n",
      "Epoch 93/300 - Train Loss: 0.0886, Val Loss: 0.0840\n",
      "Epoch 94/300 - Train Loss: 0.0860, Val Loss: 0.0821\n",
      "Epoch 95/300 - Train Loss: 0.0870, Val Loss: 0.0818\n",
      "Epoch 96/300 - Train Loss: 0.0874, Val Loss: 0.0843\n",
      "Epoch 97/300 - Train Loss: 0.0872, Val Loss: 0.0837\n",
      "Epoch 98/300 - Train Loss: 0.0865, Val Loss: 0.0831\n",
      "Epoch 99/300 - Train Loss: 0.0861, Val Loss: 0.0833\n",
      "Epoch 100/300 - Train Loss: 0.0847, Val Loss: 0.0842\n",
      "Epoch 101/300 - Train Loss: 0.0866, Val Loss: 0.0828\n",
      "Epoch 102/300 - Train Loss: 0.0851, Val Loss: 0.0833\n",
      "Epoch 103/300 - Train Loss: 0.0868, Val Loss: 0.0826\n",
      "Epoch 104/300 - Train Loss: 0.0853, Val Loss: 0.0833\n",
      "Epoch 105/300 - Train Loss: 0.0856, Val Loss: 0.0833\n",
      "Epoch 106/300 - Train Loss: 0.0866, Val Loss: 0.0822\n",
      "Epoch 107/300 - Train Loss: 0.0868, Val Loss: 0.0836\n",
      "Epoch 108/300 - Train Loss: 0.0863, Val Loss: 0.0810\n",
      "Epoch 109/300 - Train Loss: 0.0857, Val Loss: 0.0821\n",
      "Epoch 110/300 - Train Loss: 0.0850, Val Loss: 0.0835\n",
      "Epoch 111/300 - Train Loss: 0.0853, Val Loss: 0.0818\n",
      "Epoch 112/300 - Train Loss: 0.0851, Val Loss: 0.0818\n",
      "Epoch 113/300 - Train Loss: 0.0855, Val Loss: 0.0824\n",
      "Epoch 114/300 - Train Loss: 0.0851, Val Loss: 0.0828\n",
      "Epoch 115/300 - Train Loss: 0.0831, Val Loss: 0.0831\n",
      "Epoch 116/300 - Train Loss: 0.0842, Val Loss: 0.0837\n",
      "Epoch 117/300 - Train Loss: 0.0831, Val Loss: 0.0828\n",
      "Epoch 118/300 - Train Loss: 0.0844, Val Loss: 0.0813\n",
      "Epoch 119/300 - Train Loss: 0.0841, Val Loss: 0.0828\n",
      "Epoch 120/300 - Train Loss: 0.0845, Val Loss: 0.0818\n",
      "Epoch 121/300 - Train Loss: 0.0842, Val Loss: 0.0821\n",
      "Epoch 122/300 - Train Loss: 0.0854, Val Loss: 0.0817\n",
      "Epoch 123/300 - Train Loss: 0.0848, Val Loss: 0.0837\n",
      "Epoch 124/300 - Train Loss: 0.0831, Val Loss: 0.0824\n",
      "Epoch 125/300 - Train Loss: 0.0851, Val Loss: 0.0822\n",
      "Epoch 126/300 - Train Loss: 0.0838, Val Loss: 0.0829\n",
      "Epoch 127/300 - Train Loss: 0.0840, Val Loss: 0.0809\n",
      "Epoch 128/300 - Train Loss: 0.0833, Val Loss: 0.0806\n",
      "Epoch 129/300 - Train Loss: 0.0819, Val Loss: 0.0826\n",
      "Epoch 130/300 - Train Loss: 0.0833, Val Loss: 0.0826\n",
      "Epoch 131/300 - Train Loss: 0.0822, Val Loss: 0.0825\n",
      "Epoch 132/300 - Train Loss: 0.0832, Val Loss: 0.0821\n",
      "Epoch 133/300 - Train Loss: 0.0825, Val Loss: 0.0798\n",
      "Epoch 134/300 - Train Loss: 0.0825, Val Loss: 0.0825\n",
      "Epoch 135/300 - Train Loss: 0.0825, Val Loss: 0.0814\n",
      "Epoch 136/300 - Train Loss: 0.0820, Val Loss: 0.0818\n",
      "Epoch 137/300 - Train Loss: 0.0828, Val Loss: 0.0809\n",
      "Epoch 138/300 - Train Loss: 0.0837, Val Loss: 0.0816\n",
      "Epoch 139/300 - Train Loss: 0.0836, Val Loss: 0.0800\n",
      "Epoch 140/300 - Train Loss: 0.0812, Val Loss: 0.0806\n",
      "Epoch 141/300 - Train Loss: 0.0811, Val Loss: 0.0807\n",
      "Epoch 142/300 - Train Loss: 0.0821, Val Loss: 0.0798\n",
      "Epoch 143/300 - Train Loss: 0.0819, Val Loss: 0.0802\n",
      "Epoch 144/300 - Train Loss: 0.0833, Val Loss: 0.0813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0830, Val Loss: 0.0802\n",
      "Epoch 146/300 - Train Loss: 0.0821, Val Loss: 0.0823\n",
      "Epoch 147/300 - Train Loss: 0.0809, Val Loss: 0.0812\n",
      "Epoch 148/300 - Train Loss: 0.0834, Val Loss: 0.0799\n",
      "Epoch 149/300 - Train Loss: 0.0795, Val Loss: 0.0796\n",
      "Epoch 150/300 - Train Loss: 0.0809, Val Loss: 0.0802\n",
      "Epoch 151/300 - Train Loss: 0.0820, Val Loss: 0.0818\n",
      "Epoch 152/300 - Train Loss: 0.0823, Val Loss: 0.0786\n",
      "Epoch 153/300 - Train Loss: 0.0815, Val Loss: 0.0804\n",
      "Epoch 154/300 - Train Loss: 0.0822, Val Loss: 0.0789\n",
      "Epoch 155/300 - Train Loss: 0.0800, Val Loss: 0.0788\n",
      "Epoch 156/300 - Train Loss: 0.0812, Val Loss: 0.0803\n",
      "Epoch 157/300 - Train Loss: 0.0820, Val Loss: 0.0794\n",
      "Epoch 158/300 - Train Loss: 0.0812, Val Loss: 0.0804\n",
      "Epoch 159/300 - Train Loss: 0.0815, Val Loss: 0.0805\n",
      "Epoch 160/300 - Train Loss: 0.0813, Val Loss: 0.0797\n",
      "Epoch 161/300 - Train Loss: 0.0801, Val Loss: 0.0804\n",
      "Epoch 162/300 - Train Loss: 0.0805, Val Loss: 0.0807\n",
      "Epoch 163/300 - Train Loss: 0.0807, Val Loss: 0.0801\n",
      "Epoch 164/300 - Train Loss: 0.0803, Val Loss: 0.0821\n",
      "Epoch 165/300 - Train Loss: 0.0831, Val Loss: 0.0795\n",
      "Epoch 166/300 - Train Loss: 0.0811, Val Loss: 0.0797\n",
      "Epoch 167/300 - Train Loss: 0.0797, Val Loss: 0.0792\n",
      "Epoch 168/300 - Train Loss: 0.0798, Val Loss: 0.0807\n",
      "Epoch 169/300 - Train Loss: 0.0800, Val Loss: 0.0804\n",
      "Epoch 170/300 - Train Loss: 0.0808, Val Loss: 0.0794\n",
      "Epoch 171/300 - Train Loss: 0.0801, Val Loss: 0.0782\n",
      "Epoch 172/300 - Train Loss: 0.0794, Val Loss: 0.0791\n",
      "Epoch 173/300 - Train Loss: 0.0827, Val Loss: 0.0781\n",
      "Epoch 174/300 - Train Loss: 0.0790, Val Loss: 0.0790\n",
      "Epoch 175/300 - Train Loss: 0.0819, Val Loss: 0.0797\n",
      "Epoch 176/300 - Train Loss: 0.0816, Val Loss: 0.0798\n",
      "Epoch 177/300 - Train Loss: 0.0816, Val Loss: 0.0799\n",
      "Epoch 178/300 - Train Loss: 0.0799, Val Loss: 0.0805\n",
      "Epoch 179/300 - Train Loss: 0.0789, Val Loss: 0.0813\n",
      "Epoch 180/300 - Train Loss: 0.0801, Val Loss: 0.0802\n",
      "Epoch 181/300 - Train Loss: 0.0811, Val Loss: 0.0779\n",
      "Epoch 182/300 - Train Loss: 0.0804, Val Loss: 0.0788\n",
      "Epoch 183/300 - Train Loss: 0.0799, Val Loss: 0.0788\n",
      "Epoch 184/300 - Train Loss: 0.0798, Val Loss: 0.0782\n",
      "Epoch 185/300 - Train Loss: 0.0797, Val Loss: 0.0788\n",
      "Epoch 186/300 - Train Loss: 0.0805, Val Loss: 0.0792\n",
      "Epoch 187/300 - Train Loss: 0.0793, Val Loss: 0.0791\n",
      "Epoch 188/300 - Train Loss: 0.0784, Val Loss: 0.0800\n",
      "Epoch 189/300 - Train Loss: 0.0796, Val Loss: 0.0784\n",
      "Epoch 190/300 - Train Loss: 0.0789, Val Loss: 0.0788\n",
      "Epoch 191/300 - Train Loss: 0.0794, Val Loss: 0.0793\n",
      "Epoch 192/300 - Train Loss: 0.0783, Val Loss: 0.0782\n",
      "Epoch 193/300 - Train Loss: 0.0800, Val Loss: 0.0787\n",
      "Epoch 194/300 - Train Loss: 0.0787, Val Loss: 0.0799\n",
      "Epoch 195/300 - Train Loss: 0.0791, Val Loss: 0.0791\n",
      "Epoch 196/300 - Train Loss: 0.0805, Val Loss: 0.0796\n",
      "Epoch 197/300 - Train Loss: 0.0788, Val Loss: 0.0786\n",
      "Epoch 198/300 - Train Loss: 0.0794, Val Loss: 0.0793\n",
      "Epoch 199/300 - Train Loss: 0.0785, Val Loss: 0.0775\n",
      "Epoch 200/300 - Train Loss: 0.0797, Val Loss: 0.0793\n",
      "Epoch 201/300 - Train Loss: 0.0780, Val Loss: 0.0788\n",
      "Epoch 202/300 - Train Loss: 0.0790, Val Loss: 0.0801\n",
      "Epoch 203/300 - Train Loss: 0.0783, Val Loss: 0.0785\n",
      "Epoch 204/300 - Train Loss: 0.0792, Val Loss: 0.0794\n",
      "Epoch 205/300 - Train Loss: 0.0786, Val Loss: 0.0769\n",
      "Epoch 206/300 - Train Loss: 0.0786, Val Loss: 0.0790\n",
      "Epoch 207/300 - Train Loss: 0.0792, Val Loss: 0.0782\n",
      "Epoch 208/300 - Train Loss: 0.0766, Val Loss: 0.0776\n",
      "Epoch 209/300 - Train Loss: 0.0773, Val Loss: 0.0780\n",
      "Epoch 210/300 - Train Loss: 0.0787, Val Loss: 0.0806\n",
      "Epoch 211/300 - Train Loss: 0.0779, Val Loss: 0.0788\n",
      "Epoch 212/300 - Train Loss: 0.0798, Val Loss: 0.0787\n",
      "Epoch 213/300 - Train Loss: 0.0786, Val Loss: 0.0789\n",
      "Epoch 214/300 - Train Loss: 0.0792, Val Loss: 0.0785\n",
      "Epoch 215/300 - Train Loss: 0.0784, Val Loss: 0.0769\n",
      "Epoch 216/300 - Train Loss: 0.0795, Val Loss: 0.0784\n",
      "Epoch 217/300 - Train Loss: 0.0777, Val Loss: 0.0776\n",
      "Epoch 218/300 - Train Loss: 0.0784, Val Loss: 0.0784\n",
      "Epoch 219/300 - Train Loss: 0.0775, Val Loss: 0.0781\n",
      "Epoch 220/300 - Train Loss: 0.0791, Val Loss: 0.0779\n",
      "Epoch 221/300 - Train Loss: 0.0775, Val Loss: 0.0766\n",
      "Epoch 222/300 - Train Loss: 0.0789, Val Loss: 0.0779\n",
      "Epoch 223/300 - Train Loss: 0.0777, Val Loss: 0.0777\n",
      "Epoch 224/300 - Train Loss: 0.0769, Val Loss: 0.0778\n",
      "Epoch 225/300 - Train Loss: 0.0785, Val Loss: 0.0785\n",
      "Epoch 226/300 - Train Loss: 0.0786, Val Loss: 0.0775\n",
      "Epoch 227/300 - Train Loss: 0.0776, Val Loss: 0.0771\n",
      "Epoch 228/300 - Train Loss: 0.0784, Val Loss: 0.0774\n",
      "Epoch 229/300 - Train Loss: 0.0791, Val Loss: 0.0767\n",
      "Epoch 230/300 - Train Loss: 0.0765, Val Loss: 0.0773\n",
      "Epoch 231/300 - Train Loss: 0.0772, Val Loss: 0.0781\n",
      "Epoch 232/300 - Train Loss: 0.0772, Val Loss: 0.0780\n",
      "Epoch 233/300 - Train Loss: 0.0771, Val Loss: 0.0792\n",
      "Epoch 234/300 - Train Loss: 0.0796, Val Loss: 0.0788\n",
      "Epoch 235/300 - Train Loss: 0.0783, Val Loss: 0.0789\n",
      "Epoch 236/300 - Train Loss: 0.0779, Val Loss: 0.0769\n",
      "Epoch 237/300 - Train Loss: 0.0759, Val Loss: 0.0786\n",
      "Epoch 238/300 - Train Loss: 0.0779, Val Loss: 0.0775\n",
      "Epoch 239/300 - Train Loss: 0.0766, Val Loss: 0.0784\n",
      "Epoch 240/300 - Train Loss: 0.0777, Val Loss: 0.0777\n",
      "Epoch 241/300 - Train Loss: 0.0780, Val Loss: 0.0774\n",
      "Epoch 242/300 - Train Loss: 0.0781, Val Loss: 0.0772\n",
      "Epoch 243/300 - Train Loss: 0.0789, Val Loss: 0.0795\n",
      "Epoch 244/300 - Train Loss: 0.0780, Val Loss: 0.0782\n",
      "Epoch 245/300 - Train Loss: 0.0780, Val Loss: 0.0762\n",
      "Epoch 246/300 - Train Loss: 0.0779, Val Loss: 0.0764\n",
      "Epoch 247/300 - Train Loss: 0.0778, Val Loss: 0.0772\n",
      "Epoch 248/300 - Train Loss: 0.0773, Val Loss: 0.0766\n",
      "Epoch 249/300 - Train Loss: 0.0772, Val Loss: 0.0779\n",
      "Epoch 250/300 - Train Loss: 0.0766, Val Loss: 0.0771\n",
      "Epoch 251/300 - Train Loss: 0.0777, Val Loss: 0.0773\n",
      "Epoch 252/300 - Train Loss: 0.0763, Val Loss: 0.0768\n",
      "Epoch 253/300 - Train Loss: 0.0776, Val Loss: 0.0786\n",
      "Epoch 254/300 - Train Loss: 0.0761, Val Loss: 0.0776\n",
      "Epoch 255/300 - Train Loss: 0.0774, Val Loss: 0.0773\n",
      "Epoch 256/300 - Train Loss: 0.0774, Val Loss: 0.0768\n",
      "Epoch 257/300 - Train Loss: 0.0770, Val Loss: 0.0779\n",
      "Epoch 258/300 - Train Loss: 0.0777, Val Loss: 0.0776\n",
      "Epoch 259/300 - Train Loss: 0.0777, Val Loss: 0.0761\n",
      "Epoch 260/300 - Train Loss: 0.0761, Val Loss: 0.0774\n",
      "Epoch 261/300 - Train Loss: 0.0774, Val Loss: 0.0759\n",
      "Epoch 262/300 - Train Loss: 0.0766, Val Loss: 0.0767\n",
      "Epoch 263/300 - Train Loss: 0.0778, Val Loss: 0.0767\n",
      "Epoch 264/300 - Train Loss: 0.0767, Val Loss: 0.0771\n",
      "Epoch 265/300 - Train Loss: 0.0772, Val Loss: 0.0779\n",
      "Epoch 266/300 - Train Loss: 0.0777, Val Loss: 0.0778\n",
      "Epoch 267/300 - Train Loss: 0.0759, Val Loss: 0.0773\n",
      "Epoch 268/300 - Train Loss: 0.0762, Val Loss: 0.0761\n",
      "Epoch 269/300 - Train Loss: 0.0759, Val Loss: 0.0772\n",
      "Epoch 270/300 - Train Loss: 0.0777, Val Loss: 0.0771\n",
      "Epoch 271/300 - Train Loss: 0.0756, Val Loss: 0.0766\n",
      "Epoch 272/300 - Train Loss: 0.0772, Val Loss: 0.0762\n",
      "Epoch 273/300 - Train Loss: 0.0759, Val Loss: 0.0771\n",
      "Epoch 274/300 - Train Loss: 0.0762, Val Loss: 0.0767\n",
      "Epoch 275/300 - Train Loss: 0.0753, Val Loss: 0.0765\n",
      "Epoch 276/300 - Train Loss: 0.0758, Val Loss: 0.0753\n",
      "Epoch 277/300 - Train Loss: 0.0765, Val Loss: 0.0769\n",
      "Epoch 278/300 - Train Loss: 0.0760, Val Loss: 0.0763\n",
      "Epoch 279/300 - Train Loss: 0.0768, Val Loss: 0.0773\n",
      "Epoch 280/300 - Train Loss: 0.0775, Val Loss: 0.0768\n",
      "Epoch 281/300 - Train Loss: 0.0758, Val Loss: 0.0762\n",
      "Epoch 282/300 - Train Loss: 0.0764, Val Loss: 0.0762\n",
      "Epoch 283/300 - Train Loss: 0.0770, Val Loss: 0.0764\n",
      "Epoch 284/300 - Train Loss: 0.0760, Val Loss: 0.0760\n",
      "Epoch 285/300 - Train Loss: 0.0755, Val Loss: 0.0771\n",
      "Epoch 286/300 - Train Loss: 0.0744, Val Loss: 0.0772\n",
      "Epoch 287/300 - Train Loss: 0.0743, Val Loss: 0.0776\n",
      "Epoch 288/300 - Train Loss: 0.0750, Val Loss: 0.0761\n",
      "Epoch 289/300 - Train Loss: 0.0767, Val Loss: 0.0777\n",
      "Epoch 290/300 - Train Loss: 0.0771, Val Loss: 0.0763\n",
      "Epoch 291/300 - Train Loss: 0.0756, Val Loss: 0.0767\n",
      "Epoch 292/300 - Train Loss: 0.0777, Val Loss: 0.0779\n",
      "Epoch 293/300 - Train Loss: 0.0762, Val Loss: 0.0765\n",
      "Epoch 294/300 - Train Loss: 0.0762, Val Loss: 0.0771\n",
      "Epoch 295/300 - Train Loss: 0.0736, Val Loss: 0.0763\n",
      "Epoch 296/300 - Train Loss: 0.0744, Val Loss: 0.0765\n",
      "Epoch 297/300 - Train Loss: 0.0759, Val Loss: 0.0759\n",
      "Epoch 298/300 - Train Loss: 0.0749, Val Loss: 0.0769\n",
      "Epoch 299/300 - Train Loss: 0.0749, Val Loss: 0.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-04-30 23:58:25,668] Trial 21 finished with value: 0.9607462427253738 and parameters: {'F1': 8, 'F2': 8, 'D': 4, 'dropout': 0.1758130806314788, 'learning_rate': 1.1471400194518907e-05, 'batch_size': 64, 'weight_decay': 0.00015531401052541069}. Best is trial 10 with value: 0.9693616971084471.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/300 - Train Loss: 0.0755, Val Loss: 0.0777\n",
      "Macro F1 Score: 0.9607, Macro Precision: 0.9496, Macro Recall: 0.9735\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 23\n",
      "Training with F1=8, F2=8, D=4, dropout=0.2512393866864839, LR=2.055158870920062e-05, BS=64, WD=1.8947432226364434e-05\n",
      "Epoch 1/300 - Train Loss: 0.9956, Val Loss: 0.8770\n",
      "Epoch 2/300 - Train Loss: 0.7140, Val Loss: 0.6182\n",
      "Epoch 3/300 - Train Loss: 0.5067, Val Loss: 0.4407\n",
      "Epoch 4/300 - Train Loss: 0.3882, Val Loss: 0.3474\n",
      "Epoch 5/300 - Train Loss: 0.3176, Val Loss: 0.2825\n",
      "Epoch 6/300 - Train Loss: 0.2688, Val Loss: 0.2407\n",
      "Epoch 7/300 - Train Loss: 0.2334, Val Loss: 0.2054\n",
      "Epoch 8/300 - Train Loss: 0.2039, Val Loss: 0.1786\n",
      "Epoch 9/300 - Train Loss: 0.1822, Val Loss: 0.1602\n",
      "Epoch 10/300 - Train Loss: 0.1709, Val Loss: 0.1480\n",
      "Epoch 11/300 - Train Loss: 0.1585, Val Loss: 0.1369\n",
      "Epoch 12/300 - Train Loss: 0.1523, Val Loss: 0.1304\n",
      "Epoch 13/300 - Train Loss: 0.1445, Val Loss: 0.1222\n",
      "Epoch 14/300 - Train Loss: 0.1369, Val Loss: 0.1177\n",
      "Epoch 15/300 - Train Loss: 0.1329, Val Loss: 0.1136\n",
      "Epoch 16/300 - Train Loss: 0.1301, Val Loss: 0.1102\n",
      "Epoch 17/300 - Train Loss: 0.1260, Val Loss: 0.1069\n",
      "Epoch 18/300 - Train Loss: 0.1251, Val Loss: 0.1047\n",
      "Epoch 19/300 - Train Loss: 0.1198, Val Loss: 0.1038\n",
      "Epoch 20/300 - Train Loss: 0.1175, Val Loss: 0.1014\n",
      "Epoch 21/300 - Train Loss: 0.1183, Val Loss: 0.0982\n",
      "Epoch 22/300 - Train Loss: 0.1161, Val Loss: 0.0975\n",
      "Epoch 23/300 - Train Loss: 0.1140, Val Loss: 0.0990\n",
      "Epoch 24/300 - Train Loss: 0.1127, Val Loss: 0.0950\n",
      "Epoch 25/300 - Train Loss: 0.1125, Val Loss: 0.0940\n",
      "Epoch 26/300 - Train Loss: 0.1119, Val Loss: 0.0929\n",
      "Epoch 27/300 - Train Loss: 0.1087, Val Loss: 0.0914\n",
      "Epoch 28/300 - Train Loss: 0.1095, Val Loss: 0.0924\n",
      "Epoch 29/300 - Train Loss: 0.1076, Val Loss: 0.0933\n",
      "Epoch 30/300 - Train Loss: 0.1047, Val Loss: 0.0893\n",
      "Epoch 31/300 - Train Loss: 0.1060, Val Loss: 0.0914\n",
      "Epoch 32/300 - Train Loss: 0.1086, Val Loss: 0.0888\n",
      "Epoch 33/300 - Train Loss: 0.1068, Val Loss: 0.0887\n",
      "Epoch 34/300 - Train Loss: 0.1041, Val Loss: 0.0875\n",
      "Epoch 35/300 - Train Loss: 0.1035, Val Loss: 0.0870\n",
      "Epoch 36/300 - Train Loss: 0.1030, Val Loss: 0.0875\n",
      "Epoch 37/300 - Train Loss: 0.1023, Val Loss: 0.0851\n",
      "Epoch 38/300 - Train Loss: 0.1031, Val Loss: 0.0881\n",
      "Epoch 39/300 - Train Loss: 0.1021, Val Loss: 0.0857\n",
      "Epoch 40/300 - Train Loss: 0.1008, Val Loss: 0.0852\n",
      "Epoch 41/300 - Train Loss: 0.1005, Val Loss: 0.0843\n",
      "Epoch 42/300 - Train Loss: 0.1009, Val Loss: 0.0850\n",
      "Epoch 43/300 - Train Loss: 0.0995, Val Loss: 0.0838\n",
      "Epoch 44/300 - Train Loss: 0.1015, Val Loss: 0.0840\n",
      "Epoch 45/300 - Train Loss: 0.0993, Val Loss: 0.0838\n",
      "Epoch 46/300 - Train Loss: 0.1007, Val Loss: 0.0834\n",
      "Epoch 47/300 - Train Loss: 0.1001, Val Loss: 0.0826\n",
      "Epoch 48/300 - Train Loss: 0.0991, Val Loss: 0.0832\n",
      "Epoch 49/300 - Train Loss: 0.1004, Val Loss: 0.0834\n",
      "Epoch 50/300 - Train Loss: 0.1005, Val Loss: 0.0836\n",
      "Epoch 51/300 - Train Loss: 0.0973, Val Loss: 0.0837\n",
      "Epoch 52/300 - Train Loss: 0.0983, Val Loss: 0.0836\n",
      "Epoch 53/300 - Train Loss: 0.0957, Val Loss: 0.0850\n",
      "Epoch 54/300 - Train Loss: 0.0970, Val Loss: 0.0837\n",
      "Epoch 55/300 - Train Loss: 0.0964, Val Loss: 0.0809\n",
      "Epoch 56/300 - Train Loss: 0.0962, Val Loss: 0.0814\n",
      "Epoch 57/300 - Train Loss: 0.0984, Val Loss: 0.0827\n",
      "Epoch 58/300 - Train Loss: 0.0980, Val Loss: 0.0831\n",
      "Epoch 59/300 - Train Loss: 0.0976, Val Loss: 0.0819\n",
      "Epoch 60/300 - Train Loss: 0.0955, Val Loss: 0.0822\n",
      "Epoch 61/300 - Train Loss: 0.0952, Val Loss: 0.0811\n",
      "Epoch 62/300 - Train Loss: 0.0948, Val Loss: 0.0824\n",
      "Epoch 63/300 - Train Loss: 0.0939, Val Loss: 0.0812\n",
      "Epoch 64/300 - Train Loss: 0.0953, Val Loss: 0.0815\n",
      "Epoch 65/300 - Train Loss: 0.0947, Val Loss: 0.0804\n",
      "Epoch 66/300 - Train Loss: 0.0945, Val Loss: 0.0797\n",
      "Epoch 67/300 - Train Loss: 0.0950, Val Loss: 0.0819\n",
      "Epoch 68/300 - Train Loss: 0.0948, Val Loss: 0.0806\n",
      "Epoch 69/300 - Train Loss: 0.0955, Val Loss: 0.0809\n",
      "Epoch 70/300 - Train Loss: 0.0939, Val Loss: 0.0811\n",
      "Epoch 71/300 - Train Loss: 0.0940, Val Loss: 0.0805\n",
      "Epoch 72/300 - Train Loss: 0.0934, Val Loss: 0.0807\n",
      "Epoch 73/300 - Train Loss: 0.0947, Val Loss: 0.0800\n",
      "Epoch 74/300 - Train Loss: 0.0944, Val Loss: 0.0809\n",
      "Epoch 75/300 - Train Loss: 0.0928, Val Loss: 0.0820\n",
      "Epoch 76/300 - Train Loss: 0.0950, Val Loss: 0.0816\n",
      "Epoch 77/300 - Train Loss: 0.0921, Val Loss: 0.0806\n",
      "Epoch 78/300 - Train Loss: 0.0938, Val Loss: 0.0812\n",
      "Epoch 79/300 - Train Loss: 0.0929, Val Loss: 0.0805\n",
      "Epoch 80/300 - Train Loss: 0.0935, Val Loss: 0.0810\n",
      "Epoch 81/300 - Train Loss: 0.0916, Val Loss: 0.0797\n",
      "Epoch 82/300 - Train Loss: 0.0920, Val Loss: 0.0792\n",
      "Epoch 83/300 - Train Loss: 0.0917, Val Loss: 0.0797\n",
      "Epoch 84/300 - Train Loss: 0.0928, Val Loss: 0.0810\n",
      "Epoch 85/300 - Train Loss: 0.0910, Val Loss: 0.0794\n",
      "Epoch 86/300 - Train Loss: 0.0903, Val Loss: 0.0795\n",
      "Epoch 87/300 - Train Loss: 0.0911, Val Loss: 0.0801\n",
      "Epoch 88/300 - Train Loss: 0.0933, Val Loss: 0.0797\n",
      "Epoch 89/300 - Train Loss: 0.0915, Val Loss: 0.0789\n",
      "Epoch 90/300 - Train Loss: 0.0916, Val Loss: 0.0793\n",
      "Epoch 91/300 - Train Loss: 0.0911, Val Loss: 0.0785\n",
      "Epoch 92/300 - Train Loss: 0.0892, Val Loss: 0.0786\n",
      "Epoch 93/300 - Train Loss: 0.0893, Val Loss: 0.0782\n",
      "Epoch 94/300 - Train Loss: 0.0919, Val Loss: 0.0804\n",
      "Epoch 95/300 - Train Loss: 0.0903, Val Loss: 0.0785\n",
      "Epoch 96/300 - Train Loss: 0.0906, Val Loss: 0.0787\n",
      "Epoch 97/300 - Train Loss: 0.0907, Val Loss: 0.0784\n",
      "Epoch 98/300 - Train Loss: 0.0905, Val Loss: 0.0789\n",
      "Epoch 99/300 - Train Loss: 0.0894, Val Loss: 0.0788\n",
      "Epoch 100/300 - Train Loss: 0.0904, Val Loss: 0.0794\n",
      "Epoch 101/300 - Train Loss: 0.0907, Val Loss: 0.0786\n",
      "Epoch 102/300 - Train Loss: 0.0915, Val Loss: 0.0787\n",
      "Epoch 103/300 - Train Loss: 0.0905, Val Loss: 0.0787\n",
      "Epoch 104/300 - Train Loss: 0.0891, Val Loss: 0.0769\n",
      "Epoch 105/300 - Train Loss: 0.0900, Val Loss: 0.0774\n",
      "Epoch 106/300 - Train Loss: 0.0899, Val Loss: 0.0783\n",
      "Epoch 107/300 - Train Loss: 0.0897, Val Loss: 0.0781\n",
      "Epoch 108/300 - Train Loss: 0.0886, Val Loss: 0.0772\n",
      "Epoch 109/300 - Train Loss: 0.0884, Val Loss: 0.0781\n",
      "Epoch 110/300 - Train Loss: 0.0864, Val Loss: 0.0788\n",
      "Epoch 111/300 - Train Loss: 0.0877, Val Loss: 0.0785\n",
      "Epoch 112/300 - Train Loss: 0.0886, Val Loss: 0.0785\n",
      "Epoch 113/300 - Train Loss: 0.0877, Val Loss: 0.0779\n",
      "Epoch 114/300 - Train Loss: 0.0859, Val Loss: 0.0771\n",
      "Epoch 115/300 - Train Loss: 0.0869, Val Loss: 0.0771\n",
      "Epoch 116/300 - Train Loss: 0.0879, Val Loss: 0.0779\n",
      "Epoch 117/300 - Train Loss: 0.0871, Val Loss: 0.0769\n",
      "Epoch 118/300 - Train Loss: 0.0883, Val Loss: 0.0762\n",
      "Epoch 119/300 - Train Loss: 0.0888, Val Loss: 0.0766\n",
      "Epoch 120/300 - Train Loss: 0.0883, Val Loss: 0.0762\n",
      "Epoch 121/300 - Train Loss: 0.0881, Val Loss: 0.0776\n",
      "Epoch 122/300 - Train Loss: 0.0873, Val Loss: 0.0766\n",
      "Epoch 123/300 - Train Loss: 0.0861, Val Loss: 0.0780\n",
      "Epoch 124/300 - Train Loss: 0.0862, Val Loss: 0.0769\n",
      "Epoch 125/300 - Train Loss: 0.0875, Val Loss: 0.0764\n",
      "Epoch 126/300 - Train Loss: 0.0857, Val Loss: 0.0753\n",
      "Epoch 127/300 - Train Loss: 0.0869, Val Loss: 0.0757\n",
      "Epoch 128/300 - Train Loss: 0.0869, Val Loss: 0.0774\n",
      "Epoch 129/300 - Train Loss: 0.0879, Val Loss: 0.0754\n",
      "Epoch 130/300 - Train Loss: 0.0845, Val Loss: 0.0772\n",
      "Epoch 131/300 - Train Loss: 0.0857, Val Loss: 0.0762\n",
      "Epoch 132/300 - Train Loss: 0.0885, Val Loss: 0.0761\n",
      "Epoch 133/300 - Train Loss: 0.0857, Val Loss: 0.0764\n",
      "Epoch 134/300 - Train Loss: 0.0846, Val Loss: 0.0763\n",
      "Epoch 135/300 - Train Loss: 0.0843, Val Loss: 0.0768\n",
      "Epoch 136/300 - Train Loss: 0.0871, Val Loss: 0.0764\n",
      "Epoch 137/300 - Train Loss: 0.0844, Val Loss: 0.0767\n",
      "Epoch 138/300 - Train Loss: 0.0845, Val Loss: 0.0772\n",
      "Epoch 139/300 - Train Loss: 0.0843, Val Loss: 0.0766\n",
      "Epoch 140/300 - Train Loss: 0.0854, Val Loss: 0.0787\n",
      "Epoch 141/300 - Train Loss: 0.0852, Val Loss: 0.0760\n",
      "Epoch 142/300 - Train Loss: 0.0851, Val Loss: 0.0759\n",
      "Epoch 143/300 - Train Loss: 0.0846, Val Loss: 0.0768\n",
      "Epoch 144/300 - Train Loss: 0.0853, Val Loss: 0.0755\n",
      "Epoch 145/300 - Train Loss: 0.0843, Val Loss: 0.0768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/300 - Train Loss: 0.0853, Val Loss: 0.0747\n",
      "Epoch 147/300 - Train Loss: 0.0865, Val Loss: 0.0755\n",
      "Epoch 148/300 - Train Loss: 0.0842, Val Loss: 0.0759\n",
      "Epoch 149/300 - Train Loss: 0.0835, Val Loss: 0.0752\n",
      "Epoch 150/300 - Train Loss: 0.0843, Val Loss: 0.0763\n",
      "Epoch 151/300 - Train Loss: 0.0846, Val Loss: 0.0759\n",
      "Epoch 152/300 - Train Loss: 0.0848, Val Loss: 0.0784\n",
      "Epoch 153/300 - Train Loss: 0.0853, Val Loss: 0.0744\n",
      "Epoch 154/300 - Train Loss: 0.0844, Val Loss: 0.0747\n",
      "Epoch 155/300 - Train Loss: 0.0841, Val Loss: 0.0766\n",
      "Epoch 156/300 - Train Loss: 0.0839, Val Loss: 0.0777\n",
      "Epoch 157/300 - Train Loss: 0.0838, Val Loss: 0.0742\n",
      "Epoch 158/300 - Train Loss: 0.0846, Val Loss: 0.0757\n",
      "Epoch 159/300 - Train Loss: 0.0822, Val Loss: 0.0748\n",
      "Epoch 160/300 - Train Loss: 0.0833, Val Loss: 0.0763\n",
      "Epoch 161/300 - Train Loss: 0.0833, Val Loss: 0.0768\n",
      "Epoch 162/300 - Train Loss: 0.0825, Val Loss: 0.0754\n",
      "Epoch 163/300 - Train Loss: 0.0816, Val Loss: 0.0769\n",
      "Epoch 164/300 - Train Loss: 0.0849, Val Loss: 0.0752\n",
      "Epoch 165/300 - Train Loss: 0.0845, Val Loss: 0.0762\n",
      "Epoch 166/300 - Train Loss: 0.0841, Val Loss: 0.0748\n",
      "Epoch 167/300 - Train Loss: 0.0819, Val Loss: 0.0747\n",
      "Epoch 168/300 - Train Loss: 0.0820, Val Loss: 0.0751\n",
      "Epoch 169/300 - Train Loss: 0.0817, Val Loss: 0.0751\n",
      "Epoch 170/300 - Train Loss: 0.0835, Val Loss: 0.0758\n",
      "Epoch 171/300 - Train Loss: 0.0821, Val Loss: 0.0768\n",
      "Epoch 172/300 - Train Loss: 0.0827, Val Loss: 0.0747\n",
      "Epoch 173/300 - Train Loss: 0.0821, Val Loss: 0.0747\n",
      "Epoch 174/300 - Train Loss: 0.0831, Val Loss: 0.0752\n",
      "Epoch 175/300 - Train Loss: 0.0824, Val Loss: 0.0757\n",
      "Epoch 176/300 - Train Loss: 0.0795, Val Loss: 0.0756\n",
      "Epoch 177/300 - Train Loss: 0.0835, Val Loss: 0.0752\n",
      "Epoch 178/300 - Train Loss: 0.0819, Val Loss: 0.0753\n",
      "Epoch 179/300 - Train Loss: 0.0807, Val Loss: 0.0737\n",
      "Epoch 180/300 - Train Loss: 0.0824, Val Loss: 0.0748\n",
      "Epoch 181/300 - Train Loss: 0.0811, Val Loss: 0.0754\n",
      "Epoch 182/300 - Train Loss: 0.0814, Val Loss: 0.0750\n",
      "Epoch 183/300 - Train Loss: 0.0822, Val Loss: 0.0749\n",
      "Epoch 184/300 - Train Loss: 0.0816, Val Loss: 0.0758\n",
      "Epoch 185/300 - Train Loss: 0.0833, Val Loss: 0.0755\n",
      "Epoch 186/300 - Train Loss: 0.0834, Val Loss: 0.0755\n",
      "Epoch 187/300 - Train Loss: 0.0824, Val Loss: 0.0748\n",
      "Epoch 188/300 - Train Loss: 0.0825, Val Loss: 0.0740\n",
      "Epoch 189/300 - Train Loss: 0.0807, Val Loss: 0.0751\n",
      "Epoch 190/300 - Train Loss: 0.0830, Val Loss: 0.0766\n",
      "Epoch 191/300 - Train Loss: 0.0804, Val Loss: 0.0753\n",
      "Epoch 192/300 - Train Loss: 0.0817, Val Loss: 0.0763\n",
      "Epoch 193/300 - Train Loss: 0.0819, Val Loss: 0.0756\n",
      "Epoch 194/300 - Train Loss: 0.0825, Val Loss: 0.0746\n",
      "Epoch 195/300 - Train Loss: 0.0804, Val Loss: 0.0760\n",
      "Epoch 196/300 - Train Loss: 0.0809, Val Loss: 0.0757\n",
      "Epoch 197/300 - Train Loss: 0.0815, Val Loss: 0.0745\n",
      "Epoch 198/300 - Train Loss: 0.0815, Val Loss: 0.0742\n",
      "Epoch 199/300 - Train Loss: 0.0804, Val Loss: 0.0748\n",
      "Epoch 200/300 - Train Loss: 0.0808, Val Loss: 0.0756\n",
      "Epoch 201/300 - Train Loss: 0.0802, Val Loss: 0.0764\n",
      "Epoch 202/300 - Train Loss: 0.0816, Val Loss: 0.0762\n",
      "Epoch 203/300 - Train Loss: 0.0818, Val Loss: 0.0747\n",
      "Epoch 204/300 - Train Loss: 0.0806, Val Loss: 0.0757\n",
      "Epoch 205/300 - Train Loss: 0.0809, Val Loss: 0.0752\n",
      "Epoch 206/300 - Train Loss: 0.0822, Val Loss: 0.0746\n",
      "Epoch 207/300 - Train Loss: 0.0816, Val Loss: 0.0753\n",
      "Epoch 208/300 - Train Loss: 0.0803, Val Loss: 0.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:03:07,401] Trial 22 finished with value: 0.9700424024810431 and parameters: {'F1': 8, 'F2': 8, 'D': 4, 'dropout': 0.2512393866864839, 'learning_rate': 2.055158870920062e-05, 'batch_size': 64, 'weight_decay': 1.8947432226364434e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 209/300 - Train Loss: 0.0790, Val Loss: 0.0760\n",
      "Early stopping at epoch 209\n",
      "Macro F1 Score: 0.9700, Macro Precision: 0.9603, Macro Recall: 0.9809\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 24\n",
      "Training with F1=16, F2=8, D=4, dropout=0.2554674892715744, LR=2.1386710552032182e-05, BS=64, WD=2.094201283059875e-05\n",
      "Epoch 1/300 - Train Loss: 0.9699, Val Loss: 0.7089\n",
      "Epoch 2/300 - Train Loss: 0.5459, Val Loss: 0.4170\n",
      "Epoch 3/300 - Train Loss: 0.3703, Val Loss: 0.3122\n",
      "Epoch 4/300 - Train Loss: 0.2910, Val Loss: 0.2552\n",
      "Epoch 5/300 - Train Loss: 0.2416, Val Loss: 0.2127\n",
      "Epoch 6/300 - Train Loss: 0.2078, Val Loss: 0.1763\n",
      "Epoch 7/300 - Train Loss: 0.1829, Val Loss: 0.1576\n",
      "Epoch 8/300 - Train Loss: 0.1634, Val Loss: 0.1375\n",
      "Epoch 9/300 - Train Loss: 0.1486, Val Loss: 0.1279\n",
      "Epoch 10/300 - Train Loss: 0.1345, Val Loss: 0.1196\n",
      "Epoch 11/300 - Train Loss: 0.1278, Val Loss: 0.1144\n",
      "Epoch 12/300 - Train Loss: 0.1228, Val Loss: 0.1071\n",
      "Epoch 13/300 - Train Loss: 0.1168, Val Loss: 0.1046\n",
      "Epoch 14/300 - Train Loss: 0.1128, Val Loss: 0.0987\n",
      "Epoch 15/300 - Train Loss: 0.1124, Val Loss: 0.0967\n",
      "Epoch 16/300 - Train Loss: 0.1068, Val Loss: 0.0927\n",
      "Epoch 17/300 - Train Loss: 0.1032, Val Loss: 0.0917\n",
      "Epoch 18/300 - Train Loss: 0.1054, Val Loss: 0.0923\n",
      "Epoch 19/300 - Train Loss: 0.1018, Val Loss: 0.0894\n",
      "Epoch 20/300 - Train Loss: 0.1025, Val Loss: 0.0855\n",
      "Epoch 21/300 - Train Loss: 0.1009, Val Loss: 0.0832\n",
      "Epoch 22/300 - Train Loss: 0.0991, Val Loss: 0.0858\n",
      "Epoch 23/300 - Train Loss: 0.0989, Val Loss: 0.0849\n",
      "Epoch 24/300 - Train Loss: 0.0977, Val Loss: 0.0827\n",
      "Epoch 25/300 - Train Loss: 0.0974, Val Loss: 0.0823\n",
      "Epoch 26/300 - Train Loss: 0.0955, Val Loss: 0.0832\n",
      "Epoch 27/300 - Train Loss: 0.0967, Val Loss: 0.0866\n",
      "Epoch 28/300 - Train Loss: 0.0957, Val Loss: 0.0810\n",
      "Epoch 29/300 - Train Loss: 0.0950, Val Loss: 0.0815\n",
      "Epoch 30/300 - Train Loss: 0.0940, Val Loss: 0.0815\n",
      "Epoch 31/300 - Train Loss: 0.0950, Val Loss: 0.0807\n",
      "Epoch 32/300 - Train Loss: 0.0932, Val Loss: 0.0798\n",
      "Epoch 33/300 - Train Loss: 0.0906, Val Loss: 0.0804\n",
      "Epoch 34/300 - Train Loss: 0.0922, Val Loss: 0.0789\n",
      "Epoch 35/300 - Train Loss: 0.0920, Val Loss: 0.0780\n",
      "Epoch 36/300 - Train Loss: 0.0923, Val Loss: 0.0814\n",
      "Epoch 37/300 - Train Loss: 0.0915, Val Loss: 0.0770\n",
      "Epoch 38/300 - Train Loss: 0.0899, Val Loss: 0.0777\n",
      "Epoch 39/300 - Train Loss: 0.0911, Val Loss: 0.0756\n",
      "Epoch 40/300 - Train Loss: 0.0891, Val Loss: 0.0765\n",
      "Epoch 41/300 - Train Loss: 0.0911, Val Loss: 0.0768\n",
      "Epoch 42/300 - Train Loss: 0.0905, Val Loss: 0.0756\n",
      "Epoch 43/300 - Train Loss: 0.0890, Val Loss: 0.0767\n",
      "Epoch 44/300 - Train Loss: 0.0880, Val Loss: 0.0779\n",
      "Epoch 45/300 - Train Loss: 0.0891, Val Loss: 0.0759\n",
      "Epoch 46/300 - Train Loss: 0.0872, Val Loss: 0.0753\n",
      "Epoch 47/300 - Train Loss: 0.0880, Val Loss: 0.0772\n",
      "Epoch 48/300 - Train Loss: 0.0877, Val Loss: 0.0770\n",
      "Epoch 49/300 - Train Loss: 0.0858, Val Loss: 0.0751\n",
      "Epoch 50/300 - Train Loss: 0.0881, Val Loss: 0.0733\n",
      "Epoch 51/300 - Train Loss: 0.0864, Val Loss: 0.0750\n",
      "Epoch 52/300 - Train Loss: 0.0878, Val Loss: 0.0746\n",
      "Epoch 53/300 - Train Loss: 0.0872, Val Loss: 0.0753\n",
      "Epoch 54/300 - Train Loss: 0.0858, Val Loss: 0.0747\n",
      "Epoch 55/300 - Train Loss: 0.0862, Val Loss: 0.0756\n",
      "Epoch 56/300 - Train Loss: 0.0867, Val Loss: 0.0731\n",
      "Epoch 57/300 - Train Loss: 0.0877, Val Loss: 0.0738\n",
      "Epoch 58/300 - Train Loss: 0.0868, Val Loss: 0.0751\n",
      "Epoch 59/300 - Train Loss: 0.0855, Val Loss: 0.0751\n",
      "Epoch 60/300 - Train Loss: 0.0857, Val Loss: 0.0742\n",
      "Epoch 61/300 - Train Loss: 0.0852, Val Loss: 0.0744\n",
      "Epoch 62/300 - Train Loss: 0.0852, Val Loss: 0.0775\n",
      "Epoch 63/300 - Train Loss: 0.0856, Val Loss: 0.0751\n",
      "Epoch 64/300 - Train Loss: 0.0838, Val Loss: 0.0736\n",
      "Epoch 65/300 - Train Loss: 0.0848, Val Loss: 0.0732\n",
      "Epoch 66/300 - Train Loss: 0.0855, Val Loss: 0.0741\n",
      "Epoch 67/300 - Train Loss: 0.0844, Val Loss: 0.0736\n",
      "Epoch 68/300 - Train Loss: 0.0848, Val Loss: 0.0731\n",
      "Epoch 69/300 - Train Loss: 0.0841, Val Loss: 0.0729\n",
      "Epoch 70/300 - Train Loss: 0.0843, Val Loss: 0.0728\n",
      "Epoch 71/300 - Train Loss: 0.0825, Val Loss: 0.0748\n",
      "Epoch 72/300 - Train Loss: 0.0845, Val Loss: 0.0752\n",
      "Epoch 73/300 - Train Loss: 0.0843, Val Loss: 0.0727\n",
      "Epoch 74/300 - Train Loss: 0.0832, Val Loss: 0.0725\n",
      "Epoch 75/300 - Train Loss: 0.0831, Val Loss: 0.0741\n",
      "Epoch 76/300 - Train Loss: 0.0830, Val Loss: 0.0732\n",
      "Epoch 77/300 - Train Loss: 0.0822, Val Loss: 0.0748\n",
      "Epoch 78/300 - Train Loss: 0.0831, Val Loss: 0.0715\n",
      "Epoch 79/300 - Train Loss: 0.0835, Val Loss: 0.0731\n",
      "Epoch 80/300 - Train Loss: 0.0828, Val Loss: 0.0729\n",
      "Epoch 81/300 - Train Loss: 0.0814, Val Loss: 0.0746\n",
      "Epoch 82/300 - Train Loss: 0.0830, Val Loss: 0.0725\n",
      "Epoch 83/300 - Train Loss: 0.0837, Val Loss: 0.0743\n",
      "Epoch 84/300 - Train Loss: 0.0823, Val Loss: 0.0755\n",
      "Epoch 85/300 - Train Loss: 0.0810, Val Loss: 0.0725\n",
      "Epoch 86/300 - Train Loss: 0.0800, Val Loss: 0.0730\n",
      "Epoch 87/300 - Train Loss: 0.0808, Val Loss: 0.0740\n",
      "Epoch 88/300 - Train Loss: 0.0811, Val Loss: 0.0729\n",
      "Epoch 89/300 - Train Loss: 0.0819, Val Loss: 0.0716\n",
      "Epoch 90/300 - Train Loss: 0.0805, Val Loss: 0.0723\n",
      "Epoch 91/300 - Train Loss: 0.0824, Val Loss: 0.0715\n",
      "Epoch 92/300 - Train Loss: 0.0824, Val Loss: 0.0717\n",
      "Epoch 93/300 - Train Loss: 0.0813, Val Loss: 0.0714\n",
      "Epoch 94/300 - Train Loss: 0.0821, Val Loss: 0.0727\n",
      "Epoch 95/300 - Train Loss: 0.0829, Val Loss: 0.0736\n",
      "Epoch 96/300 - Train Loss: 0.0824, Val Loss: 0.0736\n",
      "Epoch 97/300 - Train Loss: 0.0798, Val Loss: 0.0743\n",
      "Epoch 98/300 - Train Loss: 0.0827, Val Loss: 0.0747\n",
      "Epoch 99/300 - Train Loss: 0.0808, Val Loss: 0.0717\n",
      "Epoch 100/300 - Train Loss: 0.0793, Val Loss: 0.0728\n",
      "Epoch 101/300 - Train Loss: 0.0803, Val Loss: 0.0715\n",
      "Epoch 102/300 - Train Loss: 0.0798, Val Loss: 0.0737\n",
      "Epoch 103/300 - Train Loss: 0.0792, Val Loss: 0.0723\n",
      "Epoch 104/300 - Train Loss: 0.0795, Val Loss: 0.0726\n",
      "Epoch 105/300 - Train Loss: 0.0786, Val Loss: 0.0725\n",
      "Epoch 106/300 - Train Loss: 0.0793, Val Loss: 0.0725\n",
      "Epoch 107/300 - Train Loss: 0.0788, Val Loss: 0.0740\n",
      "Epoch 108/300 - Train Loss: 0.0790, Val Loss: 0.0711\n",
      "Epoch 109/300 - Train Loss: 0.0802, Val Loss: 0.0733\n",
      "Epoch 110/300 - Train Loss: 0.0777, Val Loss: 0.0708\n",
      "Epoch 111/300 - Train Loss: 0.0803, Val Loss: 0.0727\n",
      "Epoch 112/300 - Train Loss: 0.0780, Val Loss: 0.0698\n",
      "Epoch 113/300 - Train Loss: 0.0790, Val Loss: 0.0734\n",
      "Epoch 114/300 - Train Loss: 0.0786, Val Loss: 0.0700\n",
      "Epoch 115/300 - Train Loss: 0.0793, Val Loss: 0.0716\n",
      "Epoch 116/300 - Train Loss: 0.0800, Val Loss: 0.0729\n",
      "Epoch 117/300 - Train Loss: 0.0779, Val Loss: 0.0717\n",
      "Epoch 118/300 - Train Loss: 0.0792, Val Loss: 0.0727\n",
      "Epoch 119/300 - Train Loss: 0.0797, Val Loss: 0.0728\n",
      "Epoch 120/300 - Train Loss: 0.0778, Val Loss: 0.0741\n",
      "Epoch 121/300 - Train Loss: 0.0777, Val Loss: 0.0712\n",
      "Epoch 122/300 - Train Loss: 0.0772, Val Loss: 0.0710\n",
      "Epoch 123/300 - Train Loss: 0.0790, Val Loss: 0.0698\n",
      "Epoch 124/300 - Train Loss: 0.0774, Val Loss: 0.0707\n",
      "Epoch 125/300 - Train Loss: 0.0805, Val Loss: 0.0706\n",
      "Epoch 126/300 - Train Loss: 0.0782, Val Loss: 0.0712\n",
      "Epoch 127/300 - Train Loss: 0.0773, Val Loss: 0.0710\n",
      "Epoch 128/300 - Train Loss: 0.0791, Val Loss: 0.0708\n",
      "Epoch 129/300 - Train Loss: 0.0782, Val Loss: 0.0711\n",
      "Epoch 130/300 - Train Loss: 0.0756, Val Loss: 0.0707\n",
      "Epoch 131/300 - Train Loss: 0.0769, Val Loss: 0.0711\n",
      "Epoch 132/300 - Train Loss: 0.0764, Val Loss: 0.0728\n",
      "Epoch 133/300 - Train Loss: 0.0755, Val Loss: 0.0698\n",
      "Epoch 134/300 - Train Loss: 0.0775, Val Loss: 0.0706\n",
      "Epoch 135/300 - Train Loss: 0.0775, Val Loss: 0.0729\n",
      "Epoch 136/300 - Train Loss: 0.0778, Val Loss: 0.0726\n",
      "Epoch 137/300 - Train Loss: 0.0773, Val Loss: 0.0704\n",
      "Epoch 138/300 - Train Loss: 0.0767, Val Loss: 0.0703\n",
      "Epoch 139/300 - Train Loss: 0.0773, Val Loss: 0.0708\n",
      "Epoch 140/300 - Train Loss: 0.0762, Val Loss: 0.0711\n",
      "Epoch 141/300 - Train Loss: 0.0776, Val Loss: 0.0725\n",
      "Epoch 142/300 - Train Loss: 0.0766, Val Loss: 0.0693\n",
      "Epoch 143/300 - Train Loss: 0.0775, Val Loss: 0.0715\n",
      "Epoch 144/300 - Train Loss: 0.0771, Val Loss: 0.0702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0775, Val Loss: 0.0710\n",
      "Epoch 146/300 - Train Loss: 0.0756, Val Loss: 0.0703\n",
      "Epoch 147/300 - Train Loss: 0.0745, Val Loss: 0.0728\n",
      "Epoch 148/300 - Train Loss: 0.0759, Val Loss: 0.0713\n",
      "Epoch 149/300 - Train Loss: 0.0753, Val Loss: 0.0709\n",
      "Epoch 150/300 - Train Loss: 0.0763, Val Loss: 0.0701\n",
      "Epoch 151/300 - Train Loss: 0.0757, Val Loss: 0.0697\n",
      "Epoch 152/300 - Train Loss: 0.0749, Val Loss: 0.0713\n",
      "Epoch 153/300 - Train Loss: 0.0746, Val Loss: 0.0702\n",
      "Epoch 154/300 - Train Loss: 0.0751, Val Loss: 0.0694\n",
      "Epoch 155/300 - Train Loss: 0.0762, Val Loss: 0.0721\n",
      "Epoch 156/300 - Train Loss: 0.0756, Val Loss: 0.0706\n",
      "Epoch 157/300 - Train Loss: 0.0760, Val Loss: 0.0712\n",
      "Epoch 158/300 - Train Loss: 0.0761, Val Loss: 0.0689\n",
      "Epoch 159/300 - Train Loss: 0.0745, Val Loss: 0.0713\n",
      "Epoch 160/300 - Train Loss: 0.0744, Val Loss: 0.0738\n",
      "Epoch 161/300 - Train Loss: 0.0749, Val Loss: 0.0701\n",
      "Epoch 162/300 - Train Loss: 0.0757, Val Loss: 0.0700\n",
      "Epoch 163/300 - Train Loss: 0.0757, Val Loss: 0.0712\n",
      "Epoch 164/300 - Train Loss: 0.0743, Val Loss: 0.0717\n",
      "Epoch 165/300 - Train Loss: 0.0753, Val Loss: 0.0703\n",
      "Epoch 166/300 - Train Loss: 0.0758, Val Loss: 0.0706\n",
      "Epoch 167/300 - Train Loss: 0.0757, Val Loss: 0.0703\n",
      "Epoch 168/300 - Train Loss: 0.0746, Val Loss: 0.0722\n",
      "Epoch 169/300 - Train Loss: 0.0748, Val Loss: 0.0710\n",
      "Epoch 170/300 - Train Loss: 0.0756, Val Loss: 0.0718\n",
      "Epoch 171/300 - Train Loss: 0.0730, Val Loss: 0.0704\n",
      "Epoch 172/300 - Train Loss: 0.0745, Val Loss: 0.0712\n",
      "Epoch 173/300 - Train Loss: 0.0754, Val Loss: 0.0688\n",
      "Epoch 174/300 - Train Loss: 0.0734, Val Loss: 0.0714\n",
      "Epoch 175/300 - Train Loss: 0.0770, Val Loss: 0.0685\n",
      "Epoch 176/300 - Train Loss: 0.0741, Val Loss: 0.0718\n",
      "Epoch 177/300 - Train Loss: 0.0751, Val Loss: 0.0717\n",
      "Epoch 178/300 - Train Loss: 0.0742, Val Loss: 0.0704\n",
      "Epoch 179/300 - Train Loss: 0.0756, Val Loss: 0.0702\n",
      "Epoch 180/300 - Train Loss: 0.0739, Val Loss: 0.0731\n",
      "Epoch 181/300 - Train Loss: 0.0744, Val Loss: 0.0714\n",
      "Epoch 182/300 - Train Loss: 0.0728, Val Loss: 0.0720\n",
      "Epoch 183/300 - Train Loss: 0.0731, Val Loss: 0.0716\n",
      "Epoch 184/300 - Train Loss: 0.0728, Val Loss: 0.0706\n",
      "Epoch 185/300 - Train Loss: 0.0726, Val Loss: 0.0714\n",
      "Epoch 186/300 - Train Loss: 0.0757, Val Loss: 0.0695\n",
      "Epoch 187/300 - Train Loss: 0.0745, Val Loss: 0.0694\n",
      "Epoch 188/300 - Train Loss: 0.0740, Val Loss: 0.0716\n",
      "Epoch 189/300 - Train Loss: 0.0729, Val Loss: 0.0705\n",
      "Epoch 190/300 - Train Loss: 0.0734, Val Loss: 0.0705\n",
      "Epoch 191/300 - Train Loss: 0.0739, Val Loss: 0.0703\n",
      "Epoch 192/300 - Train Loss: 0.0738, Val Loss: 0.0698\n",
      "Epoch 193/300 - Train Loss: 0.0740, Val Loss: 0.0701\n",
      "Epoch 194/300 - Train Loss: 0.0723, Val Loss: 0.0695\n",
      "Epoch 195/300 - Train Loss: 0.0732, Val Loss: 0.0700\n",
      "Epoch 196/300 - Train Loss: 0.0742, Val Loss: 0.0698\n",
      "Epoch 197/300 - Train Loss: 0.0747, Val Loss: 0.0700\n",
      "Epoch 198/300 - Train Loss: 0.0722, Val Loss: 0.0712\n",
      "Epoch 199/300 - Train Loss: 0.0718, Val Loss: 0.0726\n",
      "Epoch 200/300 - Train Loss: 0.0734, Val Loss: 0.0696\n",
      "Epoch 201/300 - Train Loss: 0.0737, Val Loss: 0.0709\n",
      "Epoch 202/300 - Train Loss: 0.0739, Val Loss: 0.0696\n",
      "Epoch 203/300 - Train Loss: 0.0731, Val Loss: 0.0690\n",
      "Epoch 204/300 - Train Loss: 0.0737, Val Loss: 0.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:09:54,583] Trial 23 finished with value: 0.9678781350067697 and parameters: {'F1': 16, 'F2': 8, 'D': 4, 'dropout': 0.2554674892715744, 'learning_rate': 2.1386710552032182e-05, 'batch_size': 64, 'weight_decay': 2.094201283059875e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 205/300 - Train Loss: 0.0725, Val Loss: 0.0714\n",
      "Early stopping at epoch 205\n",
      "Macro F1 Score: 0.9679, Macro Precision: 0.9557, Macro Recall: 0.9815\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 25\n",
      "Training with F1=16, F2=8, D=4, dropout=0.6992320521301457, LR=4.037810320388766e-05, BS=64, WD=5.417262574317252e-05\n",
      "Epoch 1/300 - Train Loss: 0.9424, Val Loss: 0.7585\n",
      "Epoch 2/300 - Train Loss: 0.5810, Val Loss: 0.5066\n",
      "Epoch 3/300 - Train Loss: 0.4387, Val Loss: 0.3798\n",
      "Epoch 4/300 - Train Loss: 0.3684, Val Loss: 0.3115\n",
      "Epoch 5/300 - Train Loss: 0.3170, Val Loss: 0.2715\n",
      "Epoch 6/300 - Train Loss: 0.2856, Val Loss: 0.2400\n",
      "Epoch 7/300 - Train Loss: 0.2693, Val Loss: 0.2187\n",
      "Epoch 8/300 - Train Loss: 0.2442, Val Loss: 0.1951\n",
      "Epoch 9/300 - Train Loss: 0.2209, Val Loss: 0.1728\n",
      "Epoch 10/300 - Train Loss: 0.1981, Val Loss: 0.1548\n",
      "Epoch 11/300 - Train Loss: 0.1834, Val Loss: 0.1375\n",
      "Epoch 12/300 - Train Loss: 0.1727, Val Loss: 0.1318\n",
      "Epoch 13/300 - Train Loss: 0.1602, Val Loss: 0.1224\n",
      "Epoch 14/300 - Train Loss: 0.1560, Val Loss: 0.1172\n",
      "Epoch 15/300 - Train Loss: 0.1490, Val Loss: 0.1119\n",
      "Epoch 16/300 - Train Loss: 0.1466, Val Loss: 0.1082\n",
      "Epoch 17/300 - Train Loss: 0.1381, Val Loss: 0.1049\n",
      "Epoch 18/300 - Train Loss: 0.1359, Val Loss: 0.0999\n",
      "Epoch 19/300 - Train Loss: 0.1297, Val Loss: 0.0974\n",
      "Epoch 20/300 - Train Loss: 0.1288, Val Loss: 0.0970\n",
      "Epoch 21/300 - Train Loss: 0.1266, Val Loss: 0.0939\n",
      "Epoch 22/300 - Train Loss: 0.1249, Val Loss: 0.0925\n",
      "Epoch 23/300 - Train Loss: 0.1202, Val Loss: 0.0921\n",
      "Epoch 24/300 - Train Loss: 0.1220, Val Loss: 0.0892\n",
      "Epoch 25/300 - Train Loss: 0.1192, Val Loss: 0.0886\n",
      "Epoch 26/300 - Train Loss: 0.1183, Val Loss: 0.0870\n",
      "Epoch 27/300 - Train Loss: 0.1159, Val Loss: 0.0867\n",
      "Epoch 28/300 - Train Loss: 0.1172, Val Loss: 0.0843\n",
      "Epoch 29/300 - Train Loss: 0.1143, Val Loss: 0.0859\n",
      "Epoch 30/300 - Train Loss: 0.1148, Val Loss: 0.0833\n",
      "Epoch 31/300 - Train Loss: 0.1144, Val Loss: 0.0836\n",
      "Epoch 32/300 - Train Loss: 0.1105, Val Loss: 0.0851\n",
      "Epoch 33/300 - Train Loss: 0.1141, Val Loss: 0.0841\n",
      "Epoch 34/300 - Train Loss: 0.1132, Val Loss: 0.0848\n",
      "Epoch 35/300 - Train Loss: 0.1100, Val Loss: 0.0829\n",
      "Epoch 36/300 - Train Loss: 0.1113, Val Loss: 0.0842\n",
      "Epoch 37/300 - Train Loss: 0.1105, Val Loss: 0.0817\n",
      "Epoch 38/300 - Train Loss: 0.1105, Val Loss: 0.0822\n",
      "Epoch 39/300 - Train Loss: 0.1116, Val Loss: 0.0801\n",
      "Epoch 40/300 - Train Loss: 0.1116, Val Loss: 0.0811\n",
      "Epoch 41/300 - Train Loss: 0.1111, Val Loss: 0.0831\n",
      "Epoch 42/300 - Train Loss: 0.1077, Val Loss: 0.0803\n",
      "Epoch 43/300 - Train Loss: 0.1102, Val Loss: 0.0805\n",
      "Epoch 44/300 - Train Loss: 0.1093, Val Loss: 0.0798\n",
      "Epoch 45/300 - Train Loss: 0.1078, Val Loss: 0.0826\n",
      "Epoch 46/300 - Train Loss: 0.1099, Val Loss: 0.0804\n",
      "Epoch 47/300 - Train Loss: 0.1088, Val Loss: 0.0821\n",
      "Epoch 48/300 - Train Loss: 0.1091, Val Loss: 0.0806\n",
      "Epoch 49/300 - Train Loss: 0.1073, Val Loss: 0.0797\n",
      "Epoch 50/300 - Train Loss: 0.1090, Val Loss: 0.0794\n",
      "Epoch 51/300 - Train Loss: 0.1069, Val Loss: 0.0800\n",
      "Epoch 52/300 - Train Loss: 0.1087, Val Loss: 0.0795\n",
      "Epoch 53/300 - Train Loss: 0.1041, Val Loss: 0.0799\n",
      "Epoch 54/300 - Train Loss: 0.1062, Val Loss: 0.0782\n",
      "Epoch 55/300 - Train Loss: 0.1040, Val Loss: 0.0793\n",
      "Epoch 56/300 - Train Loss: 0.1070, Val Loss: 0.0782\n",
      "Epoch 57/300 - Train Loss: 0.1082, Val Loss: 0.0781\n",
      "Epoch 58/300 - Train Loss: 0.1028, Val Loss: 0.0791\n",
      "Epoch 59/300 - Train Loss: 0.1070, Val Loss: 0.0780\n",
      "Epoch 60/300 - Train Loss: 0.1051, Val Loss: 0.0799\n",
      "Epoch 61/300 - Train Loss: 0.1035, Val Loss: 0.0780\n",
      "Epoch 62/300 - Train Loss: 0.1047, Val Loss: 0.0785\n",
      "Epoch 63/300 - Train Loss: 0.1062, Val Loss: 0.0792\n",
      "Epoch 64/300 - Train Loss: 0.1054, Val Loss: 0.0791\n",
      "Epoch 65/300 - Train Loss: 0.1027, Val Loss: 0.0771\n",
      "Epoch 66/300 - Train Loss: 0.1031, Val Loss: 0.0766\n",
      "Epoch 67/300 - Train Loss: 0.1043, Val Loss: 0.0775\n",
      "Epoch 68/300 - Train Loss: 0.1025, Val Loss: 0.0805\n",
      "Epoch 69/300 - Train Loss: 0.1025, Val Loss: 0.0767\n",
      "Epoch 70/300 - Train Loss: 0.1028, Val Loss: 0.0761\n",
      "Epoch 71/300 - Train Loss: 0.1021, Val Loss: 0.0772\n",
      "Epoch 72/300 - Train Loss: 0.1054, Val Loss: 0.0784\n",
      "Epoch 73/300 - Train Loss: 0.1032, Val Loss: 0.0785\n",
      "Epoch 74/300 - Train Loss: 0.1026, Val Loss: 0.0773\n",
      "Epoch 75/300 - Train Loss: 0.1034, Val Loss: 0.0789\n",
      "Epoch 76/300 - Train Loss: 0.1024, Val Loss: 0.0796\n",
      "Epoch 77/300 - Train Loss: 0.1015, Val Loss: 0.0786\n",
      "Epoch 78/300 - Train Loss: 0.1035, Val Loss: 0.0804\n",
      "Epoch 79/300 - Train Loss: 0.1021, Val Loss: 0.0768\n",
      "Epoch 80/300 - Train Loss: 0.1020, Val Loss: 0.0784\n",
      "Epoch 81/300 - Train Loss: 0.1030, Val Loss: 0.0772\n",
      "Epoch 82/300 - Train Loss: 0.1021, Val Loss: 0.0764\n",
      "Epoch 83/300 - Train Loss: 0.1044, Val Loss: 0.0765\n",
      "Epoch 84/300 - Train Loss: 0.1013, Val Loss: 0.0785\n",
      "Epoch 85/300 - Train Loss: 0.1047, Val Loss: 0.0774\n",
      "Epoch 86/300 - Train Loss: 0.1014, Val Loss: 0.0759\n",
      "Epoch 87/300 - Train Loss: 0.1004, Val Loss: 0.0778\n",
      "Epoch 88/300 - Train Loss: 0.1002, Val Loss: 0.0761\n",
      "Epoch 89/300 - Train Loss: 0.0992, Val Loss: 0.0775\n",
      "Epoch 90/300 - Train Loss: 0.1024, Val Loss: 0.0781\n",
      "Epoch 91/300 - Train Loss: 0.1011, Val Loss: 0.0764\n",
      "Epoch 92/300 - Train Loss: 0.1030, Val Loss: 0.0782\n",
      "Epoch 93/300 - Train Loss: 0.1003, Val Loss: 0.0772\n",
      "Epoch 94/300 - Train Loss: 0.1003, Val Loss: 0.0777\n",
      "Epoch 95/300 - Train Loss: 0.0987, Val Loss: 0.0767\n",
      "Epoch 96/300 - Train Loss: 0.1014, Val Loss: 0.0766\n",
      "Epoch 97/300 - Train Loss: 0.1014, Val Loss: 0.0761\n",
      "Epoch 98/300 - Train Loss: 0.0996, Val Loss: 0.0757\n",
      "Epoch 99/300 - Train Loss: 0.0999, Val Loss: 0.0762\n",
      "Epoch 100/300 - Train Loss: 0.1007, Val Loss: 0.0769\n",
      "Epoch 101/300 - Train Loss: 0.1020, Val Loss: 0.0776\n",
      "Epoch 102/300 - Train Loss: 0.0987, Val Loss: 0.0778\n",
      "Epoch 103/300 - Train Loss: 0.0986, Val Loss: 0.0759\n",
      "Epoch 104/300 - Train Loss: 0.1007, Val Loss: 0.0765\n",
      "Epoch 105/300 - Train Loss: 0.0999, Val Loss: 0.0757\n",
      "Epoch 106/300 - Train Loss: 0.1004, Val Loss: 0.0759\n",
      "Epoch 107/300 - Train Loss: 0.0984, Val Loss: 0.0758\n",
      "Epoch 108/300 - Train Loss: 0.0985, Val Loss: 0.0765\n",
      "Epoch 109/300 - Train Loss: 0.0965, Val Loss: 0.0764\n",
      "Epoch 110/300 - Train Loss: 0.0995, Val Loss: 0.0763\n",
      "Epoch 111/300 - Train Loss: 0.0964, Val Loss: 0.0754\n",
      "Epoch 112/300 - Train Loss: 0.0985, Val Loss: 0.0757\n",
      "Epoch 113/300 - Train Loss: 0.0976, Val Loss: 0.0751\n",
      "Epoch 114/300 - Train Loss: 0.0984, Val Loss: 0.0748\n",
      "Epoch 115/300 - Train Loss: 0.0989, Val Loss: 0.0785\n",
      "Epoch 116/300 - Train Loss: 0.1005, Val Loss: 0.0785\n",
      "Epoch 117/300 - Train Loss: 0.0955, Val Loss: 0.0765\n",
      "Epoch 118/300 - Train Loss: 0.0978, Val Loss: 0.0746\n",
      "Epoch 119/300 - Train Loss: 0.1002, Val Loss: 0.0750\n",
      "Epoch 120/300 - Train Loss: 0.0964, Val Loss: 0.0756\n",
      "Epoch 121/300 - Train Loss: 0.0989, Val Loss: 0.0755\n",
      "Epoch 122/300 - Train Loss: 0.0955, Val Loss: 0.0769\n",
      "Epoch 123/300 - Train Loss: 0.0989, Val Loss: 0.0762\n",
      "Epoch 124/300 - Train Loss: 0.0982, Val Loss: 0.0738\n",
      "Epoch 125/300 - Train Loss: 0.0952, Val Loss: 0.0747\n",
      "Epoch 126/300 - Train Loss: 0.0953, Val Loss: 0.0735\n",
      "Epoch 127/300 - Train Loss: 0.0987, Val Loss: 0.0749\n",
      "Epoch 128/300 - Train Loss: 0.0967, Val Loss: 0.0757\n",
      "Epoch 129/300 - Train Loss: 0.0953, Val Loss: 0.0757\n",
      "Epoch 130/300 - Train Loss: 0.0969, Val Loss: 0.0749\n",
      "Epoch 131/300 - Train Loss: 0.0954, Val Loss: 0.0763\n",
      "Epoch 132/300 - Train Loss: 0.0984, Val Loss: 0.0769\n",
      "Epoch 133/300 - Train Loss: 0.0958, Val Loss: 0.0755\n",
      "Epoch 134/300 - Train Loss: 0.0953, Val Loss: 0.0745\n",
      "Epoch 135/300 - Train Loss: 0.0980, Val Loss: 0.0741\n",
      "Epoch 136/300 - Train Loss: 0.0980, Val Loss: 0.0740\n",
      "Epoch 137/300 - Train Loss: 0.0933, Val Loss: 0.0737\n",
      "Epoch 138/300 - Train Loss: 0.0954, Val Loss: 0.0741\n",
      "Epoch 139/300 - Train Loss: 0.0973, Val Loss: 0.0751\n",
      "Epoch 140/300 - Train Loss: 0.0986, Val Loss: 0.0737\n",
      "Epoch 141/300 - Train Loss: 0.0968, Val Loss: 0.0732\n",
      "Epoch 142/300 - Train Loss: 0.0983, Val Loss: 0.0745\n",
      "Epoch 143/300 - Train Loss: 0.0958, Val Loss: 0.0750\n",
      "Epoch 144/300 - Train Loss: 0.0978, Val Loss: 0.0737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0954, Val Loss: 0.0764\n",
      "Epoch 146/300 - Train Loss: 0.0929, Val Loss: 0.0739\n",
      "Epoch 147/300 - Train Loss: 0.0942, Val Loss: 0.0752\n",
      "Epoch 148/300 - Train Loss: 0.0977, Val Loss: 0.0738\n",
      "Epoch 149/300 - Train Loss: 0.0929, Val Loss: 0.0733\n",
      "Epoch 150/300 - Train Loss: 0.0953, Val Loss: 0.0741\n",
      "Epoch 151/300 - Train Loss: 0.0954, Val Loss: 0.0743\n",
      "Epoch 152/300 - Train Loss: 0.0959, Val Loss: 0.0733\n",
      "Epoch 153/300 - Train Loss: 0.0951, Val Loss: 0.0745\n",
      "Epoch 154/300 - Train Loss: 0.0957, Val Loss: 0.0733\n",
      "Epoch 155/300 - Train Loss: 0.0950, Val Loss: 0.0740\n",
      "Epoch 156/300 - Train Loss: 0.0946, Val Loss: 0.0727\n",
      "Epoch 157/300 - Train Loss: 0.0932, Val Loss: 0.0737\n",
      "Epoch 158/300 - Train Loss: 0.0949, Val Loss: 0.0730\n",
      "Epoch 159/300 - Train Loss: 0.0948, Val Loss: 0.0727\n",
      "Epoch 160/300 - Train Loss: 0.0952, Val Loss: 0.0730\n",
      "Epoch 161/300 - Train Loss: 0.0960, Val Loss: 0.0749\n",
      "Epoch 162/300 - Train Loss: 0.0942, Val Loss: 0.0727\n",
      "Epoch 163/300 - Train Loss: 0.0952, Val Loss: 0.0717\n",
      "Epoch 164/300 - Train Loss: 0.0938, Val Loss: 0.0739\n",
      "Epoch 165/300 - Train Loss: 0.0919, Val Loss: 0.0732\n",
      "Epoch 166/300 - Train Loss: 0.0939, Val Loss: 0.0716\n",
      "Epoch 167/300 - Train Loss: 0.0950, Val Loss: 0.0747\n",
      "Epoch 168/300 - Train Loss: 0.0958, Val Loss: 0.0725\n",
      "Epoch 169/300 - Train Loss: 0.0966, Val Loss: 0.0731\n",
      "Epoch 170/300 - Train Loss: 0.0954, Val Loss: 0.0727\n",
      "Epoch 171/300 - Train Loss: 0.0954, Val Loss: 0.0726\n",
      "Epoch 172/300 - Train Loss: 0.0945, Val Loss: 0.0793\n",
      "Epoch 173/300 - Train Loss: 0.0954, Val Loss: 0.0751\n",
      "Epoch 174/300 - Train Loss: 0.0967, Val Loss: 0.0716\n",
      "Epoch 175/300 - Train Loss: 0.0952, Val Loss: 0.0737\n",
      "Epoch 176/300 - Train Loss: 0.0938, Val Loss: 0.0725\n",
      "Epoch 177/300 - Train Loss: 0.0934, Val Loss: 0.0723\n",
      "Epoch 178/300 - Train Loss: 0.0946, Val Loss: 0.0725\n",
      "Epoch 179/300 - Train Loss: 0.0956, Val Loss: 0.0728\n",
      "Epoch 180/300 - Train Loss: 0.0914, Val Loss: 0.0716\n",
      "Epoch 181/300 - Train Loss: 0.0927, Val Loss: 0.0739\n",
      "Epoch 182/300 - Train Loss: 0.0926, Val Loss: 0.0726\n",
      "Epoch 183/300 - Train Loss: 0.0928, Val Loss: 0.0722\n",
      "Epoch 184/300 - Train Loss: 0.0927, Val Loss: 0.0729\n",
      "Epoch 185/300 - Train Loss: 0.0947, Val Loss: 0.0713\n",
      "Epoch 186/300 - Train Loss: 0.0924, Val Loss: 0.0730\n",
      "Epoch 187/300 - Train Loss: 0.0907, Val Loss: 0.0720\n",
      "Epoch 188/300 - Train Loss: 0.0951, Val Loss: 0.0729\n",
      "Epoch 189/300 - Train Loss: 0.0936, Val Loss: 0.0728\n",
      "Epoch 190/300 - Train Loss: 0.0951, Val Loss: 0.0723\n",
      "Epoch 191/300 - Train Loss: 0.0935, Val Loss: 0.0731\n",
      "Epoch 192/300 - Train Loss: 0.0928, Val Loss: 0.0715\n",
      "Epoch 193/300 - Train Loss: 0.0924, Val Loss: 0.0731\n",
      "Epoch 194/300 - Train Loss: 0.0941, Val Loss: 0.0707\n",
      "Epoch 195/300 - Train Loss: 0.0943, Val Loss: 0.0721\n",
      "Epoch 196/300 - Train Loss: 0.0943, Val Loss: 0.0724\n",
      "Epoch 197/300 - Train Loss: 0.0934, Val Loss: 0.0726\n",
      "Epoch 198/300 - Train Loss: 0.0910, Val Loss: 0.0721\n",
      "Epoch 199/300 - Train Loss: 0.0923, Val Loss: 0.0719\n",
      "Epoch 200/300 - Train Loss: 0.0921, Val Loss: 0.0710\n",
      "Epoch 201/300 - Train Loss: 0.0918, Val Loss: 0.0724\n",
      "Epoch 202/300 - Train Loss: 0.0920, Val Loss: 0.0729\n",
      "Epoch 203/300 - Train Loss: 0.0942, Val Loss: 0.0706\n",
      "Epoch 204/300 - Train Loss: 0.0916, Val Loss: 0.0723\n",
      "Epoch 205/300 - Train Loss: 0.0925, Val Loss: 0.0736\n",
      "Epoch 206/300 - Train Loss: 0.0919, Val Loss: 0.0716\n",
      "Epoch 207/300 - Train Loss: 0.0932, Val Loss: 0.0724\n",
      "Epoch 208/300 - Train Loss: 0.0913, Val Loss: 0.0713\n",
      "Epoch 209/300 - Train Loss: 0.0918, Val Loss: 0.0727\n",
      "Epoch 210/300 - Train Loss: 0.0913, Val Loss: 0.0719\n",
      "Epoch 211/300 - Train Loss: 0.0919, Val Loss: 0.0718\n",
      "Epoch 212/300 - Train Loss: 0.0926, Val Loss: 0.0713\n",
      "Epoch 213/300 - Train Loss: 0.0916, Val Loss: 0.0739\n",
      "Epoch 214/300 - Train Loss: 0.0923, Val Loss: 0.0723\n",
      "Epoch 215/300 - Train Loss: 0.0937, Val Loss: 0.0727\n",
      "Epoch 216/300 - Train Loss: 0.0926, Val Loss: 0.0718\n",
      "Epoch 217/300 - Train Loss: 0.0907, Val Loss: 0.0706\n",
      "Epoch 218/300 - Train Loss: 0.0915, Val Loss: 0.0699\n",
      "Epoch 219/300 - Train Loss: 0.0912, Val Loss: 0.0722\n",
      "Epoch 220/300 - Train Loss: 0.0934, Val Loss: 0.0708\n",
      "Epoch 221/300 - Train Loss: 0.0926, Val Loss: 0.0708\n",
      "Epoch 222/300 - Train Loss: 0.0928, Val Loss: 0.0705\n",
      "Epoch 223/300 - Train Loss: 0.0914, Val Loss: 0.0711\n",
      "Epoch 224/300 - Train Loss: 0.0891, Val Loss: 0.0702\n",
      "Epoch 225/300 - Train Loss: 0.0922, Val Loss: 0.0713\n",
      "Epoch 226/300 - Train Loss: 0.0918, Val Loss: 0.0710\n",
      "Epoch 227/300 - Train Loss: 0.0915, Val Loss: 0.0726\n",
      "Epoch 228/300 - Train Loss: 0.0916, Val Loss: 0.0716\n",
      "Epoch 229/300 - Train Loss: 0.0914, Val Loss: 0.0724\n",
      "Epoch 230/300 - Train Loss: 0.0898, Val Loss: 0.0702\n",
      "Epoch 231/300 - Train Loss: 0.0909, Val Loss: 0.0716\n",
      "Epoch 232/300 - Train Loss: 0.0914, Val Loss: 0.0708\n",
      "Epoch 233/300 - Train Loss: 0.0933, Val Loss: 0.0703\n",
      "Epoch 234/300 - Train Loss: 0.0909, Val Loss: 0.0708\n",
      "Epoch 235/300 - Train Loss: 0.0928, Val Loss: 0.0715\n",
      "Epoch 236/300 - Train Loss: 0.0918, Val Loss: 0.0720\n",
      "Epoch 237/300 - Train Loss: 0.0928, Val Loss: 0.0719\n",
      "Epoch 238/300 - Train Loss: 0.0913, Val Loss: 0.0720\n",
      "Epoch 239/300 - Train Loss: 0.0930, Val Loss: 0.0727\n",
      "Epoch 240/300 - Train Loss: 0.0935, Val Loss: 0.0717\n",
      "Epoch 241/300 - Train Loss: 0.0933, Val Loss: 0.0726\n",
      "Epoch 242/300 - Train Loss: 0.0899, Val Loss: 0.0705\n",
      "Epoch 243/300 - Train Loss: 0.0908, Val Loss: 0.0720\n",
      "Epoch 244/300 - Train Loss: 0.0919, Val Loss: 0.0723\n",
      "Epoch 245/300 - Train Loss: 0.0915, Val Loss: 0.0705\n",
      "Epoch 246/300 - Train Loss: 0.0896, Val Loss: 0.0713\n",
      "Epoch 247/300 - Train Loss: 0.0907, Val Loss: 0.0709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:18:07,295] Trial 24 finished with value: 0.9678566813130008 and parameters: {'F1': 16, 'F2': 8, 'D': 4, 'dropout': 0.6992320521301457, 'learning_rate': 4.037810320388766e-05, 'batch_size': 64, 'weight_decay': 5.417262574317252e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/300 - Train Loss: 0.0922, Val Loss: 0.0704\n",
      "Early stopping at epoch 248\n",
      "Macro F1 Score: 0.9679, Macro Precision: 0.9565, Macro Recall: 0.9808\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       1.00      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 26\n",
      "Training with F1=8, F2=8, D=4, dropout=0.3575776184785753, LR=9.544046163404694e-05, BS=32, WD=1.8771994039570136e-05\n",
      "Epoch 1/300 - Train Loss: 0.5012, Val Loss: 0.2472\n",
      "Epoch 2/300 - Train Loss: 0.2042, Val Loss: 0.1214\n",
      "Epoch 3/300 - Train Loss: 0.1421, Val Loss: 0.0967\n",
      "Epoch 4/300 - Train Loss: 0.1232, Val Loss: 0.0962\n",
      "Epoch 5/300 - Train Loss: 0.1161, Val Loss: 0.0874\n",
      "Epoch 6/300 - Train Loss: 0.1119, Val Loss: 0.0807\n",
      "Epoch 7/300 - Train Loss: 0.1109, Val Loss: 0.0776\n",
      "Epoch 8/300 - Train Loss: 0.1063, Val Loss: 0.0771\n",
      "Epoch 9/300 - Train Loss: 0.1067, Val Loss: 0.0813\n",
      "Epoch 10/300 - Train Loss: 0.1071, Val Loss: 0.0790\n",
      "Epoch 11/300 - Train Loss: 0.1026, Val Loss: 0.0802\n",
      "Epoch 12/300 - Train Loss: 0.1025, Val Loss: 0.0804\n",
      "Epoch 13/300 - Train Loss: 0.1032, Val Loss: 0.0765\n",
      "Epoch 14/300 - Train Loss: 0.1026, Val Loss: 0.0787\n",
      "Epoch 15/300 - Train Loss: 0.1018, Val Loss: 0.0716\n",
      "Epoch 16/300 - Train Loss: 0.1010, Val Loss: 0.0779\n",
      "Epoch 17/300 - Train Loss: 0.0991, Val Loss: 0.0766\n",
      "Epoch 18/300 - Train Loss: 0.0989, Val Loss: 0.0795\n",
      "Epoch 19/300 - Train Loss: 0.0995, Val Loss: 0.0762\n",
      "Epoch 20/300 - Train Loss: 0.0985, Val Loss: 0.0854\n",
      "Epoch 21/300 - Train Loss: 0.0972, Val Loss: 0.0748\n",
      "Epoch 22/300 - Train Loss: 0.0997, Val Loss: 0.0742\n",
      "Epoch 23/300 - Train Loss: 0.0954, Val Loss: 0.0761\n",
      "Epoch 24/300 - Train Loss: 0.0958, Val Loss: 0.0740\n",
      "Epoch 25/300 - Train Loss: 0.0952, Val Loss: 0.0741\n",
      "Epoch 26/300 - Train Loss: 0.0954, Val Loss: 0.0712\n",
      "Epoch 27/300 - Train Loss: 0.0978, Val Loss: 0.0719\n",
      "Epoch 28/300 - Train Loss: 0.0938, Val Loss: 0.0727\n",
      "Epoch 29/300 - Train Loss: 0.0943, Val Loss: 0.0746\n",
      "Epoch 30/300 - Train Loss: 0.0943, Val Loss: 0.0768\n",
      "Epoch 31/300 - Train Loss: 0.0952, Val Loss: 0.0721\n",
      "Epoch 32/300 - Train Loss: 0.0949, Val Loss: 0.0732\n",
      "Epoch 33/300 - Train Loss: 0.0911, Val Loss: 0.0761\n",
      "Epoch 34/300 - Train Loss: 0.0948, Val Loss: 0.0821\n",
      "Epoch 35/300 - Train Loss: 0.0935, Val Loss: 0.0697\n",
      "Epoch 36/300 - Train Loss: 0.0902, Val Loss: 0.0723\n",
      "Epoch 37/300 - Train Loss: 0.0925, Val Loss: 0.0718\n",
      "Epoch 38/300 - Train Loss: 0.0898, Val Loss: 0.0705\n",
      "Epoch 39/300 - Train Loss: 0.0902, Val Loss: 0.0728\n",
      "Epoch 40/300 - Train Loss: 0.0898, Val Loss: 0.0690\n",
      "Epoch 41/300 - Train Loss: 0.0925, Val Loss: 0.0732\n",
      "Epoch 42/300 - Train Loss: 0.0916, Val Loss: 0.0699\n",
      "Epoch 43/300 - Train Loss: 0.0910, Val Loss: 0.0785\n",
      "Epoch 44/300 - Train Loss: 0.0911, Val Loss: 0.0726\n",
      "Epoch 45/300 - Train Loss: 0.0922, Val Loss: 0.0724\n",
      "Epoch 46/300 - Train Loss: 0.0910, Val Loss: 0.0722\n",
      "Epoch 47/300 - Train Loss: 0.0891, Val Loss: 0.0734\n",
      "Epoch 48/300 - Train Loss: 0.0904, Val Loss: 0.0729\n",
      "Epoch 49/300 - Train Loss: 0.0885, Val Loss: 0.0684\n",
      "Epoch 50/300 - Train Loss: 0.0902, Val Loss: 0.0678\n",
      "Epoch 51/300 - Train Loss: 0.0878, Val Loss: 0.0712\n",
      "Epoch 52/300 - Train Loss: 0.0901, Val Loss: 0.0689\n",
      "Epoch 53/300 - Train Loss: 0.0880, Val Loss: 0.0715\n",
      "Epoch 54/300 - Train Loss: 0.0898, Val Loss: 0.0706\n",
      "Epoch 55/300 - Train Loss: 0.0907, Val Loss: 0.0730\n",
      "Epoch 56/300 - Train Loss: 0.0887, Val Loss: 0.0739\n",
      "Epoch 57/300 - Train Loss: 0.0885, Val Loss: 0.0705\n",
      "Epoch 58/300 - Train Loss: 0.0887, Val Loss: 0.0711\n",
      "Epoch 59/300 - Train Loss: 0.0869, Val Loss: 0.0716\n",
      "Epoch 60/300 - Train Loss: 0.0876, Val Loss: 0.0716\n",
      "Epoch 61/300 - Train Loss: 0.0893, Val Loss: 0.0743\n",
      "Epoch 62/300 - Train Loss: 0.0886, Val Loss: 0.0755\n",
      "Epoch 63/300 - Train Loss: 0.0861, Val Loss: 0.0689\n",
      "Epoch 64/300 - Train Loss: 0.0865, Val Loss: 0.0684\n",
      "Epoch 65/300 - Train Loss: 0.0882, Val Loss: 0.0709\n",
      "Epoch 66/300 - Train Loss: 0.0863, Val Loss: 0.0684\n",
      "Epoch 67/300 - Train Loss: 0.0835, Val Loss: 0.0716\n",
      "Epoch 68/300 - Train Loss: 0.0855, Val Loss: 0.0724\n",
      "Epoch 69/300 - Train Loss: 0.0867, Val Loss: 0.0699\n",
      "Epoch 70/300 - Train Loss: 0.0876, Val Loss: 0.0706\n",
      "Epoch 71/300 - Train Loss: 0.0875, Val Loss: 0.0710\n",
      "Epoch 72/300 - Train Loss: 0.0863, Val Loss: 0.0694\n",
      "Epoch 73/300 - Train Loss: 0.0872, Val Loss: 0.0764\n",
      "Epoch 74/300 - Train Loss: 0.0867, Val Loss: 0.0753\n",
      "Epoch 75/300 - Train Loss: 0.0851, Val Loss: 0.0678\n",
      "Epoch 76/300 - Train Loss: 0.0861, Val Loss: 0.0685\n",
      "Epoch 77/300 - Train Loss: 0.0888, Val Loss: 0.0702\n",
      "Epoch 78/300 - Train Loss: 0.0860, Val Loss: 0.0729\n",
      "Epoch 79/300 - Train Loss: 0.0856, Val Loss: 0.0695\n",
      "Epoch 80/300 - Train Loss: 0.0850, Val Loss: 0.0732\n",
      "Epoch 81/300 - Train Loss: 0.0859, Val Loss: 0.0675\n",
      "Epoch 82/300 - Train Loss: 0.0867, Val Loss: 0.0677\n",
      "Epoch 83/300 - Train Loss: 0.0851, Val Loss: 0.0706\n",
      "Epoch 84/300 - Train Loss: 0.0857, Val Loss: 0.0675\n",
      "Epoch 85/300 - Train Loss: 0.0841, Val Loss: 0.0724\n",
      "Epoch 86/300 - Train Loss: 0.0852, Val Loss: 0.0688\n",
      "Epoch 87/300 - Train Loss: 0.0838, Val Loss: 0.0722\n",
      "Epoch 88/300 - Train Loss: 0.0863, Val Loss: 0.0704\n",
      "Epoch 89/300 - Train Loss: 0.0850, Val Loss: 0.0729\n",
      "Epoch 90/300 - Train Loss: 0.0830, Val Loss: 0.0712\n",
      "Epoch 91/300 - Train Loss: 0.0862, Val Loss: 0.0697\n",
      "Epoch 92/300 - Train Loss: 0.0841, Val Loss: 0.0680\n",
      "Epoch 93/300 - Train Loss: 0.0837, Val Loss: 0.0707\n",
      "Epoch 94/300 - Train Loss: 0.0839, Val Loss: 0.0713\n",
      "Epoch 95/300 - Train Loss: 0.0871, Val Loss: 0.0694\n",
      "Epoch 96/300 - Train Loss: 0.0861, Val Loss: 0.0736\n",
      "Epoch 97/300 - Train Loss: 0.0832, Val Loss: 0.0737\n",
      "Epoch 98/300 - Train Loss: 0.0856, Val Loss: 0.0753\n",
      "Epoch 99/300 - Train Loss: 0.0833, Val Loss: 0.0722\n",
      "Epoch 100/300 - Train Loss: 0.0830, Val Loss: 0.0725\n",
      "Epoch 101/300 - Train Loss: 0.0827, Val Loss: 0.0704\n",
      "Epoch 102/300 - Train Loss: 0.0828, Val Loss: 0.0714\n",
      "Epoch 103/300 - Train Loss: 0.0859, Val Loss: 0.0759\n",
      "Epoch 104/300 - Train Loss: 0.0814, Val Loss: 0.0728\n",
      "Epoch 105/300 - Train Loss: 0.0877, Val Loss: 0.0678\n",
      "Epoch 106/300 - Train Loss: 0.0862, Val Loss: 0.0723\n",
      "Epoch 107/300 - Train Loss: 0.0835, Val Loss: 0.0716\n",
      "Epoch 108/300 - Train Loss: 0.0844, Val Loss: 0.0704\n",
      "Epoch 109/300 - Train Loss: 0.0821, Val Loss: 0.0713\n",
      "Epoch 110/300 - Train Loss: 0.0810, Val Loss: 0.0702\n",
      "Epoch 111/300 - Train Loss: 0.0844, Val Loss: 0.0745\n",
      "Epoch 112/300 - Train Loss: 0.0847, Val Loss: 0.0683\n",
      "Epoch 113/300 - Train Loss: 0.0838, Val Loss: 0.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:21:58,365] Trial 25 finished with value: 0.9635055735520851 and parameters: {'F1': 8, 'F2': 8, 'D': 4, 'dropout': 0.3575776184785753, 'learning_rate': 9.544046163404694e-05, 'batch_size': 32, 'weight_decay': 1.8771994039570136e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/300 - Train Loss: 0.0818, Val Loss: 0.0720\n",
      "Early stopping at epoch 114\n",
      "Macro F1 Score: 0.9635, Macro Precision: 0.9581, Macro Recall: 0.9694\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 27\n",
      "Training with F1=32, F2=16, D=2, dropout=0.2933519216609159, LR=3.653279950192325e-05, BS=64, WD=1.537136691816701e-05\n",
      "Epoch 1/300 - Train Loss: 0.6334, Val Loss: 0.3452\n",
      "Epoch 2/300 - Train Loss: 0.2775, Val Loss: 0.2287\n",
      "Epoch 3/300 - Train Loss: 0.2072, Val Loss: 0.1710\n",
      "Epoch 4/300 - Train Loss: 0.1657, Val Loss: 0.1436\n",
      "Epoch 5/300 - Train Loss: 0.1406, Val Loss: 0.1295\n",
      "Epoch 6/300 - Train Loss: 0.1270, Val Loss: 0.1100\n",
      "Epoch 7/300 - Train Loss: 0.1156, Val Loss: 0.1008\n",
      "Epoch 8/300 - Train Loss: 0.1114, Val Loss: 0.0948\n",
      "Epoch 9/300 - Train Loss: 0.1063, Val Loss: 0.0926\n",
      "Epoch 10/300 - Train Loss: 0.1008, Val Loss: 0.0962\n",
      "Epoch 11/300 - Train Loss: 0.1012, Val Loss: 0.0879\n",
      "Epoch 12/300 - Train Loss: 0.0972, Val Loss: 0.0839\n",
      "Epoch 13/300 - Train Loss: 0.0965, Val Loss: 0.0819\n",
      "Epoch 14/300 - Train Loss: 0.0952, Val Loss: 0.0838\n",
      "Epoch 15/300 - Train Loss: 0.0964, Val Loss: 0.0827\n",
      "Epoch 16/300 - Train Loss: 0.0922, Val Loss: 0.0860\n",
      "Epoch 17/300 - Train Loss: 0.0935, Val Loss: 0.0832\n",
      "Epoch 18/300 - Train Loss: 0.0932, Val Loss: 0.0821\n",
      "Epoch 19/300 - Train Loss: 0.0892, Val Loss: 0.0785\n",
      "Epoch 20/300 - Train Loss: 0.0898, Val Loss: 0.0780\n",
      "Epoch 21/300 - Train Loss: 0.0906, Val Loss: 0.0776\n",
      "Epoch 22/300 - Train Loss: 0.0866, Val Loss: 0.0770\n",
      "Epoch 23/300 - Train Loss: 0.0891, Val Loss: 0.0780\n",
      "Epoch 24/300 - Train Loss: 0.0886, Val Loss: 0.0787\n",
      "Epoch 25/300 - Train Loss: 0.0847, Val Loss: 0.0761\n",
      "Epoch 26/300 - Train Loss: 0.0868, Val Loss: 0.0766\n",
      "Epoch 27/300 - Train Loss: 0.0860, Val Loss: 0.0762\n",
      "Epoch 28/300 - Train Loss: 0.0852, Val Loss: 0.0766\n",
      "Epoch 29/300 - Train Loss: 0.0851, Val Loss: 0.0795\n",
      "Epoch 30/300 - Train Loss: 0.0849, Val Loss: 0.0791\n",
      "Epoch 31/300 - Train Loss: 0.0839, Val Loss: 0.0769\n",
      "Epoch 32/300 - Train Loss: 0.0849, Val Loss: 0.0762\n",
      "Epoch 33/300 - Train Loss: 0.0827, Val Loss: 0.0736\n",
      "Epoch 34/300 - Train Loss: 0.0837, Val Loss: 0.0756\n",
      "Epoch 35/300 - Train Loss: 0.0816, Val Loss: 0.0781\n",
      "Epoch 36/300 - Train Loss: 0.0823, Val Loss: 0.0748\n",
      "Epoch 37/300 - Train Loss: 0.0818, Val Loss: 0.0742\n",
      "Epoch 38/300 - Train Loss: 0.0814, Val Loss: 0.0747\n",
      "Epoch 39/300 - Train Loss: 0.0826, Val Loss: 0.0765\n",
      "Epoch 40/300 - Train Loss: 0.0803, Val Loss: 0.0733\n",
      "Epoch 41/300 - Train Loss: 0.0812, Val Loss: 0.0733\n",
      "Epoch 42/300 - Train Loss: 0.0791, Val Loss: 0.0753\n",
      "Epoch 43/300 - Train Loss: 0.0813, Val Loss: 0.0748\n",
      "Epoch 44/300 - Train Loss: 0.0792, Val Loss: 0.0743\n",
      "Epoch 45/300 - Train Loss: 0.0804, Val Loss: 0.0734\n",
      "Epoch 46/300 - Train Loss: 0.0819, Val Loss: 0.0743\n",
      "Epoch 47/300 - Train Loss: 0.0807, Val Loss: 0.0751\n",
      "Epoch 48/300 - Train Loss: 0.0804, Val Loss: 0.0753\n",
      "Epoch 49/300 - Train Loss: 0.0803, Val Loss: 0.0757\n",
      "Epoch 50/300 - Train Loss: 0.0796, Val Loss: 0.0744\n",
      "Epoch 51/300 - Train Loss: 0.0793, Val Loss: 0.0750\n",
      "Epoch 52/300 - Train Loss: 0.0779, Val Loss: 0.0727\n",
      "Epoch 53/300 - Train Loss: 0.0769, Val Loss: 0.0753\n",
      "Epoch 54/300 - Train Loss: 0.0792, Val Loss: 0.0734\n",
      "Epoch 55/300 - Train Loss: 0.0800, Val Loss: 0.0739\n",
      "Epoch 56/300 - Train Loss: 0.0777, Val Loss: 0.0737\n",
      "Epoch 57/300 - Train Loss: 0.0776, Val Loss: 0.0745\n",
      "Epoch 58/300 - Train Loss: 0.0767, Val Loss: 0.0737\n",
      "Epoch 59/300 - Train Loss: 0.0773, Val Loss: 0.0744\n",
      "Epoch 60/300 - Train Loss: 0.0770, Val Loss: 0.0721\n",
      "Epoch 61/300 - Train Loss: 0.0786, Val Loss: 0.0738\n",
      "Epoch 62/300 - Train Loss: 0.0770, Val Loss: 0.0744\n",
      "Epoch 63/300 - Train Loss: 0.0765, Val Loss: 0.0723\n",
      "Epoch 64/300 - Train Loss: 0.0765, Val Loss: 0.0760\n",
      "Epoch 65/300 - Train Loss: 0.0753, Val Loss: 0.0755\n",
      "Epoch 66/300 - Train Loss: 0.0763, Val Loss: 0.0708\n",
      "Epoch 67/300 - Train Loss: 0.0768, Val Loss: 0.0719\n",
      "Epoch 68/300 - Train Loss: 0.0761, Val Loss: 0.0752\n",
      "Epoch 69/300 - Train Loss: 0.0774, Val Loss: 0.0750\n",
      "Epoch 70/300 - Train Loss: 0.0756, Val Loss: 0.0749\n",
      "Epoch 71/300 - Train Loss: 0.0764, Val Loss: 0.0727\n",
      "Epoch 72/300 - Train Loss: 0.0760, Val Loss: 0.0735\n",
      "Epoch 73/300 - Train Loss: 0.0748, Val Loss: 0.0739\n",
      "Epoch 74/300 - Train Loss: 0.0754, Val Loss: 0.0742\n",
      "Epoch 75/300 - Train Loss: 0.0747, Val Loss: 0.0736\n",
      "Epoch 76/300 - Train Loss: 0.0745, Val Loss: 0.0762\n",
      "Epoch 77/300 - Train Loss: 0.0745, Val Loss: 0.0721\n",
      "Epoch 78/300 - Train Loss: 0.0739, Val Loss: 0.0715\n",
      "Epoch 79/300 - Train Loss: 0.0748, Val Loss: 0.0746\n",
      "Epoch 80/300 - Train Loss: 0.0737, Val Loss: 0.0747\n",
      "Epoch 81/300 - Train Loss: 0.0762, Val Loss: 0.0736\n",
      "Epoch 82/300 - Train Loss: 0.0762, Val Loss: 0.0734\n",
      "Epoch 83/300 - Train Loss: 0.0751, Val Loss: 0.0720\n",
      "Epoch 84/300 - Train Loss: 0.0735, Val Loss: 0.0730\n",
      "Epoch 85/300 - Train Loss: 0.0734, Val Loss: 0.0751\n",
      "Epoch 86/300 - Train Loss: 0.0730, Val Loss: 0.0727\n",
      "Epoch 87/300 - Train Loss: 0.0728, Val Loss: 0.0739\n",
      "Epoch 88/300 - Train Loss: 0.0727, Val Loss: 0.0722\n",
      "Epoch 89/300 - Train Loss: 0.0733, Val Loss: 0.0729\n",
      "Epoch 90/300 - Train Loss: 0.0728, Val Loss: 0.0723\n",
      "Epoch 91/300 - Train Loss: 0.0744, Val Loss: 0.0735\n",
      "Epoch 92/300 - Train Loss: 0.0731, Val Loss: 0.0735\n",
      "Epoch 93/300 - Train Loss: 0.0742, Val Loss: 0.0728\n",
      "Epoch 94/300 - Train Loss: 0.0739, Val Loss: 0.0730\n",
      "Epoch 95/300 - Train Loss: 0.0723, Val Loss: 0.0725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:25:35,653] Trial 26 finished with value: 0.9668809761441789 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.2933519216609159, 'learning_rate': 3.653279950192325e-05, 'batch_size': 64, 'weight_decay': 1.537136691816701e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/300 - Train Loss: 0.0727, Val Loss: 0.0722\n",
      "Early stopping at epoch 96\n",
      "Macro F1 Score: 0.9669, Macro Precision: 0.9550, Macro Recall: 0.9802\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 28\n",
      "Training with F1=4, F2=32, D=4, dropout=0.23822094568157912, LR=2.0380980935138715e-05, BS=128, WD=2.940964241033842e-05\n",
      "Epoch 1/300 - Train Loss: 0.9465, Val Loss: 0.7970\n",
      "Epoch 2/300 - Train Loss: 0.6468, Val Loss: 0.5493\n",
      "Epoch 3/300 - Train Loss: 0.4659, Val Loss: 0.4092\n",
      "Epoch 4/300 - Train Loss: 0.3802, Val Loss: 0.3515\n",
      "Epoch 5/300 - Train Loss: 0.3377, Val Loss: 0.3156\n",
      "Epoch 6/300 - Train Loss: 0.3133, Val Loss: 0.3007\n",
      "Epoch 7/300 - Train Loss: 0.2963, Val Loss: 0.2722\n",
      "Epoch 8/300 - Train Loss: 0.2802, Val Loss: 0.2609\n",
      "Epoch 9/300 - Train Loss: 0.2675, Val Loss: 0.2548\n",
      "Epoch 10/300 - Train Loss: 0.2549, Val Loss: 0.2388\n",
      "Epoch 11/300 - Train Loss: 0.2498, Val Loss: 0.2301\n",
      "Epoch 12/300 - Train Loss: 0.2431, Val Loss: 0.2233\n",
      "Epoch 13/300 - Train Loss: 0.2365, Val Loss: 0.2159\n",
      "Epoch 14/300 - Train Loss: 0.2298, Val Loss: 0.2071\n",
      "Epoch 15/300 - Train Loss: 0.2246, Val Loss: 0.2041\n",
      "Epoch 16/300 - Train Loss: 0.2187, Val Loss: 0.1977\n",
      "Epoch 17/300 - Train Loss: 0.2179, Val Loss: 0.1912\n",
      "Epoch 18/300 - Train Loss: 0.2129, Val Loss: 0.1881\n",
      "Epoch 19/300 - Train Loss: 0.2064, Val Loss: 0.1842\n",
      "Epoch 20/300 - Train Loss: 0.2046, Val Loss: 0.1811\n",
      "Epoch 21/300 - Train Loss: 0.2021, Val Loss: 0.1788\n",
      "Epoch 22/300 - Train Loss: 0.1980, Val Loss: 0.1770\n",
      "Epoch 23/300 - Train Loss: 0.1953, Val Loss: 0.1746\n",
      "Epoch 24/300 - Train Loss: 0.1945, Val Loss: 0.1715\n",
      "Epoch 25/300 - Train Loss: 0.1924, Val Loss: 0.1684\n",
      "Epoch 26/300 - Train Loss: 0.1890, Val Loss: 0.1668\n",
      "Epoch 27/300 - Train Loss: 0.1877, Val Loss: 0.1668\n",
      "Epoch 28/300 - Train Loss: 0.1833, Val Loss: 0.1630\n",
      "Epoch 29/300 - Train Loss: 0.1839, Val Loss: 0.1624\n",
      "Epoch 30/300 - Train Loss: 0.1820, Val Loss: 0.1595\n",
      "Epoch 31/300 - Train Loss: 0.1817, Val Loss: 0.1584\n",
      "Epoch 32/300 - Train Loss: 0.1790, Val Loss: 0.1564\n",
      "Epoch 33/300 - Train Loss: 0.1770, Val Loss: 0.1530\n",
      "Epoch 34/300 - Train Loss: 0.1753, Val Loss: 0.1536\n",
      "Epoch 35/300 - Train Loss: 0.1726, Val Loss: 0.1518\n",
      "Epoch 36/300 - Train Loss: 0.1705, Val Loss: 0.1485\n",
      "Epoch 37/300 - Train Loss: 0.1694, Val Loss: 0.1470\n",
      "Epoch 38/300 - Train Loss: 0.1677, Val Loss: 0.1457\n",
      "Epoch 39/300 - Train Loss: 0.1670, Val Loss: 0.1463\n",
      "Epoch 40/300 - Train Loss: 0.1636, Val Loss: 0.1426\n",
      "Epoch 41/300 - Train Loss: 0.1625, Val Loss: 0.1415\n",
      "Epoch 42/300 - Train Loss: 0.1608, Val Loss: 0.1409\n",
      "Epoch 43/300 - Train Loss: 0.1592, Val Loss: 0.1383\n",
      "Epoch 44/300 - Train Loss: 0.1558, Val Loss: 0.1376\n",
      "Epoch 45/300 - Train Loss: 0.1537, Val Loss: 0.1349\n",
      "Epoch 46/300 - Train Loss: 0.1533, Val Loss: 0.1342\n",
      "Epoch 47/300 - Train Loss: 0.1504, Val Loss: 0.1319\n",
      "Epoch 48/300 - Train Loss: 0.1482, Val Loss: 0.1305\n",
      "Epoch 49/300 - Train Loss: 0.1450, Val Loss: 0.1278\n",
      "Epoch 50/300 - Train Loss: 0.1435, Val Loss: 0.1252\n",
      "Epoch 51/300 - Train Loss: 0.1391, Val Loss: 0.1220\n",
      "Epoch 52/300 - Train Loss: 0.1369, Val Loss: 0.1227\n",
      "Epoch 53/300 - Train Loss: 0.1346, Val Loss: 0.1193\n",
      "Epoch 54/300 - Train Loss: 0.1321, Val Loss: 0.1176\n",
      "Epoch 55/300 - Train Loss: 0.1312, Val Loss: 0.1161\n",
      "Epoch 56/300 - Train Loss: 0.1278, Val Loss: 0.1147\n",
      "Epoch 57/300 - Train Loss: 0.1278, Val Loss: 0.1134\n",
      "Epoch 58/300 - Train Loss: 0.1258, Val Loss: 0.1140\n",
      "Epoch 59/300 - Train Loss: 0.1251, Val Loss: 0.1113\n",
      "Epoch 60/300 - Train Loss: 0.1238, Val Loss: 0.1113\n",
      "Epoch 61/300 - Train Loss: 0.1221, Val Loss: 0.1114\n",
      "Epoch 62/300 - Train Loss: 0.1199, Val Loss: 0.1093\n",
      "Epoch 63/300 - Train Loss: 0.1209, Val Loss: 0.1112\n",
      "Epoch 64/300 - Train Loss: 0.1180, Val Loss: 0.1079\n",
      "Epoch 65/300 - Train Loss: 0.1166, Val Loss: 0.1100\n",
      "Epoch 66/300 - Train Loss: 0.1140, Val Loss: 0.1041\n",
      "Epoch 67/300 - Train Loss: 0.1155, Val Loss: 0.1062\n",
      "Epoch 68/300 - Train Loss: 0.1144, Val Loss: 0.1042\n",
      "Epoch 69/300 - Train Loss: 0.1146, Val Loss: 0.1022\n",
      "Epoch 70/300 - Train Loss: 0.1122, Val Loss: 0.1018\n",
      "Epoch 71/300 - Train Loss: 0.1119, Val Loss: 0.1051\n",
      "Epoch 72/300 - Train Loss: 0.1121, Val Loss: 0.1009\n",
      "Epoch 73/300 - Train Loss: 0.1107, Val Loss: 0.0994\n",
      "Epoch 74/300 - Train Loss: 0.1088, Val Loss: 0.1014\n",
      "Epoch 75/300 - Train Loss: 0.1076, Val Loss: 0.0984\n",
      "Epoch 76/300 - Train Loss: 0.1053, Val Loss: 0.0995\n",
      "Epoch 77/300 - Train Loss: 0.1077, Val Loss: 0.0983\n",
      "Epoch 78/300 - Train Loss: 0.1067, Val Loss: 0.1015\n",
      "Epoch 79/300 - Train Loss: 0.1068, Val Loss: 0.0968\n",
      "Epoch 80/300 - Train Loss: 0.1061, Val Loss: 0.0963\n",
      "Epoch 81/300 - Train Loss: 0.1055, Val Loss: 0.0984\n",
      "Epoch 82/300 - Train Loss: 0.1032, Val Loss: 0.0944\n",
      "Epoch 83/300 - Train Loss: 0.1035, Val Loss: 0.0958\n",
      "Epoch 84/300 - Train Loss: 0.1027, Val Loss: 0.0943\n",
      "Epoch 85/300 - Train Loss: 0.1034, Val Loss: 0.0945\n",
      "Epoch 86/300 - Train Loss: 0.1017, Val Loss: 0.0948\n",
      "Epoch 87/300 - Train Loss: 0.1018, Val Loss: 0.0934\n",
      "Epoch 88/300 - Train Loss: 0.0988, Val Loss: 0.0926\n",
      "Epoch 89/300 - Train Loss: 0.1000, Val Loss: 0.0954\n",
      "Epoch 90/300 - Train Loss: 0.0992, Val Loss: 0.0919\n",
      "Epoch 91/300 - Train Loss: 0.0994, Val Loss: 0.0936\n",
      "Epoch 92/300 - Train Loss: 0.0992, Val Loss: 0.0911\n",
      "Epoch 93/300 - Train Loss: 0.0975, Val Loss: 0.0915\n",
      "Epoch 94/300 - Train Loss: 0.0977, Val Loss: 0.0902\n",
      "Epoch 95/300 - Train Loss: 0.0981, Val Loss: 0.0915\n",
      "Epoch 96/300 - Train Loss: 0.0970, Val Loss: 0.0924\n",
      "Epoch 97/300 - Train Loss: 0.0973, Val Loss: 0.0896\n",
      "Epoch 98/300 - Train Loss: 0.0953, Val Loss: 0.0915\n",
      "Epoch 99/300 - Train Loss: 0.0951, Val Loss: 0.0887\n",
      "Epoch 100/300 - Train Loss: 0.0951, Val Loss: 0.0902\n",
      "Epoch 101/300 - Train Loss: 0.0959, Val Loss: 0.0881\n",
      "Epoch 102/300 - Train Loss: 0.0937, Val Loss: 0.0873\n",
      "Epoch 103/300 - Train Loss: 0.0938, Val Loss: 0.0884\n",
      "Epoch 104/300 - Train Loss: 0.0942, Val Loss: 0.0876\n",
      "Epoch 105/300 - Train Loss: 0.0941, Val Loss: 0.0875\n",
      "Epoch 106/300 - Train Loss: 0.0948, Val Loss: 0.0866\n",
      "Epoch 107/300 - Train Loss: 0.0918, Val Loss: 0.0911\n",
      "Epoch 108/300 - Train Loss: 0.0930, Val Loss: 0.0877\n",
      "Epoch 109/300 - Train Loss: 0.0932, Val Loss: 0.0858\n",
      "Epoch 110/300 - Train Loss: 0.0920, Val Loss: 0.0864\n",
      "Epoch 111/300 - Train Loss: 0.0913, Val Loss: 0.0856\n",
      "Epoch 112/300 - Train Loss: 0.0915, Val Loss: 0.0884\n",
      "Epoch 113/300 - Train Loss: 0.0918, Val Loss: 0.0853\n",
      "Epoch 114/300 - Train Loss: 0.0927, Val Loss: 0.0873\n",
      "Epoch 115/300 - Train Loss: 0.0897, Val Loss: 0.0869\n",
      "Epoch 116/300 - Train Loss: 0.0895, Val Loss: 0.0870\n",
      "Epoch 117/300 - Train Loss: 0.0897, Val Loss: 0.0829\n",
      "Epoch 118/300 - Train Loss: 0.0888, Val Loss: 0.0875\n",
      "Epoch 119/300 - Train Loss: 0.0895, Val Loss: 0.0869\n",
      "Epoch 120/300 - Train Loss: 0.0909, Val Loss: 0.0849\n",
      "Epoch 121/300 - Train Loss: 0.0886, Val Loss: 0.0837\n",
      "Epoch 122/300 - Train Loss: 0.0891, Val Loss: 0.0854\n",
      "Epoch 123/300 - Train Loss: 0.0893, Val Loss: 0.0874\n",
      "Epoch 124/300 - Train Loss: 0.0883, Val Loss: 0.0850\n",
      "Epoch 125/300 - Train Loss: 0.0898, Val Loss: 0.0876\n",
      "Epoch 126/300 - Train Loss: 0.0875, Val Loss: 0.0855\n",
      "Epoch 127/300 - Train Loss: 0.0874, Val Loss: 0.0853\n",
      "Epoch 128/300 - Train Loss: 0.0878, Val Loss: 0.0862\n",
      "Epoch 129/300 - Train Loss: 0.0885, Val Loss: 0.0848\n",
      "Epoch 130/300 - Train Loss: 0.0882, Val Loss: 0.0825\n",
      "Epoch 131/300 - Train Loss: 0.0882, Val Loss: 0.0844\n",
      "Epoch 132/300 - Train Loss: 0.0861, Val Loss: 0.0852\n",
      "Epoch 133/300 - Train Loss: 0.0867, Val Loss: 0.0843\n",
      "Epoch 134/300 - Train Loss: 0.0869, Val Loss: 0.0894\n",
      "Epoch 135/300 - Train Loss: 0.0856, Val Loss: 0.0839\n",
      "Epoch 136/300 - Train Loss: 0.0859, Val Loss: 0.0846\n",
      "Epoch 137/300 - Train Loss: 0.0874, Val Loss: 0.0853\n",
      "Epoch 138/300 - Train Loss: 0.0870, Val Loss: 0.0858\n",
      "Epoch 139/300 - Train Loss: 0.0854, Val Loss: 0.0858\n",
      "Epoch 140/300 - Train Loss: 0.0847, Val Loss: 0.0867\n",
      "Epoch 141/300 - Train Loss: 0.0853, Val Loss: 0.0837\n",
      "Epoch 142/300 - Train Loss: 0.0848, Val Loss: 0.0884\n",
      "Epoch 143/300 - Train Loss: 0.0857, Val Loss: 0.0858\n",
      "Epoch 144/300 - Train Loss: 0.0843, Val Loss: 0.0821\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0842, Val Loss: 0.0884\n",
      "Epoch 146/300 - Train Loss: 0.0854, Val Loss: 0.0853\n",
      "Epoch 147/300 - Train Loss: 0.0846, Val Loss: 0.0826\n",
      "Epoch 148/300 - Train Loss: 0.0864, Val Loss: 0.0849\n",
      "Epoch 149/300 - Train Loss: 0.0841, Val Loss: 0.0828\n",
      "Epoch 150/300 - Train Loss: 0.0845, Val Loss: 0.0848\n",
      "Epoch 151/300 - Train Loss: 0.0833, Val Loss: 0.0845\n",
      "Epoch 152/300 - Train Loss: 0.0842, Val Loss: 0.0834\n",
      "Epoch 153/300 - Train Loss: 0.0833, Val Loss: 0.0854\n",
      "Epoch 154/300 - Train Loss: 0.0841, Val Loss: 0.0816\n",
      "Epoch 155/300 - Train Loss: 0.0830, Val Loss: 0.0829\n",
      "Epoch 156/300 - Train Loss: 0.0832, Val Loss: 0.0832\n",
      "Epoch 157/300 - Train Loss: 0.0828, Val Loss: 0.0869\n",
      "Epoch 158/300 - Train Loss: 0.0827, Val Loss: 0.0861\n",
      "Epoch 159/300 - Train Loss: 0.0840, Val Loss: 0.0838\n",
      "Epoch 160/300 - Train Loss: 0.0829, Val Loss: 0.0843\n",
      "Epoch 161/300 - Train Loss: 0.0825, Val Loss: 0.0839\n",
      "Epoch 162/300 - Train Loss: 0.0824, Val Loss: 0.0830\n",
      "Epoch 163/300 - Train Loss: 0.0838, Val Loss: 0.0807\n",
      "Epoch 164/300 - Train Loss: 0.0822, Val Loss: 0.0835\n",
      "Epoch 165/300 - Train Loss: 0.0849, Val Loss: 0.0813\n",
      "Epoch 166/300 - Train Loss: 0.0811, Val Loss: 0.0815\n",
      "Epoch 167/300 - Train Loss: 0.0820, Val Loss: 0.0864\n",
      "Epoch 168/300 - Train Loss: 0.0826, Val Loss: 0.0832\n",
      "Epoch 169/300 - Train Loss: 0.0805, Val Loss: 0.0822\n",
      "Epoch 170/300 - Train Loss: 0.0816, Val Loss: 0.0848\n",
      "Epoch 171/300 - Train Loss: 0.0809, Val Loss: 0.0853\n",
      "Epoch 172/300 - Train Loss: 0.0820, Val Loss: 0.0852\n",
      "Epoch 173/300 - Train Loss: 0.0825, Val Loss: 0.0840\n",
      "Epoch 174/300 - Train Loss: 0.0824, Val Loss: 0.0836\n",
      "Epoch 175/300 - Train Loss: 0.0828, Val Loss: 0.0815\n",
      "Epoch 176/300 - Train Loss: 0.0816, Val Loss: 0.0875\n",
      "Epoch 177/300 - Train Loss: 0.0813, Val Loss: 0.0837\n",
      "Epoch 178/300 - Train Loss: 0.0821, Val Loss: 0.0827\n",
      "Epoch 179/300 - Train Loss: 0.0792, Val Loss: 0.0848\n",
      "Epoch 180/300 - Train Loss: 0.0804, Val Loss: 0.0849\n",
      "Epoch 181/300 - Train Loss: 0.0821, Val Loss: 0.0805\n",
      "Epoch 182/300 - Train Loss: 0.0815, Val Loss: 0.0837\n",
      "Epoch 183/300 - Train Loss: 0.0806, Val Loss: 0.0826\n",
      "Epoch 184/300 - Train Loss: 0.0812, Val Loss: 0.0844\n",
      "Epoch 185/300 - Train Loss: 0.0803, Val Loss: 0.0828\n",
      "Epoch 186/300 - Train Loss: 0.0806, Val Loss: 0.0835\n",
      "Epoch 187/300 - Train Loss: 0.0812, Val Loss: 0.0833\n",
      "Epoch 188/300 - Train Loss: 0.0813, Val Loss: 0.0842\n",
      "Epoch 189/300 - Train Loss: 0.0823, Val Loss: 0.0841\n",
      "Epoch 190/300 - Train Loss: 0.0805, Val Loss: 0.0850\n",
      "Epoch 191/300 - Train Loss: 0.0815, Val Loss: 0.0828\n",
      "Epoch 192/300 - Train Loss: 0.0805, Val Loss: 0.0818\n",
      "Epoch 193/300 - Train Loss: 0.0810, Val Loss: 0.0827\n",
      "Epoch 194/300 - Train Loss: 0.0814, Val Loss: 0.0829\n",
      "Epoch 195/300 - Train Loss: 0.0799, Val Loss: 0.0810\n",
      "Epoch 196/300 - Train Loss: 0.0798, Val Loss: 0.0838\n",
      "Epoch 197/300 - Train Loss: 0.0798, Val Loss: 0.0833\n",
      "Epoch 198/300 - Train Loss: 0.0816, Val Loss: 0.0819\n",
      "Epoch 199/300 - Train Loss: 0.0801, Val Loss: 0.0817\n",
      "Epoch 200/300 - Train Loss: 0.0807, Val Loss: 0.0829\n",
      "Epoch 201/300 - Train Loss: 0.0803, Val Loss: 0.0840\n",
      "Epoch 202/300 - Train Loss: 0.0812, Val Loss: 0.0840\n",
      "Epoch 203/300 - Train Loss: 0.0796, Val Loss: 0.0824\n",
      "Epoch 204/300 - Train Loss: 0.0786, Val Loss: 0.0844\n",
      "Epoch 205/300 - Train Loss: 0.0790, Val Loss: 0.0816\n",
      "Epoch 206/300 - Train Loss: 0.0790, Val Loss: 0.0838\n",
      "Epoch 207/300 - Train Loss: 0.0793, Val Loss: 0.0843\n",
      "Epoch 208/300 - Train Loss: 0.0782, Val Loss: 0.0840\n",
      "Epoch 209/300 - Train Loss: 0.0790, Val Loss: 0.0838\n",
      "Epoch 210/300 - Train Loss: 0.0793, Val Loss: 0.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:29:16,737] Trial 27 finished with value: 0.9544764103994675 and parameters: {'F1': 4, 'F2': 32, 'D': 4, 'dropout': 0.23822094568157912, 'learning_rate': 2.0380980935138715e-05, 'batch_size': 128, 'weight_decay': 2.940964241033842e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 211/300 - Train Loss: 0.0784, Val Loss: 0.0823\n",
      "Early stopping at epoch 211\n",
      "Macro F1 Score: 0.9545, Macro Precision: 0.9414, Macro Recall: 0.9694\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.85      0.95      0.90        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.94      0.97      0.95      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 29\n",
      "Training with F1=16, F2=16, D=8, dropout=0.4399173888122201, LR=0.00013616587911348963, BS=32, WD=6.730151773818122e-05\n",
      "Epoch 1/300 - Train Loss: 0.2995, Val Loss: 0.1167\n",
      "Epoch 2/300 - Train Loss: 0.1266, Val Loss: 0.0879\n",
      "Epoch 3/300 - Train Loss: 0.1100, Val Loss: 0.0756\n",
      "Epoch 4/300 - Train Loss: 0.1043, Val Loss: 0.0718\n",
      "Epoch 5/300 - Train Loss: 0.1015, Val Loss: 0.0749\n",
      "Epoch 6/300 - Train Loss: 0.0978, Val Loss: 0.0717\n",
      "Epoch 7/300 - Train Loss: 0.0948, Val Loss: 0.0766\n",
      "Epoch 8/300 - Train Loss: 0.0935, Val Loss: 0.0778\n",
      "Epoch 9/300 - Train Loss: 0.0934, Val Loss: 0.0703\n",
      "Epoch 10/300 - Train Loss: 0.0927, Val Loss: 0.0715\n",
      "Epoch 11/300 - Train Loss: 0.0908, Val Loss: 0.0683\n",
      "Epoch 12/300 - Train Loss: 0.0884, Val Loss: 0.0674\n",
      "Epoch 13/300 - Train Loss: 0.0904, Val Loss: 0.0676\n",
      "Epoch 14/300 - Train Loss: 0.0881, Val Loss: 0.0688\n",
      "Epoch 15/300 - Train Loss: 0.0868, Val Loss: 0.0710\n",
      "Epoch 16/300 - Train Loss: 0.0862, Val Loss: 0.0689\n",
      "Epoch 17/300 - Train Loss: 0.0860, Val Loss: 0.0689\n",
      "Epoch 18/300 - Train Loss: 0.0873, Val Loss: 0.0711\n",
      "Epoch 19/300 - Train Loss: 0.0831, Val Loss: 0.0683\n",
      "Epoch 20/300 - Train Loss: 0.0856, Val Loss: 0.0696\n",
      "Epoch 21/300 - Train Loss: 0.0862, Val Loss: 0.0653\n",
      "Epoch 22/300 - Train Loss: 0.0832, Val Loss: 0.0712\n",
      "Epoch 23/300 - Train Loss: 0.0840, Val Loss: 0.0697\n",
      "Epoch 24/300 - Train Loss: 0.0838, Val Loss: 0.0674\n",
      "Epoch 25/300 - Train Loss: 0.0846, Val Loss: 0.0705\n",
      "Epoch 26/300 - Train Loss: 0.0819, Val Loss: 0.0726\n",
      "Epoch 27/300 - Train Loss: 0.0828, Val Loss: 0.0666\n",
      "Epoch 28/300 - Train Loss: 0.0842, Val Loss: 0.0688\n",
      "Epoch 29/300 - Train Loss: 0.0810, Val Loss: 0.0703\n",
      "Epoch 30/300 - Train Loss: 0.0815, Val Loss: 0.0690\n",
      "Epoch 31/300 - Train Loss: 0.0828, Val Loss: 0.0667\n",
      "Epoch 32/300 - Train Loss: 0.0827, Val Loss: 0.0684\n",
      "Epoch 33/300 - Train Loss: 0.0817, Val Loss: 0.0683\n",
      "Epoch 34/300 - Train Loss: 0.0811, Val Loss: 0.0673\n",
      "Epoch 35/300 - Train Loss: 0.0810, Val Loss: 0.0654\n",
      "Epoch 36/300 - Train Loss: 0.0815, Val Loss: 0.0685\n",
      "Epoch 37/300 - Train Loss: 0.0801, Val Loss: 0.0662\n",
      "Epoch 38/300 - Train Loss: 0.0806, Val Loss: 0.0719\n",
      "Epoch 39/300 - Train Loss: 0.0795, Val Loss: 0.0662\n",
      "Epoch 40/300 - Train Loss: 0.0808, Val Loss: 0.0682\n",
      "Epoch 41/300 - Train Loss: 0.0778, Val Loss: 0.0675\n",
      "Epoch 42/300 - Train Loss: 0.0789, Val Loss: 0.0725\n",
      "Epoch 43/300 - Train Loss: 0.0776, Val Loss: 0.0678\n",
      "Epoch 44/300 - Train Loss: 0.0800, Val Loss: 0.0685\n",
      "Epoch 45/300 - Train Loss: 0.0798, Val Loss: 0.0657\n",
      "Epoch 46/300 - Train Loss: 0.0787, Val Loss: 0.0700\n",
      "Epoch 47/300 - Train Loss: 0.0762, Val Loss: 0.0663\n",
      "Epoch 48/300 - Train Loss: 0.0776, Val Loss: 0.0669\n",
      "Epoch 49/300 - Train Loss: 0.0786, Val Loss: 0.0672\n",
      "Epoch 50/300 - Train Loss: 0.0778, Val Loss: 0.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:32:25,511] Trial 28 finished with value: 0.9673465035116905 and parameters: {'F1': 16, 'F2': 16, 'D': 8, 'dropout': 0.4399173888122201, 'learning_rate': 0.00013616587911348963, 'batch_size': 32, 'weight_decay': 6.730151773818122e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300 - Train Loss: 0.0773, Val Loss: 0.0684\n",
      "Early stopping at epoch 51\n",
      "Macro F1 Score: 0.9673, Macro Precision: 0.9575, Macro Recall: 0.9782\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.98      0.99       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 30\n",
      "Training with F1=8, F2=8, D=2, dropout=0.31078454511877707, LR=2.7299492937477672e-05, BS=128, WD=0.0004007455862827244\n",
      "Epoch 1/300 - Train Loss: 1.0443, Val Loss: 0.9795\n",
      "Epoch 2/300 - Train Loss: 0.8711, Val Loss: 0.8044\n",
      "Epoch 3/300 - Train Loss: 0.6710, Val Loss: 0.5878\n",
      "Epoch 4/300 - Train Loss: 0.4997, Val Loss: 0.4467\n",
      "Epoch 5/300 - Train Loss: 0.3999, Val Loss: 0.3633\n",
      "Epoch 6/300 - Train Loss: 0.3433, Val Loss: 0.3192\n",
      "Epoch 7/300 - Train Loss: 0.3048, Val Loss: 0.2869\n",
      "Epoch 8/300 - Train Loss: 0.2772, Val Loss: 0.2580\n",
      "Epoch 9/300 - Train Loss: 0.2534, Val Loss: 0.2365\n",
      "Epoch 10/300 - Train Loss: 0.2368, Val Loss: 0.2202\n",
      "Epoch 11/300 - Train Loss: 0.2193, Val Loss: 0.2025\n",
      "Epoch 12/300 - Train Loss: 0.2050, Val Loss: 0.1845\n",
      "Epoch 13/300 - Train Loss: 0.1933, Val Loss: 0.1731\n",
      "Epoch 14/300 - Train Loss: 0.1825, Val Loss: 0.1603\n",
      "Epoch 15/300 - Train Loss: 0.1743, Val Loss: 0.1571\n",
      "Epoch 16/300 - Train Loss: 0.1701, Val Loss: 0.1488\n",
      "Epoch 17/300 - Train Loss: 0.1630, Val Loss: 0.1434\n",
      "Epoch 18/300 - Train Loss: 0.1595, Val Loss: 0.1414\n",
      "Epoch 19/300 - Train Loss: 0.1563, Val Loss: 0.1378\n",
      "Epoch 20/300 - Train Loss: 0.1512, Val Loss: 0.1348\n",
      "Epoch 21/300 - Train Loss: 0.1468, Val Loss: 0.1299\n",
      "Epoch 22/300 - Train Loss: 0.1467, Val Loss: 0.1282\n",
      "Epoch 23/300 - Train Loss: 0.1448, Val Loss: 0.1244\n",
      "Epoch 24/300 - Train Loss: 0.1413, Val Loss: 0.1229\n",
      "Epoch 25/300 - Train Loss: 0.1389, Val Loss: 0.1220\n",
      "Epoch 26/300 - Train Loss: 0.1362, Val Loss: 0.1179\n",
      "Epoch 27/300 - Train Loss: 0.1351, Val Loss: 0.1170\n",
      "Epoch 28/300 - Train Loss: 0.1341, Val Loss: 0.1150\n",
      "Epoch 29/300 - Train Loss: 0.1334, Val Loss: 0.1136\n",
      "Epoch 30/300 - Train Loss: 0.1291, Val Loss: 0.1120\n",
      "Epoch 31/300 - Train Loss: 0.1302, Val Loss: 0.1110\n",
      "Epoch 32/300 - Train Loss: 0.1274, Val Loss: 0.1097\n",
      "Epoch 33/300 - Train Loss: 0.1272, Val Loss: 0.1080\n",
      "Epoch 34/300 - Train Loss: 0.1260, Val Loss: 0.1088\n",
      "Epoch 35/300 - Train Loss: 0.1222, Val Loss: 0.1076\n",
      "Epoch 36/300 - Train Loss: 0.1219, Val Loss: 0.1061\n",
      "Epoch 37/300 - Train Loss: 0.1209, Val Loss: 0.1040\n",
      "Epoch 38/300 - Train Loss: 0.1202, Val Loss: 0.1035\n",
      "Epoch 39/300 - Train Loss: 0.1164, Val Loss: 0.1025\n",
      "Epoch 40/300 - Train Loss: 0.1176, Val Loss: 0.1008\n",
      "Epoch 41/300 - Train Loss: 0.1142, Val Loss: 0.1003\n",
      "Epoch 42/300 - Train Loss: 0.1156, Val Loss: 0.1007\n",
      "Epoch 43/300 - Train Loss: 0.1135, Val Loss: 0.0989\n",
      "Epoch 44/300 - Train Loss: 0.1109, Val Loss: 0.0986\n",
      "Epoch 45/300 - Train Loss: 0.1097, Val Loss: 0.1004\n",
      "Epoch 46/300 - Train Loss: 0.1116, Val Loss: 0.0970\n",
      "Epoch 47/300 - Train Loss: 0.1111, Val Loss: 0.0966\n",
      "Epoch 48/300 - Train Loss: 0.1073, Val Loss: 0.0962\n",
      "Epoch 49/300 - Train Loss: 0.1062, Val Loss: 0.0958\n",
      "Epoch 50/300 - Train Loss: 0.1054, Val Loss: 0.0966\n",
      "Epoch 51/300 - Train Loss: 0.1051, Val Loss: 0.0953\n",
      "Epoch 52/300 - Train Loss: 0.1038, Val Loss: 0.0926\n",
      "Epoch 53/300 - Train Loss: 0.1037, Val Loss: 0.0926\n",
      "Epoch 54/300 - Train Loss: 0.1024, Val Loss: 0.0920\n",
      "Epoch 55/300 - Train Loss: 0.1018, Val Loss: 0.0913\n",
      "Epoch 56/300 - Train Loss: 0.1012, Val Loss: 0.0914\n",
      "Epoch 57/300 - Train Loss: 0.1016, Val Loss: 0.0931\n",
      "Epoch 58/300 - Train Loss: 0.1004, Val Loss: 0.0929\n",
      "Epoch 59/300 - Train Loss: 0.0995, Val Loss: 0.0903\n",
      "Epoch 60/300 - Train Loss: 0.1004, Val Loss: 0.0912\n",
      "Epoch 61/300 - Train Loss: 0.0993, Val Loss: 0.0924\n",
      "Epoch 62/300 - Train Loss: 0.0983, Val Loss: 0.0891\n",
      "Epoch 63/300 - Train Loss: 0.0982, Val Loss: 0.0906\n",
      "Epoch 64/300 - Train Loss: 0.0973, Val Loss: 0.0894\n",
      "Epoch 65/300 - Train Loss: 0.0969, Val Loss: 0.0910\n",
      "Epoch 66/300 - Train Loss: 0.0983, Val Loss: 0.0911\n",
      "Epoch 67/300 - Train Loss: 0.0989, Val Loss: 0.0888\n",
      "Epoch 68/300 - Train Loss: 0.0968, Val Loss: 0.0890\n",
      "Epoch 69/300 - Train Loss: 0.0981, Val Loss: 0.0873\n",
      "Epoch 70/300 - Train Loss: 0.0969, Val Loss: 0.0861\n",
      "Epoch 71/300 - Train Loss: 0.0952, Val Loss: 0.0888\n",
      "Epoch 72/300 - Train Loss: 0.0960, Val Loss: 0.0881\n",
      "Epoch 73/300 - Train Loss: 0.0959, Val Loss: 0.0880\n",
      "Epoch 74/300 - Train Loss: 0.0976, Val Loss: 0.0867\n",
      "Epoch 75/300 - Train Loss: 0.0943, Val Loss: 0.0864\n",
      "Epoch 76/300 - Train Loss: 0.0962, Val Loss: 0.0883\n",
      "Epoch 77/300 - Train Loss: 0.0978, Val Loss: 0.0867\n",
      "Epoch 78/300 - Train Loss: 0.0954, Val Loss: 0.0860\n",
      "Epoch 79/300 - Train Loss: 0.0942, Val Loss: 0.0881\n",
      "Epoch 80/300 - Train Loss: 0.0953, Val Loss: 0.0884\n",
      "Epoch 81/300 - Train Loss: 0.0933, Val Loss: 0.0894\n",
      "Epoch 82/300 - Train Loss: 0.0951, Val Loss: 0.0863\n",
      "Epoch 83/300 - Train Loss: 0.0955, Val Loss: 0.0870\n",
      "Epoch 84/300 - Train Loss: 0.0952, Val Loss: 0.0892\n",
      "Epoch 85/300 - Train Loss: 0.0940, Val Loss: 0.0865\n",
      "Epoch 86/300 - Train Loss: 0.0943, Val Loss: 0.0880\n",
      "Epoch 87/300 - Train Loss: 0.0942, Val Loss: 0.0870\n",
      "Epoch 88/300 - Train Loss: 0.0953, Val Loss: 0.0869\n",
      "Epoch 89/300 - Train Loss: 0.0927, Val Loss: 0.0861\n",
      "Epoch 90/300 - Train Loss: 0.0930, Val Loss: 0.0869\n",
      "Epoch 91/300 - Train Loss: 0.0938, Val Loss: 0.0874\n",
      "Epoch 92/300 - Train Loss: 0.0944, Val Loss: 0.0847\n",
      "Epoch 93/300 - Train Loss: 0.0922, Val Loss: 0.0870\n",
      "Epoch 94/300 - Train Loss: 0.0949, Val Loss: 0.0850\n",
      "Epoch 95/300 - Train Loss: 0.0926, Val Loss: 0.0860\n",
      "Epoch 96/300 - Train Loss: 0.0932, Val Loss: 0.0848\n",
      "Epoch 97/300 - Train Loss: 0.0930, Val Loss: 0.0858\n",
      "Epoch 98/300 - Train Loss: 0.0941, Val Loss: 0.0884\n",
      "Epoch 99/300 - Train Loss: 0.0911, Val Loss: 0.0843\n",
      "Epoch 100/300 - Train Loss: 0.0938, Val Loss: 0.0846\n",
      "Epoch 101/300 - Train Loss: 0.0917, Val Loss: 0.0841\n",
      "Epoch 102/300 - Train Loss: 0.0933, Val Loss: 0.0851\n",
      "Epoch 103/300 - Train Loss: 0.0933, Val Loss: 0.0842\n",
      "Epoch 104/300 - Train Loss: 0.0916, Val Loss: 0.0838\n",
      "Epoch 105/300 - Train Loss: 0.0917, Val Loss: 0.0840\n",
      "Epoch 106/300 - Train Loss: 0.0907, Val Loss: 0.0852\n",
      "Epoch 107/300 - Train Loss: 0.0917, Val Loss: 0.0889\n",
      "Epoch 108/300 - Train Loss: 0.0917, Val Loss: 0.0844\n",
      "Epoch 109/300 - Train Loss: 0.0908, Val Loss: 0.0849\n",
      "Epoch 110/300 - Train Loss: 0.0917, Val Loss: 0.0831\n",
      "Epoch 111/300 - Train Loss: 0.0908, Val Loss: 0.0837\n",
      "Epoch 112/300 - Train Loss: 0.0918, Val Loss: 0.0833\n",
      "Epoch 113/300 - Train Loss: 0.0928, Val Loss: 0.0843\n",
      "Epoch 114/300 - Train Loss: 0.0923, Val Loss: 0.0844\n",
      "Epoch 115/300 - Train Loss: 0.0914, Val Loss: 0.0833\n",
      "Epoch 116/300 - Train Loss: 0.0908, Val Loss: 0.0835\n",
      "Epoch 117/300 - Train Loss: 0.0899, Val Loss: 0.0840\n",
      "Epoch 118/300 - Train Loss: 0.0906, Val Loss: 0.0842\n",
      "Epoch 119/300 - Train Loss: 0.0904, Val Loss: 0.0827\n",
      "Epoch 120/300 - Train Loss: 0.0914, Val Loss: 0.0817\n",
      "Epoch 121/300 - Train Loss: 0.0902, Val Loss: 0.0826\n",
      "Epoch 122/300 - Train Loss: 0.0904, Val Loss: 0.0831\n",
      "Epoch 123/300 - Train Loss: 0.0897, Val Loss: 0.0830\n",
      "Epoch 124/300 - Train Loss: 0.0894, Val Loss: 0.0833\n",
      "Epoch 125/300 - Train Loss: 0.0910, Val Loss: 0.0824\n",
      "Epoch 126/300 - Train Loss: 0.0921, Val Loss: 0.0829\n",
      "Epoch 127/300 - Train Loss: 0.0906, Val Loss: 0.0827\n",
      "Epoch 128/300 - Train Loss: 0.0911, Val Loss: 0.0842\n",
      "Epoch 129/300 - Train Loss: 0.0901, Val Loss: 0.0816\n",
      "Epoch 130/300 - Train Loss: 0.0885, Val Loss: 0.0830\n",
      "Epoch 131/300 - Train Loss: 0.0896, Val Loss: 0.0834\n",
      "Epoch 132/300 - Train Loss: 0.0890, Val Loss: 0.0819\n",
      "Epoch 133/300 - Train Loss: 0.0899, Val Loss: 0.0822\n",
      "Epoch 134/300 - Train Loss: 0.0900, Val Loss: 0.0808\n",
      "Epoch 135/300 - Train Loss: 0.0888, Val Loss: 0.0829\n",
      "Epoch 136/300 - Train Loss: 0.0895, Val Loss: 0.0841\n",
      "Epoch 137/300 - Train Loss: 0.0884, Val Loss: 0.0814\n",
      "Epoch 138/300 - Train Loss: 0.0891, Val Loss: 0.0821\n",
      "Epoch 139/300 - Train Loss: 0.0891, Val Loss: 0.0823\n",
      "Epoch 140/300 - Train Loss: 0.0892, Val Loss: 0.0806\n",
      "Epoch 141/300 - Train Loss: 0.0877, Val Loss: 0.0844\n",
      "Epoch 142/300 - Train Loss: 0.0888, Val Loss: 0.0815\n",
      "Epoch 143/300 - Train Loss: 0.0892, Val Loss: 0.0811\n",
      "Epoch 144/300 - Train Loss: 0.0883, Val Loss: 0.0810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0900, Val Loss: 0.0809\n",
      "Epoch 146/300 - Train Loss: 0.0881, Val Loss: 0.0829\n",
      "Epoch 147/300 - Train Loss: 0.0874, Val Loss: 0.0808\n",
      "Epoch 148/300 - Train Loss: 0.0877, Val Loss: 0.0827\n",
      "Epoch 149/300 - Train Loss: 0.0869, Val Loss: 0.0836\n",
      "Epoch 150/300 - Train Loss: 0.0874, Val Loss: 0.0814\n",
      "Epoch 151/300 - Train Loss: 0.0873, Val Loss: 0.0808\n",
      "Epoch 152/300 - Train Loss: 0.0874, Val Loss: 0.0861\n",
      "Epoch 153/300 - Train Loss: 0.0877, Val Loss: 0.0807\n",
      "Epoch 154/300 - Train Loss: 0.0876, Val Loss: 0.0817\n",
      "Epoch 155/300 - Train Loss: 0.0865, Val Loss: 0.0800\n",
      "Epoch 156/300 - Train Loss: 0.0873, Val Loss: 0.0815\n",
      "Epoch 157/300 - Train Loss: 0.0874, Val Loss: 0.0797\n",
      "Epoch 158/300 - Train Loss: 0.0864, Val Loss: 0.0812\n",
      "Epoch 159/300 - Train Loss: 0.0880, Val Loss: 0.0808\n",
      "Epoch 160/300 - Train Loss: 0.0873, Val Loss: 0.0808\n",
      "Epoch 161/300 - Train Loss: 0.0864, Val Loss: 0.0797\n",
      "Epoch 162/300 - Train Loss: 0.0871, Val Loss: 0.0806\n",
      "Epoch 163/300 - Train Loss: 0.0882, Val Loss: 0.0820\n",
      "Epoch 164/300 - Train Loss: 0.0864, Val Loss: 0.0811\n",
      "Epoch 165/300 - Train Loss: 0.0859, Val Loss: 0.0817\n",
      "Epoch 166/300 - Train Loss: 0.0860, Val Loss: 0.0790\n",
      "Epoch 167/300 - Train Loss: 0.0853, Val Loss: 0.0815\n",
      "Epoch 168/300 - Train Loss: 0.0854, Val Loss: 0.0813\n",
      "Epoch 169/300 - Train Loss: 0.0854, Val Loss: 0.0801\n",
      "Epoch 170/300 - Train Loss: 0.0856, Val Loss: 0.0799\n",
      "Epoch 171/300 - Train Loss: 0.0871, Val Loss: 0.0788\n",
      "Epoch 172/300 - Train Loss: 0.0862, Val Loss: 0.0789\n",
      "Epoch 173/300 - Train Loss: 0.0863, Val Loss: 0.0807\n",
      "Epoch 174/300 - Train Loss: 0.0868, Val Loss: 0.0795\n",
      "Epoch 175/300 - Train Loss: 0.0858, Val Loss: 0.0800\n",
      "Epoch 176/300 - Train Loss: 0.0860, Val Loss: 0.0791\n",
      "Epoch 177/300 - Train Loss: 0.0860, Val Loss: 0.0795\n",
      "Epoch 178/300 - Train Loss: 0.0850, Val Loss: 0.0794\n",
      "Epoch 179/300 - Train Loss: 0.0875, Val Loss: 0.0795\n",
      "Epoch 180/300 - Train Loss: 0.0850, Val Loss: 0.0819\n",
      "Epoch 181/300 - Train Loss: 0.0860, Val Loss: 0.0809\n",
      "Epoch 182/300 - Train Loss: 0.0855, Val Loss: 0.0770\n",
      "Epoch 183/300 - Train Loss: 0.0864, Val Loss: 0.0782\n",
      "Epoch 184/300 - Train Loss: 0.0855, Val Loss: 0.0794\n",
      "Epoch 185/300 - Train Loss: 0.0853, Val Loss: 0.0776\n",
      "Epoch 186/300 - Train Loss: 0.0866, Val Loss: 0.0781\n",
      "Epoch 187/300 - Train Loss: 0.0839, Val Loss: 0.0774\n",
      "Epoch 188/300 - Train Loss: 0.0850, Val Loss: 0.0796\n",
      "Epoch 189/300 - Train Loss: 0.0845, Val Loss: 0.0800\n",
      "Epoch 190/300 - Train Loss: 0.0845, Val Loss: 0.0784\n",
      "Epoch 191/300 - Train Loss: 0.0862, Val Loss: 0.0783\n",
      "Epoch 192/300 - Train Loss: 0.0849, Val Loss: 0.0764\n",
      "Epoch 193/300 - Train Loss: 0.0825, Val Loss: 0.0780\n",
      "Epoch 194/300 - Train Loss: 0.0861, Val Loss: 0.0805\n",
      "Epoch 195/300 - Train Loss: 0.0842, Val Loss: 0.0800\n",
      "Epoch 196/300 - Train Loss: 0.0843, Val Loss: 0.0788\n",
      "Epoch 197/300 - Train Loss: 0.0845, Val Loss: 0.0775\n",
      "Epoch 198/300 - Train Loss: 0.0831, Val Loss: 0.0779\n",
      "Epoch 199/300 - Train Loss: 0.0851, Val Loss: 0.0778\n",
      "Epoch 200/300 - Train Loss: 0.0841, Val Loss: 0.0780\n",
      "Epoch 201/300 - Train Loss: 0.0844, Val Loss: 0.0779\n",
      "Epoch 202/300 - Train Loss: 0.0842, Val Loss: 0.0774\n",
      "Epoch 203/300 - Train Loss: 0.0846, Val Loss: 0.0786\n",
      "Epoch 204/300 - Train Loss: 0.0834, Val Loss: 0.0785\n",
      "Epoch 205/300 - Train Loss: 0.0846, Val Loss: 0.0762\n",
      "Epoch 206/300 - Train Loss: 0.0832, Val Loss: 0.0768\n",
      "Epoch 207/300 - Train Loss: 0.0831, Val Loss: 0.0762\n",
      "Epoch 208/300 - Train Loss: 0.0838, Val Loss: 0.0765\n",
      "Epoch 209/300 - Train Loss: 0.0839, Val Loss: 0.0759\n",
      "Epoch 210/300 - Train Loss: 0.0831, Val Loss: 0.0776\n",
      "Epoch 211/300 - Train Loss: 0.0842, Val Loss: 0.0762\n",
      "Epoch 212/300 - Train Loss: 0.0854, Val Loss: 0.0763\n",
      "Epoch 213/300 - Train Loss: 0.0834, Val Loss: 0.0767\n",
      "Epoch 214/300 - Train Loss: 0.0829, Val Loss: 0.0773\n",
      "Epoch 215/300 - Train Loss: 0.0814, Val Loss: 0.0755\n",
      "Epoch 216/300 - Train Loss: 0.0825, Val Loss: 0.0761\n",
      "Epoch 217/300 - Train Loss: 0.0843, Val Loss: 0.0766\n",
      "Epoch 218/300 - Train Loss: 0.0838, Val Loss: 0.0774\n",
      "Epoch 219/300 - Train Loss: 0.0845, Val Loss: 0.0752\n",
      "Epoch 220/300 - Train Loss: 0.0827, Val Loss: 0.0762\n",
      "Epoch 221/300 - Train Loss: 0.0842, Val Loss: 0.0764\n",
      "Epoch 222/300 - Train Loss: 0.0843, Val Loss: 0.0749\n",
      "Epoch 223/300 - Train Loss: 0.0838, Val Loss: 0.0766\n",
      "Epoch 224/300 - Train Loss: 0.0830, Val Loss: 0.0787\n",
      "Epoch 225/300 - Train Loss: 0.0824, Val Loss: 0.0749\n",
      "Epoch 226/300 - Train Loss: 0.0832, Val Loss: 0.0770\n",
      "Epoch 227/300 - Train Loss: 0.0830, Val Loss: 0.0769\n",
      "Epoch 228/300 - Train Loss: 0.0844, Val Loss: 0.0756\n",
      "Epoch 229/300 - Train Loss: 0.0834, Val Loss: 0.0761\n",
      "Epoch 230/300 - Train Loss: 0.0833, Val Loss: 0.0762\n",
      "Epoch 231/300 - Train Loss: 0.0823, Val Loss: 0.0771\n",
      "Epoch 232/300 - Train Loss: 0.0820, Val Loss: 0.0755\n",
      "Epoch 233/300 - Train Loss: 0.0838, Val Loss: 0.0756\n",
      "Epoch 234/300 - Train Loss: 0.0834, Val Loss: 0.0766\n",
      "Epoch 235/300 - Train Loss: 0.0833, Val Loss: 0.0752\n",
      "Epoch 236/300 - Train Loss: 0.0835, Val Loss: 0.0746\n",
      "Epoch 237/300 - Train Loss: 0.0828, Val Loss: 0.0750\n",
      "Epoch 238/300 - Train Loss: 0.0838, Val Loss: 0.0760\n",
      "Epoch 239/300 - Train Loss: 0.0827, Val Loss: 0.0770\n",
      "Epoch 240/300 - Train Loss: 0.0807, Val Loss: 0.0762\n",
      "Epoch 241/300 - Train Loss: 0.0832, Val Loss: 0.0750\n",
      "Epoch 242/300 - Train Loss: 0.0827, Val Loss: 0.0759\n",
      "Epoch 243/300 - Train Loss: 0.0830, Val Loss: 0.0750\n",
      "Epoch 244/300 - Train Loss: 0.0819, Val Loss: 0.0770\n",
      "Epoch 245/300 - Train Loss: 0.0811, Val Loss: 0.0743\n",
      "Epoch 246/300 - Train Loss: 0.0846, Val Loss: 0.0747\n",
      "Epoch 247/300 - Train Loss: 0.0814, Val Loss: 0.0756\n",
      "Epoch 248/300 - Train Loss: 0.0831, Val Loss: 0.0748\n",
      "Epoch 249/300 - Train Loss: 0.0820, Val Loss: 0.0737\n",
      "Epoch 250/300 - Train Loss: 0.0814, Val Loss: 0.0758\n",
      "Epoch 251/300 - Train Loss: 0.0818, Val Loss: 0.0762\n",
      "Epoch 252/300 - Train Loss: 0.0820, Val Loss: 0.0765\n",
      "Epoch 253/300 - Train Loss: 0.0815, Val Loss: 0.0757\n",
      "Epoch 254/300 - Train Loss: 0.0820, Val Loss: 0.0745\n",
      "Epoch 255/300 - Train Loss: 0.0825, Val Loss: 0.0744\n",
      "Epoch 256/300 - Train Loss: 0.0809, Val Loss: 0.0741\n",
      "Epoch 257/300 - Train Loss: 0.0814, Val Loss: 0.0766\n",
      "Epoch 258/300 - Train Loss: 0.0817, Val Loss: 0.0740\n",
      "Epoch 259/300 - Train Loss: 0.0813, Val Loss: 0.0752\n",
      "Epoch 260/300 - Train Loss: 0.0806, Val Loss: 0.0745\n",
      "Epoch 261/300 - Train Loss: 0.0836, Val Loss: 0.0750\n",
      "Epoch 262/300 - Train Loss: 0.0820, Val Loss: 0.0748\n",
      "Epoch 263/300 - Train Loss: 0.0823, Val Loss: 0.0737\n",
      "Epoch 264/300 - Train Loss: 0.0816, Val Loss: 0.0741\n",
      "Epoch 265/300 - Train Loss: 0.0815, Val Loss: 0.0765\n",
      "Epoch 266/300 - Train Loss: 0.0822, Val Loss: 0.0752\n",
      "Epoch 267/300 - Train Loss: 0.0812, Val Loss: 0.0760\n",
      "Epoch 268/300 - Train Loss: 0.0819, Val Loss: 0.0744\n",
      "Epoch 269/300 - Train Loss: 0.0817, Val Loss: 0.0735\n",
      "Epoch 270/300 - Train Loss: 0.0823, Val Loss: 0.0735\n",
      "Epoch 271/300 - Train Loss: 0.0802, Val Loss: 0.0737\n",
      "Epoch 272/300 - Train Loss: 0.0803, Val Loss: 0.0732\n",
      "Epoch 273/300 - Train Loss: 0.0814, Val Loss: 0.0727\n",
      "Epoch 274/300 - Train Loss: 0.0814, Val Loss: 0.0743\n",
      "Epoch 275/300 - Train Loss: 0.0829, Val Loss: 0.0747\n",
      "Epoch 276/300 - Train Loss: 0.0818, Val Loss: 0.0749\n",
      "Epoch 277/300 - Train Loss: 0.0824, Val Loss: 0.0739\n",
      "Epoch 278/300 - Train Loss: 0.0813, Val Loss: 0.0753\n",
      "Epoch 279/300 - Train Loss: 0.0816, Val Loss: 0.0743\n",
      "Epoch 280/300 - Train Loss: 0.0802, Val Loss: 0.0734\n",
      "Epoch 281/300 - Train Loss: 0.0820, Val Loss: 0.0741\n",
      "Epoch 282/300 - Train Loss: 0.0818, Val Loss: 0.0732\n",
      "Epoch 283/300 - Train Loss: 0.0808, Val Loss: 0.0730\n",
      "Epoch 284/300 - Train Loss: 0.0801, Val Loss: 0.0743\n",
      "Epoch 285/300 - Train Loss: 0.0798, Val Loss: 0.0740\n",
      "Epoch 286/300 - Train Loss: 0.0821, Val Loss: 0.0744\n",
      "Epoch 287/300 - Train Loss: 0.0823, Val Loss: 0.0730\n",
      "Epoch 288/300 - Train Loss: 0.0834, Val Loss: 0.0739\n",
      "Epoch 289/300 - Train Loss: 0.0798, Val Loss: 0.0734\n",
      "Epoch 290/300 - Train Loss: 0.0813, Val Loss: 0.0718\n",
      "Epoch 291/300 - Train Loss: 0.0824, Val Loss: 0.0732\n",
      "Epoch 292/300 - Train Loss: 0.0812, Val Loss: 0.0738\n",
      "Epoch 293/300 - Train Loss: 0.0809, Val Loss: 0.0751\n",
      "Epoch 294/300 - Train Loss: 0.0790, Val Loss: 0.0747\n",
      "Epoch 295/300 - Train Loss: 0.0806, Val Loss: 0.0739\n",
      "Epoch 296/300 - Train Loss: 0.0807, Val Loss: 0.0762\n",
      "Epoch 297/300 - Train Loss: 0.0804, Val Loss: 0.0734\n",
      "Epoch 298/300 - Train Loss: 0.0796, Val Loss: 0.0734\n",
      "Epoch 299/300 - Train Loss: 0.0802, Val Loss: 0.0740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:37:05,445] Trial 29 finished with value: 0.9643817495894896 and parameters: {'F1': 8, 'F2': 8, 'D': 2, 'dropout': 0.31078454511877707, 'learning_rate': 2.7299492937477672e-05, 'batch_size': 128, 'weight_decay': 0.0004007455862827244}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/300 - Train Loss: 0.0812, Val Loss: 0.0741\n",
      "Macro F1 Score: 0.9644, Macro Precision: 0.9545, Macro Recall: 0.9753\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 31\n",
      "Training with F1=8, F2=16, D=2, dropout=0.38138952677957777, LR=6.167174911086604e-05, BS=64, WD=1.1120263385800963e-05\n",
      "Epoch 1/300 - Train Loss: 0.7344, Val Loss: 0.4308\n",
      "Epoch 2/300 - Train Loss: 0.3356, Val Loss: 0.2680\n",
      "Epoch 3/300 - Train Loss: 0.2522, Val Loss: 0.1981\n",
      "Epoch 4/300 - Train Loss: 0.2074, Val Loss: 0.1550\n",
      "Epoch 5/300 - Train Loss: 0.1712, Val Loss: 0.1372\n",
      "Epoch 6/300 - Train Loss: 0.1533, Val Loss: 0.1178\n",
      "Epoch 7/300 - Train Loss: 0.1405, Val Loss: 0.1091\n",
      "Epoch 8/300 - Train Loss: 0.1337, Val Loss: 0.1064\n",
      "Epoch 9/300 - Train Loss: 0.1275, Val Loss: 0.1023\n",
      "Epoch 10/300 - Train Loss: 0.1220, Val Loss: 0.0975\n",
      "Epoch 11/300 - Train Loss: 0.1186, Val Loss: 0.0941\n",
      "Epoch 12/300 - Train Loss: 0.1164, Val Loss: 0.0933\n",
      "Epoch 13/300 - Train Loss: 0.1147, Val Loss: 0.0941\n",
      "Epoch 14/300 - Train Loss: 0.1137, Val Loss: 0.0889\n",
      "Epoch 15/300 - Train Loss: 0.1109, Val Loss: 0.0876\n",
      "Epoch 16/300 - Train Loss: 0.1093, Val Loss: 0.0896\n",
      "Epoch 17/300 - Train Loss: 0.1088, Val Loss: 0.0877\n",
      "Epoch 18/300 - Train Loss: 0.1065, Val Loss: 0.0873\n",
      "Epoch 19/300 - Train Loss: 0.1027, Val Loss: 0.0857\n",
      "Epoch 20/300 - Train Loss: 0.1045, Val Loss: 0.0887\n",
      "Epoch 21/300 - Train Loss: 0.1025, Val Loss: 0.0868\n",
      "Epoch 22/300 - Train Loss: 0.1038, Val Loss: 0.0824\n",
      "Epoch 23/300 - Train Loss: 0.1026, Val Loss: 0.0871\n",
      "Epoch 24/300 - Train Loss: 0.0996, Val Loss: 0.0844\n",
      "Epoch 25/300 - Train Loss: 0.1017, Val Loss: 0.0853\n",
      "Epoch 26/300 - Train Loss: 0.0997, Val Loss: 0.0836\n",
      "Epoch 27/300 - Train Loss: 0.0992, Val Loss: 0.0852\n",
      "Epoch 28/300 - Train Loss: 0.1006, Val Loss: 0.0868\n",
      "Epoch 29/300 - Train Loss: 0.0997, Val Loss: 0.0848\n",
      "Epoch 30/300 - Train Loss: 0.0972, Val Loss: 0.0832\n",
      "Epoch 31/300 - Train Loss: 0.0963, Val Loss: 0.0814\n",
      "Epoch 32/300 - Train Loss: 0.0968, Val Loss: 0.0819\n",
      "Epoch 33/300 - Train Loss: 0.0989, Val Loss: 0.0804\n",
      "Epoch 34/300 - Train Loss: 0.0966, Val Loss: 0.0803\n",
      "Epoch 35/300 - Train Loss: 0.0934, Val Loss: 0.0818\n",
      "Epoch 36/300 - Train Loss: 0.0959, Val Loss: 0.0844\n",
      "Epoch 37/300 - Train Loss: 0.0947, Val Loss: 0.0834\n",
      "Epoch 38/300 - Train Loss: 0.0915, Val Loss: 0.0829\n",
      "Epoch 39/300 - Train Loss: 0.0951, Val Loss: 0.0804\n",
      "Epoch 40/300 - Train Loss: 0.0927, Val Loss: 0.0811\n",
      "Epoch 41/300 - Train Loss: 0.0926, Val Loss: 0.0821\n",
      "Epoch 42/300 - Train Loss: 0.0929, Val Loss: 0.0812\n",
      "Epoch 43/300 - Train Loss: 0.0928, Val Loss: 0.0806\n",
      "Epoch 44/300 - Train Loss: 0.0922, Val Loss: 0.0787\n",
      "Epoch 45/300 - Train Loss: 0.0917, Val Loss: 0.0808\n",
      "Epoch 46/300 - Train Loss: 0.0918, Val Loss: 0.0796\n",
      "Epoch 47/300 - Train Loss: 0.0916, Val Loss: 0.0796\n",
      "Epoch 48/300 - Train Loss: 0.0905, Val Loss: 0.0801\n",
      "Epoch 49/300 - Train Loss: 0.0927, Val Loss: 0.0798\n",
      "Epoch 50/300 - Train Loss: 0.0921, Val Loss: 0.0797\n",
      "Epoch 51/300 - Train Loss: 0.0897, Val Loss: 0.0769\n",
      "Epoch 52/300 - Train Loss: 0.0910, Val Loss: 0.0782\n",
      "Epoch 53/300 - Train Loss: 0.0882, Val Loss: 0.0790\n",
      "Epoch 54/300 - Train Loss: 0.0911, Val Loss: 0.0804\n",
      "Epoch 55/300 - Train Loss: 0.0901, Val Loss: 0.0801\n",
      "Epoch 56/300 - Train Loss: 0.0901, Val Loss: 0.0804\n",
      "Epoch 57/300 - Train Loss: 0.0878, Val Loss: 0.0786\n",
      "Epoch 58/300 - Train Loss: 0.0900, Val Loss: 0.0781\n",
      "Epoch 59/300 - Train Loss: 0.0897, Val Loss: 0.0777\n",
      "Epoch 60/300 - Train Loss: 0.0896, Val Loss: 0.0770\n",
      "Epoch 61/300 - Train Loss: 0.0884, Val Loss: 0.0761\n",
      "Epoch 62/300 - Train Loss: 0.0892, Val Loss: 0.0783\n",
      "Epoch 63/300 - Train Loss: 0.0877, Val Loss: 0.0786\n",
      "Epoch 64/300 - Train Loss: 0.0892, Val Loss: 0.0789\n",
      "Epoch 65/300 - Train Loss: 0.0874, Val Loss: 0.0774\n",
      "Epoch 66/300 - Train Loss: 0.0883, Val Loss: 0.0791\n",
      "Epoch 67/300 - Train Loss: 0.0897, Val Loss: 0.0785\n",
      "Epoch 68/300 - Train Loss: 0.0878, Val Loss: 0.0782\n",
      "Epoch 69/300 - Train Loss: 0.0880, Val Loss: 0.0782\n",
      "Epoch 70/300 - Train Loss: 0.0881, Val Loss: 0.0769\n",
      "Epoch 71/300 - Train Loss: 0.0862, Val Loss: 0.0769\n",
      "Epoch 72/300 - Train Loss: 0.0860, Val Loss: 0.0789\n",
      "Epoch 73/300 - Train Loss: 0.0861, Val Loss: 0.0797\n",
      "Epoch 74/300 - Train Loss: 0.0847, Val Loss: 0.0786\n",
      "Epoch 75/300 - Train Loss: 0.0853, Val Loss: 0.0787\n",
      "Epoch 76/300 - Train Loss: 0.0863, Val Loss: 0.0761\n",
      "Epoch 77/300 - Train Loss: 0.0864, Val Loss: 0.0803\n",
      "Epoch 78/300 - Train Loss: 0.0856, Val Loss: 0.0787\n",
      "Epoch 79/300 - Train Loss: 0.0868, Val Loss: 0.0790\n",
      "Epoch 80/300 - Train Loss: 0.0868, Val Loss: 0.0772\n",
      "Epoch 81/300 - Train Loss: 0.0855, Val Loss: 0.0803\n",
      "Epoch 82/300 - Train Loss: 0.0852, Val Loss: 0.0796\n",
      "Epoch 83/300 - Train Loss: 0.0846, Val Loss: 0.0814\n",
      "Epoch 84/300 - Train Loss: 0.0857, Val Loss: 0.0780\n",
      "Epoch 85/300 - Train Loss: 0.0854, Val Loss: 0.0757\n",
      "Epoch 86/300 - Train Loss: 0.0840, Val Loss: 0.0796\n",
      "Epoch 87/300 - Train Loss: 0.0866, Val Loss: 0.0793\n",
      "Epoch 88/300 - Train Loss: 0.0833, Val Loss: 0.0795\n",
      "Epoch 89/300 - Train Loss: 0.0861, Val Loss: 0.0798\n",
      "Epoch 90/300 - Train Loss: 0.0862, Val Loss: 0.0800\n",
      "Epoch 91/300 - Train Loss: 0.0857, Val Loss: 0.0778\n",
      "Epoch 92/300 - Train Loss: 0.0819, Val Loss: 0.0820\n",
      "Epoch 93/300 - Train Loss: 0.0845, Val Loss: 0.0765\n",
      "Epoch 94/300 - Train Loss: 0.0832, Val Loss: 0.0790\n",
      "Epoch 95/300 - Train Loss: 0.0834, Val Loss: 0.0815\n",
      "Epoch 96/300 - Train Loss: 0.0830, Val Loss: 0.0786\n",
      "Epoch 97/300 - Train Loss: 0.0845, Val Loss: 0.0824\n",
      "Epoch 98/300 - Train Loss: 0.0838, Val Loss: 0.0776\n",
      "Epoch 99/300 - Train Loss: 0.0849, Val Loss: 0.0785\n",
      "Epoch 100/300 - Train Loss: 0.0806, Val Loss: 0.0786\n",
      "Epoch 101/300 - Train Loss: 0.0838, Val Loss: 0.0785\n",
      "Epoch 102/300 - Train Loss: 0.0842, Val Loss: 0.0782\n",
      "Epoch 103/300 - Train Loss: 0.0831, Val Loss: 0.0811\n",
      "Epoch 104/300 - Train Loss: 0.0821, Val Loss: 0.0801\n",
      "Epoch 105/300 - Train Loss: 0.0845, Val Loss: 0.0798\n",
      "Epoch 106/300 - Train Loss: 0.0828, Val Loss: 0.0795\n",
      "Epoch 107/300 - Train Loss: 0.0815, Val Loss: 0.0791\n",
      "Epoch 108/300 - Train Loss: 0.0828, Val Loss: 0.0770\n",
      "Epoch 109/300 - Train Loss: 0.0840, Val Loss: 0.0775\n",
      "Epoch 110/300 - Train Loss: 0.0825, Val Loss: 0.0800\n",
      "Epoch 111/300 - Train Loss: 0.0806, Val Loss: 0.0779\n",
      "Epoch 112/300 - Train Loss: 0.0829, Val Loss: 0.0794\n",
      "Epoch 113/300 - Train Loss: 0.0835, Val Loss: 0.0806\n",
      "Epoch 114/300 - Train Loss: 0.0811, Val Loss: 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:39:25,897] Trial 30 finished with value: 0.9623227882857628 and parameters: {'F1': 8, 'F2': 16, 'D': 2, 'dropout': 0.38138952677957777, 'learning_rate': 6.167174911086604e-05, 'batch_size': 64, 'weight_decay': 1.1120263385800963e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300 - Train Loss: 0.0801, Val Loss: 0.0776\n",
      "Early stopping at epoch 115\n",
      "Macro F1 Score: 0.9623, Macro Precision: 0.9549, Macro Recall: 0.9705\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 32\n",
      "Training with F1=16, F2=8, D=4, dropout=0.2566947249800793, LR=1.8529155560335235e-05, BS=64, WD=2.0035096115672113e-05\n",
      "Epoch 1/300 - Train Loss: 0.9181, Val Loss: 0.7303\n",
      "Epoch 2/300 - Train Loss: 0.5523, Val Loss: 0.4236\n",
      "Epoch 3/300 - Train Loss: 0.3596, Val Loss: 0.3139\n",
      "Epoch 4/300 - Train Loss: 0.2815, Val Loss: 0.2470\n",
      "Epoch 5/300 - Train Loss: 0.2333, Val Loss: 0.2030\n",
      "Epoch 6/300 - Train Loss: 0.1981, Val Loss: 0.1732\n",
      "Epoch 7/300 - Train Loss: 0.1751, Val Loss: 0.1572\n",
      "Epoch 8/300 - Train Loss: 0.1590, Val Loss: 0.1433\n",
      "Epoch 9/300 - Train Loss: 0.1473, Val Loss: 0.1318\n",
      "Epoch 10/300 - Train Loss: 0.1394, Val Loss: 0.1262\n",
      "Epoch 11/300 - Train Loss: 0.1311, Val Loss: 0.1221\n",
      "Epoch 12/300 - Train Loss: 0.1272, Val Loss: 0.1138\n",
      "Epoch 13/300 - Train Loss: 0.1233, Val Loss: 0.1092\n",
      "Epoch 14/300 - Train Loss: 0.1211, Val Loss: 0.1099\n",
      "Epoch 15/300 - Train Loss: 0.1191, Val Loss: 0.1052\n",
      "Epoch 16/300 - Train Loss: 0.1138, Val Loss: 0.1045\n",
      "Epoch 17/300 - Train Loss: 0.1135, Val Loss: 0.1027\n",
      "Epoch 18/300 - Train Loss: 0.1103, Val Loss: 0.0958\n",
      "Epoch 19/300 - Train Loss: 0.1100, Val Loss: 0.0979\n",
      "Epoch 20/300 - Train Loss: 0.1065, Val Loss: 0.0958\n",
      "Epoch 21/300 - Train Loss: 0.1076, Val Loss: 0.0932\n",
      "Epoch 22/300 - Train Loss: 0.1047, Val Loss: 0.0930\n",
      "Epoch 23/300 - Train Loss: 0.1054, Val Loss: 0.0914\n",
      "Epoch 24/300 - Train Loss: 0.1050, Val Loss: 0.0901\n",
      "Epoch 25/300 - Train Loss: 0.1019, Val Loss: 0.0884\n",
      "Epoch 26/300 - Train Loss: 0.1024, Val Loss: 0.0887\n",
      "Epoch 27/300 - Train Loss: 0.1027, Val Loss: 0.0886\n",
      "Epoch 28/300 - Train Loss: 0.1030, Val Loss: 0.0877\n",
      "Epoch 29/300 - Train Loss: 0.1018, Val Loss: 0.0866\n",
      "Epoch 30/300 - Train Loss: 0.1002, Val Loss: 0.0904\n",
      "Epoch 31/300 - Train Loss: 0.0983, Val Loss: 0.0859\n",
      "Epoch 32/300 - Train Loss: 0.0994, Val Loss: 0.0852\n",
      "Epoch 33/300 - Train Loss: 0.0974, Val Loss: 0.0845\n",
      "Epoch 34/300 - Train Loss: 0.0984, Val Loss: 0.0836\n",
      "Epoch 35/300 - Train Loss: 0.0972, Val Loss: 0.0841\n",
      "Epoch 36/300 - Train Loss: 0.0975, Val Loss: 0.0834\n",
      "Epoch 37/300 - Train Loss: 0.0976, Val Loss: 0.0830\n",
      "Epoch 38/300 - Train Loss: 0.0969, Val Loss: 0.0840\n",
      "Epoch 39/300 - Train Loss: 0.0942, Val Loss: 0.0838\n",
      "Epoch 40/300 - Train Loss: 0.0946, Val Loss: 0.0814\n",
      "Epoch 41/300 - Train Loss: 0.0950, Val Loss: 0.0832\n",
      "Epoch 42/300 - Train Loss: 0.0954, Val Loss: 0.0815\n",
      "Epoch 43/300 - Train Loss: 0.0930, Val Loss: 0.0813\n",
      "Epoch 44/300 - Train Loss: 0.0915, Val Loss: 0.0811\n",
      "Epoch 45/300 - Train Loss: 0.0930, Val Loss: 0.0815\n",
      "Epoch 46/300 - Train Loss: 0.0937, Val Loss: 0.0824\n",
      "Epoch 47/300 - Train Loss: 0.0940, Val Loss: 0.0809\n",
      "Epoch 48/300 - Train Loss: 0.0935, Val Loss: 0.0809\n",
      "Epoch 49/300 - Train Loss: 0.0923, Val Loss: 0.0804\n",
      "Epoch 50/300 - Train Loss: 0.0910, Val Loss: 0.0791\n",
      "Epoch 51/300 - Train Loss: 0.0910, Val Loss: 0.0804\n",
      "Epoch 52/300 - Train Loss: 0.0914, Val Loss: 0.0795\n",
      "Epoch 53/300 - Train Loss: 0.0914, Val Loss: 0.0788\n",
      "Epoch 54/300 - Train Loss: 0.0911, Val Loss: 0.0818\n",
      "Epoch 55/300 - Train Loss: 0.0897, Val Loss: 0.0798\n",
      "Epoch 56/300 - Train Loss: 0.0885, Val Loss: 0.0820\n",
      "Epoch 57/300 - Train Loss: 0.0920, Val Loss: 0.0788\n",
      "Epoch 58/300 - Train Loss: 0.0918, Val Loss: 0.0817\n",
      "Epoch 59/300 - Train Loss: 0.0892, Val Loss: 0.0788\n",
      "Epoch 60/300 - Train Loss: 0.0900, Val Loss: 0.0791\n",
      "Epoch 61/300 - Train Loss: 0.0906, Val Loss: 0.0779\n",
      "Epoch 62/300 - Train Loss: 0.0900, Val Loss: 0.0790\n",
      "Epoch 63/300 - Train Loss: 0.0882, Val Loss: 0.0795\n",
      "Epoch 64/300 - Train Loss: 0.0884, Val Loss: 0.0772\n",
      "Epoch 65/300 - Train Loss: 0.0890, Val Loss: 0.0778\n",
      "Epoch 66/300 - Train Loss: 0.0897, Val Loss: 0.0784\n",
      "Epoch 67/300 - Train Loss: 0.0887, Val Loss: 0.0781\n",
      "Epoch 68/300 - Train Loss: 0.0876, Val Loss: 0.0780\n",
      "Epoch 69/300 - Train Loss: 0.0874, Val Loss: 0.0774\n",
      "Epoch 70/300 - Train Loss: 0.0886, Val Loss: 0.0759\n",
      "Epoch 71/300 - Train Loss: 0.0874, Val Loss: 0.0769\n",
      "Epoch 72/300 - Train Loss: 0.0883, Val Loss: 0.0786\n",
      "Epoch 73/300 - Train Loss: 0.0874, Val Loss: 0.0792\n",
      "Epoch 74/300 - Train Loss: 0.0873, Val Loss: 0.0800\n",
      "Epoch 75/300 - Train Loss: 0.0876, Val Loss: 0.0780\n",
      "Epoch 76/300 - Train Loss: 0.0860, Val Loss: 0.0793\n",
      "Epoch 77/300 - Train Loss: 0.0874, Val Loss: 0.0773\n",
      "Epoch 78/300 - Train Loss: 0.0864, Val Loss: 0.0776\n",
      "Epoch 79/300 - Train Loss: 0.0849, Val Loss: 0.0778\n",
      "Epoch 80/300 - Train Loss: 0.0862, Val Loss: 0.0765\n",
      "Epoch 81/300 - Train Loss: 0.0873, Val Loss: 0.0763\n",
      "Epoch 82/300 - Train Loss: 0.0848, Val Loss: 0.0767\n",
      "Epoch 83/300 - Train Loss: 0.0849, Val Loss: 0.0771\n",
      "Epoch 84/300 - Train Loss: 0.0866, Val Loss: 0.0770\n",
      "Epoch 85/300 - Train Loss: 0.0860, Val Loss: 0.0773\n",
      "Epoch 86/300 - Train Loss: 0.0853, Val Loss: 0.0768\n",
      "Epoch 87/300 - Train Loss: 0.0849, Val Loss: 0.0778\n",
      "Epoch 88/300 - Train Loss: 0.0864, Val Loss: 0.0776\n",
      "Epoch 89/300 - Train Loss: 0.0844, Val Loss: 0.0762\n",
      "Epoch 90/300 - Train Loss: 0.0844, Val Loss: 0.0775\n",
      "Epoch 91/300 - Train Loss: 0.0833, Val Loss: 0.0764\n",
      "Epoch 92/300 - Train Loss: 0.0854, Val Loss: 0.0759\n",
      "Epoch 93/300 - Train Loss: 0.0834, Val Loss: 0.0762\n",
      "Epoch 94/300 - Train Loss: 0.0841, Val Loss: 0.0756\n",
      "Epoch 95/300 - Train Loss: 0.0842, Val Loss: 0.0755\n",
      "Epoch 96/300 - Train Loss: 0.0839, Val Loss: 0.0770\n",
      "Epoch 97/300 - Train Loss: 0.0845, Val Loss: 0.0772\n",
      "Epoch 98/300 - Train Loss: 0.0834, Val Loss: 0.0768\n",
      "Epoch 99/300 - Train Loss: 0.0845, Val Loss: 0.0778\n",
      "Epoch 100/300 - Train Loss: 0.0842, Val Loss: 0.0770\n",
      "Epoch 101/300 - Train Loss: 0.0838, Val Loss: 0.0765\n",
      "Epoch 102/300 - Train Loss: 0.0852, Val Loss: 0.0777\n",
      "Epoch 103/300 - Train Loss: 0.0841, Val Loss: 0.0780\n",
      "Epoch 104/300 - Train Loss: 0.0815, Val Loss: 0.0761\n",
      "Epoch 105/300 - Train Loss: 0.0837, Val Loss: 0.0775\n",
      "Epoch 106/300 - Train Loss: 0.0828, Val Loss: 0.0762\n",
      "Epoch 107/300 - Train Loss: 0.0836, Val Loss: 0.0749\n",
      "Epoch 108/300 - Train Loss: 0.0819, Val Loss: 0.0761\n",
      "Epoch 109/300 - Train Loss: 0.0838, Val Loss: 0.0753\n",
      "Epoch 110/300 - Train Loss: 0.0827, Val Loss: 0.0754\n",
      "Epoch 111/300 - Train Loss: 0.0805, Val Loss: 0.0755\n",
      "Epoch 112/300 - Train Loss: 0.0824, Val Loss: 0.0780\n",
      "Epoch 113/300 - Train Loss: 0.0826, Val Loss: 0.0786\n",
      "Epoch 114/300 - Train Loss: 0.0828, Val Loss: 0.0774\n",
      "Epoch 115/300 - Train Loss: 0.0804, Val Loss: 0.0764\n",
      "Epoch 116/300 - Train Loss: 0.0812, Val Loss: 0.0756\n",
      "Epoch 117/300 - Train Loss: 0.0806, Val Loss: 0.0759\n",
      "Epoch 118/300 - Train Loss: 0.0811, Val Loss: 0.0751\n",
      "Epoch 119/300 - Train Loss: 0.0810, Val Loss: 0.0760\n",
      "Epoch 120/300 - Train Loss: 0.0810, Val Loss: 0.0769\n",
      "Epoch 121/300 - Train Loss: 0.0817, Val Loss: 0.0770\n",
      "Epoch 122/300 - Train Loss: 0.0813, Val Loss: 0.0744\n",
      "Epoch 123/300 - Train Loss: 0.0822, Val Loss: 0.0740\n",
      "Epoch 124/300 - Train Loss: 0.0811, Val Loss: 0.0763\n",
      "Epoch 125/300 - Train Loss: 0.0814, Val Loss: 0.0751\n",
      "Epoch 126/300 - Train Loss: 0.0817, Val Loss: 0.0743\n",
      "Epoch 127/300 - Train Loss: 0.0808, Val Loss: 0.0765\n",
      "Epoch 128/300 - Train Loss: 0.0796, Val Loss: 0.0762\n",
      "Epoch 129/300 - Train Loss: 0.0809, Val Loss: 0.0743\n",
      "Epoch 130/300 - Train Loss: 0.0798, Val Loss: 0.0757\n",
      "Epoch 131/300 - Train Loss: 0.0795, Val Loss: 0.0755\n",
      "Epoch 132/300 - Train Loss: 0.0805, Val Loss: 0.0750\n",
      "Epoch 133/300 - Train Loss: 0.0801, Val Loss: 0.0751\n",
      "Epoch 134/300 - Train Loss: 0.0799, Val Loss: 0.0751\n",
      "Epoch 135/300 - Train Loss: 0.0801, Val Loss: 0.0750\n",
      "Epoch 136/300 - Train Loss: 0.0801, Val Loss: 0.0752\n",
      "Epoch 137/300 - Train Loss: 0.0796, Val Loss: 0.0761\n",
      "Epoch 138/300 - Train Loss: 0.0798, Val Loss: 0.0753\n",
      "Epoch 139/300 - Train Loss: 0.0798, Val Loss: 0.0751\n",
      "Epoch 140/300 - Train Loss: 0.0811, Val Loss: 0.0749\n",
      "Epoch 141/300 - Train Loss: 0.0793, Val Loss: 0.0749\n",
      "Epoch 142/300 - Train Loss: 0.0797, Val Loss: 0.0735\n",
      "Epoch 143/300 - Train Loss: 0.0777, Val Loss: 0.0755\n",
      "Epoch 144/300 - Train Loss: 0.0781, Val Loss: 0.0750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0782, Val Loss: 0.0737\n",
      "Epoch 146/300 - Train Loss: 0.0797, Val Loss: 0.0754\n",
      "Epoch 147/300 - Train Loss: 0.0777, Val Loss: 0.0741\n",
      "Epoch 148/300 - Train Loss: 0.0786, Val Loss: 0.0750\n",
      "Epoch 149/300 - Train Loss: 0.0786, Val Loss: 0.0739\n",
      "Epoch 150/300 - Train Loss: 0.0778, Val Loss: 0.0727\n",
      "Epoch 151/300 - Train Loss: 0.0780, Val Loss: 0.0740\n",
      "Epoch 152/300 - Train Loss: 0.0786, Val Loss: 0.0744\n",
      "Epoch 153/300 - Train Loss: 0.0787, Val Loss: 0.0758\n",
      "Epoch 154/300 - Train Loss: 0.0784, Val Loss: 0.0749\n",
      "Epoch 155/300 - Train Loss: 0.0770, Val Loss: 0.0740\n",
      "Epoch 156/300 - Train Loss: 0.0794, Val Loss: 0.0732\n",
      "Epoch 157/300 - Train Loss: 0.0784, Val Loss: 0.0755\n",
      "Epoch 158/300 - Train Loss: 0.0775, Val Loss: 0.0745\n",
      "Epoch 159/300 - Train Loss: 0.0769, Val Loss: 0.0740\n",
      "Epoch 160/300 - Train Loss: 0.0790, Val Loss: 0.0750\n",
      "Epoch 161/300 - Train Loss: 0.0781, Val Loss: 0.0731\n",
      "Epoch 162/300 - Train Loss: 0.0782, Val Loss: 0.0742\n",
      "Epoch 163/300 - Train Loss: 0.0764, Val Loss: 0.0755\n",
      "Epoch 164/300 - Train Loss: 0.0772, Val Loss: 0.0747\n",
      "Epoch 165/300 - Train Loss: 0.0782, Val Loss: 0.0732\n",
      "Epoch 166/300 - Train Loss: 0.0771, Val Loss: 0.0759\n",
      "Epoch 167/300 - Train Loss: 0.0783, Val Loss: 0.0745\n",
      "Epoch 168/300 - Train Loss: 0.0780, Val Loss: 0.0722\n",
      "Epoch 169/300 - Train Loss: 0.0771, Val Loss: 0.0761\n",
      "Epoch 170/300 - Train Loss: 0.0773, Val Loss: 0.0734\n",
      "Epoch 171/300 - Train Loss: 0.0770, Val Loss: 0.0735\n",
      "Epoch 172/300 - Train Loss: 0.0778, Val Loss: 0.0743\n",
      "Epoch 173/300 - Train Loss: 0.0778, Val Loss: 0.0729\n",
      "Epoch 174/300 - Train Loss: 0.0765, Val Loss: 0.0736\n",
      "Epoch 175/300 - Train Loss: 0.0768, Val Loss: 0.0730\n",
      "Epoch 176/300 - Train Loss: 0.0768, Val Loss: 0.0734\n",
      "Epoch 177/300 - Train Loss: 0.0767, Val Loss: 0.0754\n",
      "Epoch 178/300 - Train Loss: 0.0756, Val Loss: 0.0740\n",
      "Epoch 179/300 - Train Loss: 0.0761, Val Loss: 0.0741\n",
      "Epoch 180/300 - Train Loss: 0.0745, Val Loss: 0.0722\n",
      "Epoch 181/300 - Train Loss: 0.0776, Val Loss: 0.0728\n",
      "Epoch 182/300 - Train Loss: 0.0782, Val Loss: 0.0737\n",
      "Epoch 183/300 - Train Loss: 0.0749, Val Loss: 0.0731\n",
      "Epoch 184/300 - Train Loss: 0.0770, Val Loss: 0.0737\n",
      "Epoch 185/300 - Train Loss: 0.0762, Val Loss: 0.0730\n",
      "Epoch 186/300 - Train Loss: 0.0789, Val Loss: 0.0731\n",
      "Epoch 187/300 - Train Loss: 0.0774, Val Loss: 0.0732\n",
      "Epoch 188/300 - Train Loss: 0.0758, Val Loss: 0.0726\n",
      "Epoch 189/300 - Train Loss: 0.0747, Val Loss: 0.0742\n",
      "Epoch 190/300 - Train Loss: 0.0768, Val Loss: 0.0725\n",
      "Epoch 191/300 - Train Loss: 0.0759, Val Loss: 0.0740\n",
      "Epoch 192/300 - Train Loss: 0.0764, Val Loss: 0.0731\n",
      "Epoch 193/300 - Train Loss: 0.0762, Val Loss: 0.0728\n",
      "Epoch 194/300 - Train Loss: 0.0768, Val Loss: 0.0736\n",
      "Epoch 195/300 - Train Loss: 0.0747, Val Loss: 0.0743\n",
      "Epoch 196/300 - Train Loss: 0.0758, Val Loss: 0.0743\n",
      "Epoch 197/300 - Train Loss: 0.0760, Val Loss: 0.0743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:45:57,865] Trial 31 finished with value: 0.9653774266717964 and parameters: {'F1': 16, 'F2': 8, 'D': 4, 'dropout': 0.2566947249800793, 'learning_rate': 1.8529155560335235e-05, 'batch_size': 64, 'weight_decay': 2.0035096115672113e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 198/300 - Train Loss: 0.0752, Val Loss: 0.0728\n",
      "Early stopping at epoch 198\n",
      "Macro F1 Score: 0.9654, Macro Precision: 0.9552, Macro Recall: 0.9766\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 33\n",
      "Training with F1=16, F2=8, D=4, dropout=0.21068276085531562, LR=3.209968774533337e-05, BS=64, WD=1.7917863146446622e-05\n",
      "Epoch 1/300 - Train Loss: 0.8325, Val Loss: 0.5008\n",
      "Epoch 2/300 - Train Loss: 0.3657, Val Loss: 0.2651\n",
      "Epoch 3/300 - Train Loss: 0.2350, Val Loss: 0.1963\n",
      "Epoch 4/300 - Train Loss: 0.1875, Val Loss: 0.1542\n",
      "Epoch 5/300 - Train Loss: 0.1587, Val Loss: 0.1341\n",
      "Epoch 6/300 - Train Loss: 0.1409, Val Loss: 0.1147\n",
      "Epoch 7/300 - Train Loss: 0.1269, Val Loss: 0.1102\n",
      "Epoch 8/300 - Train Loss: 0.1167, Val Loss: 0.1029\n",
      "Epoch 9/300 - Train Loss: 0.1129, Val Loss: 0.0942\n",
      "Epoch 10/300 - Train Loss: 0.1090, Val Loss: 0.0925\n",
      "Epoch 11/300 - Train Loss: 0.1056, Val Loss: 0.0883\n",
      "Epoch 12/300 - Train Loss: 0.1016, Val Loss: 0.0881\n",
      "Epoch 13/300 - Train Loss: 0.0985, Val Loss: 0.0840\n",
      "Epoch 14/300 - Train Loss: 0.0989, Val Loss: 0.0885\n",
      "Epoch 15/300 - Train Loss: 0.0958, Val Loss: 0.0802\n",
      "Epoch 16/300 - Train Loss: 0.0951, Val Loss: 0.0811\n",
      "Epoch 17/300 - Train Loss: 0.0936, Val Loss: 0.0793\n",
      "Epoch 18/300 - Train Loss: 0.0921, Val Loss: 0.0786\n",
      "Epoch 19/300 - Train Loss: 0.0921, Val Loss: 0.0783\n",
      "Epoch 20/300 - Train Loss: 0.0913, Val Loss: 0.0820\n",
      "Epoch 21/300 - Train Loss: 0.0912, Val Loss: 0.0782\n",
      "Epoch 22/300 - Train Loss: 0.0904, Val Loss: 0.0801\n",
      "Epoch 23/300 - Train Loss: 0.0874, Val Loss: 0.0781\n",
      "Epoch 24/300 - Train Loss: 0.0873, Val Loss: 0.0778\n",
      "Epoch 25/300 - Train Loss: 0.0891, Val Loss: 0.0772\n",
      "Epoch 26/300 - Train Loss: 0.0868, Val Loss: 0.0750\n",
      "Epoch 27/300 - Train Loss: 0.0861, Val Loss: 0.0772\n",
      "Epoch 28/300 - Train Loss: 0.0852, Val Loss: 0.0756\n",
      "Epoch 29/300 - Train Loss: 0.0860, Val Loss: 0.0760\n",
      "Epoch 30/300 - Train Loss: 0.0854, Val Loss: 0.0769\n",
      "Epoch 31/300 - Train Loss: 0.0859, Val Loss: 0.0741\n",
      "Epoch 32/300 - Train Loss: 0.0837, Val Loss: 0.0771\n",
      "Epoch 33/300 - Train Loss: 0.0837, Val Loss: 0.0751\n",
      "Epoch 34/300 - Train Loss: 0.0846, Val Loss: 0.0741\n",
      "Epoch 35/300 - Train Loss: 0.0848, Val Loss: 0.0739\n",
      "Epoch 36/300 - Train Loss: 0.0829, Val Loss: 0.0752\n",
      "Epoch 37/300 - Train Loss: 0.0825, Val Loss: 0.0735\n",
      "Epoch 38/300 - Train Loss: 0.0806, Val Loss: 0.0730\n",
      "Epoch 39/300 - Train Loss: 0.0822, Val Loss: 0.0739\n",
      "Epoch 40/300 - Train Loss: 0.0815, Val Loss: 0.0741\n",
      "Epoch 41/300 - Train Loss: 0.0831, Val Loss: 0.0717\n",
      "Epoch 42/300 - Train Loss: 0.0828, Val Loss: 0.0722\n",
      "Epoch 43/300 - Train Loss: 0.0826, Val Loss: 0.0748\n",
      "Epoch 44/300 - Train Loss: 0.0816, Val Loss: 0.0716\n",
      "Epoch 45/300 - Train Loss: 0.0796, Val Loss: 0.0728\n",
      "Epoch 46/300 - Train Loss: 0.0809, Val Loss: 0.0732\n",
      "Epoch 47/300 - Train Loss: 0.0807, Val Loss: 0.0737\n",
      "Epoch 48/300 - Train Loss: 0.0807, Val Loss: 0.0721\n",
      "Epoch 49/300 - Train Loss: 0.0806, Val Loss: 0.0733\n",
      "Epoch 50/300 - Train Loss: 0.0796, Val Loss: 0.0719\n",
      "Epoch 51/300 - Train Loss: 0.0799, Val Loss: 0.0695\n",
      "Epoch 52/300 - Train Loss: 0.0795, Val Loss: 0.0714\n",
      "Epoch 53/300 - Train Loss: 0.0818, Val Loss: 0.0698\n",
      "Epoch 54/300 - Train Loss: 0.0809, Val Loss: 0.0721\n",
      "Epoch 55/300 - Train Loss: 0.0789, Val Loss: 0.0716\n",
      "Epoch 56/300 - Train Loss: 0.0796, Val Loss: 0.0712\n",
      "Epoch 57/300 - Train Loss: 0.0796, Val Loss: 0.0718\n",
      "Epoch 58/300 - Train Loss: 0.0773, Val Loss: 0.0697\n",
      "Epoch 59/300 - Train Loss: 0.0802, Val Loss: 0.0696\n",
      "Epoch 60/300 - Train Loss: 0.0789, Val Loss: 0.0704\n",
      "Epoch 61/300 - Train Loss: 0.0776, Val Loss: 0.0722\n",
      "Epoch 62/300 - Train Loss: 0.0772, Val Loss: 0.0698\n",
      "Epoch 63/300 - Train Loss: 0.0769, Val Loss: 0.0710\n",
      "Epoch 64/300 - Train Loss: 0.0784, Val Loss: 0.0691\n",
      "Epoch 65/300 - Train Loss: 0.0787, Val Loss: 0.0716\n",
      "Epoch 66/300 - Train Loss: 0.0767, Val Loss: 0.0722\n",
      "Epoch 67/300 - Train Loss: 0.0777, Val Loss: 0.0726\n",
      "Epoch 68/300 - Train Loss: 0.0776, Val Loss: 0.0707\n",
      "Epoch 69/300 - Train Loss: 0.0775, Val Loss: 0.0696\n",
      "Epoch 70/300 - Train Loss: 0.0781, Val Loss: 0.0691\n",
      "Epoch 71/300 - Train Loss: 0.0771, Val Loss: 0.0714\n",
      "Epoch 72/300 - Train Loss: 0.0765, Val Loss: 0.0706\n",
      "Epoch 73/300 - Train Loss: 0.0773, Val Loss: 0.0696\n",
      "Epoch 74/300 - Train Loss: 0.0776, Val Loss: 0.0713\n",
      "Epoch 75/300 - Train Loss: 0.0773, Val Loss: 0.0692\n",
      "Epoch 76/300 - Train Loss: 0.0765, Val Loss: 0.0711\n",
      "Epoch 77/300 - Train Loss: 0.0775, Val Loss: 0.0705\n",
      "Epoch 78/300 - Train Loss: 0.0766, Val Loss: 0.0717\n",
      "Epoch 79/300 - Train Loss: 0.0762, Val Loss: 0.0702\n",
      "Epoch 80/300 - Train Loss: 0.0767, Val Loss: 0.0702\n",
      "Epoch 81/300 - Train Loss: 0.0754, Val Loss: 0.0685\n",
      "Epoch 82/300 - Train Loss: 0.0765, Val Loss: 0.0710\n",
      "Epoch 83/300 - Train Loss: 0.0753, Val Loss: 0.0733\n",
      "Epoch 84/300 - Train Loss: 0.0767, Val Loss: 0.0686\n",
      "Epoch 85/300 - Train Loss: 0.0758, Val Loss: 0.0685\n",
      "Epoch 86/300 - Train Loss: 0.0764, Val Loss: 0.0686\n",
      "Epoch 87/300 - Train Loss: 0.0759, Val Loss: 0.0712\n",
      "Epoch 88/300 - Train Loss: 0.0770, Val Loss: 0.0676\n",
      "Epoch 89/300 - Train Loss: 0.0759, Val Loss: 0.0695\n",
      "Epoch 90/300 - Train Loss: 0.0754, Val Loss: 0.0705\n",
      "Epoch 91/300 - Train Loss: 0.0747, Val Loss: 0.0681\n",
      "Epoch 92/300 - Train Loss: 0.0732, Val Loss: 0.0686\n",
      "Epoch 93/300 - Train Loss: 0.0748, Val Loss: 0.0668\n",
      "Epoch 94/300 - Train Loss: 0.0738, Val Loss: 0.0707\n",
      "Epoch 95/300 - Train Loss: 0.0743, Val Loss: 0.0693\n",
      "Epoch 96/300 - Train Loss: 0.0734, Val Loss: 0.0685\n",
      "Epoch 97/300 - Train Loss: 0.0751, Val Loss: 0.0706\n",
      "Epoch 98/300 - Train Loss: 0.0754, Val Loss: 0.0689\n",
      "Epoch 99/300 - Train Loss: 0.0738, Val Loss: 0.0697\n",
      "Epoch 100/300 - Train Loss: 0.0721, Val Loss: 0.0699\n",
      "Epoch 101/300 - Train Loss: 0.0729, Val Loss: 0.0686\n",
      "Epoch 102/300 - Train Loss: 0.0738, Val Loss: 0.0676\n",
      "Epoch 103/300 - Train Loss: 0.0730, Val Loss: 0.0690\n",
      "Epoch 104/300 - Train Loss: 0.0739, Val Loss: 0.0693\n",
      "Epoch 105/300 - Train Loss: 0.0726, Val Loss: 0.0682\n",
      "Epoch 106/300 - Train Loss: 0.0733, Val Loss: 0.0687\n",
      "Epoch 107/300 - Train Loss: 0.0748, Val Loss: 0.0682\n",
      "Epoch 108/300 - Train Loss: 0.0739, Val Loss: 0.0678\n",
      "Epoch 109/300 - Train Loss: 0.0726, Val Loss: 0.0687\n",
      "Epoch 110/300 - Train Loss: 0.0736, Val Loss: 0.0682\n",
      "Epoch 111/300 - Train Loss: 0.0727, Val Loss: 0.0707\n",
      "Epoch 112/300 - Train Loss: 0.0738, Val Loss: 0.0695\n",
      "Epoch 113/300 - Train Loss: 0.0738, Val Loss: 0.0680\n",
      "Epoch 114/300 - Train Loss: 0.0725, Val Loss: 0.0675\n",
      "Epoch 115/300 - Train Loss: 0.0719, Val Loss: 0.0688\n",
      "Epoch 116/300 - Train Loss: 0.0727, Val Loss: 0.0671\n",
      "Epoch 117/300 - Train Loss: 0.0728, Val Loss: 0.0677\n",
      "Epoch 118/300 - Train Loss: 0.0730, Val Loss: 0.0701\n",
      "Epoch 119/300 - Train Loss: 0.0722, Val Loss: 0.0687\n",
      "Epoch 120/300 - Train Loss: 0.0718, Val Loss: 0.0700\n",
      "Epoch 121/300 - Train Loss: 0.0724, Val Loss: 0.0683\n",
      "Epoch 122/300 - Train Loss: 0.0727, Val Loss: 0.0693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:50:01,340] Trial 32 finished with value: 0.9647452184933031 and parameters: {'F1': 16, 'F2': 8, 'D': 4, 'dropout': 0.21068276085531562, 'learning_rate': 3.209968774533337e-05, 'batch_size': 64, 'weight_decay': 1.7917863146446622e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 123/300 - Train Loss: 0.0709, Val Loss: 0.0682\n",
      "Early stopping at epoch 123\n",
      "Macro F1 Score: 0.9647, Macro Precision: 0.9509, Macro Recall: 0.9805\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.88      0.98      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 34\n",
      "Training with F1=16, F2=8, D=4, dropout=0.2725140095810382, LR=2.4196185570449757e-05, BS=64, WD=2.939724454441985e-05\n",
      "Epoch 1/300 - Train Loss: 0.8139, Val Loss: 0.5655\n",
      "Epoch 2/300 - Train Loss: 0.4404, Val Loss: 0.3473\n",
      "Epoch 3/300 - Train Loss: 0.3161, Val Loss: 0.2755\n",
      "Epoch 4/300 - Train Loss: 0.2596, Val Loss: 0.2203\n",
      "Epoch 5/300 - Train Loss: 0.2190, Val Loss: 0.1872\n",
      "Epoch 6/300 - Train Loss: 0.1873, Val Loss: 0.1583\n",
      "Epoch 7/300 - Train Loss: 0.1626, Val Loss: 0.1416\n",
      "Epoch 8/300 - Train Loss: 0.1482, Val Loss: 0.1315\n",
      "Epoch 9/300 - Train Loss: 0.1344, Val Loss: 0.1211\n",
      "Epoch 10/300 - Train Loss: 0.1275, Val Loss: 0.1131\n",
      "Epoch 11/300 - Train Loss: 0.1220, Val Loss: 0.1110\n",
      "Epoch 12/300 - Train Loss: 0.1172, Val Loss: 0.1015\n",
      "Epoch 13/300 - Train Loss: 0.1134, Val Loss: 0.1000\n",
      "Epoch 14/300 - Train Loss: 0.1108, Val Loss: 0.0980\n",
      "Epoch 15/300 - Train Loss: 0.1082, Val Loss: 0.0927\n",
      "Epoch 16/300 - Train Loss: 0.1053, Val Loss: 0.0952\n",
      "Epoch 17/300 - Train Loss: 0.1066, Val Loss: 0.0914\n",
      "Epoch 18/300 - Train Loss: 0.1029, Val Loss: 0.0890\n",
      "Epoch 19/300 - Train Loss: 0.1027, Val Loss: 0.0900\n",
      "Epoch 20/300 - Train Loss: 0.1010, Val Loss: 0.0862\n",
      "Epoch 21/300 - Train Loss: 0.0991, Val Loss: 0.0844\n",
      "Epoch 22/300 - Train Loss: 0.0991, Val Loss: 0.0829\n",
      "Epoch 23/300 - Train Loss: 0.0990, Val Loss: 0.0847\n",
      "Epoch 24/300 - Train Loss: 0.0993, Val Loss: 0.0811\n",
      "Epoch 25/300 - Train Loss: 0.0970, Val Loss: 0.0826\n",
      "Epoch 26/300 - Train Loss: 0.0969, Val Loss: 0.0824\n",
      "Epoch 27/300 - Train Loss: 0.0968, Val Loss: 0.0838\n",
      "Epoch 28/300 - Train Loss: 0.0962, Val Loss: 0.0814\n",
      "Epoch 29/300 - Train Loss: 0.0975, Val Loss: 0.0820\n",
      "Epoch 30/300 - Train Loss: 0.0943, Val Loss: 0.0789\n",
      "Epoch 31/300 - Train Loss: 0.0941, Val Loss: 0.0788\n",
      "Epoch 32/300 - Train Loss: 0.0948, Val Loss: 0.0784\n",
      "Epoch 33/300 - Train Loss: 0.0939, Val Loss: 0.0849\n",
      "Epoch 34/300 - Train Loss: 0.0925, Val Loss: 0.0792\n",
      "Epoch 35/300 - Train Loss: 0.0917, Val Loss: 0.0786\n",
      "Epoch 36/300 - Train Loss: 0.0923, Val Loss: 0.0773\n",
      "Epoch 37/300 - Train Loss: 0.0917, Val Loss: 0.0777\n",
      "Epoch 38/300 - Train Loss: 0.0925, Val Loss: 0.0781\n",
      "Epoch 39/300 - Train Loss: 0.0904, Val Loss: 0.0802\n",
      "Epoch 40/300 - Train Loss: 0.0904, Val Loss: 0.0773\n",
      "Epoch 41/300 - Train Loss: 0.0890, Val Loss: 0.0786\n",
      "Epoch 42/300 - Train Loss: 0.0909, Val Loss: 0.0773\n",
      "Epoch 43/300 - Train Loss: 0.0898, Val Loss: 0.0766\n",
      "Epoch 44/300 - Train Loss: 0.0895, Val Loss: 0.0770\n",
      "Epoch 45/300 - Train Loss: 0.0885, Val Loss: 0.0749\n",
      "Epoch 46/300 - Train Loss: 0.0916, Val Loss: 0.0768\n",
      "Epoch 47/300 - Train Loss: 0.0881, Val Loss: 0.0763\n",
      "Epoch 48/300 - Train Loss: 0.0893, Val Loss: 0.0759\n",
      "Epoch 49/300 - Train Loss: 0.0864, Val Loss: 0.0774\n",
      "Epoch 50/300 - Train Loss: 0.0879, Val Loss: 0.0779\n",
      "Epoch 51/300 - Train Loss: 0.0879, Val Loss: 0.0783\n",
      "Epoch 52/300 - Train Loss: 0.0882, Val Loss: 0.0753\n",
      "Epoch 53/300 - Train Loss: 0.0876, Val Loss: 0.0773\n",
      "Epoch 54/300 - Train Loss: 0.0865, Val Loss: 0.0751\n",
      "Epoch 55/300 - Train Loss: 0.0874, Val Loss: 0.0757\n",
      "Epoch 56/300 - Train Loss: 0.0852, Val Loss: 0.0736\n",
      "Epoch 57/300 - Train Loss: 0.0863, Val Loss: 0.0745\n",
      "Epoch 58/300 - Train Loss: 0.0852, Val Loss: 0.0783\n",
      "Epoch 59/300 - Train Loss: 0.0881, Val Loss: 0.0748\n",
      "Epoch 60/300 - Train Loss: 0.0868, Val Loss: 0.0748\n",
      "Epoch 61/300 - Train Loss: 0.0862, Val Loss: 0.0753\n",
      "Epoch 62/300 - Train Loss: 0.0848, Val Loss: 0.0736\n",
      "Epoch 63/300 - Train Loss: 0.0854, Val Loss: 0.0740\n",
      "Epoch 64/300 - Train Loss: 0.0839, Val Loss: 0.0741\n",
      "Epoch 65/300 - Train Loss: 0.0846, Val Loss: 0.0754\n",
      "Epoch 66/300 - Train Loss: 0.0852, Val Loss: 0.0744\n",
      "Epoch 67/300 - Train Loss: 0.0847, Val Loss: 0.0742\n",
      "Epoch 68/300 - Train Loss: 0.0858, Val Loss: 0.0769\n",
      "Epoch 69/300 - Train Loss: 0.0835, Val Loss: 0.0735\n",
      "Epoch 70/300 - Train Loss: 0.0831, Val Loss: 0.0752\n",
      "Epoch 71/300 - Train Loss: 0.0826, Val Loss: 0.0784\n",
      "Epoch 72/300 - Train Loss: 0.0843, Val Loss: 0.0744\n",
      "Epoch 73/300 - Train Loss: 0.0841, Val Loss: 0.0739\n",
      "Epoch 74/300 - Train Loss: 0.0832, Val Loss: 0.0731\n",
      "Epoch 75/300 - Train Loss: 0.0838, Val Loss: 0.0744\n",
      "Epoch 76/300 - Train Loss: 0.0831, Val Loss: 0.0736\n",
      "Epoch 77/300 - Train Loss: 0.0830, Val Loss: 0.0727\n",
      "Epoch 78/300 - Train Loss: 0.0837, Val Loss: 0.0732\n",
      "Epoch 79/300 - Train Loss: 0.0822, Val Loss: 0.0737\n",
      "Epoch 80/300 - Train Loss: 0.0835, Val Loss: 0.0709\n",
      "Epoch 81/300 - Train Loss: 0.0817, Val Loss: 0.0736\n",
      "Epoch 82/300 - Train Loss: 0.0829, Val Loss: 0.0726\n",
      "Epoch 83/300 - Train Loss: 0.0829, Val Loss: 0.0719\n",
      "Epoch 84/300 - Train Loss: 0.0827, Val Loss: 0.0726\n",
      "Epoch 85/300 - Train Loss: 0.0819, Val Loss: 0.0731\n",
      "Epoch 86/300 - Train Loss: 0.0805, Val Loss: 0.0751\n",
      "Epoch 87/300 - Train Loss: 0.0833, Val Loss: 0.0740\n",
      "Epoch 88/300 - Train Loss: 0.0814, Val Loss: 0.0729\n",
      "Epoch 89/300 - Train Loss: 0.0813, Val Loss: 0.0713\n",
      "Epoch 90/300 - Train Loss: 0.0804, Val Loss: 0.0725\n",
      "Epoch 91/300 - Train Loss: 0.0807, Val Loss: 0.0724\n",
      "Epoch 92/300 - Train Loss: 0.0821, Val Loss: 0.0723\n",
      "Epoch 93/300 - Train Loss: 0.0801, Val Loss: 0.0736\n",
      "Epoch 94/300 - Train Loss: 0.0813, Val Loss: 0.0727\n",
      "Epoch 95/300 - Train Loss: 0.0798, Val Loss: 0.0726\n",
      "Epoch 96/300 - Train Loss: 0.0816, Val Loss: 0.0728\n",
      "Epoch 97/300 - Train Loss: 0.0802, Val Loss: 0.0753\n",
      "Epoch 98/300 - Train Loss: 0.0820, Val Loss: 0.0721\n",
      "Epoch 99/300 - Train Loss: 0.0808, Val Loss: 0.0704\n",
      "Epoch 100/300 - Train Loss: 0.0804, Val Loss: 0.0732\n",
      "Epoch 101/300 - Train Loss: 0.0804, Val Loss: 0.0712\n",
      "Epoch 102/300 - Train Loss: 0.0805, Val Loss: 0.0737\n",
      "Epoch 103/300 - Train Loss: 0.0814, Val Loss: 0.0715\n",
      "Epoch 104/300 - Train Loss: 0.0812, Val Loss: 0.0721\n",
      "Epoch 105/300 - Train Loss: 0.0809, Val Loss: 0.0713\n",
      "Epoch 106/300 - Train Loss: 0.0807, Val Loss: 0.0730\n",
      "Epoch 107/300 - Train Loss: 0.0788, Val Loss: 0.0724\n",
      "Epoch 108/300 - Train Loss: 0.0800, Val Loss: 0.0713\n",
      "Epoch 109/300 - Train Loss: 0.0791, Val Loss: 0.0726\n",
      "Epoch 110/300 - Train Loss: 0.0796, Val Loss: 0.0720\n",
      "Epoch 111/300 - Train Loss: 0.0789, Val Loss: 0.0733\n",
      "Epoch 112/300 - Train Loss: 0.0790, Val Loss: 0.0717\n",
      "Epoch 113/300 - Train Loss: 0.0806, Val Loss: 0.0702\n",
      "Epoch 114/300 - Train Loss: 0.0815, Val Loss: 0.0710\n",
      "Epoch 115/300 - Train Loss: 0.0783, Val Loss: 0.0721\n",
      "Epoch 116/300 - Train Loss: 0.0773, Val Loss: 0.0715\n",
      "Epoch 117/300 - Train Loss: 0.0799, Val Loss: 0.0717\n",
      "Epoch 118/300 - Train Loss: 0.0787, Val Loss: 0.0721\n",
      "Epoch 119/300 - Train Loss: 0.0783, Val Loss: 0.0725\n",
      "Epoch 120/300 - Train Loss: 0.0781, Val Loss: 0.0730\n",
      "Epoch 121/300 - Train Loss: 0.0791, Val Loss: 0.0705\n",
      "Epoch 122/300 - Train Loss: 0.0768, Val Loss: 0.0725\n",
      "Epoch 123/300 - Train Loss: 0.0778, Val Loss: 0.0722\n",
      "Epoch 124/300 - Train Loss: 0.0791, Val Loss: 0.0750\n",
      "Epoch 125/300 - Train Loss: 0.0773, Val Loss: 0.0717\n",
      "Epoch 126/300 - Train Loss: 0.0785, Val Loss: 0.0720\n",
      "Epoch 127/300 - Train Loss: 0.0780, Val Loss: 0.0720\n",
      "Epoch 128/300 - Train Loss: 0.0795, Val Loss: 0.0730\n",
      "Epoch 129/300 - Train Loss: 0.0768, Val Loss: 0.0720\n",
      "Epoch 130/300 - Train Loss: 0.0775, Val Loss: 0.0727\n",
      "Epoch 131/300 - Train Loss: 0.0786, Val Loss: 0.0715\n",
      "Epoch 132/300 - Train Loss: 0.0768, Val Loss: 0.0715\n",
      "Epoch 133/300 - Train Loss: 0.0775, Val Loss: 0.0705\n",
      "Epoch 134/300 - Train Loss: 0.0772, Val Loss: 0.0731\n",
      "Epoch 135/300 - Train Loss: 0.0774, Val Loss: 0.0729\n",
      "Epoch 136/300 - Train Loss: 0.0757, Val Loss: 0.0708\n",
      "Epoch 137/300 - Train Loss: 0.0771, Val Loss: 0.0701\n",
      "Epoch 138/300 - Train Loss: 0.0776, Val Loss: 0.0720\n",
      "Epoch 139/300 - Train Loss: 0.0757, Val Loss: 0.0720\n",
      "Epoch 140/300 - Train Loss: 0.0776, Val Loss: 0.0708\n",
      "Epoch 141/300 - Train Loss: 0.0770, Val Loss: 0.0713\n",
      "Epoch 142/300 - Train Loss: 0.0767, Val Loss: 0.0703\n",
      "Epoch 143/300 - Train Loss: 0.0763, Val Loss: 0.0704\n",
      "Epoch 144/300 - Train Loss: 0.0767, Val Loss: 0.0719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0783, Val Loss: 0.0686\n",
      "Epoch 146/300 - Train Loss: 0.0757, Val Loss: 0.0713\n",
      "Epoch 147/300 - Train Loss: 0.0776, Val Loss: 0.0719\n",
      "Epoch 148/300 - Train Loss: 0.0764, Val Loss: 0.0734\n",
      "Epoch 149/300 - Train Loss: 0.0770, Val Loss: 0.0694\n",
      "Epoch 150/300 - Train Loss: 0.0777, Val Loss: 0.0705\n",
      "Epoch 151/300 - Train Loss: 0.0770, Val Loss: 0.0703\n",
      "Epoch 152/300 - Train Loss: 0.0752, Val Loss: 0.0710\n",
      "Epoch 153/300 - Train Loss: 0.0765, Val Loss: 0.0714\n",
      "Epoch 154/300 - Train Loss: 0.0752, Val Loss: 0.0732\n",
      "Epoch 155/300 - Train Loss: 0.0768, Val Loss: 0.0712\n",
      "Epoch 156/300 - Train Loss: 0.0759, Val Loss: 0.0699\n",
      "Epoch 157/300 - Train Loss: 0.0737, Val Loss: 0.0699\n",
      "Epoch 158/300 - Train Loss: 0.0738, Val Loss: 0.0705\n",
      "Epoch 159/300 - Train Loss: 0.0759, Val Loss: 0.0699\n",
      "Epoch 160/300 - Train Loss: 0.0761, Val Loss: 0.0705\n",
      "Epoch 161/300 - Train Loss: 0.0753, Val Loss: 0.0719\n",
      "Epoch 162/300 - Train Loss: 0.0760, Val Loss: 0.0693\n",
      "Epoch 163/300 - Train Loss: 0.0744, Val Loss: 0.0710\n",
      "Epoch 164/300 - Train Loss: 0.0760, Val Loss: 0.0715\n",
      "Epoch 165/300 - Train Loss: 0.0758, Val Loss: 0.0706\n",
      "Epoch 166/300 - Train Loss: 0.0759, Val Loss: 0.0713\n",
      "Epoch 167/300 - Train Loss: 0.0751, Val Loss: 0.0709\n",
      "Epoch 168/300 - Train Loss: 0.0747, Val Loss: 0.0711\n",
      "Epoch 169/300 - Train Loss: 0.0758, Val Loss: 0.0712\n",
      "Epoch 170/300 - Train Loss: 0.0762, Val Loss: 0.0702\n",
      "Epoch 171/300 - Train Loss: 0.0742, Val Loss: 0.0709\n",
      "Epoch 172/300 - Train Loss: 0.0742, Val Loss: 0.0697\n",
      "Epoch 173/300 - Train Loss: 0.0748, Val Loss: 0.0710\n",
      "Epoch 174/300 - Train Loss: 0.0758, Val Loss: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 00:55:47,772] Trial 33 finished with value: 0.9658658936471493 and parameters: {'F1': 16, 'F2': 8, 'D': 4, 'dropout': 0.2725140095810382, 'learning_rate': 2.4196185570449757e-05, 'batch_size': 64, 'weight_decay': 2.939724454441985e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/300 - Train Loss: 0.0761, Val Loss: 0.0729\n",
      "Early stopping at epoch 175\n",
      "Macro F1 Score: 0.9659, Macro Precision: 0.9559, Macro Recall: 0.9768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 35\n",
      "Training with F1=16, F2=8, D=4, dropout=0.3284251592850694, LR=1.4511081462236066e-05, BS=64, WD=4.849964797817115e-05\n",
      "Epoch 1/300 - Train Loss: 0.9501, Val Loss: 0.8325\n",
      "Epoch 2/300 - Train Loss: 0.7024, Val Loss: 0.5951\n",
      "Epoch 3/300 - Train Loss: 0.5154, Val Loss: 0.4521\n",
      "Epoch 4/300 - Train Loss: 0.4120, Val Loss: 0.3561\n",
      "Epoch 5/300 - Train Loss: 0.3419, Val Loss: 0.3083\n",
      "Epoch 6/300 - Train Loss: 0.2948, Val Loss: 0.2663\n",
      "Epoch 7/300 - Train Loss: 0.2602, Val Loss: 0.2310\n",
      "Epoch 8/300 - Train Loss: 0.2312, Val Loss: 0.2031\n",
      "Epoch 9/300 - Train Loss: 0.2062, Val Loss: 0.1818\n",
      "Epoch 10/300 - Train Loss: 0.1910, Val Loss: 0.1649\n",
      "Epoch 11/300 - Train Loss: 0.1721, Val Loss: 0.1486\n",
      "Epoch 12/300 - Train Loss: 0.1617, Val Loss: 0.1397\n",
      "Epoch 13/300 - Train Loss: 0.1516, Val Loss: 0.1304\n",
      "Epoch 14/300 - Train Loss: 0.1425, Val Loss: 0.1287\n",
      "Epoch 15/300 - Train Loss: 0.1377, Val Loss: 0.1240\n",
      "Epoch 16/300 - Train Loss: 0.1338, Val Loss: 0.1146\n",
      "Epoch 17/300 - Train Loss: 0.1288, Val Loss: 0.1139\n",
      "Epoch 18/300 - Train Loss: 0.1221, Val Loss: 0.1095\n",
      "Epoch 19/300 - Train Loss: 0.1215, Val Loss: 0.1053\n",
      "Epoch 20/300 - Train Loss: 0.1181, Val Loss: 0.1026\n",
      "Epoch 21/300 - Train Loss: 0.1162, Val Loss: 0.1021\n",
      "Epoch 22/300 - Train Loss: 0.1132, Val Loss: 0.1007\n",
      "Epoch 23/300 - Train Loss: 0.1116, Val Loss: 0.0973\n",
      "Epoch 24/300 - Train Loss: 0.1109, Val Loss: 0.1008\n",
      "Epoch 25/300 - Train Loss: 0.1085, Val Loss: 0.0962\n",
      "Epoch 26/300 - Train Loss: 0.1083, Val Loss: 0.0944\n",
      "Epoch 27/300 - Train Loss: 0.1068, Val Loss: 0.0919\n",
      "Epoch 28/300 - Train Loss: 0.1045, Val Loss: 0.0926\n",
      "Epoch 29/300 - Train Loss: 0.1049, Val Loss: 0.0907\n",
      "Epoch 30/300 - Train Loss: 0.1040, Val Loss: 0.0896\n",
      "Epoch 31/300 - Train Loss: 0.1029, Val Loss: 0.0890\n",
      "Epoch 32/300 - Train Loss: 0.1024, Val Loss: 0.0878\n",
      "Epoch 33/300 - Train Loss: 0.0999, Val Loss: 0.0867\n",
      "Epoch 34/300 - Train Loss: 0.1019, Val Loss: 0.0870\n",
      "Epoch 35/300 - Train Loss: 0.0998, Val Loss: 0.0865\n",
      "Epoch 36/300 - Train Loss: 0.0993, Val Loss: 0.0873\n",
      "Epoch 37/300 - Train Loss: 0.0984, Val Loss: 0.0847\n",
      "Epoch 38/300 - Train Loss: 0.0982, Val Loss: 0.0854\n",
      "Epoch 39/300 - Train Loss: 0.0996, Val Loss: 0.0836\n",
      "Epoch 40/300 - Train Loss: 0.0988, Val Loss: 0.0834\n",
      "Epoch 41/300 - Train Loss: 0.0975, Val Loss: 0.0844\n",
      "Epoch 42/300 - Train Loss: 0.0970, Val Loss: 0.0836\n",
      "Epoch 43/300 - Train Loss: 0.0968, Val Loss: 0.0825\n",
      "Epoch 44/300 - Train Loss: 0.0965, Val Loss: 0.0822\n",
      "Epoch 45/300 - Train Loss: 0.0963, Val Loss: 0.0827\n",
      "Epoch 46/300 - Train Loss: 0.0954, Val Loss: 0.0836\n",
      "Epoch 47/300 - Train Loss: 0.0944, Val Loss: 0.0822\n",
      "Epoch 48/300 - Train Loss: 0.0942, Val Loss: 0.0818\n",
      "Epoch 49/300 - Train Loss: 0.0958, Val Loss: 0.0807\n",
      "Epoch 50/300 - Train Loss: 0.0946, Val Loss: 0.0820\n",
      "Epoch 51/300 - Train Loss: 0.0928, Val Loss: 0.0826\n",
      "Epoch 52/300 - Train Loss: 0.0939, Val Loss: 0.0828\n",
      "Epoch 53/300 - Train Loss: 0.0942, Val Loss: 0.0807\n",
      "Epoch 54/300 - Train Loss: 0.0928, Val Loss: 0.0822\n",
      "Epoch 55/300 - Train Loss: 0.0925, Val Loss: 0.0827\n",
      "Epoch 56/300 - Train Loss: 0.0937, Val Loss: 0.0806\n",
      "Epoch 57/300 - Train Loss: 0.0929, Val Loss: 0.0798\n",
      "Epoch 58/300 - Train Loss: 0.0929, Val Loss: 0.0791\n",
      "Epoch 59/300 - Train Loss: 0.0924, Val Loss: 0.0803\n",
      "Epoch 60/300 - Train Loss: 0.0927, Val Loss: 0.0791\n",
      "Epoch 61/300 - Train Loss: 0.0913, Val Loss: 0.0786\n",
      "Epoch 62/300 - Train Loss: 0.0915, Val Loss: 0.0790\n",
      "Epoch 63/300 - Train Loss: 0.0919, Val Loss: 0.0794\n",
      "Epoch 64/300 - Train Loss: 0.0921, Val Loss: 0.0780\n",
      "Epoch 65/300 - Train Loss: 0.0908, Val Loss: 0.0776\n",
      "Epoch 66/300 - Train Loss: 0.0904, Val Loss: 0.0785\n",
      "Epoch 67/300 - Train Loss: 0.0892, Val Loss: 0.0792\n",
      "Epoch 68/300 - Train Loss: 0.0920, Val Loss: 0.0808\n",
      "Epoch 69/300 - Train Loss: 0.0901, Val Loss: 0.0804\n",
      "Epoch 70/300 - Train Loss: 0.0896, Val Loss: 0.0794\n",
      "Epoch 71/300 - Train Loss: 0.0904, Val Loss: 0.0777\n",
      "Epoch 72/300 - Train Loss: 0.0907, Val Loss: 0.0783\n",
      "Epoch 73/300 - Train Loss: 0.0902, Val Loss: 0.0782\n",
      "Epoch 74/300 - Train Loss: 0.0905, Val Loss: 0.0779\n",
      "Epoch 75/300 - Train Loss: 0.0899, Val Loss: 0.0780\n",
      "Epoch 76/300 - Train Loss: 0.0907, Val Loss: 0.0784\n",
      "Epoch 77/300 - Train Loss: 0.0909, Val Loss: 0.0781\n",
      "Epoch 78/300 - Train Loss: 0.0880, Val Loss: 0.0789\n",
      "Epoch 79/300 - Train Loss: 0.0897, Val Loss: 0.0766\n",
      "Epoch 80/300 - Train Loss: 0.0899, Val Loss: 0.0770\n",
      "Epoch 81/300 - Train Loss: 0.0894, Val Loss: 0.0767\n",
      "Epoch 82/300 - Train Loss: 0.0881, Val Loss: 0.0755\n",
      "Epoch 83/300 - Train Loss: 0.0883, Val Loss: 0.0774\n",
      "Epoch 84/300 - Train Loss: 0.0882, Val Loss: 0.0769\n",
      "Epoch 85/300 - Train Loss: 0.0904, Val Loss: 0.0793\n",
      "Epoch 86/300 - Train Loss: 0.0893, Val Loss: 0.0790\n",
      "Epoch 87/300 - Train Loss: 0.0889, Val Loss: 0.0763\n",
      "Epoch 88/300 - Train Loss: 0.0883, Val Loss: 0.0766\n",
      "Epoch 89/300 - Train Loss: 0.0871, Val Loss: 0.0765\n",
      "Epoch 90/300 - Train Loss: 0.0872, Val Loss: 0.0770\n",
      "Epoch 91/300 - Train Loss: 0.0871, Val Loss: 0.0775\n",
      "Epoch 92/300 - Train Loss: 0.0876, Val Loss: 0.0773\n",
      "Epoch 93/300 - Train Loss: 0.0880, Val Loss: 0.0767\n",
      "Epoch 94/300 - Train Loss: 0.0884, Val Loss: 0.0768\n",
      "Epoch 95/300 - Train Loss: 0.0888, Val Loss: 0.0774\n",
      "Epoch 96/300 - Train Loss: 0.0877, Val Loss: 0.0769\n",
      "Epoch 97/300 - Train Loss: 0.0861, Val Loss: 0.0767\n",
      "Epoch 98/300 - Train Loss: 0.0870, Val Loss: 0.0769\n",
      "Epoch 99/300 - Train Loss: 0.0876, Val Loss: 0.0762\n",
      "Epoch 100/300 - Train Loss: 0.0874, Val Loss: 0.0776\n",
      "Epoch 101/300 - Train Loss: 0.0867, Val Loss: 0.0767\n",
      "Epoch 102/300 - Train Loss: 0.0869, Val Loss: 0.0768\n",
      "Epoch 103/300 - Train Loss: 0.0880, Val Loss: 0.0781\n",
      "Epoch 104/300 - Train Loss: 0.0864, Val Loss: 0.0755\n",
      "Epoch 105/300 - Train Loss: 0.0880, Val Loss: 0.0769\n",
      "Epoch 106/300 - Train Loss: 0.0877, Val Loss: 0.0757\n",
      "Epoch 107/300 - Train Loss: 0.0855, Val Loss: 0.0754\n",
      "Epoch 108/300 - Train Loss: 0.0854, Val Loss: 0.0762\n",
      "Epoch 109/300 - Train Loss: 0.0855, Val Loss: 0.0753\n",
      "Epoch 110/300 - Train Loss: 0.0863, Val Loss: 0.0756\n",
      "Epoch 111/300 - Train Loss: 0.0872, Val Loss: 0.0753\n",
      "Epoch 112/300 - Train Loss: 0.0858, Val Loss: 0.0767\n",
      "Epoch 113/300 - Train Loss: 0.0865, Val Loss: 0.0757\n",
      "Epoch 114/300 - Train Loss: 0.0860, Val Loss: 0.0755\n",
      "Epoch 115/300 - Train Loss: 0.0867, Val Loss: 0.0750\n",
      "Epoch 116/300 - Train Loss: 0.0861, Val Loss: 0.0749\n",
      "Epoch 117/300 - Train Loss: 0.0858, Val Loss: 0.0756\n",
      "Epoch 118/300 - Train Loss: 0.0864, Val Loss: 0.0760\n",
      "Epoch 119/300 - Train Loss: 0.0838, Val Loss: 0.0755\n",
      "Epoch 120/300 - Train Loss: 0.0857, Val Loss: 0.0751\n",
      "Epoch 121/300 - Train Loss: 0.0850, Val Loss: 0.0759\n",
      "Epoch 122/300 - Train Loss: 0.0851, Val Loss: 0.0756\n",
      "Epoch 123/300 - Train Loss: 0.0853, Val Loss: 0.0752\n",
      "Epoch 124/300 - Train Loss: 0.0868, Val Loss: 0.0750\n",
      "Epoch 125/300 - Train Loss: 0.0857, Val Loss: 0.0757\n",
      "Epoch 126/300 - Train Loss: 0.0843, Val Loss: 0.0739\n",
      "Epoch 127/300 - Train Loss: 0.0835, Val Loss: 0.0753\n",
      "Epoch 128/300 - Train Loss: 0.0848, Val Loss: 0.0739\n",
      "Epoch 129/300 - Train Loss: 0.0850, Val Loss: 0.0741\n",
      "Epoch 130/300 - Train Loss: 0.0836, Val Loss: 0.0745\n",
      "Epoch 131/300 - Train Loss: 0.0846, Val Loss: 0.0726\n",
      "Epoch 132/300 - Train Loss: 0.0844, Val Loss: 0.0772\n",
      "Epoch 133/300 - Train Loss: 0.0836, Val Loss: 0.0745\n",
      "Epoch 134/300 - Train Loss: 0.0847, Val Loss: 0.0746\n",
      "Epoch 135/300 - Train Loss: 0.0846, Val Loss: 0.0766\n",
      "Epoch 136/300 - Train Loss: 0.0831, Val Loss: 0.0758\n",
      "Epoch 137/300 - Train Loss: 0.0846, Val Loss: 0.0750\n",
      "Epoch 138/300 - Train Loss: 0.0852, Val Loss: 0.0741\n",
      "Epoch 139/300 - Train Loss: 0.0845, Val Loss: 0.0752\n",
      "Epoch 140/300 - Train Loss: 0.0826, Val Loss: 0.0746\n",
      "Epoch 141/300 - Train Loss: 0.0827, Val Loss: 0.0747\n",
      "Epoch 142/300 - Train Loss: 0.0848, Val Loss: 0.0739\n",
      "Epoch 143/300 - Train Loss: 0.0846, Val Loss: 0.0742\n",
      "Epoch 144/300 - Train Loss: 0.0842, Val Loss: 0.0743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0839, Val Loss: 0.0750\n",
      "Epoch 146/300 - Train Loss: 0.0836, Val Loss: 0.0736\n",
      "Epoch 147/300 - Train Loss: 0.0846, Val Loss: 0.0757\n",
      "Epoch 148/300 - Train Loss: 0.0820, Val Loss: 0.0751\n",
      "Epoch 149/300 - Train Loss: 0.0857, Val Loss: 0.0749\n",
      "Epoch 150/300 - Train Loss: 0.0820, Val Loss: 0.0731\n",
      "Epoch 151/300 - Train Loss: 0.0836, Val Loss: 0.0732\n",
      "Epoch 152/300 - Train Loss: 0.0833, Val Loss: 0.0731\n",
      "Epoch 153/300 - Train Loss: 0.0824, Val Loss: 0.0744\n",
      "Epoch 154/300 - Train Loss: 0.0832, Val Loss: 0.0731\n",
      "Epoch 155/300 - Train Loss: 0.0845, Val Loss: 0.0749\n",
      "Epoch 156/300 - Train Loss: 0.0839, Val Loss: 0.0736\n",
      "Epoch 157/300 - Train Loss: 0.0826, Val Loss: 0.0742\n",
      "Epoch 158/300 - Train Loss: 0.0822, Val Loss: 0.0743\n",
      "Epoch 159/300 - Train Loss: 0.0831, Val Loss: 0.0743\n",
      "Epoch 160/300 - Train Loss: 0.0833, Val Loss: 0.0723\n",
      "Epoch 161/300 - Train Loss: 0.0831, Val Loss: 0.0738\n",
      "Epoch 162/300 - Train Loss: 0.0819, Val Loss: 0.0729\n",
      "Epoch 163/300 - Train Loss: 0.0827, Val Loss: 0.0735\n",
      "Epoch 164/300 - Train Loss: 0.0810, Val Loss: 0.0726\n",
      "Epoch 165/300 - Train Loss: 0.0820, Val Loss: 0.0746\n",
      "Epoch 166/300 - Train Loss: 0.0815, Val Loss: 0.0733\n",
      "Epoch 167/300 - Train Loss: 0.0820, Val Loss: 0.0721\n",
      "Epoch 168/300 - Train Loss: 0.0819, Val Loss: 0.0751\n",
      "Epoch 169/300 - Train Loss: 0.0807, Val Loss: 0.0725\n",
      "Epoch 170/300 - Train Loss: 0.0832, Val Loss: 0.0739\n",
      "Epoch 171/300 - Train Loss: 0.0796, Val Loss: 0.0724\n",
      "Epoch 172/300 - Train Loss: 0.0835, Val Loss: 0.0734\n",
      "Epoch 173/300 - Train Loss: 0.0815, Val Loss: 0.0742\n",
      "Epoch 174/300 - Train Loss: 0.0816, Val Loss: 0.0742\n",
      "Epoch 175/300 - Train Loss: 0.0796, Val Loss: 0.0740\n",
      "Epoch 176/300 - Train Loss: 0.0823, Val Loss: 0.0735\n",
      "Epoch 177/300 - Train Loss: 0.0814, Val Loss: 0.0733\n",
      "Epoch 178/300 - Train Loss: 0.0815, Val Loss: 0.0730\n",
      "Epoch 179/300 - Train Loss: 0.0812, Val Loss: 0.0730\n",
      "Epoch 180/300 - Train Loss: 0.0794, Val Loss: 0.0722\n",
      "Epoch 181/300 - Train Loss: 0.0793, Val Loss: 0.0733\n",
      "Epoch 182/300 - Train Loss: 0.0792, Val Loss: 0.0723\n",
      "Epoch 183/300 - Train Loss: 0.0797, Val Loss: 0.0722\n",
      "Epoch 184/300 - Train Loss: 0.0815, Val Loss: 0.0738\n",
      "Epoch 185/300 - Train Loss: 0.0824, Val Loss: 0.0726\n",
      "Epoch 186/300 - Train Loss: 0.0805, Val Loss: 0.0737\n",
      "Epoch 187/300 - Train Loss: 0.0804, Val Loss: 0.0724\n",
      "Epoch 188/300 - Train Loss: 0.0806, Val Loss: 0.0730\n",
      "Epoch 189/300 - Train Loss: 0.0810, Val Loss: 0.0725\n",
      "Epoch 190/300 - Train Loss: 0.0802, Val Loss: 0.0717\n",
      "Epoch 191/300 - Train Loss: 0.0813, Val Loss: 0.0732\n",
      "Epoch 192/300 - Train Loss: 0.0801, Val Loss: 0.0743\n",
      "Epoch 193/300 - Train Loss: 0.0809, Val Loss: 0.0742\n",
      "Epoch 194/300 - Train Loss: 0.0815, Val Loss: 0.0739\n",
      "Epoch 195/300 - Train Loss: 0.0803, Val Loss: 0.0717\n",
      "Epoch 196/300 - Train Loss: 0.0817, Val Loss: 0.0721\n",
      "Epoch 197/300 - Train Loss: 0.0799, Val Loss: 0.0720\n",
      "Epoch 198/300 - Train Loss: 0.0809, Val Loss: 0.0706\n",
      "Epoch 199/300 - Train Loss: 0.0816, Val Loss: 0.0714\n",
      "Epoch 200/300 - Train Loss: 0.0797, Val Loss: 0.0714\n",
      "Epoch 201/300 - Train Loss: 0.0800, Val Loss: 0.0727\n",
      "Epoch 202/300 - Train Loss: 0.0795, Val Loss: 0.0732\n",
      "Epoch 203/300 - Train Loss: 0.0796, Val Loss: 0.0726\n",
      "Epoch 204/300 - Train Loss: 0.0804, Val Loss: 0.0722\n",
      "Epoch 205/300 - Train Loss: 0.0809, Val Loss: 0.0715\n",
      "Epoch 206/300 - Train Loss: 0.0785, Val Loss: 0.0715\n",
      "Epoch 207/300 - Train Loss: 0.0787, Val Loss: 0.0723\n",
      "Epoch 208/300 - Train Loss: 0.0797, Val Loss: 0.0722\n",
      "Epoch 209/300 - Train Loss: 0.0804, Val Loss: 0.0717\n",
      "Epoch 210/300 - Train Loss: 0.0800, Val Loss: 0.0739\n",
      "Epoch 211/300 - Train Loss: 0.0803, Val Loss: 0.0709\n",
      "Epoch 212/300 - Train Loss: 0.0795, Val Loss: 0.0716\n",
      "Epoch 213/300 - Train Loss: 0.0782, Val Loss: 0.0722\n",
      "Epoch 214/300 - Train Loss: 0.0797, Val Loss: 0.0715\n",
      "Epoch 215/300 - Train Loss: 0.0797, Val Loss: 0.0716\n",
      "Epoch 216/300 - Train Loss: 0.0794, Val Loss: 0.0717\n",
      "Epoch 217/300 - Train Loss: 0.0791, Val Loss: 0.0717\n",
      "Epoch 218/300 - Train Loss: 0.0787, Val Loss: 0.0725\n",
      "Epoch 219/300 - Train Loss: 0.0794, Val Loss: 0.0729\n",
      "Epoch 220/300 - Train Loss: 0.0787, Val Loss: 0.0718\n",
      "Epoch 221/300 - Train Loss: 0.0801, Val Loss: 0.0719\n",
      "Epoch 222/300 - Train Loss: 0.0800, Val Loss: 0.0702\n",
      "Epoch 223/300 - Train Loss: 0.0797, Val Loss: 0.0707\n",
      "Epoch 224/300 - Train Loss: 0.0798, Val Loss: 0.0707\n",
      "Epoch 225/300 - Train Loss: 0.0783, Val Loss: 0.0718\n",
      "Epoch 226/300 - Train Loss: 0.0790, Val Loss: 0.0706\n",
      "Epoch 227/300 - Train Loss: 0.0801, Val Loss: 0.0704\n",
      "Epoch 228/300 - Train Loss: 0.0792, Val Loss: 0.0709\n",
      "Epoch 229/300 - Train Loss: 0.0793, Val Loss: 0.0700\n",
      "Epoch 230/300 - Train Loss: 0.0799, Val Loss: 0.0716\n",
      "Epoch 231/300 - Train Loss: 0.0788, Val Loss: 0.0703\n",
      "Epoch 232/300 - Train Loss: 0.0801, Val Loss: 0.0714\n",
      "Epoch 233/300 - Train Loss: 0.0775, Val Loss: 0.0716\n",
      "Epoch 234/300 - Train Loss: 0.0776, Val Loss: 0.0699\n",
      "Epoch 235/300 - Train Loss: 0.0782, Val Loss: 0.0717\n",
      "Epoch 236/300 - Train Loss: 0.0796, Val Loss: 0.0705\n",
      "Epoch 237/300 - Train Loss: 0.0779, Val Loss: 0.0712\n",
      "Epoch 238/300 - Train Loss: 0.0785, Val Loss: 0.0710\n",
      "Epoch 239/300 - Train Loss: 0.0785, Val Loss: 0.0711\n",
      "Epoch 240/300 - Train Loss: 0.0795, Val Loss: 0.0707\n",
      "Epoch 241/300 - Train Loss: 0.0796, Val Loss: 0.0726\n",
      "Epoch 242/300 - Train Loss: 0.0800, Val Loss: 0.0699\n",
      "Epoch 243/300 - Train Loss: 0.0780, Val Loss: 0.0718\n",
      "Epoch 244/300 - Train Loss: 0.0779, Val Loss: 0.0707\n",
      "Epoch 245/300 - Train Loss: 0.0771, Val Loss: 0.0707\n",
      "Epoch 246/300 - Train Loss: 0.0768, Val Loss: 0.0702\n",
      "Epoch 247/300 - Train Loss: 0.0794, Val Loss: 0.0703\n",
      "Epoch 248/300 - Train Loss: 0.0782, Val Loss: 0.0702\n",
      "Epoch 249/300 - Train Loss: 0.0785, Val Loss: 0.0694\n",
      "Epoch 250/300 - Train Loss: 0.0787, Val Loss: 0.0710\n",
      "Epoch 251/300 - Train Loss: 0.0781, Val Loss: 0.0707\n",
      "Epoch 252/300 - Train Loss: 0.0768, Val Loss: 0.0707\n",
      "Epoch 253/300 - Train Loss: 0.0777, Val Loss: 0.0712\n",
      "Epoch 254/300 - Train Loss: 0.0777, Val Loss: 0.0698\n",
      "Epoch 255/300 - Train Loss: 0.0793, Val Loss: 0.0698\n",
      "Epoch 256/300 - Train Loss: 0.0779, Val Loss: 0.0714\n",
      "Epoch 257/300 - Train Loss: 0.0774, Val Loss: 0.0699\n",
      "Epoch 258/300 - Train Loss: 0.0772, Val Loss: 0.0693\n",
      "Epoch 259/300 - Train Loss: 0.0779, Val Loss: 0.0709\n",
      "Epoch 260/300 - Train Loss: 0.0781, Val Loss: 0.0701\n",
      "Epoch 261/300 - Train Loss: 0.0766, Val Loss: 0.0713\n",
      "Epoch 262/300 - Train Loss: 0.0763, Val Loss: 0.0716\n",
      "Epoch 263/300 - Train Loss: 0.0778, Val Loss: 0.0694\n",
      "Epoch 264/300 - Train Loss: 0.0776, Val Loss: 0.0703\n",
      "Epoch 265/300 - Train Loss: 0.0758, Val Loss: 0.0703\n",
      "Epoch 266/300 - Train Loss: 0.0767, Val Loss: 0.0698\n",
      "Epoch 267/300 - Train Loss: 0.0762, Val Loss: 0.0716\n",
      "Epoch 268/300 - Train Loss: 0.0782, Val Loss: 0.0695\n",
      "Epoch 269/300 - Train Loss: 0.0774, Val Loss: 0.0720\n",
      "Epoch 270/300 - Train Loss: 0.0773, Val Loss: 0.0693\n",
      "Epoch 271/300 - Train Loss: 0.0773, Val Loss: 0.0706\n",
      "Epoch 272/300 - Train Loss: 0.0767, Val Loss: 0.0708\n",
      "Epoch 273/300 - Train Loss: 0.0772, Val Loss: 0.0706\n",
      "Epoch 274/300 - Train Loss: 0.0791, Val Loss: 0.0724\n",
      "Epoch 275/300 - Train Loss: 0.0762, Val Loss: 0.0703\n",
      "Epoch 276/300 - Train Loss: 0.0791, Val Loss: 0.0703\n",
      "Epoch 277/300 - Train Loss: 0.0793, Val Loss: 0.0700\n",
      "Epoch 278/300 - Train Loss: 0.0770, Val Loss: 0.0696\n",
      "Epoch 279/300 - Train Loss: 0.0781, Val Loss: 0.0707\n",
      "Epoch 280/300 - Train Loss: 0.0763, Val Loss: 0.0707\n",
      "Epoch 281/300 - Train Loss: 0.0791, Val Loss: 0.0709\n",
      "Epoch 282/300 - Train Loss: 0.0771, Val Loss: 0.0702\n",
      "Epoch 283/300 - Train Loss: 0.0780, Val Loss: 0.0710\n",
      "Epoch 284/300 - Train Loss: 0.0768, Val Loss: 0.0688\n",
      "Epoch 285/300 - Train Loss: 0.0774, Val Loss: 0.0694\n",
      "Epoch 286/300 - Train Loss: 0.0764, Val Loss: 0.0697\n",
      "Epoch 287/300 - Train Loss: 0.0782, Val Loss: 0.0710\n",
      "Epoch 288/300 - Train Loss: 0.0778, Val Loss: 0.0709\n",
      "Epoch 289/300 - Train Loss: 0.0763, Val Loss: 0.0709\n",
      "Epoch 290/300 - Train Loss: 0.0760, Val Loss: 0.0693\n",
      "Epoch 291/300 - Train Loss: 0.0782, Val Loss: 0.0692\n",
      "Epoch 292/300 - Train Loss: 0.0750, Val Loss: 0.0710\n",
      "Epoch 293/300 - Train Loss: 0.0780, Val Loss: 0.0690\n",
      "Epoch 294/300 - Train Loss: 0.0781, Val Loss: 0.0698\n",
      "Epoch 295/300 - Train Loss: 0.0794, Val Loss: 0.0684\n",
      "Epoch 296/300 - Train Loss: 0.0754, Val Loss: 0.0709\n",
      "Epoch 297/300 - Train Loss: 0.0764, Val Loss: 0.0694\n",
      "Epoch 298/300 - Train Loss: 0.0751, Val Loss: 0.0691\n",
      "Epoch 299/300 - Train Loss: 0.0760, Val Loss: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 01:05:41,490] Trial 34 finished with value: 0.9688622499147151 and parameters: {'F1': 16, 'F2': 8, 'D': 4, 'dropout': 0.3284251592850694, 'learning_rate': 1.4511081462236066e-05, 'batch_size': 64, 'weight_decay': 4.849964797817115e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/300 - Train Loss: 0.0777, Val Loss: 0.0719\n",
      "Macro F1 Score: 0.9689, Macro Precision: 0.9569, Macro Recall: 0.9823\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 36\n",
      "Training with F1=8, F2=8, D=4, dropout=0.35019853399787093, LR=1.577994558504693e-05, BS=64, WD=5.838562427500637e-05\n",
      "Epoch 1/300 - Train Loss: 0.9649, Val Loss: 0.8884\n",
      "Epoch 2/300 - Train Loss: 0.7440, Val Loss: 0.6765\n",
      "Epoch 3/300 - Train Loss: 0.5516, Val Loss: 0.4960\n",
      "Epoch 4/300 - Train Loss: 0.4262, Val Loss: 0.4020\n",
      "Epoch 5/300 - Train Loss: 0.3530, Val Loss: 0.3262\n",
      "Epoch 6/300 - Train Loss: 0.3091, Val Loss: 0.2961\n",
      "Epoch 7/300 - Train Loss: 0.2819, Val Loss: 0.2721\n",
      "Epoch 8/300 - Train Loss: 0.2617, Val Loss: 0.2438\n",
      "Epoch 9/300 - Train Loss: 0.2453, Val Loss: 0.2330\n",
      "Epoch 10/300 - Train Loss: 0.2366, Val Loss: 0.2180\n",
      "Epoch 11/300 - Train Loss: 0.2246, Val Loss: 0.2099\n",
      "Epoch 12/300 - Train Loss: 0.2190, Val Loss: 0.1996\n",
      "Epoch 13/300 - Train Loss: 0.2124, Val Loss: 0.1878\n",
      "Epoch 14/300 - Train Loss: 0.2036, Val Loss: 0.1812\n",
      "Epoch 15/300 - Train Loss: 0.1988, Val Loss: 0.1745\n",
      "Epoch 16/300 - Train Loss: 0.1930, Val Loss: 0.1680\n",
      "Epoch 17/300 - Train Loss: 0.1878, Val Loss: 0.1620\n",
      "Epoch 18/300 - Train Loss: 0.1824, Val Loss: 0.1573\n",
      "Epoch 19/300 - Train Loss: 0.1771, Val Loss: 0.1512\n",
      "Epoch 20/300 - Train Loss: 0.1727, Val Loss: 0.1478\n",
      "Epoch 21/300 - Train Loss: 0.1680, Val Loss: 0.1388\n",
      "Epoch 22/300 - Train Loss: 0.1600, Val Loss: 0.1311\n",
      "Epoch 23/300 - Train Loss: 0.1543, Val Loss: 0.1253\n",
      "Epoch 24/300 - Train Loss: 0.1482, Val Loss: 0.1221\n",
      "Epoch 25/300 - Train Loss: 0.1407, Val Loss: 0.1179\n",
      "Epoch 26/300 - Train Loss: 0.1386, Val Loss: 0.1104\n",
      "Epoch 27/300 - Train Loss: 0.1353, Val Loss: 0.1086\n",
      "Epoch 28/300 - Train Loss: 0.1309, Val Loss: 0.1059\n",
      "Epoch 29/300 - Train Loss: 0.1277, Val Loss: 0.1044\n",
      "Epoch 30/300 - Train Loss: 0.1232, Val Loss: 0.1031\n",
      "Epoch 31/300 - Train Loss: 0.1194, Val Loss: 0.0981\n",
      "Epoch 32/300 - Train Loss: 0.1230, Val Loss: 0.0982\n",
      "Epoch 33/300 - Train Loss: 0.1184, Val Loss: 0.0977\n",
      "Epoch 34/300 - Train Loss: 0.1167, Val Loss: 0.0952\n",
      "Epoch 35/300 - Train Loss: 0.1141, Val Loss: 0.0958\n",
      "Epoch 36/300 - Train Loss: 0.1130, Val Loss: 0.0920\n",
      "Epoch 37/300 - Train Loss: 0.1111, Val Loss: 0.0926\n",
      "Epoch 38/300 - Train Loss: 0.1113, Val Loss: 0.0914\n",
      "Epoch 39/300 - Train Loss: 0.1085, Val Loss: 0.0916\n",
      "Epoch 40/300 - Train Loss: 0.1088, Val Loss: 0.0899\n",
      "Epoch 41/300 - Train Loss: 0.1081, Val Loss: 0.0880\n",
      "Epoch 42/300 - Train Loss: 0.1083, Val Loss: 0.0867\n",
      "Epoch 43/300 - Train Loss: 0.1067, Val Loss: 0.0919\n",
      "Epoch 44/300 - Train Loss: 0.1062, Val Loss: 0.0882\n",
      "Epoch 45/300 - Train Loss: 0.1056, Val Loss: 0.0867\n",
      "Epoch 46/300 - Train Loss: 0.1048, Val Loss: 0.0859\n",
      "Epoch 47/300 - Train Loss: 0.1049, Val Loss: 0.0867\n",
      "Epoch 48/300 - Train Loss: 0.1040, Val Loss: 0.0845\n",
      "Epoch 49/300 - Train Loss: 0.1035, Val Loss: 0.0857\n",
      "Epoch 50/300 - Train Loss: 0.1017, Val Loss: 0.0888\n",
      "Epoch 51/300 - Train Loss: 0.1019, Val Loss: 0.0856\n",
      "Epoch 52/300 - Train Loss: 0.1022, Val Loss: 0.0853\n",
      "Epoch 53/300 - Train Loss: 0.1023, Val Loss: 0.0841\n",
      "Epoch 54/300 - Train Loss: 0.1000, Val Loss: 0.0851\n",
      "Epoch 55/300 - Train Loss: 0.1029, Val Loss: 0.0844\n",
      "Epoch 56/300 - Train Loss: 0.1025, Val Loss: 0.0844\n",
      "Epoch 57/300 - Train Loss: 0.0988, Val Loss: 0.0836\n",
      "Epoch 58/300 - Train Loss: 0.1002, Val Loss: 0.0845\n",
      "Epoch 59/300 - Train Loss: 0.0991, Val Loss: 0.0822\n",
      "Epoch 60/300 - Train Loss: 0.0993, Val Loss: 0.0838\n",
      "Epoch 61/300 - Train Loss: 0.0968, Val Loss: 0.0829\n",
      "Epoch 62/300 - Train Loss: 0.0970, Val Loss: 0.0814\n",
      "Epoch 63/300 - Train Loss: 0.0980, Val Loss: 0.0821\n",
      "Epoch 64/300 - Train Loss: 0.0984, Val Loss: 0.0821\n",
      "Epoch 65/300 - Train Loss: 0.0970, Val Loss: 0.0837\n",
      "Epoch 66/300 - Train Loss: 0.0972, Val Loss: 0.0825\n",
      "Epoch 67/300 - Train Loss: 0.0956, Val Loss: 0.0816\n",
      "Epoch 68/300 - Train Loss: 0.0955, Val Loss: 0.0819\n",
      "Epoch 69/300 - Train Loss: 0.0973, Val Loss: 0.0804\n",
      "Epoch 70/300 - Train Loss: 0.0958, Val Loss: 0.0802\n",
      "Epoch 71/300 - Train Loss: 0.0957, Val Loss: 0.0813\n",
      "Epoch 72/300 - Train Loss: 0.0958, Val Loss: 0.0811\n",
      "Epoch 73/300 - Train Loss: 0.0956, Val Loss: 0.0802\n",
      "Epoch 74/300 - Train Loss: 0.0950, Val Loss: 0.0806\n",
      "Epoch 75/300 - Train Loss: 0.0934, Val Loss: 0.0792\n",
      "Epoch 76/300 - Train Loss: 0.0940, Val Loss: 0.0801\n",
      "Epoch 77/300 - Train Loss: 0.0945, Val Loss: 0.0787\n",
      "Epoch 78/300 - Train Loss: 0.0945, Val Loss: 0.0795\n",
      "Epoch 79/300 - Train Loss: 0.0937, Val Loss: 0.0801\n",
      "Epoch 80/300 - Train Loss: 0.0949, Val Loss: 0.0789\n",
      "Epoch 81/300 - Train Loss: 0.0938, Val Loss: 0.0796\n",
      "Epoch 82/300 - Train Loss: 0.0936, Val Loss: 0.0792\n",
      "Epoch 83/300 - Train Loss: 0.0935, Val Loss: 0.0797\n",
      "Epoch 84/300 - Train Loss: 0.0928, Val Loss: 0.0797\n",
      "Epoch 85/300 - Train Loss: 0.0944, Val Loss: 0.0794\n",
      "Epoch 86/300 - Train Loss: 0.0932, Val Loss: 0.0800\n",
      "Epoch 87/300 - Train Loss: 0.0939, Val Loss: 0.0780\n",
      "Epoch 88/300 - Train Loss: 0.0943, Val Loss: 0.0786\n",
      "Epoch 89/300 - Train Loss: 0.0937, Val Loss: 0.0782\n",
      "Epoch 90/300 - Train Loss: 0.0910, Val Loss: 0.0781\n",
      "Epoch 91/300 - Train Loss: 0.0932, Val Loss: 0.0773\n",
      "Epoch 92/300 - Train Loss: 0.0916, Val Loss: 0.0790\n",
      "Epoch 93/300 - Train Loss: 0.0926, Val Loss: 0.0785\n",
      "Epoch 94/300 - Train Loss: 0.0907, Val Loss: 0.0787\n",
      "Epoch 95/300 - Train Loss: 0.0920, Val Loss: 0.0785\n",
      "Epoch 96/300 - Train Loss: 0.0918, Val Loss: 0.0771\n",
      "Epoch 97/300 - Train Loss: 0.0918, Val Loss: 0.0788\n",
      "Epoch 98/300 - Train Loss: 0.0921, Val Loss: 0.0777\n",
      "Epoch 99/300 - Train Loss: 0.0929, Val Loss: 0.0775\n",
      "Epoch 100/300 - Train Loss: 0.0919, Val Loss: 0.0768\n",
      "Epoch 101/300 - Train Loss: 0.0921, Val Loss: 0.0768\n",
      "Epoch 102/300 - Train Loss: 0.0912, Val Loss: 0.0765\n",
      "Epoch 103/300 - Train Loss: 0.0914, Val Loss: 0.0769\n",
      "Epoch 104/300 - Train Loss: 0.0900, Val Loss: 0.0772\n",
      "Epoch 105/300 - Train Loss: 0.0898, Val Loss: 0.0765\n",
      "Epoch 106/300 - Train Loss: 0.0906, Val Loss: 0.0790\n",
      "Epoch 107/300 - Train Loss: 0.0910, Val Loss: 0.0769\n",
      "Epoch 108/300 - Train Loss: 0.0909, Val Loss: 0.0748\n",
      "Epoch 109/300 - Train Loss: 0.0911, Val Loss: 0.0759\n",
      "Epoch 110/300 - Train Loss: 0.0902, Val Loss: 0.0759\n",
      "Epoch 111/300 - Train Loss: 0.0907, Val Loss: 0.0768\n",
      "Epoch 112/300 - Train Loss: 0.0913, Val Loss: 0.0752\n",
      "Epoch 113/300 - Train Loss: 0.0896, Val Loss: 0.0770\n",
      "Epoch 114/300 - Train Loss: 0.0891, Val Loss: 0.0758\n",
      "Epoch 115/300 - Train Loss: 0.0872, Val Loss: 0.0751\n",
      "Epoch 116/300 - Train Loss: 0.0883, Val Loss: 0.0756\n",
      "Epoch 117/300 - Train Loss: 0.0903, Val Loss: 0.0766\n",
      "Epoch 118/300 - Train Loss: 0.0891, Val Loss: 0.0766\n",
      "Epoch 119/300 - Train Loss: 0.0886, Val Loss: 0.0743\n",
      "Epoch 120/300 - Train Loss: 0.0894, Val Loss: 0.0750\n",
      "Epoch 121/300 - Train Loss: 0.0881, Val Loss: 0.0755\n",
      "Epoch 122/300 - Train Loss: 0.0879, Val Loss: 0.0762\n",
      "Epoch 123/300 - Train Loss: 0.0875, Val Loss: 0.0756\n",
      "Epoch 124/300 - Train Loss: 0.0878, Val Loss: 0.0767\n",
      "Epoch 125/300 - Train Loss: 0.0904, Val Loss: 0.0766\n",
      "Epoch 126/300 - Train Loss: 0.0889, Val Loss: 0.0747\n",
      "Epoch 127/300 - Train Loss: 0.0855, Val Loss: 0.0761\n",
      "Epoch 128/300 - Train Loss: 0.0894, Val Loss: 0.0757\n",
      "Epoch 129/300 - Train Loss: 0.0898, Val Loss: 0.0758\n",
      "Epoch 130/300 - Train Loss: 0.0877, Val Loss: 0.0754\n",
      "Epoch 131/300 - Train Loss: 0.0877, Val Loss: 0.0743\n",
      "Epoch 132/300 - Train Loss: 0.0869, Val Loss: 0.0756\n",
      "Epoch 133/300 - Train Loss: 0.0890, Val Loss: 0.0747\n",
      "Epoch 134/300 - Train Loss: 0.0890, Val Loss: 0.0783\n",
      "Epoch 135/300 - Train Loss: 0.0868, Val Loss: 0.0780\n",
      "Epoch 136/300 - Train Loss: 0.0872, Val Loss: 0.0752\n",
      "Epoch 137/300 - Train Loss: 0.0875, Val Loss: 0.0765\n",
      "Epoch 138/300 - Train Loss: 0.0879, Val Loss: 0.0746\n",
      "Epoch 139/300 - Train Loss: 0.0881, Val Loss: 0.0739\n",
      "Epoch 140/300 - Train Loss: 0.0859, Val Loss: 0.0746\n",
      "Epoch 141/300 - Train Loss: 0.0885, Val Loss: 0.0754\n",
      "Epoch 142/300 - Train Loss: 0.0878, Val Loss: 0.0762\n",
      "Epoch 143/300 - Train Loss: 0.0863, Val Loss: 0.0741\n",
      "Epoch 144/300 - Train Loss: 0.0880, Val Loss: 0.0747\n",
      "Epoch 145/300 - Train Loss: 0.0869, Val Loss: 0.0749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/300 - Train Loss: 0.0875, Val Loss: 0.0749\n",
      "Epoch 147/300 - Train Loss: 0.0858, Val Loss: 0.0735\n",
      "Epoch 148/300 - Train Loss: 0.0873, Val Loss: 0.0749\n",
      "Epoch 149/300 - Train Loss: 0.0862, Val Loss: 0.0741\n",
      "Epoch 150/300 - Train Loss: 0.0880, Val Loss: 0.0740\n",
      "Epoch 151/300 - Train Loss: 0.0857, Val Loss: 0.0761\n",
      "Epoch 152/300 - Train Loss: 0.0859, Val Loss: 0.0751\n",
      "Epoch 153/300 - Train Loss: 0.0863, Val Loss: 0.0758\n",
      "Epoch 154/300 - Train Loss: 0.0881, Val Loss: 0.0751\n",
      "Epoch 155/300 - Train Loss: 0.0844, Val Loss: 0.0750\n",
      "Epoch 156/300 - Train Loss: 0.0873, Val Loss: 0.0737\n",
      "Epoch 157/300 - Train Loss: 0.0864, Val Loss: 0.0763\n",
      "Epoch 158/300 - Train Loss: 0.0860, Val Loss: 0.0748\n",
      "Epoch 159/300 - Train Loss: 0.0857, Val Loss: 0.0749\n",
      "Epoch 160/300 - Train Loss: 0.0853, Val Loss: 0.0729\n",
      "Epoch 161/300 - Train Loss: 0.0836, Val Loss: 0.0734\n",
      "Epoch 162/300 - Train Loss: 0.0859, Val Loss: 0.0723\n",
      "Epoch 163/300 - Train Loss: 0.0852, Val Loss: 0.0757\n",
      "Epoch 164/300 - Train Loss: 0.0865, Val Loss: 0.0768\n",
      "Epoch 165/300 - Train Loss: 0.0863, Val Loss: 0.0736\n",
      "Epoch 166/300 - Train Loss: 0.0864, Val Loss: 0.0733\n",
      "Epoch 167/300 - Train Loss: 0.0857, Val Loss: 0.0744\n",
      "Epoch 168/300 - Train Loss: 0.0843, Val Loss: 0.0742\n",
      "Epoch 169/300 - Train Loss: 0.0860, Val Loss: 0.0739\n",
      "Epoch 170/300 - Train Loss: 0.0861, Val Loss: 0.0727\n",
      "Epoch 171/300 - Train Loss: 0.0870, Val Loss: 0.0733\n",
      "Epoch 172/300 - Train Loss: 0.0869, Val Loss: 0.0738\n",
      "Epoch 173/300 - Train Loss: 0.0849, Val Loss: 0.0749\n",
      "Epoch 174/300 - Train Loss: 0.0865, Val Loss: 0.0729\n",
      "Epoch 175/300 - Train Loss: 0.0850, Val Loss: 0.0742\n",
      "Epoch 176/300 - Train Loss: 0.0869, Val Loss: 0.0744\n",
      "Epoch 177/300 - Train Loss: 0.0839, Val Loss: 0.0740\n",
      "Epoch 178/300 - Train Loss: 0.0862, Val Loss: 0.0726\n",
      "Epoch 179/300 - Train Loss: 0.0848, Val Loss: 0.0738\n",
      "Epoch 180/300 - Train Loss: 0.0868, Val Loss: 0.0759\n",
      "Epoch 181/300 - Train Loss: 0.0865, Val Loss: 0.0725\n",
      "Epoch 182/300 - Train Loss: 0.0863, Val Loss: 0.0724\n",
      "Epoch 183/300 - Train Loss: 0.0850, Val Loss: 0.0739\n",
      "Epoch 184/300 - Train Loss: 0.0851, Val Loss: 0.0737\n",
      "Epoch 185/300 - Train Loss: 0.0879, Val Loss: 0.0730\n",
      "Epoch 186/300 - Train Loss: 0.0847, Val Loss: 0.0733\n",
      "Epoch 187/300 - Train Loss: 0.0841, Val Loss: 0.0734\n",
      "Epoch 188/300 - Train Loss: 0.0847, Val Loss: 0.0717\n",
      "Epoch 189/300 - Train Loss: 0.0852, Val Loss: 0.0731\n",
      "Epoch 190/300 - Train Loss: 0.0849, Val Loss: 0.0731\n",
      "Epoch 191/300 - Train Loss: 0.0862, Val Loss: 0.0718\n",
      "Epoch 192/300 - Train Loss: 0.0845, Val Loss: 0.0727\n",
      "Epoch 193/300 - Train Loss: 0.0829, Val Loss: 0.0734\n",
      "Epoch 194/300 - Train Loss: 0.0843, Val Loss: 0.0723\n",
      "Epoch 195/300 - Train Loss: 0.0861, Val Loss: 0.0732\n",
      "Epoch 196/300 - Train Loss: 0.0827, Val Loss: 0.0725\n",
      "Epoch 197/300 - Train Loss: 0.0857, Val Loss: 0.0721\n",
      "Epoch 198/300 - Train Loss: 0.0853, Val Loss: 0.0735\n",
      "Epoch 199/300 - Train Loss: 0.0837, Val Loss: 0.0725\n",
      "Epoch 200/300 - Train Loss: 0.0834, Val Loss: 0.0724\n",
      "Epoch 201/300 - Train Loss: 0.0858, Val Loss: 0.0728\n",
      "Epoch 202/300 - Train Loss: 0.0834, Val Loss: 0.0750\n",
      "Epoch 203/300 - Train Loss: 0.0859, Val Loss: 0.0728\n",
      "Epoch 204/300 - Train Loss: 0.0840, Val Loss: 0.0719\n",
      "Epoch 205/300 - Train Loss: 0.0836, Val Loss: 0.0730\n",
      "Epoch 206/300 - Train Loss: 0.0851, Val Loss: 0.0719\n",
      "Epoch 207/300 - Train Loss: 0.0835, Val Loss: 0.0728\n",
      "Epoch 208/300 - Train Loss: 0.0837, Val Loss: 0.0723\n",
      "Epoch 209/300 - Train Loss: 0.0827, Val Loss: 0.0724\n",
      "Epoch 210/300 - Train Loss: 0.0833, Val Loss: 0.0721\n",
      "Epoch 211/300 - Train Loss: 0.0842, Val Loss: 0.0711\n",
      "Epoch 212/300 - Train Loss: 0.0815, Val Loss: 0.0719\n",
      "Epoch 213/300 - Train Loss: 0.0827, Val Loss: 0.0730\n",
      "Epoch 214/300 - Train Loss: 0.0834, Val Loss: 0.0740\n",
      "Epoch 215/300 - Train Loss: 0.0823, Val Loss: 0.0741\n",
      "Epoch 216/300 - Train Loss: 0.0843, Val Loss: 0.0717\n",
      "Epoch 217/300 - Train Loss: 0.0835, Val Loss: 0.0729\n",
      "Epoch 218/300 - Train Loss: 0.0819, Val Loss: 0.0730\n",
      "Epoch 219/300 - Train Loss: 0.0835, Val Loss: 0.0744\n",
      "Epoch 220/300 - Train Loss: 0.0827, Val Loss: 0.0715\n",
      "Epoch 221/300 - Train Loss: 0.0849, Val Loss: 0.0720\n",
      "Epoch 222/300 - Train Loss: 0.0826, Val Loss: 0.0726\n",
      "Epoch 223/300 - Train Loss: 0.0834, Val Loss: 0.0716\n",
      "Epoch 224/300 - Train Loss: 0.0836, Val Loss: 0.0722\n",
      "Epoch 225/300 - Train Loss: 0.0822, Val Loss: 0.0736\n",
      "Epoch 226/300 - Train Loss: 0.0834, Val Loss: 0.0721\n",
      "Epoch 227/300 - Train Loss: 0.0836, Val Loss: 0.0715\n",
      "Epoch 228/300 - Train Loss: 0.0827, Val Loss: 0.0727\n",
      "Epoch 229/300 - Train Loss: 0.0840, Val Loss: 0.0725\n",
      "Epoch 230/300 - Train Loss: 0.0833, Val Loss: 0.0725\n",
      "Epoch 231/300 - Train Loss: 0.0824, Val Loss: 0.0719\n",
      "Epoch 232/300 - Train Loss: 0.0843, Val Loss: 0.0709\n",
      "Epoch 233/300 - Train Loss: 0.0827, Val Loss: 0.0714\n",
      "Epoch 234/300 - Train Loss: 0.0828, Val Loss: 0.0720\n",
      "Epoch 235/300 - Train Loss: 0.0813, Val Loss: 0.0729\n",
      "Epoch 236/300 - Train Loss: 0.0834, Val Loss: 0.0705\n",
      "Epoch 237/300 - Train Loss: 0.0823, Val Loss: 0.0723\n",
      "Epoch 238/300 - Train Loss: 0.0828, Val Loss: 0.0719\n",
      "Epoch 239/300 - Train Loss: 0.0813, Val Loss: 0.0708\n",
      "Epoch 240/300 - Train Loss: 0.0812, Val Loss: 0.0731\n",
      "Epoch 241/300 - Train Loss: 0.0849, Val Loss: 0.0722\n",
      "Epoch 242/300 - Train Loss: 0.0831, Val Loss: 0.0713\n",
      "Epoch 243/300 - Train Loss: 0.0837, Val Loss: 0.0740\n",
      "Epoch 244/300 - Train Loss: 0.0821, Val Loss: 0.0713\n",
      "Epoch 245/300 - Train Loss: 0.0829, Val Loss: 0.0727\n",
      "Epoch 246/300 - Train Loss: 0.0816, Val Loss: 0.0715\n",
      "Epoch 247/300 - Train Loss: 0.0827, Val Loss: 0.0719\n",
      "Epoch 248/300 - Train Loss: 0.0816, Val Loss: 0.0718\n",
      "Epoch 249/300 - Train Loss: 0.0819, Val Loss: 0.0719\n",
      "Epoch 250/300 - Train Loss: 0.0816, Val Loss: 0.0721\n",
      "Epoch 251/300 - Train Loss: 0.0817, Val Loss: 0.0721\n",
      "Epoch 252/300 - Train Loss: 0.0808, Val Loss: 0.0713\n",
      "Epoch 253/300 - Train Loss: 0.0808, Val Loss: 0.0707\n",
      "Epoch 254/300 - Train Loss: 0.0825, Val Loss: 0.0728\n",
      "Epoch 255/300 - Train Loss: 0.0812, Val Loss: 0.0718\n",
      "Epoch 256/300 - Train Loss: 0.0828, Val Loss: 0.0718\n",
      "Epoch 257/300 - Train Loss: 0.0816, Val Loss: 0.0715\n",
      "Epoch 258/300 - Train Loss: 0.0821, Val Loss: 0.0709\n",
      "Epoch 259/300 - Train Loss: 0.0825, Val Loss: 0.0725\n",
      "Epoch 260/300 - Train Loss: 0.0805, Val Loss: 0.0712\n",
      "Epoch 261/300 - Train Loss: 0.0802, Val Loss: 0.0708\n",
      "Epoch 262/300 - Train Loss: 0.0826, Val Loss: 0.0716\n",
      "Epoch 263/300 - Train Loss: 0.0806, Val Loss: 0.0724\n",
      "Epoch 264/300 - Train Loss: 0.0816, Val Loss: 0.0711\n",
      "Epoch 265/300 - Train Loss: 0.0812, Val Loss: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 01:11:38,396] Trial 35 finished with value: 0.9670490334339995 and parameters: {'F1': 8, 'F2': 8, 'D': 4, 'dropout': 0.35019853399787093, 'learning_rate': 1.577994558504693e-05, 'batch_size': 64, 'weight_decay': 5.838562427500637e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 266/300 - Train Loss: 0.0811, Val Loss: 0.0712\n",
      "Early stopping at epoch 266\n",
      "Macro F1 Score: 0.9670, Macro Precision: 0.9593, Macro Recall: 0.9756\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 37\n",
      "Training with F1=16, F2=8, D=8, dropout=0.3219919160653276, LR=7.917707186775857e-05, BS=256, WD=4.5249298925721016e-05\n",
      "Epoch 1/300 - Train Loss: 0.7861, Val Loss: 0.5117\n",
      "Epoch 2/300 - Train Loss: 0.3968, Val Loss: 0.3132\n",
      "Epoch 3/300 - Train Loss: 0.2702, Val Loss: 0.2343\n",
      "Epoch 4/300 - Train Loss: 0.2139, Val Loss: 0.1855\n",
      "Epoch 5/300 - Train Loss: 0.1781, Val Loss: 0.1533\n",
      "Epoch 6/300 - Train Loss: 0.1556, Val Loss: 0.1391\n",
      "Epoch 7/300 - Train Loss: 0.1401, Val Loss: 0.1211\n",
      "Epoch 8/300 - Train Loss: 0.1287, Val Loss: 0.1138\n",
      "Epoch 9/300 - Train Loss: 0.1187, Val Loss: 0.1046\n",
      "Epoch 10/300 - Train Loss: 0.1136, Val Loss: 0.1032\n",
      "Epoch 11/300 - Train Loss: 0.1086, Val Loss: 0.0969\n",
      "Epoch 12/300 - Train Loss: 0.1066, Val Loss: 0.0927\n",
      "Epoch 13/300 - Train Loss: 0.1031, Val Loss: 0.0935\n",
      "Epoch 14/300 - Train Loss: 0.1006, Val Loss: 0.0916\n",
      "Epoch 15/300 - Train Loss: 0.0984, Val Loss: 0.0875\n",
      "Epoch 16/300 - Train Loss: 0.0977, Val Loss: 0.0908\n",
      "Epoch 17/300 - Train Loss: 0.0953, Val Loss: 0.0965\n",
      "Epoch 18/300 - Train Loss: 0.0946, Val Loss: 0.0826\n",
      "Epoch 19/300 - Train Loss: 0.0934, Val Loss: 0.0802\n",
      "Epoch 20/300 - Train Loss: 0.0920, Val Loss: 0.0802\n",
      "Epoch 21/300 - Train Loss: 0.0906, Val Loss: 0.0826\n",
      "Epoch 22/300 - Train Loss: 0.0905, Val Loss: 0.0783\n",
      "Epoch 23/300 - Train Loss: 0.0895, Val Loss: 0.0759\n",
      "Epoch 24/300 - Train Loss: 0.0878, Val Loss: 0.0812\n",
      "Epoch 25/300 - Train Loss: 0.0874, Val Loss: 0.0777\n",
      "Epoch 26/300 - Train Loss: 0.0876, Val Loss: 0.0806\n",
      "Epoch 27/300 - Train Loss: 0.0858, Val Loss: 0.0727\n",
      "Epoch 28/300 - Train Loss: 0.0853, Val Loss: 0.0766\n",
      "Epoch 29/300 - Train Loss: 0.0869, Val Loss: 0.0718\n",
      "Epoch 30/300 - Train Loss: 0.0851, Val Loss: 0.0745\n",
      "Epoch 31/300 - Train Loss: 0.0831, Val Loss: 0.0733\n",
      "Epoch 32/300 - Train Loss: 0.0833, Val Loss: 0.0733\n",
      "Epoch 33/300 - Train Loss: 0.0822, Val Loss: 0.0727\n",
      "Epoch 34/300 - Train Loss: 0.0833, Val Loss: 0.0707\n",
      "Epoch 35/300 - Train Loss: 0.0837, Val Loss: 0.0752\n",
      "Epoch 36/300 - Train Loss: 0.0820, Val Loss: 0.0699\n",
      "Epoch 37/300 - Train Loss: 0.0814, Val Loss: 0.0702\n",
      "Epoch 38/300 - Train Loss: 0.0826, Val Loss: 0.0713\n",
      "Epoch 39/300 - Train Loss: 0.0823, Val Loss: 0.0730\n",
      "Epoch 40/300 - Train Loss: 0.0791, Val Loss: 0.0699\n",
      "Epoch 41/300 - Train Loss: 0.0796, Val Loss: 0.0723\n",
      "Epoch 42/300 - Train Loss: 0.0800, Val Loss: 0.0729\n",
      "Epoch 43/300 - Train Loss: 0.0805, Val Loss: 0.0716\n",
      "Epoch 44/300 - Train Loss: 0.0801, Val Loss: 0.0766\n",
      "Epoch 45/300 - Train Loss: 0.0779, Val Loss: 0.0693\n",
      "Epoch 46/300 - Train Loss: 0.0778, Val Loss: 0.0690\n",
      "Epoch 47/300 - Train Loss: 0.0786, Val Loss: 0.0696\n",
      "Epoch 48/300 - Train Loss: 0.0784, Val Loss: 0.0684\n",
      "Epoch 49/300 - Train Loss: 0.0768, Val Loss: 0.0707\n",
      "Epoch 50/300 - Train Loss: 0.0786, Val Loss: 0.0659\n",
      "Epoch 51/300 - Train Loss: 0.0785, Val Loss: 0.0688\n",
      "Epoch 52/300 - Train Loss: 0.0779, Val Loss: 0.0672\n",
      "Epoch 53/300 - Train Loss: 0.0762, Val Loss: 0.0680\n",
      "Epoch 54/300 - Train Loss: 0.0773, Val Loss: 0.0691\n",
      "Epoch 55/300 - Train Loss: 0.0778, Val Loss: 0.0674\n",
      "Epoch 56/300 - Train Loss: 0.0748, Val Loss: 0.0692\n",
      "Epoch 57/300 - Train Loss: 0.0783, Val Loss: 0.0676\n",
      "Epoch 58/300 - Train Loss: 0.0768, Val Loss: 0.0682\n",
      "Epoch 59/300 - Train Loss: 0.0749, Val Loss: 0.0678\n",
      "Epoch 60/300 - Train Loss: 0.0756, Val Loss: 0.0679\n",
      "Epoch 61/300 - Train Loss: 0.0766, Val Loss: 0.0676\n",
      "Epoch 62/300 - Train Loss: 0.0755, Val Loss: 0.0661\n",
      "Epoch 63/300 - Train Loss: 0.0758, Val Loss: 0.0677\n",
      "Epoch 64/300 - Train Loss: 0.0759, Val Loss: 0.0664\n",
      "Epoch 65/300 - Train Loss: 0.0747, Val Loss: 0.0714\n",
      "Epoch 66/300 - Train Loss: 0.0750, Val Loss: 0.0657\n",
      "Epoch 67/300 - Train Loss: 0.0740, Val Loss: 0.0678\n",
      "Epoch 68/300 - Train Loss: 0.0731, Val Loss: 0.0674\n",
      "Epoch 69/300 - Train Loss: 0.0751, Val Loss: 0.0705\n",
      "Epoch 70/300 - Train Loss: 0.0741, Val Loss: 0.0677\n",
      "Epoch 71/300 - Train Loss: 0.0734, Val Loss: 0.0673\n",
      "Epoch 72/300 - Train Loss: 0.0744, Val Loss: 0.0667\n",
      "Epoch 73/300 - Train Loss: 0.0738, Val Loss: 0.0690\n",
      "Epoch 74/300 - Train Loss: 0.0738, Val Loss: 0.0666\n",
      "Epoch 75/300 - Train Loss: 0.0735, Val Loss: 0.0696\n",
      "Epoch 76/300 - Train Loss: 0.0745, Val Loss: 0.0668\n",
      "Epoch 77/300 - Train Loss: 0.0730, Val Loss: 0.0662\n",
      "Epoch 78/300 - Train Loss: 0.0726, Val Loss: 0.0683\n",
      "Epoch 79/300 - Train Loss: 0.0751, Val Loss: 0.0672\n",
      "Epoch 80/300 - Train Loss: 0.0728, Val Loss: 0.0674\n",
      "Epoch 81/300 - Train Loss: 0.0719, Val Loss: 0.0657\n",
      "Epoch 82/300 - Train Loss: 0.0722, Val Loss: 0.0674\n",
      "Epoch 83/300 - Train Loss: 0.0717, Val Loss: 0.0659\n",
      "Epoch 84/300 - Train Loss: 0.0729, Val Loss: 0.0657\n",
      "Epoch 85/300 - Train Loss: 0.0726, Val Loss: 0.0673\n",
      "Epoch 86/300 - Train Loss: 0.0719, Val Loss: 0.0655\n",
      "Epoch 87/300 - Train Loss: 0.0721, Val Loss: 0.0696\n",
      "Epoch 88/300 - Train Loss: 0.0736, Val Loss: 0.0667\n",
      "Epoch 89/300 - Train Loss: 0.0730, Val Loss: 0.0673\n",
      "Epoch 90/300 - Train Loss: 0.0721, Val Loss: 0.0660\n",
      "Epoch 91/300 - Train Loss: 0.0725, Val Loss: 0.0667\n",
      "Epoch 92/300 - Train Loss: 0.0726, Val Loss: 0.0666\n",
      "Epoch 93/300 - Train Loss: 0.0706, Val Loss: 0.0652\n",
      "Epoch 94/300 - Train Loss: 0.0713, Val Loss: 0.0666\n",
      "Epoch 95/300 - Train Loss: 0.0715, Val Loss: 0.0652\n",
      "Epoch 96/300 - Train Loss: 0.0716, Val Loss: 0.0668\n",
      "Epoch 97/300 - Train Loss: 0.0725, Val Loss: 0.0660\n",
      "Epoch 98/300 - Train Loss: 0.0704, Val Loss: 0.0674\n",
      "Epoch 99/300 - Train Loss: 0.0708, Val Loss: 0.0676\n",
      "Epoch 100/300 - Train Loss: 0.0711, Val Loss: 0.0669\n",
      "Epoch 101/300 - Train Loss: 0.0705, Val Loss: 0.0669\n",
      "Epoch 102/300 - Train Loss: 0.0712, Val Loss: 0.0647\n",
      "Epoch 103/300 - Train Loss: 0.0722, Val Loss: 0.0672\n",
      "Epoch 104/300 - Train Loss: 0.0701, Val Loss: 0.0665\n",
      "Epoch 105/300 - Train Loss: 0.0703, Val Loss: 0.0660\n",
      "Epoch 106/300 - Train Loss: 0.0711, Val Loss: 0.0671\n",
      "Epoch 107/300 - Train Loss: 0.0695, Val Loss: 0.0672\n",
      "Epoch 108/300 - Train Loss: 0.0706, Val Loss: 0.0644\n",
      "Epoch 109/300 - Train Loss: 0.0700, Val Loss: 0.0680\n",
      "Epoch 110/300 - Train Loss: 0.0701, Val Loss: 0.0651\n",
      "Epoch 111/300 - Train Loss: 0.0694, Val Loss: 0.0664\n",
      "Epoch 112/300 - Train Loss: 0.0709, Val Loss: 0.0668\n",
      "Epoch 113/300 - Train Loss: 0.0695, Val Loss: 0.0662\n",
      "Epoch 114/300 - Train Loss: 0.0722, Val Loss: 0.0661\n",
      "Epoch 115/300 - Train Loss: 0.0693, Val Loss: 0.0658\n",
      "Epoch 116/300 - Train Loss: 0.0700, Val Loss: 0.0654\n",
      "Epoch 117/300 - Train Loss: 0.0701, Val Loss: 0.0658\n",
      "Epoch 118/300 - Train Loss: 0.0709, Val Loss: 0.0660\n",
      "Epoch 119/300 - Train Loss: 0.0697, Val Loss: 0.0665\n",
      "Epoch 120/300 - Train Loss: 0.0687, Val Loss: 0.0641\n",
      "Epoch 121/300 - Train Loss: 0.0682, Val Loss: 0.0666\n",
      "Epoch 122/300 - Train Loss: 0.0711, Val Loss: 0.0646\n",
      "Epoch 123/300 - Train Loss: 0.0703, Val Loss: 0.0651\n",
      "Epoch 124/300 - Train Loss: 0.0691, Val Loss: 0.0660\n",
      "Epoch 125/300 - Train Loss: 0.0696, Val Loss: 0.0666\n",
      "Epoch 126/300 - Train Loss: 0.0696, Val Loss: 0.0643\n",
      "Epoch 127/300 - Train Loss: 0.0692, Val Loss: 0.0653\n",
      "Epoch 128/300 - Train Loss: 0.0697, Val Loss: 0.0659\n",
      "Epoch 129/300 - Train Loss: 0.0733, Val Loss: 0.0675\n",
      "Epoch 130/300 - Train Loss: 0.0689, Val Loss: 0.0667\n",
      "Epoch 131/300 - Train Loss: 0.0721, Val Loss: 0.0654\n",
      "Epoch 132/300 - Train Loss: 0.0681, Val Loss: 0.0649\n",
      "Epoch 133/300 - Train Loss: 0.0687, Val Loss: 0.0666\n",
      "Epoch 134/300 - Train Loss: 0.0684, Val Loss: 0.0649\n",
      "Epoch 135/300 - Train Loss: 0.0696, Val Loss: 0.0654\n",
      "Epoch 136/300 - Train Loss: 0.0680, Val Loss: 0.0655\n",
      "Epoch 137/300 - Train Loss: 0.0692, Val Loss: 0.0645\n",
      "Epoch 138/300 - Train Loss: 0.0680, Val Loss: 0.0665\n",
      "Epoch 139/300 - Train Loss: 0.0679, Val Loss: 0.0650\n",
      "Epoch 140/300 - Train Loss: 0.0683, Val Loss: 0.0666\n",
      "Epoch 141/300 - Train Loss: 0.0683, Val Loss: 0.0654\n",
      "Epoch 142/300 - Train Loss: 0.0696, Val Loss: 0.0654\n",
      "Epoch 143/300 - Train Loss: 0.0691, Val Loss: 0.0639\n",
      "Epoch 144/300 - Train Loss: 0.0679, Val Loss: 0.0658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0693, Val Loss: 0.0646\n",
      "Epoch 146/300 - Train Loss: 0.0681, Val Loss: 0.0653\n",
      "Epoch 147/300 - Train Loss: 0.0672, Val Loss: 0.0655\n",
      "Epoch 148/300 - Train Loss: 0.0676, Val Loss: 0.0650\n",
      "Epoch 149/300 - Train Loss: 0.0683, Val Loss: 0.0674\n",
      "Epoch 150/300 - Train Loss: 0.0681, Val Loss: 0.0655\n",
      "Epoch 151/300 - Train Loss: 0.0679, Val Loss: 0.0663\n",
      "Epoch 152/300 - Train Loss: 0.0691, Val Loss: 0.0656\n",
      "Epoch 153/300 - Train Loss: 0.0689, Val Loss: 0.0650\n",
      "Epoch 154/300 - Train Loss: 0.0679, Val Loss: 0.0648\n",
      "Epoch 155/300 - Train Loss: 0.0667, Val Loss: 0.0655\n",
      "Epoch 156/300 - Train Loss: 0.0666, Val Loss: 0.0659\n",
      "Epoch 157/300 - Train Loss: 0.0656, Val Loss: 0.0653\n",
      "Epoch 158/300 - Train Loss: 0.0679, Val Loss: 0.0647\n",
      "Epoch 159/300 - Train Loss: 0.0656, Val Loss: 0.0654\n",
      "Epoch 160/300 - Train Loss: 0.0661, Val Loss: 0.0654\n",
      "Epoch 161/300 - Train Loss: 0.0690, Val Loss: 0.0659\n",
      "Epoch 162/300 - Train Loss: 0.0676, Val Loss: 0.0649\n",
      "Epoch 163/300 - Train Loss: 0.0686, Val Loss: 0.0643\n",
      "Epoch 164/300 - Train Loss: 0.0663, Val Loss: 0.0674\n",
      "Epoch 165/300 - Train Loss: 0.0682, Val Loss: 0.0661\n",
      "Epoch 166/300 - Train Loss: 0.0677, Val Loss: 0.0655\n",
      "Epoch 167/300 - Train Loss: 0.0667, Val Loss: 0.0658\n",
      "Epoch 168/300 - Train Loss: 0.0659, Val Loss: 0.0657\n",
      "Epoch 169/300 - Train Loss: 0.0650, Val Loss: 0.0643\n",
      "Epoch 170/300 - Train Loss: 0.0664, Val Loss: 0.0651\n",
      "Epoch 171/300 - Train Loss: 0.0686, Val Loss: 0.0642\n",
      "Epoch 172/300 - Train Loss: 0.0690, Val Loss: 0.0649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 01:20:10,592] Trial 36 finished with value: 0.96754539176412 and parameters: {'F1': 16, 'F2': 8, 'D': 8, 'dropout': 0.3219919160653276, 'learning_rate': 7.917707186775857e-05, 'batch_size': 256, 'weight_decay': 4.5249298925721016e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 173/300 - Train Loss: 0.0686, Val Loss: 0.0657\n",
      "Early stopping at epoch 173\n",
      "Macro F1 Score: 0.9675, Macro Precision: 0.9597, Macro Recall: 0.9761\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 38\n",
      "Training with F1=8, F2=16, D=4, dropout=0.15773428581167293, LR=1.3068005412856696e-05, BS=64, WD=9.586196343161078e-05\n",
      "Epoch 1/300 - Train Loss: 0.9223, Val Loss: 0.7813\n",
      "Epoch 2/300 - Train Loss: 0.6235, Val Loss: 0.4903\n",
      "Epoch 3/300 - Train Loss: 0.4294, Val Loss: 0.3658\n",
      "Epoch 4/300 - Train Loss: 0.3481, Val Loss: 0.3034\n",
      "Epoch 5/300 - Train Loss: 0.2958, Val Loss: 0.2622\n",
      "Epoch 6/300 - Train Loss: 0.2565, Val Loss: 0.2313\n",
      "Epoch 7/300 - Train Loss: 0.2275, Val Loss: 0.2032\n",
      "Epoch 8/300 - Train Loss: 0.2043, Val Loss: 0.1824\n",
      "Epoch 9/300 - Train Loss: 0.1864, Val Loss: 0.1650\n",
      "Epoch 10/300 - Train Loss: 0.1724, Val Loss: 0.1553\n",
      "Epoch 11/300 - Train Loss: 0.1635, Val Loss: 0.1428\n",
      "Epoch 12/300 - Train Loss: 0.1532, Val Loss: 0.1341\n",
      "Epoch 13/300 - Train Loss: 0.1463, Val Loss: 0.1274\n",
      "Epoch 14/300 - Train Loss: 0.1374, Val Loss: 0.1253\n",
      "Epoch 15/300 - Train Loss: 0.1337, Val Loss: 0.1164\n",
      "Epoch 16/300 - Train Loss: 0.1289, Val Loss: 0.1136\n",
      "Epoch 17/300 - Train Loss: 0.1237, Val Loss: 0.1064\n",
      "Epoch 18/300 - Train Loss: 0.1225, Val Loss: 0.1089\n",
      "Epoch 19/300 - Train Loss: 0.1206, Val Loss: 0.1062\n",
      "Epoch 20/300 - Train Loss: 0.1149, Val Loss: 0.1030\n",
      "Epoch 21/300 - Train Loss: 0.1150, Val Loss: 0.1027\n",
      "Epoch 22/300 - Train Loss: 0.1136, Val Loss: 0.0985\n",
      "Epoch 23/300 - Train Loss: 0.1126, Val Loss: 0.1002\n",
      "Epoch 24/300 - Train Loss: 0.1098, Val Loss: 0.0949\n",
      "Epoch 25/300 - Train Loss: 0.1080, Val Loss: 0.0966\n",
      "Epoch 26/300 - Train Loss: 0.1097, Val Loss: 0.0930\n",
      "Epoch 27/300 - Train Loss: 0.1065, Val Loss: 0.0947\n",
      "Epoch 28/300 - Train Loss: 0.1045, Val Loss: 0.0915\n",
      "Epoch 29/300 - Train Loss: 0.1041, Val Loss: 0.0915\n",
      "Epoch 30/300 - Train Loss: 0.1031, Val Loss: 0.0917\n",
      "Epoch 31/300 - Train Loss: 0.1019, Val Loss: 0.0897\n",
      "Epoch 32/300 - Train Loss: 0.1025, Val Loss: 0.0888\n",
      "Epoch 33/300 - Train Loss: 0.1014, Val Loss: 0.0930\n",
      "Epoch 34/300 - Train Loss: 0.1017, Val Loss: 0.0901\n",
      "Epoch 35/300 - Train Loss: 0.0996, Val Loss: 0.0887\n",
      "Epoch 36/300 - Train Loss: 0.0987, Val Loss: 0.0878\n",
      "Epoch 37/300 - Train Loss: 0.0993, Val Loss: 0.0864\n",
      "Epoch 38/300 - Train Loss: 0.0966, Val Loss: 0.0879\n",
      "Epoch 39/300 - Train Loss: 0.0959, Val Loss: 0.0903\n",
      "Epoch 40/300 - Train Loss: 0.0970, Val Loss: 0.0879\n",
      "Epoch 41/300 - Train Loss: 0.0971, Val Loss: 0.0870\n",
      "Epoch 42/300 - Train Loss: 0.0957, Val Loss: 0.0889\n",
      "Epoch 43/300 - Train Loss: 0.0965, Val Loss: 0.0867\n",
      "Epoch 44/300 - Train Loss: 0.0952, Val Loss: 0.0850\n",
      "Epoch 45/300 - Train Loss: 0.0969, Val Loss: 0.0840\n",
      "Epoch 46/300 - Train Loss: 0.0957, Val Loss: 0.0864\n",
      "Epoch 47/300 - Train Loss: 0.0951, Val Loss: 0.0841\n",
      "Epoch 48/300 - Train Loss: 0.0943, Val Loss: 0.0841\n",
      "Epoch 49/300 - Train Loss: 0.0938, Val Loss: 0.0847\n",
      "Epoch 50/300 - Train Loss: 0.0938, Val Loss: 0.0851\n",
      "Epoch 51/300 - Train Loss: 0.0944, Val Loss: 0.0835\n",
      "Epoch 52/300 - Train Loss: 0.0937, Val Loss: 0.0852\n",
      "Epoch 53/300 - Train Loss: 0.0918, Val Loss: 0.0833\n",
      "Epoch 54/300 - Train Loss: 0.0926, Val Loss: 0.0850\n",
      "Epoch 55/300 - Train Loss: 0.0923, Val Loss: 0.0825\n",
      "Epoch 56/300 - Train Loss: 0.0916, Val Loss: 0.0825\n",
      "Epoch 57/300 - Train Loss: 0.0931, Val Loss: 0.0830\n",
      "Epoch 58/300 - Train Loss: 0.0904, Val Loss: 0.0820\n",
      "Epoch 59/300 - Train Loss: 0.0921, Val Loss: 0.0829\n",
      "Epoch 60/300 - Train Loss: 0.0915, Val Loss: 0.0826\n",
      "Epoch 61/300 - Train Loss: 0.0897, Val Loss: 0.0835\n",
      "Epoch 62/300 - Train Loss: 0.0915, Val Loss: 0.0832\n",
      "Epoch 63/300 - Train Loss: 0.0900, Val Loss: 0.0817\n",
      "Epoch 64/300 - Train Loss: 0.0907, Val Loss: 0.0831\n",
      "Epoch 65/300 - Train Loss: 0.0893, Val Loss: 0.0822\n",
      "Epoch 66/300 - Train Loss: 0.0881, Val Loss: 0.0825\n",
      "Epoch 67/300 - Train Loss: 0.0883, Val Loss: 0.0833\n",
      "Epoch 68/300 - Train Loss: 0.0883, Val Loss: 0.0808\n",
      "Epoch 69/300 - Train Loss: 0.0883, Val Loss: 0.0813\n",
      "Epoch 70/300 - Train Loss: 0.0878, Val Loss: 0.0817\n",
      "Epoch 71/300 - Train Loss: 0.0878, Val Loss: 0.0834\n",
      "Epoch 72/300 - Train Loss: 0.0880, Val Loss: 0.0818\n",
      "Epoch 73/300 - Train Loss: 0.0890, Val Loss: 0.0808\n",
      "Epoch 74/300 - Train Loss: 0.0871, Val Loss: 0.0815\n",
      "Epoch 75/300 - Train Loss: 0.0876, Val Loss: 0.0802\n",
      "Epoch 76/300 - Train Loss: 0.0872, Val Loss: 0.0806\n",
      "Epoch 77/300 - Train Loss: 0.0863, Val Loss: 0.0802\n",
      "Epoch 78/300 - Train Loss: 0.0842, Val Loss: 0.0801\n",
      "Epoch 79/300 - Train Loss: 0.0865, Val Loss: 0.0801\n",
      "Epoch 80/300 - Train Loss: 0.0867, Val Loss: 0.0806\n",
      "Epoch 81/300 - Train Loss: 0.0855, Val Loss: 0.0803\n",
      "Epoch 82/300 - Train Loss: 0.0850, Val Loss: 0.0830\n",
      "Epoch 83/300 - Train Loss: 0.0865, Val Loss: 0.0801\n",
      "Epoch 84/300 - Train Loss: 0.0861, Val Loss: 0.0810\n",
      "Epoch 85/300 - Train Loss: 0.0856, Val Loss: 0.0807\n",
      "Epoch 86/300 - Train Loss: 0.0856, Val Loss: 0.0794\n",
      "Epoch 87/300 - Train Loss: 0.0858, Val Loss: 0.0806\n",
      "Epoch 88/300 - Train Loss: 0.0868, Val Loss: 0.0804\n",
      "Epoch 89/300 - Train Loss: 0.0832, Val Loss: 0.0796\n",
      "Epoch 90/300 - Train Loss: 0.0848, Val Loss: 0.0800\n",
      "Epoch 91/300 - Train Loss: 0.0845, Val Loss: 0.0798\n",
      "Epoch 92/300 - Train Loss: 0.0842, Val Loss: 0.0817\n",
      "Epoch 93/300 - Train Loss: 0.0843, Val Loss: 0.0794\n",
      "Epoch 94/300 - Train Loss: 0.0842, Val Loss: 0.0788\n",
      "Epoch 95/300 - Train Loss: 0.0841, Val Loss: 0.0820\n",
      "Epoch 96/300 - Train Loss: 0.0841, Val Loss: 0.0804\n",
      "Epoch 97/300 - Train Loss: 0.0838, Val Loss: 0.0812\n",
      "Epoch 98/300 - Train Loss: 0.0825, Val Loss: 0.0796\n",
      "Epoch 99/300 - Train Loss: 0.0835, Val Loss: 0.0798\n",
      "Epoch 100/300 - Train Loss: 0.0833, Val Loss: 0.0775\n",
      "Epoch 101/300 - Train Loss: 0.0828, Val Loss: 0.0791\n",
      "Epoch 102/300 - Train Loss: 0.0825, Val Loss: 0.0805\n",
      "Epoch 103/300 - Train Loss: 0.0829, Val Loss: 0.0783\n",
      "Epoch 104/300 - Train Loss: 0.0818, Val Loss: 0.0790\n",
      "Epoch 105/300 - Train Loss: 0.0831, Val Loss: 0.0789\n",
      "Epoch 106/300 - Train Loss: 0.0826, Val Loss: 0.0799\n",
      "Epoch 107/300 - Train Loss: 0.0822, Val Loss: 0.0783\n",
      "Epoch 108/300 - Train Loss: 0.0837, Val Loss: 0.0775\n",
      "Epoch 109/300 - Train Loss: 0.0820, Val Loss: 0.0790\n",
      "Epoch 110/300 - Train Loss: 0.0824, Val Loss: 0.0795\n",
      "Epoch 111/300 - Train Loss: 0.0824, Val Loss: 0.0803\n",
      "Epoch 112/300 - Train Loss: 0.0812, Val Loss: 0.0785\n",
      "Epoch 113/300 - Train Loss: 0.0822, Val Loss: 0.0818\n",
      "Epoch 114/300 - Train Loss: 0.0806, Val Loss: 0.0800\n",
      "Epoch 115/300 - Train Loss: 0.0802, Val Loss: 0.0774\n",
      "Epoch 116/300 - Train Loss: 0.0804, Val Loss: 0.0775\n",
      "Epoch 117/300 - Train Loss: 0.0823, Val Loss: 0.0775\n",
      "Epoch 118/300 - Train Loss: 0.0810, Val Loss: 0.0777\n",
      "Epoch 119/300 - Train Loss: 0.0811, Val Loss: 0.0784\n",
      "Epoch 120/300 - Train Loss: 0.0810, Val Loss: 0.0781\n",
      "Epoch 121/300 - Train Loss: 0.0803, Val Loss: 0.0798\n",
      "Epoch 122/300 - Train Loss: 0.0814, Val Loss: 0.0780\n",
      "Epoch 123/300 - Train Loss: 0.0802, Val Loss: 0.0788\n",
      "Epoch 124/300 - Train Loss: 0.0793, Val Loss: 0.0793\n",
      "Epoch 125/300 - Train Loss: 0.0802, Val Loss: 0.0783\n",
      "Epoch 126/300 - Train Loss: 0.0800, Val Loss: 0.0772\n",
      "Epoch 127/300 - Train Loss: 0.0806, Val Loss: 0.0781\n",
      "Epoch 128/300 - Train Loss: 0.0821, Val Loss: 0.0796\n",
      "Epoch 129/300 - Train Loss: 0.0800, Val Loss: 0.0784\n",
      "Epoch 130/300 - Train Loss: 0.0814, Val Loss: 0.0782\n",
      "Epoch 131/300 - Train Loss: 0.0797, Val Loss: 0.0785\n",
      "Epoch 132/300 - Train Loss: 0.0795, Val Loss: 0.0775\n",
      "Epoch 133/300 - Train Loss: 0.0788, Val Loss: 0.0780\n",
      "Epoch 134/300 - Train Loss: 0.0801, Val Loss: 0.0782\n",
      "Epoch 135/300 - Train Loss: 0.0804, Val Loss: 0.0781\n",
      "Epoch 136/300 - Train Loss: 0.0810, Val Loss: 0.0770\n",
      "Epoch 137/300 - Train Loss: 0.0780, Val Loss: 0.0770\n",
      "Epoch 138/300 - Train Loss: 0.0796, Val Loss: 0.0776\n",
      "Epoch 139/300 - Train Loss: 0.0788, Val Loss: 0.0786\n",
      "Epoch 140/300 - Train Loss: 0.0802, Val Loss: 0.0788\n",
      "Epoch 141/300 - Train Loss: 0.0782, Val Loss: 0.0762\n",
      "Epoch 142/300 - Train Loss: 0.0776, Val Loss: 0.0775\n",
      "Epoch 143/300 - Train Loss: 0.0785, Val Loss: 0.0773\n",
      "Epoch 144/300 - Train Loss: 0.0786, Val Loss: 0.0761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0778, Val Loss: 0.0782\n",
      "Epoch 146/300 - Train Loss: 0.0790, Val Loss: 0.0785\n",
      "Epoch 147/300 - Train Loss: 0.0780, Val Loss: 0.0771\n",
      "Epoch 148/300 - Train Loss: 0.0794, Val Loss: 0.0768\n",
      "Epoch 149/300 - Train Loss: 0.0780, Val Loss: 0.0771\n",
      "Epoch 150/300 - Train Loss: 0.0783, Val Loss: 0.0785\n",
      "Epoch 151/300 - Train Loss: 0.0780, Val Loss: 0.0771\n",
      "Epoch 152/300 - Train Loss: 0.0761, Val Loss: 0.0774\n",
      "Epoch 153/300 - Train Loss: 0.0774, Val Loss: 0.0781\n",
      "Epoch 154/300 - Train Loss: 0.0775, Val Loss: 0.0784\n",
      "Epoch 155/300 - Train Loss: 0.0769, Val Loss: 0.0759\n",
      "Epoch 156/300 - Train Loss: 0.0782, Val Loss: 0.0774\n",
      "Epoch 157/300 - Train Loss: 0.0773, Val Loss: 0.0771\n",
      "Epoch 158/300 - Train Loss: 0.0791, Val Loss: 0.0797\n",
      "Epoch 159/300 - Train Loss: 0.0778, Val Loss: 0.0763\n",
      "Epoch 160/300 - Train Loss: 0.0783, Val Loss: 0.0787\n",
      "Epoch 161/300 - Train Loss: 0.0778, Val Loss: 0.0764\n",
      "Epoch 162/300 - Train Loss: 0.0777, Val Loss: 0.0763\n",
      "Epoch 163/300 - Train Loss: 0.0778, Val Loss: 0.0780\n",
      "Epoch 164/300 - Train Loss: 0.0776, Val Loss: 0.0780\n",
      "Epoch 165/300 - Train Loss: 0.0769, Val Loss: 0.0755\n",
      "Epoch 166/300 - Train Loss: 0.0753, Val Loss: 0.0760\n",
      "Epoch 167/300 - Train Loss: 0.0767, Val Loss: 0.0769\n",
      "Epoch 168/300 - Train Loss: 0.0768, Val Loss: 0.0780\n",
      "Epoch 169/300 - Train Loss: 0.0765, Val Loss: 0.0764\n",
      "Epoch 170/300 - Train Loss: 0.0760, Val Loss: 0.0758\n",
      "Epoch 171/300 - Train Loss: 0.0751, Val Loss: 0.0755\n",
      "Epoch 172/300 - Train Loss: 0.0748, Val Loss: 0.0767\n",
      "Epoch 173/300 - Train Loss: 0.0775, Val Loss: 0.0775\n",
      "Epoch 174/300 - Train Loss: 0.0774, Val Loss: 0.0781\n",
      "Epoch 175/300 - Train Loss: 0.0763, Val Loss: 0.0759\n",
      "Epoch 176/300 - Train Loss: 0.0765, Val Loss: 0.0766\n",
      "Epoch 177/300 - Train Loss: 0.0759, Val Loss: 0.0778\n",
      "Epoch 178/300 - Train Loss: 0.0754, Val Loss: 0.0798\n",
      "Epoch 179/300 - Train Loss: 0.0760, Val Loss: 0.0765\n",
      "Epoch 180/300 - Train Loss: 0.0774, Val Loss: 0.0755\n",
      "Epoch 181/300 - Train Loss: 0.0763, Val Loss: 0.0772\n",
      "Epoch 182/300 - Train Loss: 0.0759, Val Loss: 0.0774\n",
      "Epoch 183/300 - Train Loss: 0.0754, Val Loss: 0.0758\n",
      "Epoch 184/300 - Train Loss: 0.0770, Val Loss: 0.0785\n",
      "Epoch 185/300 - Train Loss: 0.0750, Val Loss: 0.0765\n",
      "Epoch 186/300 - Train Loss: 0.0741, Val Loss: 0.0760\n",
      "Epoch 187/300 - Train Loss: 0.0750, Val Loss: 0.0768\n",
      "Epoch 188/300 - Train Loss: 0.0745, Val Loss: 0.0760\n",
      "Epoch 189/300 - Train Loss: 0.0751, Val Loss: 0.0757\n",
      "Epoch 190/300 - Train Loss: 0.0757, Val Loss: 0.0762\n",
      "Epoch 191/300 - Train Loss: 0.0764, Val Loss: 0.0765\n",
      "Epoch 192/300 - Train Loss: 0.0737, Val Loss: 0.0752\n",
      "Epoch 193/300 - Train Loss: 0.0749, Val Loss: 0.0759\n",
      "Epoch 194/300 - Train Loss: 0.0747, Val Loss: 0.0758\n",
      "Epoch 195/300 - Train Loss: 0.0752, Val Loss: 0.0765\n",
      "Epoch 196/300 - Train Loss: 0.0745, Val Loss: 0.0759\n",
      "Epoch 197/300 - Train Loss: 0.0740, Val Loss: 0.0768\n",
      "Epoch 198/300 - Train Loss: 0.0748, Val Loss: 0.0751\n",
      "Epoch 199/300 - Train Loss: 0.0751, Val Loss: 0.0766\n",
      "Epoch 200/300 - Train Loss: 0.0742, Val Loss: 0.0770\n",
      "Epoch 201/300 - Train Loss: 0.0745, Val Loss: 0.0765\n",
      "Epoch 202/300 - Train Loss: 0.0752, Val Loss: 0.0751\n",
      "Epoch 203/300 - Train Loss: 0.0751, Val Loss: 0.0748\n",
      "Epoch 204/300 - Train Loss: 0.0742, Val Loss: 0.0765\n",
      "Epoch 205/300 - Train Loss: 0.0736, Val Loss: 0.0749\n",
      "Epoch 206/300 - Train Loss: 0.0735, Val Loss: 0.0765\n",
      "Epoch 207/300 - Train Loss: 0.0757, Val Loss: 0.0760\n",
      "Epoch 208/300 - Train Loss: 0.0740, Val Loss: 0.0760\n",
      "Epoch 209/300 - Train Loss: 0.0733, Val Loss: 0.0766\n",
      "Epoch 210/300 - Train Loss: 0.0730, Val Loss: 0.0744\n",
      "Epoch 211/300 - Train Loss: 0.0758, Val Loss: 0.0763\n",
      "Epoch 212/300 - Train Loss: 0.0735, Val Loss: 0.0756\n",
      "Epoch 213/300 - Train Loss: 0.0730, Val Loss: 0.0755\n",
      "Epoch 214/300 - Train Loss: 0.0741, Val Loss: 0.0748\n",
      "Epoch 215/300 - Train Loss: 0.0762, Val Loss: 0.0749\n",
      "Epoch 216/300 - Train Loss: 0.0742, Val Loss: 0.0754\n",
      "Epoch 217/300 - Train Loss: 0.0732, Val Loss: 0.0749\n",
      "Epoch 218/300 - Train Loss: 0.0732, Val Loss: 0.0744\n",
      "Epoch 219/300 - Train Loss: 0.0745, Val Loss: 0.0753\n",
      "Epoch 220/300 - Train Loss: 0.0750, Val Loss: 0.0750\n",
      "Epoch 221/300 - Train Loss: 0.0748, Val Loss: 0.0763\n",
      "Epoch 222/300 - Train Loss: 0.0718, Val Loss: 0.0755\n",
      "Epoch 223/300 - Train Loss: 0.0742, Val Loss: 0.0760\n",
      "Epoch 224/300 - Train Loss: 0.0754, Val Loss: 0.0752\n",
      "Epoch 225/300 - Train Loss: 0.0742, Val Loss: 0.0753\n",
      "Epoch 226/300 - Train Loss: 0.0735, Val Loss: 0.0739\n",
      "Epoch 227/300 - Train Loss: 0.0756, Val Loss: 0.0786\n",
      "Epoch 228/300 - Train Loss: 0.0736, Val Loss: 0.0743\n",
      "Epoch 229/300 - Train Loss: 0.0730, Val Loss: 0.0755\n",
      "Epoch 230/300 - Train Loss: 0.0724, Val Loss: 0.0758\n",
      "Epoch 231/300 - Train Loss: 0.0737, Val Loss: 0.0748\n",
      "Epoch 232/300 - Train Loss: 0.0716, Val Loss: 0.0749\n",
      "Epoch 233/300 - Train Loss: 0.0733, Val Loss: 0.0776\n",
      "Epoch 234/300 - Train Loss: 0.0719, Val Loss: 0.0766\n",
      "Epoch 235/300 - Train Loss: 0.0737, Val Loss: 0.0762\n",
      "Epoch 236/300 - Train Loss: 0.0728, Val Loss: 0.0743\n",
      "Epoch 237/300 - Train Loss: 0.0727, Val Loss: 0.0743\n",
      "Epoch 238/300 - Train Loss: 0.0719, Val Loss: 0.0744\n",
      "Epoch 239/300 - Train Loss: 0.0733, Val Loss: 0.0753\n",
      "Epoch 240/300 - Train Loss: 0.0716, Val Loss: 0.0762\n",
      "Epoch 241/300 - Train Loss: 0.0723, Val Loss: 0.0776\n",
      "Epoch 242/300 - Train Loss: 0.0738, Val Loss: 0.0743\n",
      "Epoch 243/300 - Train Loss: 0.0721, Val Loss: 0.0743\n",
      "Epoch 244/300 - Train Loss: 0.0715, Val Loss: 0.0737\n",
      "Epoch 245/300 - Train Loss: 0.0728, Val Loss: 0.0747\n",
      "Epoch 246/300 - Train Loss: 0.0711, Val Loss: 0.0745\n",
      "Epoch 247/300 - Train Loss: 0.0733, Val Loss: 0.0750\n",
      "Epoch 248/300 - Train Loss: 0.0726, Val Loss: 0.0740\n",
      "Epoch 249/300 - Train Loss: 0.0713, Val Loss: 0.0738\n",
      "Epoch 250/300 - Train Loss: 0.0727, Val Loss: 0.0753\n",
      "Epoch 251/300 - Train Loss: 0.0719, Val Loss: 0.0738\n",
      "Epoch 252/300 - Train Loss: 0.0734, Val Loss: 0.0740\n",
      "Epoch 253/300 - Train Loss: 0.0721, Val Loss: 0.0742\n",
      "Epoch 254/300 - Train Loss: 0.0720, Val Loss: 0.0756\n",
      "Epoch 255/300 - Train Loss: 0.0707, Val Loss: 0.0745\n",
      "Epoch 256/300 - Train Loss: 0.0714, Val Loss: 0.0764\n",
      "Epoch 257/300 - Train Loss: 0.0720, Val Loss: 0.0747\n",
      "Epoch 258/300 - Train Loss: 0.0715, Val Loss: 0.0744\n",
      "Epoch 259/300 - Train Loss: 0.0721, Val Loss: 0.0750\n",
      "Epoch 260/300 - Train Loss: 0.0721, Val Loss: 0.0738\n",
      "Epoch 261/300 - Train Loss: 0.0720, Val Loss: 0.0760\n",
      "Epoch 262/300 - Train Loss: 0.0718, Val Loss: 0.0760\n",
      "Epoch 263/300 - Train Loss: 0.0715, Val Loss: 0.0745\n",
      "Epoch 264/300 - Train Loss: 0.0709, Val Loss: 0.0740\n",
      "Epoch 265/300 - Train Loss: 0.0734, Val Loss: 0.0743\n",
      "Epoch 266/300 - Train Loss: 0.0730, Val Loss: 0.0762\n",
      "Epoch 267/300 - Train Loss: 0.0709, Val Loss: 0.0770\n",
      "Epoch 268/300 - Train Loss: 0.0711, Val Loss: 0.0758\n",
      "Epoch 269/300 - Train Loss: 0.0721, Val Loss: 0.0766\n",
      "Epoch 270/300 - Train Loss: 0.0716, Val Loss: 0.0729\n",
      "Epoch 271/300 - Train Loss: 0.0699, Val Loss: 0.0744\n",
      "Epoch 272/300 - Train Loss: 0.0716, Val Loss: 0.0743\n",
      "Epoch 273/300 - Train Loss: 0.0723, Val Loss: 0.0739\n",
      "Epoch 274/300 - Train Loss: 0.0713, Val Loss: 0.0769\n",
      "Epoch 275/300 - Train Loss: 0.0712, Val Loss: 0.0760\n",
      "Epoch 276/300 - Train Loss: 0.0713, Val Loss: 0.0735\n",
      "Epoch 277/300 - Train Loss: 0.0714, Val Loss: 0.0738\n",
      "Epoch 278/300 - Train Loss: 0.0709, Val Loss: 0.0723\n",
      "Epoch 279/300 - Train Loss: 0.0705, Val Loss: 0.0743\n",
      "Epoch 280/300 - Train Loss: 0.0726, Val Loss: 0.0756\n",
      "Epoch 281/300 - Train Loss: 0.0698, Val Loss: 0.0733\n",
      "Epoch 282/300 - Train Loss: 0.0704, Val Loss: 0.0745\n",
      "Epoch 283/300 - Train Loss: 0.0712, Val Loss: 0.0754\n",
      "Epoch 284/300 - Train Loss: 0.0709, Val Loss: 0.0730\n",
      "Epoch 285/300 - Train Loss: 0.0703, Val Loss: 0.0754\n",
      "Epoch 286/300 - Train Loss: 0.0714, Val Loss: 0.0736\n",
      "Epoch 287/300 - Train Loss: 0.0730, Val Loss: 0.0752\n",
      "Epoch 288/300 - Train Loss: 0.0719, Val Loss: 0.0753\n",
      "Epoch 289/300 - Train Loss: 0.0714, Val Loss: 0.0744\n",
      "Epoch 290/300 - Train Loss: 0.0701, Val Loss: 0.0731\n",
      "Epoch 291/300 - Train Loss: 0.0698, Val Loss: 0.0758\n",
      "Epoch 292/300 - Train Loss: 0.0706, Val Loss: 0.0738\n",
      "Epoch 293/300 - Train Loss: 0.0715, Val Loss: 0.0748\n",
      "Epoch 294/300 - Train Loss: 0.0711, Val Loss: 0.0735\n",
      "Epoch 295/300 - Train Loss: 0.0720, Val Loss: 0.0743\n",
      "Epoch 296/300 - Train Loss: 0.0693, Val Loss: 0.0744\n",
      "Epoch 297/300 - Train Loss: 0.0705, Val Loss: 0.0741\n",
      "Epoch 298/300 - Train Loss: 0.0722, Val Loss: 0.0759\n",
      "Epoch 299/300 - Train Loss: 0.0704, Val Loss: 0.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 01:27:03,235] Trial 37 finished with value: 0.9683620485637677 and parameters: {'F1': 8, 'F2': 16, 'D': 4, 'dropout': 0.15773428581167293, 'learning_rate': 1.3068005412856696e-05, 'batch_size': 64, 'weight_decay': 9.586196343161078e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/300 - Train Loss: 0.0702, Val Loss: 0.0737\n",
      "Macro F1 Score: 0.9684, Macro Precision: 0.9566, Macro Recall: 0.9816\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 39\n",
      "Training with F1=8, F2=8, D=4, dropout=0.14762836162008836, LR=1.2521608563647914e-05, BS=64, WD=9.524880237840324e-05\n",
      "Epoch 1/300 - Train Loss: 1.0425, Val Loss: 0.9303\n",
      "Epoch 2/300 - Train Loss: 0.8004, Val Loss: 0.6736\n",
      "Epoch 3/300 - Train Loss: 0.5822, Val Loss: 0.5045\n",
      "Epoch 4/300 - Train Loss: 0.4536, Val Loss: 0.4016\n",
      "Epoch 5/300 - Train Loss: 0.3783, Val Loss: 0.3464\n",
      "Epoch 6/300 - Train Loss: 0.3295, Val Loss: 0.3052\n",
      "Epoch 7/300 - Train Loss: 0.3002, Val Loss: 0.2802\n",
      "Epoch 8/300 - Train Loss: 0.2754, Val Loss: 0.2592\n",
      "Epoch 9/300 - Train Loss: 0.2595, Val Loss: 0.2413\n",
      "Epoch 10/300 - Train Loss: 0.2459, Val Loss: 0.2326\n",
      "Epoch 11/300 - Train Loss: 0.2356, Val Loss: 0.2218\n",
      "Epoch 12/300 - Train Loss: 0.2284, Val Loss: 0.2114\n",
      "Epoch 13/300 - Train Loss: 0.2186, Val Loss: 0.2016\n",
      "Epoch 14/300 - Train Loss: 0.2092, Val Loss: 0.1921\n",
      "Epoch 15/300 - Train Loss: 0.2025, Val Loss: 0.1807\n",
      "Epoch 16/300 - Train Loss: 0.1933, Val Loss: 0.1740\n",
      "Epoch 17/300 - Train Loss: 0.1848, Val Loss: 0.1709\n",
      "Epoch 18/300 - Train Loss: 0.1773, Val Loss: 0.1568\n",
      "Epoch 19/300 - Train Loss: 0.1684, Val Loss: 0.1455\n",
      "Epoch 20/300 - Train Loss: 0.1603, Val Loss: 0.1383\n",
      "Epoch 21/300 - Train Loss: 0.1526, Val Loss: 0.1318\n",
      "Epoch 22/300 - Train Loss: 0.1467, Val Loss: 0.1273\n",
      "Epoch 23/300 - Train Loss: 0.1397, Val Loss: 0.1211\n",
      "Epoch 24/300 - Train Loss: 0.1353, Val Loss: 0.1157\n",
      "Epoch 25/300 - Train Loss: 0.1307, Val Loss: 0.1128\n",
      "Epoch 26/300 - Train Loss: 0.1259, Val Loss: 0.1094\n",
      "Epoch 27/300 - Train Loss: 0.1242, Val Loss: 0.1057\n",
      "Epoch 28/300 - Train Loss: 0.1197, Val Loss: 0.1035\n",
      "Epoch 29/300 - Train Loss: 0.1178, Val Loss: 0.1014\n",
      "Epoch 30/300 - Train Loss: 0.1169, Val Loss: 0.0979\n",
      "Epoch 31/300 - Train Loss: 0.1141, Val Loss: 0.0958\n",
      "Epoch 32/300 - Train Loss: 0.1100, Val Loss: 0.0974\n",
      "Epoch 33/300 - Train Loss: 0.1109, Val Loss: 0.0945\n",
      "Epoch 34/300 - Train Loss: 0.1100, Val Loss: 0.0941\n",
      "Epoch 35/300 - Train Loss: 0.1088, Val Loss: 0.0937\n",
      "Epoch 36/300 - Train Loss: 0.1078, Val Loss: 0.0922\n",
      "Epoch 37/300 - Train Loss: 0.1047, Val Loss: 0.0920\n",
      "Epoch 38/300 - Train Loss: 0.1057, Val Loss: 0.0895\n",
      "Epoch 39/300 - Train Loss: 0.1052, Val Loss: 0.0898\n",
      "Epoch 40/300 - Train Loss: 0.1036, Val Loss: 0.0897\n",
      "Epoch 41/300 - Train Loss: 0.1042, Val Loss: 0.0884\n",
      "Epoch 42/300 - Train Loss: 0.1016, Val Loss: 0.0872\n",
      "Epoch 43/300 - Train Loss: 0.1016, Val Loss: 0.0881\n",
      "Epoch 44/300 - Train Loss: 0.0994, Val Loss: 0.0873\n",
      "Epoch 45/300 - Train Loss: 0.1004, Val Loss: 0.0878\n",
      "Epoch 46/300 - Train Loss: 0.0992, Val Loss: 0.0865\n",
      "Epoch 47/300 - Train Loss: 0.0996, Val Loss: 0.0856\n",
      "Epoch 48/300 - Train Loss: 0.1003, Val Loss: 0.0855\n",
      "Epoch 49/300 - Train Loss: 0.0981, Val Loss: 0.0863\n",
      "Epoch 50/300 - Train Loss: 0.0998, Val Loss: 0.0836\n",
      "Epoch 51/300 - Train Loss: 0.0996, Val Loss: 0.0833\n",
      "Epoch 52/300 - Train Loss: 0.0981, Val Loss: 0.0840\n",
      "Epoch 53/300 - Train Loss: 0.0980, Val Loss: 0.0859\n",
      "Epoch 54/300 - Train Loss: 0.0970, Val Loss: 0.0848\n",
      "Epoch 55/300 - Train Loss: 0.0979, Val Loss: 0.0839\n",
      "Epoch 56/300 - Train Loss: 0.0993, Val Loss: 0.0851\n",
      "Epoch 57/300 - Train Loss: 0.0954, Val Loss: 0.0839\n",
      "Epoch 58/300 - Train Loss: 0.0971, Val Loss: 0.0831\n",
      "Epoch 59/300 - Train Loss: 0.0953, Val Loss: 0.0845\n",
      "Epoch 60/300 - Train Loss: 0.0945, Val Loss: 0.0841\n",
      "Epoch 61/300 - Train Loss: 0.0955, Val Loss: 0.0838\n",
      "Epoch 62/300 - Train Loss: 0.0945, Val Loss: 0.0831\n",
      "Epoch 63/300 - Train Loss: 0.0948, Val Loss: 0.0820\n",
      "Epoch 64/300 - Train Loss: 0.0958, Val Loss: 0.0819\n",
      "Epoch 65/300 - Train Loss: 0.0942, Val Loss: 0.0839\n",
      "Epoch 66/300 - Train Loss: 0.0943, Val Loss: 0.0829\n",
      "Epoch 67/300 - Train Loss: 0.0940, Val Loss: 0.0816\n",
      "Epoch 68/300 - Train Loss: 0.0958, Val Loss: 0.0820\n",
      "Epoch 69/300 - Train Loss: 0.0963, Val Loss: 0.0828\n",
      "Epoch 70/300 - Train Loss: 0.0947, Val Loss: 0.0816\n",
      "Epoch 71/300 - Train Loss: 0.0934, Val Loss: 0.0806\n",
      "Epoch 72/300 - Train Loss: 0.0949, Val Loss: 0.0822\n",
      "Epoch 73/300 - Train Loss: 0.0950, Val Loss: 0.0819\n",
      "Epoch 74/300 - Train Loss: 0.0947, Val Loss: 0.0810\n",
      "Epoch 75/300 - Train Loss: 0.0928, Val Loss: 0.0826\n",
      "Epoch 76/300 - Train Loss: 0.0924, Val Loss: 0.0824\n",
      "Epoch 77/300 - Train Loss: 0.0923, Val Loss: 0.0822\n",
      "Epoch 78/300 - Train Loss: 0.0942, Val Loss: 0.0814\n",
      "Epoch 79/300 - Train Loss: 0.0929, Val Loss: 0.0816\n",
      "Epoch 80/300 - Train Loss: 0.0927, Val Loss: 0.0811\n",
      "Epoch 81/300 - Train Loss: 0.0919, Val Loss: 0.0818\n",
      "Epoch 82/300 - Train Loss: 0.0913, Val Loss: 0.0800\n",
      "Epoch 83/300 - Train Loss: 0.0933, Val Loss: 0.0805\n",
      "Epoch 84/300 - Train Loss: 0.0919, Val Loss: 0.0813\n",
      "Epoch 85/300 - Train Loss: 0.0914, Val Loss: 0.0806\n",
      "Epoch 86/300 - Train Loss: 0.0918, Val Loss: 0.0806\n",
      "Epoch 87/300 - Train Loss: 0.0929, Val Loss: 0.0826\n",
      "Epoch 88/300 - Train Loss: 0.0913, Val Loss: 0.0819\n",
      "Epoch 89/300 - Train Loss: 0.0915, Val Loss: 0.0807\n",
      "Epoch 90/300 - Train Loss: 0.0893, Val Loss: 0.0815\n",
      "Epoch 91/300 - Train Loss: 0.0904, Val Loss: 0.0798\n",
      "Epoch 92/300 - Train Loss: 0.0900, Val Loss: 0.0815\n",
      "Epoch 93/300 - Train Loss: 0.0910, Val Loss: 0.0797\n",
      "Epoch 94/300 - Train Loss: 0.0888, Val Loss: 0.0805\n",
      "Epoch 95/300 - Train Loss: 0.0893, Val Loss: 0.0800\n",
      "Epoch 96/300 - Train Loss: 0.0898, Val Loss: 0.0802\n",
      "Epoch 97/300 - Train Loss: 0.0895, Val Loss: 0.0802\n",
      "Epoch 98/300 - Train Loss: 0.0903, Val Loss: 0.0800\n",
      "Epoch 99/300 - Train Loss: 0.0893, Val Loss: 0.0796\n",
      "Epoch 100/300 - Train Loss: 0.0906, Val Loss: 0.0784\n",
      "Epoch 101/300 - Train Loss: 0.0889, Val Loss: 0.0792\n",
      "Epoch 102/300 - Train Loss: 0.0902, Val Loss: 0.0801\n",
      "Epoch 103/300 - Train Loss: 0.0889, Val Loss: 0.0789\n",
      "Epoch 104/300 - Train Loss: 0.0895, Val Loss: 0.0792\n",
      "Epoch 105/300 - Train Loss: 0.0889, Val Loss: 0.0792\n",
      "Epoch 106/300 - Train Loss: 0.0883, Val Loss: 0.0793\n",
      "Epoch 107/300 - Train Loss: 0.0898, Val Loss: 0.0792\n",
      "Epoch 108/300 - Train Loss: 0.0889, Val Loss: 0.0797\n",
      "Epoch 109/300 - Train Loss: 0.0869, Val Loss: 0.0809\n",
      "Epoch 110/300 - Train Loss: 0.0877, Val Loss: 0.0795\n",
      "Epoch 111/300 - Train Loss: 0.0900, Val Loss: 0.0797\n",
      "Epoch 112/300 - Train Loss: 0.0885, Val Loss: 0.0803\n",
      "Epoch 113/300 - Train Loss: 0.0883, Val Loss: 0.0791\n",
      "Epoch 114/300 - Train Loss: 0.0877, Val Loss: 0.0789\n",
      "Epoch 115/300 - Train Loss: 0.0891, Val Loss: 0.0805\n",
      "Epoch 116/300 - Train Loss: 0.0867, Val Loss: 0.0800\n",
      "Epoch 117/300 - Train Loss: 0.0864, Val Loss: 0.0788\n",
      "Epoch 118/300 - Train Loss: 0.0885, Val Loss: 0.0791\n",
      "Epoch 119/300 - Train Loss: 0.0879, Val Loss: 0.0794\n",
      "Epoch 120/300 - Train Loss: 0.0895, Val Loss: 0.0784\n",
      "Epoch 121/300 - Train Loss: 0.0876, Val Loss: 0.0793\n",
      "Epoch 122/300 - Train Loss: 0.0875, Val Loss: 0.0783\n",
      "Epoch 123/300 - Train Loss: 0.0864, Val Loss: 0.0786\n",
      "Epoch 124/300 - Train Loss: 0.0892, Val Loss: 0.0786\n",
      "Epoch 125/300 - Train Loss: 0.0872, Val Loss: 0.0782\n",
      "Epoch 126/300 - Train Loss: 0.0869, Val Loss: 0.0787\n",
      "Epoch 127/300 - Train Loss: 0.0856, Val Loss: 0.0784\n",
      "Epoch 128/300 - Train Loss: 0.0868, Val Loss: 0.0783\n",
      "Epoch 129/300 - Train Loss: 0.0858, Val Loss: 0.0787\n",
      "Epoch 130/300 - Train Loss: 0.0855, Val Loss: 0.0788\n",
      "Epoch 131/300 - Train Loss: 0.0875, Val Loss: 0.0792\n",
      "Epoch 132/300 - Train Loss: 0.0869, Val Loss: 0.0792\n",
      "Epoch 133/300 - Train Loss: 0.0872, Val Loss: 0.0788\n",
      "Epoch 134/300 - Train Loss: 0.0867, Val Loss: 0.0785\n",
      "Epoch 135/300 - Train Loss: 0.0879, Val Loss: 0.0770\n",
      "Epoch 136/300 - Train Loss: 0.0871, Val Loss: 0.0775\n",
      "Epoch 137/300 - Train Loss: 0.0856, Val Loss: 0.0776\n",
      "Epoch 138/300 - Train Loss: 0.0862, Val Loss: 0.0773\n",
      "Epoch 139/300 - Train Loss: 0.0879, Val Loss: 0.0800\n",
      "Epoch 140/300 - Train Loss: 0.0842, Val Loss: 0.0815\n",
      "Epoch 141/300 - Train Loss: 0.0839, Val Loss: 0.0775\n",
      "Epoch 142/300 - Train Loss: 0.0856, Val Loss: 0.0777\n",
      "Epoch 143/300 - Train Loss: 0.0849, Val Loss: 0.0789\n",
      "Epoch 144/300 - Train Loss: 0.0866, Val Loss: 0.0791\n",
      "Epoch 145/300 - Train Loss: 0.0870, Val Loss: 0.0776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/300 - Train Loss: 0.0860, Val Loss: 0.0783\n",
      "Epoch 147/300 - Train Loss: 0.0855, Val Loss: 0.0776\n",
      "Epoch 148/300 - Train Loss: 0.0843, Val Loss: 0.0787\n",
      "Epoch 149/300 - Train Loss: 0.0842, Val Loss: 0.0775\n",
      "Epoch 150/300 - Train Loss: 0.0852, Val Loss: 0.0778\n",
      "Epoch 151/300 - Train Loss: 0.0855, Val Loss: 0.0775\n",
      "Epoch 152/300 - Train Loss: 0.0860, Val Loss: 0.0778\n",
      "Epoch 153/300 - Train Loss: 0.0853, Val Loss: 0.0774\n",
      "Epoch 154/300 - Train Loss: 0.0833, Val Loss: 0.0795\n",
      "Epoch 155/300 - Train Loss: 0.0845, Val Loss: 0.0773\n",
      "Epoch 156/300 - Train Loss: 0.0847, Val Loss: 0.0788\n",
      "Epoch 157/300 - Train Loss: 0.0840, Val Loss: 0.0776\n",
      "Epoch 158/300 - Train Loss: 0.0834, Val Loss: 0.0787\n",
      "Epoch 159/300 - Train Loss: 0.0839, Val Loss: 0.0769\n",
      "Epoch 160/300 - Train Loss: 0.0838, Val Loss: 0.0771\n",
      "Epoch 161/300 - Train Loss: 0.0850, Val Loss: 0.0774\n",
      "Epoch 162/300 - Train Loss: 0.0833, Val Loss: 0.0772\n",
      "Epoch 163/300 - Train Loss: 0.0835, Val Loss: 0.0780\n",
      "Epoch 164/300 - Train Loss: 0.0845, Val Loss: 0.0779\n",
      "Epoch 165/300 - Train Loss: 0.0836, Val Loss: 0.0780\n",
      "Epoch 166/300 - Train Loss: 0.0838, Val Loss: 0.0768\n",
      "Epoch 167/300 - Train Loss: 0.0836, Val Loss: 0.0784\n",
      "Epoch 168/300 - Train Loss: 0.0853, Val Loss: 0.0771\n",
      "Epoch 169/300 - Train Loss: 0.0831, Val Loss: 0.0779\n",
      "Epoch 170/300 - Train Loss: 0.0838, Val Loss: 0.0785\n",
      "Epoch 171/300 - Train Loss: 0.0835, Val Loss: 0.0779\n",
      "Epoch 172/300 - Train Loss: 0.0849, Val Loss: 0.0784\n",
      "Epoch 173/300 - Train Loss: 0.0824, Val Loss: 0.0778\n",
      "Epoch 174/300 - Train Loss: 0.0839, Val Loss: 0.0777\n",
      "Epoch 175/300 - Train Loss: 0.0823, Val Loss: 0.0771\n",
      "Epoch 176/300 - Train Loss: 0.0833, Val Loss: 0.0772\n",
      "Epoch 177/300 - Train Loss: 0.0827, Val Loss: 0.0768\n",
      "Epoch 178/300 - Train Loss: 0.0833, Val Loss: 0.0760\n",
      "Epoch 179/300 - Train Loss: 0.0831, Val Loss: 0.0774\n",
      "Epoch 180/300 - Train Loss: 0.0834, Val Loss: 0.0763\n",
      "Epoch 181/300 - Train Loss: 0.0839, Val Loss: 0.0773\n",
      "Epoch 182/300 - Train Loss: 0.0834, Val Loss: 0.0778\n",
      "Epoch 183/300 - Train Loss: 0.0823, Val Loss: 0.0781\n",
      "Epoch 184/300 - Train Loss: 0.0832, Val Loss: 0.0775\n",
      "Epoch 185/300 - Train Loss: 0.0827, Val Loss: 0.0774\n",
      "Epoch 186/300 - Train Loss: 0.0816, Val Loss: 0.0756\n",
      "Epoch 187/300 - Train Loss: 0.0820, Val Loss: 0.0779\n",
      "Epoch 188/300 - Train Loss: 0.0819, Val Loss: 0.0768\n",
      "Epoch 189/300 - Train Loss: 0.0834, Val Loss: 0.0767\n",
      "Epoch 190/300 - Train Loss: 0.0823, Val Loss: 0.0766\n",
      "Epoch 191/300 - Train Loss: 0.0821, Val Loss: 0.0765\n",
      "Epoch 192/300 - Train Loss: 0.0813, Val Loss: 0.0766\n",
      "Epoch 193/300 - Train Loss: 0.0814, Val Loss: 0.0757\n",
      "Epoch 194/300 - Train Loss: 0.0819, Val Loss: 0.0766\n",
      "Epoch 195/300 - Train Loss: 0.0819, Val Loss: 0.0759\n",
      "Epoch 196/300 - Train Loss: 0.0823, Val Loss: 0.0761\n",
      "Epoch 197/300 - Train Loss: 0.0806, Val Loss: 0.0781\n",
      "Epoch 198/300 - Train Loss: 0.0828, Val Loss: 0.0768\n",
      "Epoch 199/300 - Train Loss: 0.0816, Val Loss: 0.0767\n",
      "Epoch 200/300 - Train Loss: 0.0811, Val Loss: 0.0772\n",
      "Epoch 201/300 - Train Loss: 0.0811, Val Loss: 0.0784\n",
      "Epoch 202/300 - Train Loss: 0.0816, Val Loss: 0.0761\n",
      "Epoch 203/300 - Train Loss: 0.0820, Val Loss: 0.0760\n",
      "Epoch 204/300 - Train Loss: 0.0813, Val Loss: 0.0757\n",
      "Epoch 205/300 - Train Loss: 0.0809, Val Loss: 0.0763\n",
      "Epoch 206/300 - Train Loss: 0.0806, Val Loss: 0.0762\n",
      "Epoch 207/300 - Train Loss: 0.0822, Val Loss: 0.0766\n",
      "Epoch 208/300 - Train Loss: 0.0804, Val Loss: 0.0771\n",
      "Epoch 209/300 - Train Loss: 0.0803, Val Loss: 0.0762\n",
      "Epoch 210/300 - Train Loss: 0.0816, Val Loss: 0.0784\n",
      "Epoch 211/300 - Train Loss: 0.0804, Val Loss: 0.0786\n",
      "Epoch 212/300 - Train Loss: 0.0805, Val Loss: 0.0766\n",
      "Epoch 213/300 - Train Loss: 0.0802, Val Loss: 0.0755\n",
      "Epoch 214/300 - Train Loss: 0.0808, Val Loss: 0.0772\n",
      "Epoch 215/300 - Train Loss: 0.0809, Val Loss: 0.0773\n",
      "Epoch 216/300 - Train Loss: 0.0810, Val Loss: 0.0766\n",
      "Epoch 217/300 - Train Loss: 0.0802, Val Loss: 0.0763\n",
      "Epoch 218/300 - Train Loss: 0.0803, Val Loss: 0.0762\n",
      "Epoch 219/300 - Train Loss: 0.0800, Val Loss: 0.0760\n",
      "Epoch 220/300 - Train Loss: 0.0804, Val Loss: 0.0765\n",
      "Epoch 221/300 - Train Loss: 0.0777, Val Loss: 0.0761\n",
      "Epoch 222/300 - Train Loss: 0.0816, Val Loss: 0.0759\n",
      "Epoch 223/300 - Train Loss: 0.0797, Val Loss: 0.0767\n",
      "Epoch 224/300 - Train Loss: 0.0802, Val Loss: 0.0782\n",
      "Epoch 225/300 - Train Loss: 0.0803, Val Loss: 0.0747\n",
      "Epoch 226/300 - Train Loss: 0.0801, Val Loss: 0.0769\n",
      "Epoch 227/300 - Train Loss: 0.0803, Val Loss: 0.0754\n",
      "Epoch 228/300 - Train Loss: 0.0802, Val Loss: 0.0763\n",
      "Epoch 229/300 - Train Loss: 0.0801, Val Loss: 0.0754\n",
      "Epoch 230/300 - Train Loss: 0.0787, Val Loss: 0.0756\n",
      "Epoch 231/300 - Train Loss: 0.0809, Val Loss: 0.0751\n",
      "Epoch 232/300 - Train Loss: 0.0786, Val Loss: 0.0760\n",
      "Epoch 233/300 - Train Loss: 0.0793, Val Loss: 0.0754\n",
      "Epoch 234/300 - Train Loss: 0.0803, Val Loss: 0.0752\n",
      "Epoch 235/300 - Train Loss: 0.0790, Val Loss: 0.0753\n",
      "Epoch 236/300 - Train Loss: 0.0782, Val Loss: 0.0766\n",
      "Epoch 237/300 - Train Loss: 0.0778, Val Loss: 0.0761\n",
      "Epoch 238/300 - Train Loss: 0.0803, Val Loss: 0.0761\n",
      "Epoch 239/300 - Train Loss: 0.0807, Val Loss: 0.0767\n",
      "Epoch 240/300 - Train Loss: 0.0800, Val Loss: 0.0766\n",
      "Epoch 241/300 - Train Loss: 0.0781, Val Loss: 0.0757\n",
      "Epoch 242/300 - Train Loss: 0.0798, Val Loss: 0.0758\n",
      "Epoch 243/300 - Train Loss: 0.0791, Val Loss: 0.0769\n",
      "Epoch 244/300 - Train Loss: 0.0805, Val Loss: 0.0762\n",
      "Epoch 245/300 - Train Loss: 0.0795, Val Loss: 0.0774\n",
      "Epoch 246/300 - Train Loss: 0.0800, Val Loss: 0.0764\n",
      "Epoch 247/300 - Train Loss: 0.0799, Val Loss: 0.0760\n",
      "Epoch 248/300 - Train Loss: 0.0786, Val Loss: 0.0747\n",
      "Epoch 249/300 - Train Loss: 0.0790, Val Loss: 0.0745\n",
      "Epoch 250/300 - Train Loss: 0.0781, Val Loss: 0.0759\n",
      "Epoch 251/300 - Train Loss: 0.0789, Val Loss: 0.0760\n",
      "Epoch 252/300 - Train Loss: 0.0783, Val Loss: 0.0762\n",
      "Epoch 253/300 - Train Loss: 0.0794, Val Loss: 0.0756\n",
      "Epoch 254/300 - Train Loss: 0.0786, Val Loss: 0.0750\n",
      "Epoch 255/300 - Train Loss: 0.0786, Val Loss: 0.0761\n",
      "Epoch 256/300 - Train Loss: 0.0794, Val Loss: 0.0758\n",
      "Epoch 257/300 - Train Loss: 0.0769, Val Loss: 0.0767\n",
      "Epoch 258/300 - Train Loss: 0.0775, Val Loss: 0.0752\n",
      "Epoch 259/300 - Train Loss: 0.0785, Val Loss: 0.0754\n",
      "Epoch 260/300 - Train Loss: 0.0791, Val Loss: 0.0749\n",
      "Epoch 261/300 - Train Loss: 0.0801, Val Loss: 0.0754\n",
      "Epoch 262/300 - Train Loss: 0.0779, Val Loss: 0.0759\n",
      "Epoch 263/300 - Train Loss: 0.0774, Val Loss: 0.0751\n",
      "Epoch 264/300 - Train Loss: 0.0780, Val Loss: 0.0757\n",
      "Epoch 265/300 - Train Loss: 0.0786, Val Loss: 0.0754\n",
      "Epoch 266/300 - Train Loss: 0.0770, Val Loss: 0.0756\n",
      "Epoch 267/300 - Train Loss: 0.0771, Val Loss: 0.0775\n",
      "Epoch 268/300 - Train Loss: 0.0757, Val Loss: 0.0756\n",
      "Epoch 269/300 - Train Loss: 0.0780, Val Loss: 0.0758\n",
      "Epoch 270/300 - Train Loss: 0.0784, Val Loss: 0.0752\n",
      "Epoch 271/300 - Train Loss: 0.0789, Val Loss: 0.0751\n",
      "Epoch 272/300 - Train Loss: 0.0773, Val Loss: 0.0743\n",
      "Epoch 273/300 - Train Loss: 0.0765, Val Loss: 0.0759\n",
      "Epoch 274/300 - Train Loss: 0.0778, Val Loss: 0.0745\n",
      "Epoch 275/300 - Train Loss: 0.0753, Val Loss: 0.0751\n",
      "Epoch 276/300 - Train Loss: 0.0777, Val Loss: 0.0749\n",
      "Epoch 277/300 - Train Loss: 0.0768, Val Loss: 0.0756\n",
      "Epoch 278/300 - Train Loss: 0.0781, Val Loss: 0.0771\n",
      "Epoch 279/300 - Train Loss: 0.0753, Val Loss: 0.0740\n",
      "Epoch 280/300 - Train Loss: 0.0774, Val Loss: 0.0776\n",
      "Epoch 281/300 - Train Loss: 0.0768, Val Loss: 0.0755\n",
      "Epoch 282/300 - Train Loss: 0.0772, Val Loss: 0.0745\n",
      "Epoch 283/300 - Train Loss: 0.0768, Val Loss: 0.0752\n",
      "Epoch 284/300 - Train Loss: 0.0758, Val Loss: 0.0752\n",
      "Epoch 285/300 - Train Loss: 0.0755, Val Loss: 0.0759\n",
      "Epoch 286/300 - Train Loss: 0.0773, Val Loss: 0.0746\n",
      "Epoch 287/300 - Train Loss: 0.0774, Val Loss: 0.0746\n",
      "Epoch 288/300 - Train Loss: 0.0770, Val Loss: 0.0751\n",
      "Epoch 289/300 - Train Loss: 0.0760, Val Loss: 0.0759\n",
      "Epoch 290/300 - Train Loss: 0.0759, Val Loss: 0.0738\n",
      "Epoch 291/300 - Train Loss: 0.0767, Val Loss: 0.0750\n",
      "Epoch 292/300 - Train Loss: 0.0752, Val Loss: 0.0743\n",
      "Epoch 293/300 - Train Loss: 0.0762, Val Loss: 0.0752\n",
      "Epoch 294/300 - Train Loss: 0.0772, Val Loss: 0.0759\n",
      "Epoch 295/300 - Train Loss: 0.0775, Val Loss: 0.0738\n",
      "Epoch 296/300 - Train Loss: 0.0755, Val Loss: 0.0746\n",
      "Epoch 297/300 - Train Loss: 0.0772, Val Loss: 0.0755\n",
      "Epoch 298/300 - Train Loss: 0.0769, Val Loss: 0.0731\n",
      "Epoch 299/300 - Train Loss: 0.0759, Val Loss: 0.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 01:33:46,748] Trial 38 finished with value: 0.9628287739680982 and parameters: {'F1': 8, 'F2': 8, 'D': 4, 'dropout': 0.14762836162008836, 'learning_rate': 1.2521608563647914e-05, 'batch_size': 64, 'weight_decay': 9.524880237840324e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/300 - Train Loss: 0.0758, Val Loss: 0.0744\n",
      "Macro F1 Score: 0.9628, Macro Precision: 0.9548, Macro Recall: 0.9715\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 40\n",
      "Training with F1=8, F2=16, D=4, dropout=0.15193653725695327, LR=1.0188791065699855e-05, BS=64, WD=0.00020705117260985486\n",
      "Epoch 1/300 - Train Loss: 0.9760, Val Loss: 0.8869\n",
      "Epoch 2/300 - Train Loss: 0.7793, Val Loss: 0.6853\n",
      "Epoch 3/300 - Train Loss: 0.5747, Val Loss: 0.4816\n",
      "Epoch 4/300 - Train Loss: 0.4212, Val Loss: 0.3605\n",
      "Epoch 5/300 - Train Loss: 0.3430, Val Loss: 0.3100\n",
      "Epoch 6/300 - Train Loss: 0.2996, Val Loss: 0.2656\n",
      "Epoch 7/300 - Train Loss: 0.2686, Val Loss: 0.2370\n",
      "Epoch 8/300 - Train Loss: 0.2478, Val Loss: 0.2170\n",
      "Epoch 9/300 - Train Loss: 0.2328, Val Loss: 0.2056\n",
      "Epoch 10/300 - Train Loss: 0.2213, Val Loss: 0.1940\n",
      "Epoch 11/300 - Train Loss: 0.2126, Val Loss: 0.1874\n",
      "Epoch 12/300 - Train Loss: 0.2025, Val Loss: 0.1765\n",
      "Epoch 13/300 - Train Loss: 0.1951, Val Loss: 0.1709\n",
      "Epoch 14/300 - Train Loss: 0.1885, Val Loss: 0.1621\n",
      "Epoch 15/300 - Train Loss: 0.1798, Val Loss: 0.1539\n",
      "Epoch 16/300 - Train Loss: 0.1731, Val Loss: 0.1501\n",
      "Epoch 17/300 - Train Loss: 0.1648, Val Loss: 0.1430\n",
      "Epoch 18/300 - Train Loss: 0.1602, Val Loss: 0.1379\n",
      "Epoch 19/300 - Train Loss: 0.1541, Val Loss: 0.1316\n",
      "Epoch 20/300 - Train Loss: 0.1474, Val Loss: 0.1276\n",
      "Epoch 21/300 - Train Loss: 0.1416, Val Loss: 0.1245\n",
      "Epoch 22/300 - Train Loss: 0.1380, Val Loss: 0.1199\n",
      "Epoch 23/300 - Train Loss: 0.1327, Val Loss: 0.1170\n",
      "Epoch 24/300 - Train Loss: 0.1288, Val Loss: 0.1128\n",
      "Epoch 25/300 - Train Loss: 0.1272, Val Loss: 0.1138\n",
      "Epoch 26/300 - Train Loss: 0.1224, Val Loss: 0.1079\n",
      "Epoch 27/300 - Train Loss: 0.1193, Val Loss: 0.1069\n",
      "Epoch 28/300 - Train Loss: 0.1185, Val Loss: 0.1066\n",
      "Epoch 29/300 - Train Loss: 0.1140, Val Loss: 0.1039\n",
      "Epoch 30/300 - Train Loss: 0.1133, Val Loss: 0.1018\n",
      "Epoch 31/300 - Train Loss: 0.1115, Val Loss: 0.1020\n",
      "Epoch 32/300 - Train Loss: 0.1115, Val Loss: 0.0975\n",
      "Epoch 33/300 - Train Loss: 0.1089, Val Loss: 0.0965\n",
      "Epoch 34/300 - Train Loss: 0.1087, Val Loss: 0.0972\n",
      "Epoch 35/300 - Train Loss: 0.1089, Val Loss: 0.0967\n",
      "Epoch 36/300 - Train Loss: 0.1048, Val Loss: 0.0949\n",
      "Epoch 37/300 - Train Loss: 0.1054, Val Loss: 0.0955\n",
      "Epoch 38/300 - Train Loss: 0.1029, Val Loss: 0.0927\n",
      "Epoch 39/300 - Train Loss: 0.1034, Val Loss: 0.0923\n",
      "Epoch 40/300 - Train Loss: 0.1015, Val Loss: 0.0920\n",
      "Epoch 41/300 - Train Loss: 0.1021, Val Loss: 0.0912\n",
      "Epoch 42/300 - Train Loss: 0.1016, Val Loss: 0.0918\n",
      "Epoch 43/300 - Train Loss: 0.1012, Val Loss: 0.0920\n",
      "Epoch 44/300 - Train Loss: 0.1019, Val Loss: 0.0885\n",
      "Epoch 45/300 - Train Loss: 0.1009, Val Loss: 0.0892\n",
      "Epoch 46/300 - Train Loss: 0.0991, Val Loss: 0.0893\n",
      "Epoch 47/300 - Train Loss: 0.0999, Val Loss: 0.0880\n",
      "Epoch 48/300 - Train Loss: 0.0980, Val Loss: 0.0864\n",
      "Epoch 49/300 - Train Loss: 0.0966, Val Loss: 0.0887\n",
      "Epoch 50/300 - Train Loss: 0.0964, Val Loss: 0.0880\n",
      "Epoch 51/300 - Train Loss: 0.0976, Val Loss: 0.0868\n",
      "Epoch 52/300 - Train Loss: 0.0962, Val Loss: 0.0852\n",
      "Epoch 53/300 - Train Loss: 0.0955, Val Loss: 0.0862\n",
      "Epoch 54/300 - Train Loss: 0.0955, Val Loss: 0.0852\n",
      "Epoch 55/300 - Train Loss: 0.0959, Val Loss: 0.0861\n",
      "Epoch 56/300 - Train Loss: 0.0948, Val Loss: 0.0867\n",
      "Epoch 57/300 - Train Loss: 0.0947, Val Loss: 0.0845\n",
      "Epoch 58/300 - Train Loss: 0.0960, Val Loss: 0.0856\n",
      "Epoch 59/300 - Train Loss: 0.0927, Val Loss: 0.0840\n",
      "Epoch 60/300 - Train Loss: 0.0938, Val Loss: 0.0841\n",
      "Epoch 61/300 - Train Loss: 0.0945, Val Loss: 0.0826\n",
      "Epoch 62/300 - Train Loss: 0.0930, Val Loss: 0.0829\n",
      "Epoch 63/300 - Train Loss: 0.0925, Val Loss: 0.0835\n",
      "Epoch 64/300 - Train Loss: 0.0924, Val Loss: 0.0854\n",
      "Epoch 65/300 - Train Loss: 0.0936, Val Loss: 0.0840\n",
      "Epoch 66/300 - Train Loss: 0.0915, Val Loss: 0.0831\n",
      "Epoch 67/300 - Train Loss: 0.0917, Val Loss: 0.0837\n",
      "Epoch 68/300 - Train Loss: 0.0914, Val Loss: 0.0836\n",
      "Epoch 69/300 - Train Loss: 0.0908, Val Loss: 0.0826\n",
      "Epoch 70/300 - Train Loss: 0.0909, Val Loss: 0.0834\n",
      "Epoch 71/300 - Train Loss: 0.0914, Val Loss: 0.0835\n",
      "Epoch 72/300 - Train Loss: 0.0927, Val Loss: 0.0826\n",
      "Epoch 73/300 - Train Loss: 0.0902, Val Loss: 0.0829\n",
      "Epoch 74/300 - Train Loss: 0.0896, Val Loss: 0.0809\n",
      "Epoch 75/300 - Train Loss: 0.0906, Val Loss: 0.0804\n",
      "Epoch 76/300 - Train Loss: 0.0897, Val Loss: 0.0824\n",
      "Epoch 77/300 - Train Loss: 0.0900, Val Loss: 0.0801\n",
      "Epoch 78/300 - Train Loss: 0.0893, Val Loss: 0.0816\n",
      "Epoch 79/300 - Train Loss: 0.0906, Val Loss: 0.0809\n",
      "Epoch 80/300 - Train Loss: 0.0897, Val Loss: 0.0833\n",
      "Epoch 81/300 - Train Loss: 0.0883, Val Loss: 0.0818\n",
      "Epoch 82/300 - Train Loss: 0.0889, Val Loss: 0.0815\n",
      "Epoch 83/300 - Train Loss: 0.0885, Val Loss: 0.0813\n",
      "Epoch 84/300 - Train Loss: 0.0895, Val Loss: 0.0821\n",
      "Epoch 85/300 - Train Loss: 0.0880, Val Loss: 0.0813\n",
      "Epoch 86/300 - Train Loss: 0.0872, Val Loss: 0.0807\n",
      "Epoch 87/300 - Train Loss: 0.0879, Val Loss: 0.0799\n",
      "Epoch 88/300 - Train Loss: 0.0886, Val Loss: 0.0812\n",
      "Epoch 89/300 - Train Loss: 0.0899, Val Loss: 0.0812\n",
      "Epoch 90/300 - Train Loss: 0.0872, Val Loss: 0.0808\n",
      "Epoch 91/300 - Train Loss: 0.0871, Val Loss: 0.0816\n",
      "Epoch 92/300 - Train Loss: 0.0877, Val Loss: 0.0799\n",
      "Epoch 93/300 - Train Loss: 0.0865, Val Loss: 0.0802\n",
      "Epoch 94/300 - Train Loss: 0.0870, Val Loss: 0.0797\n",
      "Epoch 95/300 - Train Loss: 0.0872, Val Loss: 0.0805\n",
      "Epoch 96/300 - Train Loss: 0.0877, Val Loss: 0.0795\n",
      "Epoch 97/300 - Train Loss: 0.0854, Val Loss: 0.0801\n",
      "Epoch 98/300 - Train Loss: 0.0866, Val Loss: 0.0805\n",
      "Epoch 99/300 - Train Loss: 0.0872, Val Loss: 0.0797\n",
      "Epoch 100/300 - Train Loss: 0.0865, Val Loss: 0.0794\n",
      "Epoch 101/300 - Train Loss: 0.0847, Val Loss: 0.0816\n",
      "Epoch 102/300 - Train Loss: 0.0855, Val Loss: 0.0814\n",
      "Epoch 103/300 - Train Loss: 0.0863, Val Loss: 0.0793\n",
      "Epoch 104/300 - Train Loss: 0.0858, Val Loss: 0.0807\n",
      "Epoch 105/300 - Train Loss: 0.0860, Val Loss: 0.0788\n",
      "Epoch 106/300 - Train Loss: 0.0853, Val Loss: 0.0798\n",
      "Epoch 107/300 - Train Loss: 0.0851, Val Loss: 0.0792\n",
      "Epoch 108/300 - Train Loss: 0.0857, Val Loss: 0.0804\n",
      "Epoch 109/300 - Train Loss: 0.0849, Val Loss: 0.0796\n",
      "Epoch 110/300 - Train Loss: 0.0844, Val Loss: 0.0796\n",
      "Epoch 111/300 - Train Loss: 0.0842, Val Loss: 0.0801\n",
      "Epoch 112/300 - Train Loss: 0.0838, Val Loss: 0.0801\n",
      "Epoch 113/300 - Train Loss: 0.0839, Val Loss: 0.0803\n",
      "Epoch 114/300 - Train Loss: 0.0846, Val Loss: 0.0794\n",
      "Epoch 115/300 - Train Loss: 0.0846, Val Loss: 0.0785\n",
      "Epoch 116/300 - Train Loss: 0.0847, Val Loss: 0.0793\n",
      "Epoch 117/300 - Train Loss: 0.0862, Val Loss: 0.0794\n",
      "Epoch 118/300 - Train Loss: 0.0839, Val Loss: 0.0811\n",
      "Epoch 119/300 - Train Loss: 0.0846, Val Loss: 0.0797\n",
      "Epoch 120/300 - Train Loss: 0.0839, Val Loss: 0.0805\n",
      "Epoch 121/300 - Train Loss: 0.0848, Val Loss: 0.0784\n",
      "Epoch 122/300 - Train Loss: 0.0815, Val Loss: 0.0801\n",
      "Epoch 123/300 - Train Loss: 0.0832, Val Loss: 0.0787\n",
      "Epoch 124/300 - Train Loss: 0.0828, Val Loss: 0.0787\n",
      "Epoch 125/300 - Train Loss: 0.0845, Val Loss: 0.0784\n",
      "Epoch 126/300 - Train Loss: 0.0844, Val Loss: 0.0790\n",
      "Epoch 127/300 - Train Loss: 0.0837, Val Loss: 0.0781\n",
      "Epoch 128/300 - Train Loss: 0.0834, Val Loss: 0.0797\n",
      "Epoch 129/300 - Train Loss: 0.0819, Val Loss: 0.0787\n",
      "Epoch 130/300 - Train Loss: 0.0815, Val Loss: 0.0781\n",
      "Epoch 131/300 - Train Loss: 0.0821, Val Loss: 0.0787\n",
      "Epoch 132/300 - Train Loss: 0.0828, Val Loss: 0.0799\n",
      "Epoch 133/300 - Train Loss: 0.0830, Val Loss: 0.0775\n",
      "Epoch 134/300 - Train Loss: 0.0811, Val Loss: 0.0790\n",
      "Epoch 135/300 - Train Loss: 0.0820, Val Loss: 0.0789\n",
      "Epoch 136/300 - Train Loss: 0.0822, Val Loss: 0.0777\n",
      "Epoch 137/300 - Train Loss: 0.0816, Val Loss: 0.0787\n",
      "Epoch 138/300 - Train Loss: 0.0822, Val Loss: 0.0792\n",
      "Epoch 139/300 - Train Loss: 0.0820, Val Loss: 0.0786\n",
      "Epoch 140/300 - Train Loss: 0.0823, Val Loss: 0.0780\n",
      "Epoch 141/300 - Train Loss: 0.0822, Val Loss: 0.0778\n",
      "Epoch 142/300 - Train Loss: 0.0804, Val Loss: 0.0797\n",
      "Epoch 143/300 - Train Loss: 0.0820, Val Loss: 0.0793\n",
      "Epoch 144/300 - Train Loss: 0.0807, Val Loss: 0.0786\n",
      "Epoch 145/300 - Train Loss: 0.0811, Val Loss: 0.0786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 146/300 - Train Loss: 0.0800, Val Loss: 0.0779\n",
      "Epoch 147/300 - Train Loss: 0.0799, Val Loss: 0.0781\n",
      "Epoch 148/300 - Train Loss: 0.0812, Val Loss: 0.0790\n",
      "Epoch 149/300 - Train Loss: 0.0818, Val Loss: 0.0786\n",
      "Epoch 150/300 - Train Loss: 0.0809, Val Loss: 0.0778\n",
      "Epoch 151/300 - Train Loss: 0.0823, Val Loss: 0.0789\n",
      "Epoch 152/300 - Train Loss: 0.0815, Val Loss: 0.0773\n",
      "Epoch 153/300 - Train Loss: 0.0808, Val Loss: 0.0776\n",
      "Epoch 154/300 - Train Loss: 0.0811, Val Loss: 0.0788\n",
      "Epoch 155/300 - Train Loss: 0.0805, Val Loss: 0.0792\n",
      "Epoch 156/300 - Train Loss: 0.0817, Val Loss: 0.0788\n",
      "Epoch 157/300 - Train Loss: 0.0794, Val Loss: 0.0781\n",
      "Epoch 158/300 - Train Loss: 0.0791, Val Loss: 0.0791\n",
      "Epoch 159/300 - Train Loss: 0.0806, Val Loss: 0.0784\n",
      "Epoch 160/300 - Train Loss: 0.0793, Val Loss: 0.0798\n",
      "Epoch 161/300 - Train Loss: 0.0812, Val Loss: 0.0791\n",
      "Epoch 162/300 - Train Loss: 0.0790, Val Loss: 0.0786\n",
      "Epoch 163/300 - Train Loss: 0.0793, Val Loss: 0.0782\n",
      "Epoch 164/300 - Train Loss: 0.0792, Val Loss: 0.0792\n",
      "Epoch 165/300 - Train Loss: 0.0786, Val Loss: 0.0782\n",
      "Epoch 166/300 - Train Loss: 0.0811, Val Loss: 0.0792\n",
      "Epoch 167/300 - Train Loss: 0.0792, Val Loss: 0.0783\n",
      "Epoch 168/300 - Train Loss: 0.0782, Val Loss: 0.0778\n",
      "Epoch 169/300 - Train Loss: 0.0795, Val Loss: 0.0778\n",
      "Epoch 170/300 - Train Loss: 0.0797, Val Loss: 0.0787\n",
      "Epoch 171/300 - Train Loss: 0.0790, Val Loss: 0.0777\n",
      "Epoch 172/300 - Train Loss: 0.0791, Val Loss: 0.0771\n",
      "Epoch 173/300 - Train Loss: 0.0791, Val Loss: 0.0789\n",
      "Epoch 174/300 - Train Loss: 0.0789, Val Loss: 0.0800\n",
      "Epoch 175/300 - Train Loss: 0.0794, Val Loss: 0.0782\n",
      "Epoch 176/300 - Train Loss: 0.0784, Val Loss: 0.0778\n",
      "Epoch 177/300 - Train Loss: 0.0789, Val Loss: 0.0789\n",
      "Epoch 178/300 - Train Loss: 0.0776, Val Loss: 0.0793\n",
      "Epoch 179/300 - Train Loss: 0.0787, Val Loss: 0.0780\n",
      "Epoch 180/300 - Train Loss: 0.0789, Val Loss: 0.0768\n",
      "Epoch 181/300 - Train Loss: 0.0780, Val Loss: 0.0794\n",
      "Epoch 182/300 - Train Loss: 0.0790, Val Loss: 0.0777\n",
      "Epoch 183/300 - Train Loss: 0.0793, Val Loss: 0.0786\n",
      "Epoch 184/300 - Train Loss: 0.0789, Val Loss: 0.0774\n",
      "Epoch 185/300 - Train Loss: 0.0772, Val Loss: 0.0777\n",
      "Epoch 186/300 - Train Loss: 0.0789, Val Loss: 0.0774\n",
      "Epoch 187/300 - Train Loss: 0.0781, Val Loss: 0.0778\n",
      "Epoch 188/300 - Train Loss: 0.0780, Val Loss: 0.0790\n",
      "Epoch 189/300 - Train Loss: 0.0770, Val Loss: 0.0771\n",
      "Epoch 190/300 - Train Loss: 0.0772, Val Loss: 0.0776\n",
      "Epoch 191/300 - Train Loss: 0.0771, Val Loss: 0.0776\n",
      "Epoch 192/300 - Train Loss: 0.0771, Val Loss: 0.0769\n",
      "Epoch 193/300 - Train Loss: 0.0779, Val Loss: 0.0790\n",
      "Epoch 194/300 - Train Loss: 0.0770, Val Loss: 0.0777\n",
      "Epoch 195/300 - Train Loss: 0.0772, Val Loss: 0.0771\n",
      "Epoch 196/300 - Train Loss: 0.0787, Val Loss: 0.0772\n",
      "Epoch 197/300 - Train Loss: 0.0785, Val Loss: 0.0764\n",
      "Epoch 198/300 - Train Loss: 0.0759, Val Loss: 0.0773\n",
      "Epoch 199/300 - Train Loss: 0.0772, Val Loss: 0.0783\n",
      "Epoch 200/300 - Train Loss: 0.0760, Val Loss: 0.0772\n",
      "Epoch 201/300 - Train Loss: 0.0769, Val Loss: 0.0795\n",
      "Epoch 202/300 - Train Loss: 0.0776, Val Loss: 0.0780\n",
      "Epoch 203/300 - Train Loss: 0.0772, Val Loss: 0.0775\n",
      "Epoch 204/300 - Train Loss: 0.0771, Val Loss: 0.0781\n",
      "Epoch 205/300 - Train Loss: 0.0758, Val Loss: 0.0780\n",
      "Epoch 206/300 - Train Loss: 0.0765, Val Loss: 0.0772\n",
      "Epoch 207/300 - Train Loss: 0.0759, Val Loss: 0.0777\n",
      "Epoch 208/300 - Train Loss: 0.0762, Val Loss: 0.0778\n",
      "Epoch 209/300 - Train Loss: 0.0757, Val Loss: 0.0769\n",
      "Epoch 210/300 - Train Loss: 0.0762, Val Loss: 0.0768\n",
      "Epoch 211/300 - Train Loss: 0.0777, Val Loss: 0.0779\n",
      "Epoch 212/300 - Train Loss: 0.0774, Val Loss: 0.0769\n",
      "Epoch 213/300 - Train Loss: 0.0761, Val Loss: 0.0785\n",
      "Epoch 214/300 - Train Loss: 0.0758, Val Loss: 0.0765\n",
      "Epoch 215/300 - Train Loss: 0.0763, Val Loss: 0.0774\n",
      "Epoch 216/300 - Train Loss: 0.0760, Val Loss: 0.0775\n",
      "Epoch 217/300 - Train Loss: 0.0759, Val Loss: 0.0787\n",
      "Epoch 218/300 - Train Loss: 0.0760, Val Loss: 0.0785\n",
      "Epoch 219/300 - Train Loss: 0.0768, Val Loss: 0.0773\n",
      "Epoch 220/300 - Train Loss: 0.0773, Val Loss: 0.0771\n",
      "Epoch 221/300 - Train Loss: 0.0772, Val Loss: 0.0803\n",
      "Epoch 222/300 - Train Loss: 0.0748, Val Loss: 0.0785\n",
      "Epoch 223/300 - Train Loss: 0.0760, Val Loss: 0.0772\n",
      "Epoch 224/300 - Train Loss: 0.0766, Val Loss: 0.0766\n",
      "Epoch 225/300 - Train Loss: 0.0744, Val Loss: 0.0775\n",
      "Epoch 226/300 - Train Loss: 0.0759, Val Loss: 0.0778\n",
      "Epoch 227/300 - Train Loss: 0.0741, Val Loss: 0.0760\n",
      "Epoch 228/300 - Train Loss: 0.0742, Val Loss: 0.0766\n",
      "Epoch 229/300 - Train Loss: 0.0738, Val Loss: 0.0787\n",
      "Epoch 230/300 - Train Loss: 0.0762, Val Loss: 0.0772\n",
      "Epoch 231/300 - Train Loss: 0.0752, Val Loss: 0.0769\n",
      "Epoch 232/300 - Train Loss: 0.0743, Val Loss: 0.0776\n",
      "Epoch 233/300 - Train Loss: 0.0763, Val Loss: 0.0784\n",
      "Epoch 234/300 - Train Loss: 0.0761, Val Loss: 0.0770\n",
      "Epoch 235/300 - Train Loss: 0.0758, Val Loss: 0.0771\n",
      "Epoch 236/300 - Train Loss: 0.0749, Val Loss: 0.0775\n",
      "Epoch 237/300 - Train Loss: 0.0761, Val Loss: 0.0756\n",
      "Epoch 238/300 - Train Loss: 0.0755, Val Loss: 0.0766\n",
      "Epoch 239/300 - Train Loss: 0.0750, Val Loss: 0.0774\n",
      "Epoch 240/300 - Train Loss: 0.0731, Val Loss: 0.0758\n",
      "Epoch 241/300 - Train Loss: 0.0747, Val Loss: 0.0767\n",
      "Epoch 242/300 - Train Loss: 0.0750, Val Loss: 0.0760\n",
      "Epoch 243/300 - Train Loss: 0.0742, Val Loss: 0.0765\n",
      "Epoch 244/300 - Train Loss: 0.0755, Val Loss: 0.0766\n",
      "Epoch 245/300 - Train Loss: 0.0763, Val Loss: 0.0778\n",
      "Epoch 246/300 - Train Loss: 0.0753, Val Loss: 0.0766\n",
      "Epoch 247/300 - Train Loss: 0.0740, Val Loss: 0.0762\n",
      "Epoch 248/300 - Train Loss: 0.0747, Val Loss: 0.0772\n",
      "Epoch 249/300 - Train Loss: 0.0752, Val Loss: 0.0760\n",
      "Epoch 250/300 - Train Loss: 0.0734, Val Loss: 0.0766\n",
      "Epoch 251/300 - Train Loss: 0.0745, Val Loss: 0.0762\n",
      "Epoch 252/300 - Train Loss: 0.0752, Val Loss: 0.0779\n",
      "Epoch 253/300 - Train Loss: 0.0760, Val Loss: 0.0796\n",
      "Epoch 254/300 - Train Loss: 0.0751, Val Loss: 0.0757\n",
      "Epoch 255/300 - Train Loss: 0.0737, Val Loss: 0.0774\n",
      "Epoch 256/300 - Train Loss: 0.0748, Val Loss: 0.0763\n",
      "Epoch 257/300 - Train Loss: 0.0742, Val Loss: 0.0785\n",
      "Epoch 258/300 - Train Loss: 0.0736, Val Loss: 0.0766\n",
      "Epoch 259/300 - Train Loss: 0.0744, Val Loss: 0.0779\n",
      "Epoch 260/300 - Train Loss: 0.0742, Val Loss: 0.0775\n",
      "Epoch 261/300 - Train Loss: 0.0738, Val Loss: 0.0772\n",
      "Epoch 262/300 - Train Loss: 0.0731, Val Loss: 0.0759\n",
      "Epoch 263/300 - Train Loss: 0.0743, Val Loss: 0.0772\n",
      "Epoch 264/300 - Train Loss: 0.0735, Val Loss: 0.0763\n",
      "Epoch 265/300 - Train Loss: 0.0743, Val Loss: 0.0763\n",
      "Epoch 266/300 - Train Loss: 0.0731, Val Loss: 0.0766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 01:39:53,950] Trial 39 finished with value: 0.964373243367746 and parameters: {'F1': 8, 'F2': 16, 'D': 4, 'dropout': 0.15193653725695327, 'learning_rate': 1.0188791065699855e-05, 'batch_size': 64, 'weight_decay': 0.00020705117260985486}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267/300 - Train Loss: 0.0724, Val Loss: 0.0776\n",
      "Early stopping at epoch 267\n",
      "Macro F1 Score: 0.9644, Macro Precision: 0.9548, Macro Recall: 0.9750\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 41\n",
      "Training with F1=8, F2=8, D=4, dropout=0.21433898679903307, LR=1.5362229115469584e-05, BS=64, WD=8.91694294899232e-05\n",
      "Epoch 1/300 - Train Loss: 0.9937, Val Loss: 0.8745\n",
      "Epoch 2/300 - Train Loss: 0.7364, Val Loss: 0.6213\n",
      "Epoch 3/300 - Train Loss: 0.5459, Val Loss: 0.4781\n",
      "Epoch 4/300 - Train Loss: 0.4346, Val Loss: 0.3863\n",
      "Epoch 5/300 - Train Loss: 0.3682, Val Loss: 0.3303\n",
      "Epoch 6/300 - Train Loss: 0.3199, Val Loss: 0.2932\n",
      "Epoch 7/300 - Train Loss: 0.2918, Val Loss: 0.2589\n",
      "Epoch 8/300 - Train Loss: 0.2694, Val Loss: 0.2370\n",
      "Epoch 9/300 - Train Loss: 0.2495, Val Loss: 0.2210\n",
      "Epoch 10/300 - Train Loss: 0.2338, Val Loss: 0.2042\n",
      "Epoch 11/300 - Train Loss: 0.2180, Val Loss: 0.1871\n",
      "Epoch 12/300 - Train Loss: 0.2062, Val Loss: 0.1751\n",
      "Epoch 13/300 - Train Loss: 0.1955, Val Loss: 0.1689\n",
      "Epoch 14/300 - Train Loss: 0.1843, Val Loss: 0.1555\n",
      "Epoch 15/300 - Train Loss: 0.1747, Val Loss: 0.1453\n",
      "Epoch 16/300 - Train Loss: 0.1644, Val Loss: 0.1382\n",
      "Epoch 17/300 - Train Loss: 0.1592, Val Loss: 0.1345\n",
      "Epoch 18/300 - Train Loss: 0.1531, Val Loss: 0.1263\n",
      "Epoch 19/300 - Train Loss: 0.1462, Val Loss: 0.1232\n",
      "Epoch 20/300 - Train Loss: 0.1423, Val Loss: 0.1176\n",
      "Epoch 21/300 - Train Loss: 0.1356, Val Loss: 0.1108\n",
      "Epoch 22/300 - Train Loss: 0.1301, Val Loss: 0.1106\n",
      "Epoch 23/300 - Train Loss: 0.1279, Val Loss: 0.1089\n",
      "Epoch 24/300 - Train Loss: 0.1252, Val Loss: 0.1041\n",
      "Epoch 25/300 - Train Loss: 0.1228, Val Loss: 0.1003\n",
      "Epoch 26/300 - Train Loss: 0.1182, Val Loss: 0.1010\n",
      "Epoch 27/300 - Train Loss: 0.1188, Val Loss: 0.0980\n",
      "Epoch 28/300 - Train Loss: 0.1134, Val Loss: 0.0969\n",
      "Epoch 29/300 - Train Loss: 0.1138, Val Loss: 0.0967\n",
      "Epoch 30/300 - Train Loss: 0.1114, Val Loss: 0.0943\n",
      "Epoch 31/300 - Train Loss: 0.1093, Val Loss: 0.0941\n",
      "Epoch 32/300 - Train Loss: 0.1085, Val Loss: 0.0918\n",
      "Epoch 33/300 - Train Loss: 0.1079, Val Loss: 0.0917\n",
      "Epoch 34/300 - Train Loss: 0.1068, Val Loss: 0.0894\n",
      "Epoch 35/300 - Train Loss: 0.1046, Val Loss: 0.0893\n",
      "Epoch 36/300 - Train Loss: 0.1044, Val Loss: 0.0902\n",
      "Epoch 37/300 - Train Loss: 0.1026, Val Loss: 0.0886\n",
      "Epoch 38/300 - Train Loss: 0.1038, Val Loss: 0.0861\n",
      "Epoch 39/300 - Train Loss: 0.1019, Val Loss: 0.0865\n",
      "Epoch 40/300 - Train Loss: 0.1005, Val Loss: 0.0856\n",
      "Epoch 41/300 - Train Loss: 0.1004, Val Loss: 0.0865\n",
      "Epoch 42/300 - Train Loss: 0.0989, Val Loss: 0.0853\n",
      "Epoch 43/300 - Train Loss: 0.0995, Val Loss: 0.0865\n",
      "Epoch 44/300 - Train Loss: 0.1008, Val Loss: 0.0837\n",
      "Epoch 45/300 - Train Loss: 0.0979, Val Loss: 0.0829\n",
      "Epoch 46/300 - Train Loss: 0.0987, Val Loss: 0.0839\n",
      "Epoch 47/300 - Train Loss: 0.0970, Val Loss: 0.0837\n",
      "Epoch 48/300 - Train Loss: 0.0981, Val Loss: 0.0832\n",
      "Epoch 49/300 - Train Loss: 0.0977, Val Loss: 0.0826\n",
      "Epoch 50/300 - Train Loss: 0.0957, Val Loss: 0.0821\n",
      "Epoch 51/300 - Train Loss: 0.0964, Val Loss: 0.0819\n",
      "Epoch 52/300 - Train Loss: 0.0949, Val Loss: 0.0828\n",
      "Epoch 53/300 - Train Loss: 0.0943, Val Loss: 0.0807\n",
      "Epoch 54/300 - Train Loss: 0.0960, Val Loss: 0.0818\n",
      "Epoch 55/300 - Train Loss: 0.0950, Val Loss: 0.0841\n",
      "Epoch 56/300 - Train Loss: 0.0938, Val Loss: 0.0817\n",
      "Epoch 57/300 - Train Loss: 0.0921, Val Loss: 0.0820\n",
      "Epoch 58/300 - Train Loss: 0.0956, Val Loss: 0.0802\n",
      "Epoch 59/300 - Train Loss: 0.0938, Val Loss: 0.0825\n",
      "Epoch 60/300 - Train Loss: 0.0936, Val Loss: 0.0827\n",
      "Epoch 61/300 - Train Loss: 0.0923, Val Loss: 0.0791\n",
      "Epoch 62/300 - Train Loss: 0.0920, Val Loss: 0.0816\n",
      "Epoch 63/300 - Train Loss: 0.0924, Val Loss: 0.0829\n",
      "Epoch 64/300 - Train Loss: 0.0922, Val Loss: 0.0798\n",
      "Epoch 65/300 - Train Loss: 0.0925, Val Loss: 0.0792\n",
      "Epoch 66/300 - Train Loss: 0.0925, Val Loss: 0.0786\n",
      "Epoch 67/300 - Train Loss: 0.0917, Val Loss: 0.0798\n",
      "Epoch 68/300 - Train Loss: 0.0904, Val Loss: 0.0798\n",
      "Epoch 69/300 - Train Loss: 0.0929, Val Loss: 0.0798\n",
      "Epoch 70/300 - Train Loss: 0.0910, Val Loss: 0.0784\n",
      "Epoch 71/300 - Train Loss: 0.0898, Val Loss: 0.0794\n",
      "Epoch 72/300 - Train Loss: 0.0901, Val Loss: 0.0781\n",
      "Epoch 73/300 - Train Loss: 0.0897, Val Loss: 0.0786\n",
      "Epoch 74/300 - Train Loss: 0.0893, Val Loss: 0.0772\n",
      "Epoch 75/300 - Train Loss: 0.0904, Val Loss: 0.0811\n",
      "Epoch 76/300 - Train Loss: 0.0909, Val Loss: 0.0787\n",
      "Epoch 77/300 - Train Loss: 0.0909, Val Loss: 0.0797\n",
      "Epoch 78/300 - Train Loss: 0.0892, Val Loss: 0.0776\n",
      "Epoch 79/300 - Train Loss: 0.0906, Val Loss: 0.0770\n",
      "Epoch 80/300 - Train Loss: 0.0897, Val Loss: 0.0788\n",
      "Epoch 81/300 - Train Loss: 0.0907, Val Loss: 0.0797\n",
      "Epoch 82/300 - Train Loss: 0.0888, Val Loss: 0.0774\n",
      "Epoch 83/300 - Train Loss: 0.0897, Val Loss: 0.0824\n",
      "Epoch 84/300 - Train Loss: 0.0885, Val Loss: 0.0779\n",
      "Epoch 85/300 - Train Loss: 0.0888, Val Loss: 0.0775\n",
      "Epoch 86/300 - Train Loss: 0.0890, Val Loss: 0.0785\n",
      "Epoch 87/300 - Train Loss: 0.0883, Val Loss: 0.0784\n",
      "Epoch 88/300 - Train Loss: 0.0887, Val Loss: 0.0782\n",
      "Epoch 89/300 - Train Loss: 0.0883, Val Loss: 0.0756\n",
      "Epoch 90/300 - Train Loss: 0.0901, Val Loss: 0.0769\n",
      "Epoch 91/300 - Train Loss: 0.0887, Val Loss: 0.0777\n",
      "Epoch 92/300 - Train Loss: 0.0887, Val Loss: 0.0770\n",
      "Epoch 93/300 - Train Loss: 0.0883, Val Loss: 0.0773\n",
      "Epoch 94/300 - Train Loss: 0.0893, Val Loss: 0.0764\n",
      "Epoch 95/300 - Train Loss: 0.0873, Val Loss: 0.0761\n",
      "Epoch 96/300 - Train Loss: 0.0866, Val Loss: 0.0754\n",
      "Epoch 97/300 - Train Loss: 0.0896, Val Loss: 0.0771\n",
      "Epoch 98/300 - Train Loss: 0.0884, Val Loss: 0.0784\n",
      "Epoch 99/300 - Train Loss: 0.0879, Val Loss: 0.0760\n",
      "Epoch 100/300 - Train Loss: 0.0873, Val Loss: 0.0756\n",
      "Epoch 101/300 - Train Loss: 0.0872, Val Loss: 0.0760\n",
      "Epoch 102/300 - Train Loss: 0.0851, Val Loss: 0.0759\n",
      "Epoch 103/300 - Train Loss: 0.0864, Val Loss: 0.0770\n",
      "Epoch 104/300 - Train Loss: 0.0850, Val Loss: 0.0770\n",
      "Epoch 105/300 - Train Loss: 0.0858, Val Loss: 0.0776\n",
      "Epoch 106/300 - Train Loss: 0.0858, Val Loss: 0.0774\n",
      "Epoch 107/300 - Train Loss: 0.0854, Val Loss: 0.0757\n",
      "Epoch 108/300 - Train Loss: 0.0845, Val Loss: 0.0756\n",
      "Epoch 109/300 - Train Loss: 0.0875, Val Loss: 0.0745\n",
      "Epoch 110/300 - Train Loss: 0.0858, Val Loss: 0.0771\n",
      "Epoch 111/300 - Train Loss: 0.0861, Val Loss: 0.0743\n",
      "Epoch 112/300 - Train Loss: 0.0846, Val Loss: 0.0765\n",
      "Epoch 113/300 - Train Loss: 0.0841, Val Loss: 0.0743\n",
      "Epoch 114/300 - Train Loss: 0.0852, Val Loss: 0.0742\n",
      "Epoch 115/300 - Train Loss: 0.0844, Val Loss: 0.0766\n",
      "Epoch 116/300 - Train Loss: 0.0866, Val Loss: 0.0759\n",
      "Epoch 117/300 - Train Loss: 0.0844, Val Loss: 0.0770\n",
      "Epoch 118/300 - Train Loss: 0.0837, Val Loss: 0.0780\n",
      "Epoch 119/300 - Train Loss: 0.0851, Val Loss: 0.0760\n",
      "Epoch 120/300 - Train Loss: 0.0850, Val Loss: 0.0749\n",
      "Epoch 121/300 - Train Loss: 0.0858, Val Loss: 0.0751\n",
      "Epoch 122/300 - Train Loss: 0.0849, Val Loss: 0.0749\n",
      "Epoch 123/300 - Train Loss: 0.0838, Val Loss: 0.0736\n",
      "Epoch 124/300 - Train Loss: 0.0825, Val Loss: 0.0756\n",
      "Epoch 125/300 - Train Loss: 0.0843, Val Loss: 0.0741\n",
      "Epoch 126/300 - Train Loss: 0.0839, Val Loss: 0.0741\n",
      "Epoch 127/300 - Train Loss: 0.0849, Val Loss: 0.0744\n",
      "Epoch 128/300 - Train Loss: 0.0840, Val Loss: 0.0750\n",
      "Epoch 129/300 - Train Loss: 0.0842, Val Loss: 0.0753\n",
      "Epoch 130/300 - Train Loss: 0.0842, Val Loss: 0.0761\n",
      "Epoch 131/300 - Train Loss: 0.0836, Val Loss: 0.0744\n",
      "Epoch 132/300 - Train Loss: 0.0834, Val Loss: 0.0768\n",
      "Epoch 133/300 - Train Loss: 0.0848, Val Loss: 0.0755\n",
      "Epoch 134/300 - Train Loss: 0.0836, Val Loss: 0.0734\n",
      "Epoch 135/300 - Train Loss: 0.0844, Val Loss: 0.0749\n",
      "Epoch 136/300 - Train Loss: 0.0840, Val Loss: 0.0730\n",
      "Epoch 137/300 - Train Loss: 0.0833, Val Loss: 0.0740\n",
      "Epoch 138/300 - Train Loss: 0.0838, Val Loss: 0.0738\n",
      "Epoch 139/300 - Train Loss: 0.0839, Val Loss: 0.0736\n",
      "Epoch 140/300 - Train Loss: 0.0849, Val Loss: 0.0732\n",
      "Epoch 141/300 - Train Loss: 0.0841, Val Loss: 0.0738\n",
      "Epoch 142/300 - Train Loss: 0.0833, Val Loss: 0.0749\n",
      "Epoch 143/300 - Train Loss: 0.0835, Val Loss: 0.0746\n",
      "Epoch 144/300 - Train Loss: 0.0818, Val Loss: 0.0732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0820, Val Loss: 0.0734\n",
      "Epoch 146/300 - Train Loss: 0.0816, Val Loss: 0.0749\n",
      "Epoch 147/300 - Train Loss: 0.0821, Val Loss: 0.0730\n",
      "Epoch 148/300 - Train Loss: 0.0825, Val Loss: 0.0755\n",
      "Epoch 149/300 - Train Loss: 0.0821, Val Loss: 0.0738\n",
      "Epoch 150/300 - Train Loss: 0.0800, Val Loss: 0.0734\n",
      "Epoch 151/300 - Train Loss: 0.0831, Val Loss: 0.0736\n",
      "Epoch 152/300 - Train Loss: 0.0828, Val Loss: 0.0735\n",
      "Epoch 153/300 - Train Loss: 0.0835, Val Loss: 0.0730\n",
      "Epoch 154/300 - Train Loss: 0.0834, Val Loss: 0.0743\n",
      "Epoch 155/300 - Train Loss: 0.0817, Val Loss: 0.0735\n",
      "Epoch 156/300 - Train Loss: 0.0816, Val Loss: 0.0736\n",
      "Epoch 157/300 - Train Loss: 0.0804, Val Loss: 0.0732\n",
      "Epoch 158/300 - Train Loss: 0.0830, Val Loss: 0.0729\n",
      "Epoch 159/300 - Train Loss: 0.0812, Val Loss: 0.0726\n",
      "Epoch 160/300 - Train Loss: 0.0826, Val Loss: 0.0744\n",
      "Epoch 161/300 - Train Loss: 0.0821, Val Loss: 0.0734\n",
      "Epoch 162/300 - Train Loss: 0.0813, Val Loss: 0.0728\n",
      "Epoch 163/300 - Train Loss: 0.0814, Val Loss: 0.0724\n",
      "Epoch 164/300 - Train Loss: 0.0801, Val Loss: 0.0734\n",
      "Epoch 165/300 - Train Loss: 0.0818, Val Loss: 0.0741\n",
      "Epoch 166/300 - Train Loss: 0.0799, Val Loss: 0.0751\n",
      "Epoch 167/300 - Train Loss: 0.0811, Val Loss: 0.0739\n",
      "Epoch 168/300 - Train Loss: 0.0805, Val Loss: 0.0734\n",
      "Epoch 169/300 - Train Loss: 0.0805, Val Loss: 0.0737\n",
      "Epoch 170/300 - Train Loss: 0.0805, Val Loss: 0.0762\n",
      "Epoch 171/300 - Train Loss: 0.0817, Val Loss: 0.0750\n",
      "Epoch 172/300 - Train Loss: 0.0818, Val Loss: 0.0730\n",
      "Epoch 173/300 - Train Loss: 0.0813, Val Loss: 0.0724\n",
      "Epoch 174/300 - Train Loss: 0.0801, Val Loss: 0.0731\n",
      "Epoch 175/300 - Train Loss: 0.0802, Val Loss: 0.0721\n",
      "Epoch 176/300 - Train Loss: 0.0808, Val Loss: 0.0730\n",
      "Epoch 177/300 - Train Loss: 0.0808, Val Loss: 0.0750\n",
      "Epoch 178/300 - Train Loss: 0.0800, Val Loss: 0.0730\n",
      "Epoch 179/300 - Train Loss: 0.0815, Val Loss: 0.0720\n",
      "Epoch 180/300 - Train Loss: 0.0789, Val Loss: 0.0728\n",
      "Epoch 181/300 - Train Loss: 0.0811, Val Loss: 0.0726\n",
      "Epoch 182/300 - Train Loss: 0.0793, Val Loss: 0.0725\n",
      "Epoch 183/300 - Train Loss: 0.0805, Val Loss: 0.0733\n",
      "Epoch 184/300 - Train Loss: 0.0783, Val Loss: 0.0764\n",
      "Epoch 185/300 - Train Loss: 0.0798, Val Loss: 0.0723\n",
      "Epoch 186/300 - Train Loss: 0.0808, Val Loss: 0.0731\n",
      "Epoch 187/300 - Train Loss: 0.0805, Val Loss: 0.0723\n",
      "Epoch 188/300 - Train Loss: 0.0808, Val Loss: 0.0731\n",
      "Epoch 189/300 - Train Loss: 0.0809, Val Loss: 0.0728\n",
      "Epoch 190/300 - Train Loss: 0.0804, Val Loss: 0.0728\n",
      "Epoch 191/300 - Train Loss: 0.0786, Val Loss: 0.0728\n",
      "Epoch 192/300 - Train Loss: 0.0791, Val Loss: 0.0730\n",
      "Epoch 193/300 - Train Loss: 0.0799, Val Loss: 0.0729\n",
      "Epoch 194/300 - Train Loss: 0.0802, Val Loss: 0.0738\n",
      "Epoch 195/300 - Train Loss: 0.0788, Val Loss: 0.0715\n",
      "Epoch 196/300 - Train Loss: 0.0817, Val Loss: 0.0725\n",
      "Epoch 197/300 - Train Loss: 0.0787, Val Loss: 0.0725\n",
      "Epoch 198/300 - Train Loss: 0.0794, Val Loss: 0.0731\n",
      "Epoch 199/300 - Train Loss: 0.0788, Val Loss: 0.0732\n",
      "Epoch 200/300 - Train Loss: 0.0795, Val Loss: 0.0726\n",
      "Epoch 201/300 - Train Loss: 0.0796, Val Loss: 0.0753\n",
      "Epoch 202/300 - Train Loss: 0.0790, Val Loss: 0.0725\n",
      "Epoch 203/300 - Train Loss: 0.0791, Val Loss: 0.0727\n",
      "Epoch 204/300 - Train Loss: 0.0778, Val Loss: 0.0736\n",
      "Epoch 205/300 - Train Loss: 0.0799, Val Loss: 0.0728\n",
      "Epoch 206/300 - Train Loss: 0.0792, Val Loss: 0.0725\n",
      "Epoch 207/300 - Train Loss: 0.0798, Val Loss: 0.0737\n",
      "Epoch 208/300 - Train Loss: 0.0799, Val Loss: 0.0726\n",
      "Epoch 209/300 - Train Loss: 0.0797, Val Loss: 0.0739\n",
      "Epoch 210/300 - Train Loss: 0.0785, Val Loss: 0.0718\n",
      "Epoch 211/300 - Train Loss: 0.0792, Val Loss: 0.0732\n",
      "Epoch 212/300 - Train Loss: 0.0783, Val Loss: 0.0727\n",
      "Epoch 213/300 - Train Loss: 0.0794, Val Loss: 0.0729\n",
      "Epoch 214/300 - Train Loss: 0.0777, Val Loss: 0.0739\n",
      "Epoch 215/300 - Train Loss: 0.0787, Val Loss: 0.0715\n",
      "Epoch 216/300 - Train Loss: 0.0791, Val Loss: 0.0718\n",
      "Epoch 217/300 - Train Loss: 0.0768, Val Loss: 0.0728\n",
      "Epoch 218/300 - Train Loss: 0.0784, Val Loss: 0.0731\n",
      "Epoch 219/300 - Train Loss: 0.0798, Val Loss: 0.0719\n",
      "Epoch 220/300 - Train Loss: 0.0791, Val Loss: 0.0714\n",
      "Epoch 221/300 - Train Loss: 0.0773, Val Loss: 0.0741\n",
      "Epoch 222/300 - Train Loss: 0.0798, Val Loss: 0.0740\n",
      "Epoch 223/300 - Train Loss: 0.0802, Val Loss: 0.0750\n",
      "Epoch 224/300 - Train Loss: 0.0773, Val Loss: 0.0721\n",
      "Epoch 225/300 - Train Loss: 0.0778, Val Loss: 0.0722\n",
      "Epoch 226/300 - Train Loss: 0.0774, Val Loss: 0.0715\n",
      "Epoch 227/300 - Train Loss: 0.0774, Val Loss: 0.0715\n",
      "Epoch 228/300 - Train Loss: 0.0777, Val Loss: 0.0725\n",
      "Epoch 229/300 - Train Loss: 0.0784, Val Loss: 0.0727\n",
      "Epoch 230/300 - Train Loss: 0.0789, Val Loss: 0.0722\n",
      "Epoch 231/300 - Train Loss: 0.0782, Val Loss: 0.0733\n",
      "Epoch 232/300 - Train Loss: 0.0780, Val Loss: 0.0721\n",
      "Epoch 233/300 - Train Loss: 0.0781, Val Loss: 0.0719\n",
      "Epoch 234/300 - Train Loss: 0.0770, Val Loss: 0.0734\n",
      "Epoch 235/300 - Train Loss: 0.0789, Val Loss: 0.0714\n",
      "Epoch 236/300 - Train Loss: 0.0783, Val Loss: 0.0721\n",
      "Epoch 237/300 - Train Loss: 0.0777, Val Loss: 0.0731\n",
      "Epoch 238/300 - Train Loss: 0.0777, Val Loss: 0.0725\n",
      "Epoch 239/300 - Train Loss: 0.0781, Val Loss: 0.0725\n",
      "Epoch 240/300 - Train Loss: 0.0784, Val Loss: 0.0722\n",
      "Epoch 241/300 - Train Loss: 0.0771, Val Loss: 0.0719\n",
      "Epoch 242/300 - Train Loss: 0.0770, Val Loss: 0.0712\n",
      "Epoch 243/300 - Train Loss: 0.0754, Val Loss: 0.0714\n",
      "Epoch 244/300 - Train Loss: 0.0761, Val Loss: 0.0721\n",
      "Epoch 245/300 - Train Loss: 0.0788, Val Loss: 0.0711\n",
      "Epoch 246/300 - Train Loss: 0.0774, Val Loss: 0.0729\n",
      "Epoch 247/300 - Train Loss: 0.0767, Val Loss: 0.0717\n",
      "Epoch 248/300 - Train Loss: 0.0765, Val Loss: 0.0710\n",
      "Epoch 249/300 - Train Loss: 0.0774, Val Loss: 0.0719\n",
      "Epoch 250/300 - Train Loss: 0.0777, Val Loss: 0.0713\n",
      "Epoch 251/300 - Train Loss: 0.0767, Val Loss: 0.0714\n",
      "Epoch 252/300 - Train Loss: 0.0771, Val Loss: 0.0727\n",
      "Epoch 253/300 - Train Loss: 0.0768, Val Loss: 0.0719\n",
      "Epoch 254/300 - Train Loss: 0.0775, Val Loss: 0.0732\n",
      "Epoch 255/300 - Train Loss: 0.0771, Val Loss: 0.0735\n",
      "Epoch 256/300 - Train Loss: 0.0792, Val Loss: 0.0730\n",
      "Epoch 257/300 - Train Loss: 0.0757, Val Loss: 0.0707\n",
      "Epoch 258/300 - Train Loss: 0.0768, Val Loss: 0.0732\n",
      "Epoch 259/300 - Train Loss: 0.0782, Val Loss: 0.0730\n",
      "Epoch 260/300 - Train Loss: 0.0760, Val Loss: 0.0705\n",
      "Epoch 261/300 - Train Loss: 0.0759, Val Loss: 0.0731\n",
      "Epoch 262/300 - Train Loss: 0.0763, Val Loss: 0.0716\n",
      "Epoch 263/300 - Train Loss: 0.0764, Val Loss: 0.0720\n",
      "Epoch 264/300 - Train Loss: 0.0769, Val Loss: 0.0730\n",
      "Epoch 265/300 - Train Loss: 0.0759, Val Loss: 0.0727\n",
      "Epoch 266/300 - Train Loss: 0.0762, Val Loss: 0.0721\n",
      "Epoch 267/300 - Train Loss: 0.0767, Val Loss: 0.0723\n",
      "Epoch 268/300 - Train Loss: 0.0786, Val Loss: 0.0720\n",
      "Epoch 269/300 - Train Loss: 0.0769, Val Loss: 0.0722\n",
      "Epoch 270/300 - Train Loss: 0.0758, Val Loss: 0.0719\n",
      "Epoch 271/300 - Train Loss: 0.0762, Val Loss: 0.0732\n",
      "Epoch 272/300 - Train Loss: 0.0750, Val Loss: 0.0708\n",
      "Epoch 273/300 - Train Loss: 0.0775, Val Loss: 0.0730\n",
      "Epoch 274/300 - Train Loss: 0.0762, Val Loss: 0.0726\n",
      "Epoch 275/300 - Train Loss: 0.0753, Val Loss: 0.0730\n",
      "Epoch 276/300 - Train Loss: 0.0780, Val Loss: 0.0720\n",
      "Epoch 277/300 - Train Loss: 0.0766, Val Loss: 0.0709\n",
      "Epoch 278/300 - Train Loss: 0.0763, Val Loss: 0.0727\n",
      "Epoch 279/300 - Train Loss: 0.0773, Val Loss: 0.0723\n",
      "Epoch 280/300 - Train Loss: 0.0762, Val Loss: 0.0726\n",
      "Epoch 281/300 - Train Loss: 0.0762, Val Loss: 0.0740\n",
      "Epoch 282/300 - Train Loss: 0.0759, Val Loss: 0.0719\n",
      "Epoch 283/300 - Train Loss: 0.0769, Val Loss: 0.0725\n",
      "Epoch 284/300 - Train Loss: 0.0765, Val Loss: 0.0714\n",
      "Epoch 285/300 - Train Loss: 0.0760, Val Loss: 0.0725\n",
      "Epoch 286/300 - Train Loss: 0.0760, Val Loss: 0.0727\n",
      "Epoch 287/300 - Train Loss: 0.0782, Val Loss: 0.0716\n",
      "Epoch 288/300 - Train Loss: 0.0766, Val Loss: 0.0731\n",
      "Epoch 289/300 - Train Loss: 0.0762, Val Loss: 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 01:46:23,530] Trial 40 finished with value: 0.9628255342541056 and parameters: {'F1': 8, 'F2': 8, 'D': 4, 'dropout': 0.21433898679903307, 'learning_rate': 1.5362229115469584e-05, 'batch_size': 64, 'weight_decay': 8.91694294899232e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/300 - Train Loss: 0.0765, Val Loss: 0.0731\n",
      "Early stopping at epoch 290\n",
      "Macro F1 Score: 0.9628, Macro Precision: 0.9550, Macro Recall: 0.9714\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 42\n",
      "Training with F1=4, F2=16, D=4, dropout=0.17537090434184385, LR=1.4661711544491558e-05, BS=256, WD=3.2325332407111046e-05\n",
      "Epoch 1/300 - Train Loss: 1.0348, Val Loss: 0.9958\n",
      "Epoch 2/300 - Train Loss: 0.9471, Val Loss: 0.9221\n",
      "Epoch 3/300 - Train Loss: 0.8712, Val Loss: 0.8532\n",
      "Epoch 4/300 - Train Loss: 0.8013, Val Loss: 0.7789\n",
      "Epoch 5/300 - Train Loss: 0.7307, Val Loss: 0.7057\n",
      "Epoch 6/300 - Train Loss: 0.6606, Val Loss: 0.6345\n",
      "Epoch 7/300 - Train Loss: 0.5915, Val Loss: 0.5646\n",
      "Epoch 8/300 - Train Loss: 0.5285, Val Loss: 0.5008\n",
      "Epoch 9/300 - Train Loss: 0.4742, Val Loss: 0.4507\n",
      "Epoch 10/300 - Train Loss: 0.4339, Val Loss: 0.4054\n",
      "Epoch 11/300 - Train Loss: 0.3920, Val Loss: 0.3693\n",
      "Epoch 12/300 - Train Loss: 0.3576, Val Loss: 0.3348\n",
      "Epoch 13/300 - Train Loss: 0.3290, Val Loss: 0.3091\n",
      "Epoch 14/300 - Train Loss: 0.3058, Val Loss: 0.2885\n",
      "Epoch 15/300 - Train Loss: 0.2884, Val Loss: 0.2678\n",
      "Epoch 16/300 - Train Loss: 0.2709, Val Loss: 0.2559\n",
      "Epoch 17/300 - Train Loss: 0.2577, Val Loss: 0.2398\n",
      "Epoch 18/300 - Train Loss: 0.2456, Val Loss: 0.2318\n",
      "Epoch 19/300 - Train Loss: 0.2361, Val Loss: 0.2212\n",
      "Epoch 20/300 - Train Loss: 0.2297, Val Loss: 0.2121\n",
      "Epoch 21/300 - Train Loss: 0.2243, Val Loss: 0.2053\n",
      "Epoch 22/300 - Train Loss: 0.2159, Val Loss: 0.1972\n",
      "Epoch 23/300 - Train Loss: 0.2123, Val Loss: 0.1934\n",
      "Epoch 24/300 - Train Loss: 0.2036, Val Loss: 0.1844\n",
      "Epoch 25/300 - Train Loss: 0.1979, Val Loss: 0.1796\n",
      "Epoch 26/300 - Train Loss: 0.1935, Val Loss: 0.1741\n",
      "Epoch 27/300 - Train Loss: 0.1890, Val Loss: 0.1675\n",
      "Epoch 28/300 - Train Loss: 0.1853, Val Loss: 0.1664\n",
      "Epoch 29/300 - Train Loss: 0.1800, Val Loss: 0.1579\n",
      "Epoch 30/300 - Train Loss: 0.1765, Val Loss: 0.1563\n",
      "Epoch 31/300 - Train Loss: 0.1727, Val Loss: 0.1537\n",
      "Epoch 32/300 - Train Loss: 0.1697, Val Loss: 0.1470\n",
      "Epoch 33/300 - Train Loss: 0.1679, Val Loss: 0.1443\n",
      "Epoch 34/300 - Train Loss: 0.1613, Val Loss: 0.1420\n",
      "Epoch 35/300 - Train Loss: 0.1594, Val Loss: 0.1380\n",
      "Epoch 36/300 - Train Loss: 0.1568, Val Loss: 0.1364\n",
      "Epoch 37/300 - Train Loss: 0.1552, Val Loss: 0.1334\n",
      "Epoch 38/300 - Train Loss: 0.1513, Val Loss: 0.1301\n",
      "Epoch 39/300 - Train Loss: 0.1517, Val Loss: 0.1286\n",
      "Epoch 40/300 - Train Loss: 0.1474, Val Loss: 0.1254\n",
      "Epoch 41/300 - Train Loss: 0.1477, Val Loss: 0.1241\n",
      "Epoch 42/300 - Train Loss: 0.1438, Val Loss: 0.1227\n",
      "Epoch 43/300 - Train Loss: 0.1423, Val Loss: 0.1212\n",
      "Epoch 44/300 - Train Loss: 0.1415, Val Loss: 0.1183\n",
      "Epoch 45/300 - Train Loss: 0.1391, Val Loss: 0.1178\n",
      "Epoch 46/300 - Train Loss: 0.1380, Val Loss: 0.1166\n",
      "Epoch 47/300 - Train Loss: 0.1364, Val Loss: 0.1154\n",
      "Epoch 48/300 - Train Loss: 0.1366, Val Loss: 0.1128\n",
      "Epoch 49/300 - Train Loss: 0.1357, Val Loss: 0.1121\n",
      "Epoch 50/300 - Train Loss: 0.1343, Val Loss: 0.1109\n",
      "Epoch 51/300 - Train Loss: 0.1319, Val Loss: 0.1104\n",
      "Epoch 52/300 - Train Loss: 0.1304, Val Loss: 0.1095\n",
      "Epoch 53/300 - Train Loss: 0.1294, Val Loss: 0.1071\n",
      "Epoch 54/300 - Train Loss: 0.1280, Val Loss: 0.1067\n",
      "Epoch 55/300 - Train Loss: 0.1272, Val Loss: 0.1056\n",
      "Epoch 56/300 - Train Loss: 0.1269, Val Loss: 0.1055\n",
      "Epoch 57/300 - Train Loss: 0.1266, Val Loss: 0.1060\n",
      "Epoch 58/300 - Train Loss: 0.1250, Val Loss: 0.1039\n",
      "Epoch 59/300 - Train Loss: 0.1258, Val Loss: 0.1034\n",
      "Epoch 60/300 - Train Loss: 0.1229, Val Loss: 0.1030\n",
      "Epoch 61/300 - Train Loss: 0.1216, Val Loss: 0.1009\n",
      "Epoch 62/300 - Train Loss: 0.1226, Val Loss: 0.1010\n",
      "Epoch 63/300 - Train Loss: 0.1205, Val Loss: 0.0997\n",
      "Epoch 64/300 - Train Loss: 0.1200, Val Loss: 0.0993\n",
      "Epoch 65/300 - Train Loss: 0.1181, Val Loss: 0.0981\n",
      "Epoch 66/300 - Train Loss: 0.1185, Val Loss: 0.0988\n",
      "Epoch 67/300 - Train Loss: 0.1187, Val Loss: 0.0995\n",
      "Epoch 68/300 - Train Loss: 0.1167, Val Loss: 0.0969\n",
      "Epoch 69/300 - Train Loss: 0.1164, Val Loss: 0.0961\n",
      "Epoch 70/300 - Train Loss: 0.1168, Val Loss: 0.0958\n",
      "Epoch 71/300 - Train Loss: 0.1145, Val Loss: 0.0948\n",
      "Epoch 72/300 - Train Loss: 0.1158, Val Loss: 0.0947\n",
      "Epoch 73/300 - Train Loss: 0.1128, Val Loss: 0.0932\n",
      "Epoch 74/300 - Train Loss: 0.1123, Val Loss: 0.0949\n",
      "Epoch 75/300 - Train Loss: 0.1130, Val Loss: 0.0926\n",
      "Epoch 76/300 - Train Loss: 0.1119, Val Loss: 0.0926\n",
      "Epoch 77/300 - Train Loss: 0.1116, Val Loss: 0.0925\n",
      "Epoch 78/300 - Train Loss: 0.1103, Val Loss: 0.0911\n",
      "Epoch 79/300 - Train Loss: 0.1101, Val Loss: 0.0907\n",
      "Epoch 80/300 - Train Loss: 0.1099, Val Loss: 0.0895\n",
      "Epoch 81/300 - Train Loss: 0.1085, Val Loss: 0.0896\n",
      "Epoch 82/300 - Train Loss: 0.1073, Val Loss: 0.0897\n",
      "Epoch 83/300 - Train Loss: 0.1055, Val Loss: 0.0895\n",
      "Epoch 84/300 - Train Loss: 0.1068, Val Loss: 0.0887\n",
      "Epoch 85/300 - Train Loss: 0.1064, Val Loss: 0.0882\n",
      "Epoch 86/300 - Train Loss: 0.1068, Val Loss: 0.0880\n",
      "Epoch 87/300 - Train Loss: 0.1072, Val Loss: 0.0880\n",
      "Epoch 88/300 - Train Loss: 0.1036, Val Loss: 0.0883\n",
      "Epoch 89/300 - Train Loss: 0.1048, Val Loss: 0.0869\n",
      "Epoch 90/300 - Train Loss: 0.1034, Val Loss: 0.0865\n",
      "Epoch 91/300 - Train Loss: 0.1031, Val Loss: 0.0870\n",
      "Epoch 92/300 - Train Loss: 0.1027, Val Loss: 0.0859\n",
      "Epoch 93/300 - Train Loss: 0.1023, Val Loss: 0.0856\n",
      "Epoch 94/300 - Train Loss: 0.1018, Val Loss: 0.0856\n",
      "Epoch 95/300 - Train Loss: 0.1043, Val Loss: 0.0853\n",
      "Epoch 96/300 - Train Loss: 0.0997, Val Loss: 0.0864\n",
      "Epoch 97/300 - Train Loss: 0.1007, Val Loss: 0.0851\n",
      "Epoch 98/300 - Train Loss: 0.1010, Val Loss: 0.0853\n",
      "Epoch 99/300 - Train Loss: 0.1002, Val Loss: 0.0849\n",
      "Epoch 100/300 - Train Loss: 0.1000, Val Loss: 0.0856\n",
      "Epoch 101/300 - Train Loss: 0.1016, Val Loss: 0.0851\n",
      "Epoch 102/300 - Train Loss: 0.1000, Val Loss: 0.0844\n",
      "Epoch 103/300 - Train Loss: 0.0992, Val Loss: 0.0844\n",
      "Epoch 104/300 - Train Loss: 0.0990, Val Loss: 0.0827\n",
      "Epoch 105/300 - Train Loss: 0.0997, Val Loss: 0.0844\n",
      "Epoch 106/300 - Train Loss: 0.0966, Val Loss: 0.0835\n",
      "Epoch 107/300 - Train Loss: 0.0965, Val Loss: 0.0837\n",
      "Epoch 108/300 - Train Loss: 0.0976, Val Loss: 0.0855\n",
      "Epoch 109/300 - Train Loss: 0.0969, Val Loss: 0.0826\n",
      "Epoch 110/300 - Train Loss: 0.0961, Val Loss: 0.0828\n",
      "Epoch 111/300 - Train Loss: 0.0967, Val Loss: 0.0822\n",
      "Epoch 112/300 - Train Loss: 0.0962, Val Loss: 0.0839\n",
      "Epoch 113/300 - Train Loss: 0.0964, Val Loss: 0.0822\n",
      "Epoch 114/300 - Train Loss: 0.0955, Val Loss: 0.0813\n",
      "Epoch 115/300 - Train Loss: 0.0950, Val Loss: 0.0815\n",
      "Epoch 116/300 - Train Loss: 0.0958, Val Loss: 0.0820\n",
      "Epoch 117/300 - Train Loss: 0.0950, Val Loss: 0.0813\n",
      "Epoch 118/300 - Train Loss: 0.0949, Val Loss: 0.0805\n",
      "Epoch 119/300 - Train Loss: 0.0953, Val Loss: 0.0807\n",
      "Epoch 120/300 - Train Loss: 0.0937, Val Loss: 0.0808\n",
      "Epoch 121/300 - Train Loss: 0.0966, Val Loss: 0.0806\n",
      "Epoch 122/300 - Train Loss: 0.0942, Val Loss: 0.0810\n",
      "Epoch 123/300 - Train Loss: 0.0944, Val Loss: 0.0798\n",
      "Epoch 124/300 - Train Loss: 0.0941, Val Loss: 0.0815\n",
      "Epoch 125/300 - Train Loss: 0.0928, Val Loss: 0.0813\n",
      "Epoch 126/300 - Train Loss: 0.0932, Val Loss: 0.0798\n",
      "Epoch 127/300 - Train Loss: 0.0932, Val Loss: 0.0810\n",
      "Epoch 128/300 - Train Loss: 0.0913, Val Loss: 0.0800\n",
      "Epoch 129/300 - Train Loss: 0.0925, Val Loss: 0.0798\n",
      "Epoch 130/300 - Train Loss: 0.0930, Val Loss: 0.0813\n",
      "Epoch 131/300 - Train Loss: 0.0915, Val Loss: 0.0802\n",
      "Epoch 132/300 - Train Loss: 0.0903, Val Loss: 0.0792\n",
      "Epoch 133/300 - Train Loss: 0.0911, Val Loss: 0.0797\n",
      "Epoch 134/300 - Train Loss: 0.0944, Val Loss: 0.0795\n",
      "Epoch 135/300 - Train Loss: 0.0909, Val Loss: 0.0805\n",
      "Epoch 136/300 - Train Loss: 0.0907, Val Loss: 0.0777\n",
      "Epoch 137/300 - Train Loss: 0.0914, Val Loss: 0.0799\n",
      "Epoch 138/300 - Train Loss: 0.0921, Val Loss: 0.0798\n",
      "Epoch 139/300 - Train Loss: 0.0904, Val Loss: 0.0794\n",
      "Epoch 140/300 - Train Loss: 0.0920, Val Loss: 0.0792\n",
      "Epoch 141/300 - Train Loss: 0.0894, Val Loss: 0.0783\n",
      "Epoch 142/300 - Train Loss: 0.0903, Val Loss: 0.0787\n",
      "Epoch 143/300 - Train Loss: 0.0906, Val Loss: 0.0779\n",
      "Epoch 144/300 - Train Loss: 0.0894, Val Loss: 0.0782\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0903, Val Loss: 0.0774\n",
      "Epoch 146/300 - Train Loss: 0.0883, Val Loss: 0.0790\n",
      "Epoch 147/300 - Train Loss: 0.0894, Val Loss: 0.0781\n",
      "Epoch 148/300 - Train Loss: 0.0894, Val Loss: 0.0779\n",
      "Epoch 149/300 - Train Loss: 0.0888, Val Loss: 0.0782\n",
      "Epoch 150/300 - Train Loss: 0.0877, Val Loss: 0.0782\n",
      "Epoch 151/300 - Train Loss: 0.0885, Val Loss: 0.0773\n",
      "Epoch 152/300 - Train Loss: 0.0881, Val Loss: 0.0775\n",
      "Epoch 153/300 - Train Loss: 0.0885, Val Loss: 0.0773\n",
      "Epoch 154/300 - Train Loss: 0.0900, Val Loss: 0.0779\n",
      "Epoch 155/300 - Train Loss: 0.0886, Val Loss: 0.0785\n",
      "Epoch 156/300 - Train Loss: 0.0882, Val Loss: 0.0776\n",
      "Epoch 157/300 - Train Loss: 0.0877, Val Loss: 0.0761\n",
      "Epoch 158/300 - Train Loss: 0.0880, Val Loss: 0.0765\n",
      "Epoch 159/300 - Train Loss: 0.0881, Val Loss: 0.0771\n",
      "Epoch 160/300 - Train Loss: 0.0883, Val Loss: 0.0773\n",
      "Epoch 161/300 - Train Loss: 0.0876, Val Loss: 0.0774\n",
      "Epoch 162/300 - Train Loss: 0.0878, Val Loss: 0.0771\n",
      "Epoch 163/300 - Train Loss: 0.0881, Val Loss: 0.0767\n",
      "Epoch 164/300 - Train Loss: 0.0872, Val Loss: 0.0780\n",
      "Epoch 165/300 - Train Loss: 0.0877, Val Loss: 0.0779\n",
      "Epoch 166/300 - Train Loss: 0.0870, Val Loss: 0.0760\n",
      "Epoch 167/300 - Train Loss: 0.0886, Val Loss: 0.0763\n",
      "Epoch 168/300 - Train Loss: 0.0880, Val Loss: 0.0766\n",
      "Epoch 169/300 - Train Loss: 0.0886, Val Loss: 0.0790\n",
      "Epoch 170/300 - Train Loss: 0.0893, Val Loss: 0.0757\n",
      "Epoch 171/300 - Train Loss: 0.0880, Val Loss: 0.0796\n",
      "Epoch 172/300 - Train Loss: 0.0884, Val Loss: 0.0787\n",
      "Epoch 173/300 - Train Loss: 0.0877, Val Loss: 0.0772\n",
      "Epoch 174/300 - Train Loss: 0.0880, Val Loss: 0.0767\n",
      "Epoch 175/300 - Train Loss: 0.0885, Val Loss: 0.0769\n",
      "Epoch 176/300 - Train Loss: 0.0863, Val Loss: 0.0764\n",
      "Epoch 177/300 - Train Loss: 0.0882, Val Loss: 0.0800\n",
      "Epoch 178/300 - Train Loss: 0.0879, Val Loss: 0.0761\n",
      "Epoch 179/300 - Train Loss: 0.0863, Val Loss: 0.0772\n",
      "Epoch 180/300 - Train Loss: 0.0874, Val Loss: 0.0756\n",
      "Epoch 181/300 - Train Loss: 0.0865, Val Loss: 0.0774\n",
      "Epoch 182/300 - Train Loss: 0.0876, Val Loss: 0.0763\n",
      "Epoch 183/300 - Train Loss: 0.0881, Val Loss: 0.0753\n",
      "Epoch 184/300 - Train Loss: 0.0896, Val Loss: 0.0758\n",
      "Epoch 185/300 - Train Loss: 0.0852, Val Loss: 0.0761\n",
      "Epoch 186/300 - Train Loss: 0.0864, Val Loss: 0.0766\n",
      "Epoch 187/300 - Train Loss: 0.0869, Val Loss: 0.0761\n",
      "Epoch 188/300 - Train Loss: 0.0863, Val Loss: 0.0761\n",
      "Epoch 189/300 - Train Loss: 0.0879, Val Loss: 0.0749\n",
      "Epoch 190/300 - Train Loss: 0.0863, Val Loss: 0.0765\n",
      "Epoch 191/300 - Train Loss: 0.0882, Val Loss: 0.0752\n",
      "Epoch 192/300 - Train Loss: 0.0843, Val Loss: 0.0768\n",
      "Epoch 193/300 - Train Loss: 0.0870, Val Loss: 0.0762\n",
      "Epoch 194/300 - Train Loss: 0.0857, Val Loss: 0.0767\n",
      "Epoch 195/300 - Train Loss: 0.0862, Val Loss: 0.0758\n",
      "Epoch 196/300 - Train Loss: 0.0862, Val Loss: 0.0760\n",
      "Epoch 197/300 - Train Loss: 0.0854, Val Loss: 0.0761\n",
      "Epoch 198/300 - Train Loss: 0.0869, Val Loss: 0.0762\n",
      "Epoch 199/300 - Train Loss: 0.0854, Val Loss: 0.0770\n",
      "Epoch 200/300 - Train Loss: 0.0851, Val Loss: 0.0760\n",
      "Epoch 201/300 - Train Loss: 0.0878, Val Loss: 0.0756\n",
      "Epoch 202/300 - Train Loss: 0.0859, Val Loss: 0.0774\n",
      "Epoch 203/300 - Train Loss: 0.0841, Val Loss: 0.0753\n",
      "Epoch 204/300 - Train Loss: 0.0853, Val Loss: 0.0754\n",
      "Epoch 205/300 - Train Loss: 0.0855, Val Loss: 0.0761\n",
      "Epoch 206/300 - Train Loss: 0.0886, Val Loss: 0.0777\n",
      "Epoch 207/300 - Train Loss: 0.0853, Val Loss: 0.0764\n",
      "Epoch 208/300 - Train Loss: 0.0849, Val Loss: 0.0763\n",
      "Epoch 209/300 - Train Loss: 0.0851, Val Loss: 0.0758\n",
      "Epoch 210/300 - Train Loss: 0.0858, Val Loss: 0.0772\n",
      "Epoch 211/300 - Train Loss: 0.0867, Val Loss: 0.0759\n",
      "Epoch 212/300 - Train Loss: 0.0857, Val Loss: 0.0755\n",
      "Epoch 213/300 - Train Loss: 0.0849, Val Loss: 0.0780\n",
      "Epoch 214/300 - Train Loss: 0.0869, Val Loss: 0.0756\n",
      "Epoch 215/300 - Train Loss: 0.0852, Val Loss: 0.0751\n",
      "Epoch 216/300 - Train Loss: 0.0845, Val Loss: 0.0783\n",
      "Epoch 217/300 - Train Loss: 0.0848, Val Loss: 0.0750\n",
      "Epoch 218/300 - Train Loss: 0.0847, Val Loss: 0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 01:49:41,740] Trial 41 finished with value: 0.9582115130374408 and parameters: {'F1': 4, 'F2': 16, 'D': 4, 'dropout': 0.17537090434184385, 'learning_rate': 1.4661711544491558e-05, 'batch_size': 256, 'weight_decay': 3.2325332407111046e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 219/300 - Train Loss: 0.0846, Val Loss: 0.0773\n",
      "Early stopping at epoch 219\n",
      "Macro F1 Score: 0.9582, Macro Precision: 0.9488, Macro Recall: 0.9687\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.88      0.95      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 43\n",
      "Training with F1=8, F2=16, D=8, dropout=0.40995592560734545, LR=4.965277694591995e-05, BS=64, WD=1.4662814702577793e-05\n",
      "Epoch 1/300 - Train Loss: 0.6570, Val Loss: 0.3635\n",
      "Epoch 2/300 - Train Loss: 0.2846, Val Loss: 0.2567\n",
      "Epoch 3/300 - Train Loss: 0.2205, Val Loss: 0.1893\n",
      "Epoch 4/300 - Train Loss: 0.1769, Val Loss: 0.1510\n",
      "Epoch 5/300 - Train Loss: 0.1497, Val Loss: 0.1259\n",
      "Epoch 6/300 - Train Loss: 0.1300, Val Loss: 0.1149\n",
      "Epoch 7/300 - Train Loss: 0.1211, Val Loss: 0.1019\n",
      "Epoch 8/300 - Train Loss: 0.1147, Val Loss: 0.1014\n",
      "Epoch 9/300 - Train Loss: 0.1111, Val Loss: 0.0964\n",
      "Epoch 10/300 - Train Loss: 0.1068, Val Loss: 0.0928\n",
      "Epoch 11/300 - Train Loss: 0.1051, Val Loss: 0.0898\n",
      "Epoch 12/300 - Train Loss: 0.1030, Val Loss: 0.0919\n",
      "Epoch 13/300 - Train Loss: 0.1016, Val Loss: 0.0861\n",
      "Epoch 14/300 - Train Loss: 0.1001, Val Loss: 0.0843\n",
      "Epoch 15/300 - Train Loss: 0.0990, Val Loss: 0.0858\n",
      "Epoch 16/300 - Train Loss: 0.0973, Val Loss: 0.0829\n",
      "Epoch 17/300 - Train Loss: 0.0989, Val Loss: 0.0815\n",
      "Epoch 18/300 - Train Loss: 0.0970, Val Loss: 0.0797\n",
      "Epoch 19/300 - Train Loss: 0.0964, Val Loss: 0.0778\n",
      "Epoch 20/300 - Train Loss: 0.0968, Val Loss: 0.0809\n",
      "Epoch 21/300 - Train Loss: 0.0935, Val Loss: 0.0767\n",
      "Epoch 22/300 - Train Loss: 0.0905, Val Loss: 0.0789\n",
      "Epoch 23/300 - Train Loss: 0.0932, Val Loss: 0.0802\n",
      "Epoch 24/300 - Train Loss: 0.0910, Val Loss: 0.0772\n",
      "Epoch 25/300 - Train Loss: 0.0905, Val Loss: 0.0783\n",
      "Epoch 26/300 - Train Loss: 0.0903, Val Loss: 0.0780\n",
      "Epoch 27/300 - Train Loss: 0.0893, Val Loss: 0.0767\n",
      "Epoch 28/300 - Train Loss: 0.0901, Val Loss: 0.0786\n",
      "Epoch 29/300 - Train Loss: 0.0886, Val Loss: 0.0796\n",
      "Epoch 30/300 - Train Loss: 0.0905, Val Loss: 0.0760\n",
      "Epoch 31/300 - Train Loss: 0.0882, Val Loss: 0.0752\n",
      "Epoch 32/300 - Train Loss: 0.0878, Val Loss: 0.0795\n",
      "Epoch 33/300 - Train Loss: 0.0866, Val Loss: 0.0767\n",
      "Epoch 34/300 - Train Loss: 0.0874, Val Loss: 0.0765\n",
      "Epoch 35/300 - Train Loss: 0.0865, Val Loss: 0.0775\n",
      "Epoch 36/300 - Train Loss: 0.0856, Val Loss: 0.0808\n",
      "Epoch 37/300 - Train Loss: 0.0875, Val Loss: 0.0757\n",
      "Epoch 38/300 - Train Loss: 0.0860, Val Loss: 0.0767\n",
      "Epoch 39/300 - Train Loss: 0.0864, Val Loss: 0.0739\n",
      "Epoch 40/300 - Train Loss: 0.0836, Val Loss: 0.0744\n",
      "Epoch 41/300 - Train Loss: 0.0830, Val Loss: 0.0764\n",
      "Epoch 42/300 - Train Loss: 0.0854, Val Loss: 0.0747\n",
      "Epoch 43/300 - Train Loss: 0.0830, Val Loss: 0.0751\n",
      "Epoch 44/300 - Train Loss: 0.0856, Val Loss: 0.0757\n",
      "Epoch 45/300 - Train Loss: 0.0844, Val Loss: 0.0752\n",
      "Epoch 46/300 - Train Loss: 0.0832, Val Loss: 0.0759\n",
      "Epoch 47/300 - Train Loss: 0.0833, Val Loss: 0.0758\n",
      "Epoch 48/300 - Train Loss: 0.0814, Val Loss: 0.0758\n",
      "Epoch 49/300 - Train Loss: 0.0847, Val Loss: 0.0758\n",
      "Epoch 50/300 - Train Loss: 0.0824, Val Loss: 0.0732\n",
      "Epoch 51/300 - Train Loss: 0.0835, Val Loss: 0.0758\n",
      "Epoch 52/300 - Train Loss: 0.0833, Val Loss: 0.0752\n",
      "Epoch 53/300 - Train Loss: 0.0818, Val Loss: 0.0757\n",
      "Epoch 54/300 - Train Loss: 0.0825, Val Loss: 0.0773\n",
      "Epoch 55/300 - Train Loss: 0.0838, Val Loss: 0.0741\n",
      "Epoch 56/300 - Train Loss: 0.0812, Val Loss: 0.0732\n",
      "Epoch 57/300 - Train Loss: 0.0800, Val Loss: 0.0729\n",
      "Epoch 58/300 - Train Loss: 0.0804, Val Loss: 0.0725\n",
      "Epoch 59/300 - Train Loss: 0.0811, Val Loss: 0.0733\n",
      "Epoch 60/300 - Train Loss: 0.0796, Val Loss: 0.0748\n",
      "Epoch 61/300 - Train Loss: 0.0802, Val Loss: 0.0714\n",
      "Epoch 62/300 - Train Loss: 0.0799, Val Loss: 0.0761\n",
      "Epoch 63/300 - Train Loss: 0.0811, Val Loss: 0.0746\n",
      "Epoch 64/300 - Train Loss: 0.0805, Val Loss: 0.0757\n",
      "Epoch 65/300 - Train Loss: 0.0795, Val Loss: 0.0723\n",
      "Epoch 66/300 - Train Loss: 0.0792, Val Loss: 0.0744\n",
      "Epoch 67/300 - Train Loss: 0.0804, Val Loss: 0.0728\n",
      "Epoch 68/300 - Train Loss: 0.0787, Val Loss: 0.0748\n",
      "Epoch 69/300 - Train Loss: 0.0797, Val Loss: 0.0728\n",
      "Epoch 70/300 - Train Loss: 0.0797, Val Loss: 0.0716\n",
      "Epoch 71/300 - Train Loss: 0.0798, Val Loss: 0.0739\n",
      "Epoch 72/300 - Train Loss: 0.0780, Val Loss: 0.0725\n",
      "Epoch 73/300 - Train Loss: 0.0775, Val Loss: 0.0746\n",
      "Epoch 74/300 - Train Loss: 0.0786, Val Loss: 0.0733\n",
      "Epoch 75/300 - Train Loss: 0.0791, Val Loss: 0.0728\n",
      "Epoch 76/300 - Train Loss: 0.0762, Val Loss: 0.0740\n",
      "Epoch 77/300 - Train Loss: 0.0776, Val Loss: 0.0734\n",
      "Epoch 78/300 - Train Loss: 0.0788, Val Loss: 0.0707\n",
      "Epoch 79/300 - Train Loss: 0.0785, Val Loss: 0.0730\n",
      "Epoch 80/300 - Train Loss: 0.0771, Val Loss: 0.0715\n",
      "Epoch 81/300 - Train Loss: 0.0759, Val Loss: 0.0711\n",
      "Epoch 82/300 - Train Loss: 0.0776, Val Loss: 0.0733\n",
      "Epoch 83/300 - Train Loss: 0.0773, Val Loss: 0.0717\n",
      "Epoch 84/300 - Train Loss: 0.0776, Val Loss: 0.0718\n",
      "Epoch 85/300 - Train Loss: 0.0765, Val Loss: 0.0730\n",
      "Epoch 86/300 - Train Loss: 0.0762, Val Loss: 0.0712\n",
      "Epoch 87/300 - Train Loss: 0.0767, Val Loss: 0.0725\n",
      "Epoch 88/300 - Train Loss: 0.0769, Val Loss: 0.0710\n",
      "Epoch 89/300 - Train Loss: 0.0772, Val Loss: 0.0713\n",
      "Epoch 90/300 - Train Loss: 0.0768, Val Loss: 0.0718\n",
      "Epoch 91/300 - Train Loss: 0.0780, Val Loss: 0.0723\n",
      "Epoch 92/300 - Train Loss: 0.0747, Val Loss: 0.0714\n",
      "Epoch 93/300 - Train Loss: 0.0748, Val Loss: 0.0719\n",
      "Epoch 94/300 - Train Loss: 0.0764, Val Loss: 0.0705\n",
      "Epoch 95/300 - Train Loss: 0.0793, Val Loss: 0.0710\n",
      "Epoch 96/300 - Train Loss: 0.0755, Val Loss: 0.0722\n",
      "Epoch 97/300 - Train Loss: 0.0752, Val Loss: 0.0706\n",
      "Epoch 98/300 - Train Loss: 0.0781, Val Loss: 0.0718\n",
      "Epoch 99/300 - Train Loss: 0.0755, Val Loss: 0.0698\n",
      "Epoch 100/300 - Train Loss: 0.0774, Val Loss: 0.0710\n",
      "Epoch 101/300 - Train Loss: 0.0751, Val Loss: 0.0714\n",
      "Epoch 102/300 - Train Loss: 0.0760, Val Loss: 0.0710\n",
      "Epoch 103/300 - Train Loss: 0.0775, Val Loss: 0.0736\n",
      "Epoch 104/300 - Train Loss: 0.0752, Val Loss: 0.0706\n",
      "Epoch 105/300 - Train Loss: 0.0756, Val Loss: 0.0720\n",
      "Epoch 106/300 - Train Loss: 0.0763, Val Loss: 0.0710\n",
      "Epoch 107/300 - Train Loss: 0.0738, Val Loss: 0.0715\n",
      "Epoch 108/300 - Train Loss: 0.0745, Val Loss: 0.0755\n",
      "Epoch 109/300 - Train Loss: 0.0755, Val Loss: 0.0739\n",
      "Epoch 110/300 - Train Loss: 0.0749, Val Loss: 0.0696\n",
      "Epoch 111/300 - Train Loss: 0.0742, Val Loss: 0.0710\n",
      "Epoch 112/300 - Train Loss: 0.0753, Val Loss: 0.0711\n",
      "Epoch 113/300 - Train Loss: 0.0752, Val Loss: 0.0721\n",
      "Epoch 114/300 - Train Loss: 0.0762, Val Loss: 0.0703\n",
      "Epoch 115/300 - Train Loss: 0.0753, Val Loss: 0.0718\n",
      "Epoch 116/300 - Train Loss: 0.0747, Val Loss: 0.0709\n",
      "Epoch 117/300 - Train Loss: 0.0729, Val Loss: 0.0726\n",
      "Epoch 118/300 - Train Loss: 0.0733, Val Loss: 0.0716\n",
      "Epoch 119/300 - Train Loss: 0.0738, Val Loss: 0.0720\n",
      "Epoch 120/300 - Train Loss: 0.0753, Val Loss: 0.0723\n",
      "Epoch 121/300 - Train Loss: 0.0742, Val Loss: 0.0696\n",
      "Epoch 122/300 - Train Loss: 0.0735, Val Loss: 0.0698\n",
      "Epoch 123/300 - Train Loss: 0.0724, Val Loss: 0.0697\n",
      "Epoch 124/300 - Train Loss: 0.0747, Val Loss: 0.0706\n",
      "Epoch 125/300 - Train Loss: 0.0736, Val Loss: 0.0724\n",
      "Epoch 126/300 - Train Loss: 0.0759, Val Loss: 0.0716\n",
      "Epoch 127/300 - Train Loss: 0.0753, Val Loss: 0.0708\n",
      "Epoch 128/300 - Train Loss: 0.0725, Val Loss: 0.0704\n",
      "Epoch 129/300 - Train Loss: 0.0740, Val Loss: 0.0711\n",
      "Epoch 130/300 - Train Loss: 0.0720, Val Loss: 0.0717\n",
      "Epoch 131/300 - Train Loss: 0.0753, Val Loss: 0.0722\n",
      "Epoch 132/300 - Train Loss: 0.0733, Val Loss: 0.0709\n",
      "Epoch 133/300 - Train Loss: 0.0742, Val Loss: 0.0706\n",
      "Epoch 134/300 - Train Loss: 0.0732, Val Loss: 0.0717\n",
      "Epoch 135/300 - Train Loss: 0.0754, Val Loss: 0.0694\n",
      "Epoch 136/300 - Train Loss: 0.0724, Val Loss: 0.0703\n",
      "Epoch 137/300 - Train Loss: 0.0726, Val Loss: 0.0704\n",
      "Epoch 138/300 - Train Loss: 0.0737, Val Loss: 0.0712\n",
      "Epoch 139/300 - Train Loss: 0.0712, Val Loss: 0.0719\n",
      "Epoch 140/300 - Train Loss: 0.0724, Val Loss: 0.0738\n",
      "Epoch 141/300 - Train Loss: 0.0707, Val Loss: 0.0723\n",
      "Epoch 142/300 - Train Loss: 0.0705, Val Loss: 0.0713\n",
      "Epoch 143/300 - Train Loss: 0.0712, Val Loss: 0.0729\n",
      "Epoch 144/300 - Train Loss: 0.0727, Val Loss: 0.0712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0725, Val Loss: 0.0714\n",
      "Epoch 146/300 - Train Loss: 0.0719, Val Loss: 0.0708\n",
      "Epoch 147/300 - Train Loss: 0.0741, Val Loss: 0.0698\n",
      "Epoch 148/300 - Train Loss: 0.0748, Val Loss: 0.0721\n",
      "Epoch 149/300 - Train Loss: 0.0732, Val Loss: 0.0703\n",
      "Epoch 150/300 - Train Loss: 0.0749, Val Loss: 0.0687\n",
      "Epoch 151/300 - Train Loss: 0.0714, Val Loss: 0.0696\n",
      "Epoch 152/300 - Train Loss: 0.0729, Val Loss: 0.0705\n",
      "Epoch 153/300 - Train Loss: 0.0702, Val Loss: 0.0706\n",
      "Epoch 154/300 - Train Loss: 0.0737, Val Loss: 0.0700\n",
      "Epoch 155/300 - Train Loss: 0.0717, Val Loss: 0.0723\n",
      "Epoch 156/300 - Train Loss: 0.0724, Val Loss: 0.0690\n",
      "Epoch 157/300 - Train Loss: 0.0728, Val Loss: 0.0704\n",
      "Epoch 158/300 - Train Loss: 0.0714, Val Loss: 0.0713\n",
      "Epoch 159/300 - Train Loss: 0.0724, Val Loss: 0.0705\n",
      "Epoch 160/300 - Train Loss: 0.0727, Val Loss: 0.0718\n",
      "Epoch 161/300 - Train Loss: 0.0709, Val Loss: 0.0700\n",
      "Epoch 162/300 - Train Loss: 0.0722, Val Loss: 0.0686\n",
      "Epoch 163/300 - Train Loss: 0.0714, Val Loss: 0.0681\n",
      "Epoch 164/300 - Train Loss: 0.0694, Val Loss: 0.0719\n",
      "Epoch 165/300 - Train Loss: 0.0708, Val Loss: 0.0704\n",
      "Epoch 166/300 - Train Loss: 0.0709, Val Loss: 0.0685\n",
      "Epoch 167/300 - Train Loss: 0.0737, Val Loss: 0.0703\n",
      "Epoch 168/300 - Train Loss: 0.0704, Val Loss: 0.0695\n",
      "Epoch 169/300 - Train Loss: 0.0719, Val Loss: 0.0695\n",
      "Epoch 170/300 - Train Loss: 0.0720, Val Loss: 0.0703\n",
      "Epoch 171/300 - Train Loss: 0.0722, Val Loss: 0.0697\n",
      "Epoch 172/300 - Train Loss: 0.0692, Val Loss: 0.0708\n",
      "Epoch 173/300 - Train Loss: 0.0721, Val Loss: 0.0688\n",
      "Epoch 174/300 - Train Loss: 0.0727, Val Loss: 0.0674\n",
      "Epoch 175/300 - Train Loss: 0.0713, Val Loss: 0.0683\n",
      "Epoch 176/300 - Train Loss: 0.0698, Val Loss: 0.0692\n",
      "Epoch 177/300 - Train Loss: 0.0716, Val Loss: 0.0702\n",
      "Epoch 178/300 - Train Loss: 0.0705, Val Loss: 0.0711\n",
      "Epoch 179/300 - Train Loss: 0.0695, Val Loss: 0.0713\n",
      "Epoch 180/300 - Train Loss: 0.0713, Val Loss: 0.0693\n",
      "Epoch 181/300 - Train Loss: 0.0690, Val Loss: 0.0721\n",
      "Epoch 182/300 - Train Loss: 0.0698, Val Loss: 0.0691\n",
      "Epoch 183/300 - Train Loss: 0.0717, Val Loss: 0.0697\n",
      "Epoch 184/300 - Train Loss: 0.0709, Val Loss: 0.0712\n",
      "Epoch 185/300 - Train Loss: 0.0710, Val Loss: 0.0727\n",
      "Epoch 186/300 - Train Loss: 0.0704, Val Loss: 0.0705\n",
      "Epoch 187/300 - Train Loss: 0.0685, Val Loss: 0.0682\n",
      "Epoch 188/300 - Train Loss: 0.0693, Val Loss: 0.0702\n",
      "Epoch 189/300 - Train Loss: 0.0712, Val Loss: 0.0712\n",
      "Epoch 190/300 - Train Loss: 0.0691, Val Loss: 0.0708\n",
      "Epoch 191/300 - Train Loss: 0.0706, Val Loss: 0.0738\n",
      "Epoch 192/300 - Train Loss: 0.0711, Val Loss: 0.0700\n",
      "Epoch 193/300 - Train Loss: 0.0700, Val Loss: 0.0717\n",
      "Epoch 194/300 - Train Loss: 0.0703, Val Loss: 0.0715\n",
      "Epoch 195/300 - Train Loss: 0.0703, Val Loss: 0.0689\n",
      "Epoch 196/300 - Train Loss: 0.0697, Val Loss: 0.0701\n",
      "Epoch 197/300 - Train Loss: 0.0698, Val Loss: 0.0708\n",
      "Epoch 198/300 - Train Loss: 0.0687, Val Loss: 0.0698\n",
      "Epoch 199/300 - Train Loss: 0.0699, Val Loss: 0.0690\n",
      "Epoch 200/300 - Train Loss: 0.0710, Val Loss: 0.0706\n",
      "Epoch 201/300 - Train Loss: 0.0699, Val Loss: 0.0692\n",
      "Epoch 202/300 - Train Loss: 0.0694, Val Loss: 0.0698\n",
      "Epoch 203/300 - Train Loss: 0.0702, Val Loss: 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 01:56:22,459] Trial 42 finished with value: 0.9681980638526193 and parameters: {'F1': 8, 'F2': 16, 'D': 8, 'dropout': 0.40995592560734545, 'learning_rate': 4.965277694591995e-05, 'batch_size': 64, 'weight_decay': 1.4662814702577793e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204/300 - Train Loss: 0.0696, Val Loss: 0.0697\n",
      "Early stopping at epoch 204\n",
      "Macro F1 Score: 0.9682, Macro Precision: 0.9646, Macro Recall: 0.9721\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 44\n",
      "Training with F1=8, F2=16, D=8, dropout=0.40897987153051607, LR=2.4306564976986812e-05, BS=64, WD=0.0035241282826506287\n",
      "Epoch 1/300 - Train Loss: 0.7428, Val Loss: 0.5270\n",
      "Epoch 2/300 - Train Loss: 0.4027, Val Loss: 0.3259\n",
      "Epoch 3/300 - Train Loss: 0.2778, Val Loss: 0.2389\n",
      "Epoch 4/300 - Train Loss: 0.2188, Val Loss: 0.1985\n",
      "Epoch 5/300 - Train Loss: 0.1927, Val Loss: 0.1704\n",
      "Epoch 6/300 - Train Loss: 0.1731, Val Loss: 0.1553\n",
      "Epoch 7/300 - Train Loss: 0.1593, Val Loss: 0.1428\n",
      "Epoch 8/300 - Train Loss: 0.1494, Val Loss: 0.1348\n",
      "Epoch 9/300 - Train Loss: 0.1382, Val Loss: 0.1273\n",
      "Epoch 10/300 - Train Loss: 0.1302, Val Loss: 0.1173\n",
      "Epoch 11/300 - Train Loss: 0.1246, Val Loss: 0.1144\n",
      "Epoch 12/300 - Train Loss: 0.1199, Val Loss: 0.1081\n",
      "Epoch 13/300 - Train Loss: 0.1158, Val Loss: 0.1053\n",
      "Epoch 14/300 - Train Loss: 0.1146, Val Loss: 0.1108\n",
      "Epoch 15/300 - Train Loss: 0.1124, Val Loss: 0.1034\n",
      "Epoch 16/300 - Train Loss: 0.1096, Val Loss: 0.1000\n",
      "Epoch 17/300 - Train Loss: 0.1082, Val Loss: 0.0984\n",
      "Epoch 18/300 - Train Loss: 0.1063, Val Loss: 0.0930\n",
      "Epoch 19/300 - Train Loss: 0.1059, Val Loss: 0.0959\n",
      "Epoch 20/300 - Train Loss: 0.1058, Val Loss: 0.0923\n",
      "Epoch 21/300 - Train Loss: 0.1047, Val Loss: 0.0927\n",
      "Epoch 22/300 - Train Loss: 0.1034, Val Loss: 0.0909\n",
      "Epoch 23/300 - Train Loss: 0.1033, Val Loss: 0.0921\n",
      "Epoch 24/300 - Train Loss: 0.1011, Val Loss: 0.0894\n",
      "Epoch 25/300 - Train Loss: 0.1018, Val Loss: 0.0899\n",
      "Epoch 26/300 - Train Loss: 0.0998, Val Loss: 0.0861\n",
      "Epoch 27/300 - Train Loss: 0.1012, Val Loss: 0.0862\n",
      "Epoch 28/300 - Train Loss: 0.1012, Val Loss: 0.0882\n",
      "Epoch 29/300 - Train Loss: 0.0985, Val Loss: 0.0855\n",
      "Epoch 30/300 - Train Loss: 0.0988, Val Loss: 0.0858\n",
      "Epoch 31/300 - Train Loss: 0.0984, Val Loss: 0.0830\n",
      "Epoch 32/300 - Train Loss: 0.0973, Val Loss: 0.0875\n",
      "Epoch 33/300 - Train Loss: 0.0985, Val Loss: 0.0845\n",
      "Epoch 34/300 - Train Loss: 0.0956, Val Loss: 0.0838\n",
      "Epoch 35/300 - Train Loss: 0.0974, Val Loss: 0.0836\n",
      "Epoch 36/300 - Train Loss: 0.0949, Val Loss: 0.0863\n",
      "Epoch 37/300 - Train Loss: 0.0961, Val Loss: 0.0831\n",
      "Epoch 38/300 - Train Loss: 0.0972, Val Loss: 0.0825\n",
      "Epoch 39/300 - Train Loss: 0.0949, Val Loss: 0.0824\n",
      "Epoch 40/300 - Train Loss: 0.0956, Val Loss: 0.0829\n",
      "Epoch 41/300 - Train Loss: 0.0942, Val Loss: 0.0809\n",
      "Epoch 42/300 - Train Loss: 0.0960, Val Loss: 0.0827\n",
      "Epoch 43/300 - Train Loss: 0.0933, Val Loss: 0.0802\n",
      "Epoch 44/300 - Train Loss: 0.0944, Val Loss: 0.0784\n",
      "Epoch 45/300 - Train Loss: 0.0945, Val Loss: 0.0792\n",
      "Epoch 46/300 - Train Loss: 0.0930, Val Loss: 0.0802\n",
      "Epoch 47/300 - Train Loss: 0.0942, Val Loss: 0.0795\n",
      "Epoch 48/300 - Train Loss: 0.0947, Val Loss: 0.0790\n",
      "Epoch 49/300 - Train Loss: 0.0928, Val Loss: 0.0777\n",
      "Epoch 50/300 - Train Loss: 0.0921, Val Loss: 0.0804\n",
      "Epoch 51/300 - Train Loss: 0.0927, Val Loss: 0.0778\n",
      "Epoch 52/300 - Train Loss: 0.0935, Val Loss: 0.0804\n",
      "Epoch 53/300 - Train Loss: 0.0915, Val Loss: 0.0789\n",
      "Epoch 54/300 - Train Loss: 0.0936, Val Loss: 0.0787\n",
      "Epoch 55/300 - Train Loss: 0.0929, Val Loss: 0.0771\n",
      "Epoch 56/300 - Train Loss: 0.0943, Val Loss: 0.0777\n",
      "Epoch 57/300 - Train Loss: 0.0926, Val Loss: 0.0786\n",
      "Epoch 58/300 - Train Loss: 0.0927, Val Loss: 0.0776\n",
      "Epoch 59/300 - Train Loss: 0.0908, Val Loss: 0.0783\n",
      "Epoch 60/300 - Train Loss: 0.0911, Val Loss: 0.0769\n",
      "Epoch 61/300 - Train Loss: 0.0917, Val Loss: 0.0769\n",
      "Epoch 62/300 - Train Loss: 0.0914, Val Loss: 0.0790\n",
      "Epoch 63/300 - Train Loss: 0.0923, Val Loss: 0.0776\n",
      "Epoch 64/300 - Train Loss: 0.0914, Val Loss: 0.0773\n",
      "Epoch 65/300 - Train Loss: 0.0913, Val Loss: 0.0773\n",
      "Epoch 66/300 - Train Loss: 0.0897, Val Loss: 0.0790\n",
      "Epoch 67/300 - Train Loss: 0.0908, Val Loss: 0.0768\n",
      "Epoch 68/300 - Train Loss: 0.0899, Val Loss: 0.0777\n",
      "Epoch 69/300 - Train Loss: 0.0893, Val Loss: 0.0765\n",
      "Epoch 70/300 - Train Loss: 0.0901, Val Loss: 0.0750\n",
      "Epoch 71/300 - Train Loss: 0.0908, Val Loss: 0.0764\n",
      "Epoch 72/300 - Train Loss: 0.0900, Val Loss: 0.0755\n",
      "Epoch 73/300 - Train Loss: 0.0897, Val Loss: 0.0772\n",
      "Epoch 74/300 - Train Loss: 0.0890, Val Loss: 0.0772\n",
      "Epoch 75/300 - Train Loss: 0.0897, Val Loss: 0.0771\n",
      "Epoch 76/300 - Train Loss: 0.0897, Val Loss: 0.0757\n",
      "Epoch 77/300 - Train Loss: 0.0913, Val Loss: 0.0764\n",
      "Epoch 78/300 - Train Loss: 0.0908, Val Loss: 0.0765\n",
      "Epoch 79/300 - Train Loss: 0.0913, Val Loss: 0.0760\n",
      "Epoch 80/300 - Train Loss: 0.0894, Val Loss: 0.0766\n",
      "Epoch 81/300 - Train Loss: 0.0903, Val Loss: 0.0766\n",
      "Epoch 82/300 - Train Loss: 0.0896, Val Loss: 0.0755\n",
      "Epoch 83/300 - Train Loss: 0.0889, Val Loss: 0.0747\n",
      "Epoch 84/300 - Train Loss: 0.0898, Val Loss: 0.0752\n",
      "Epoch 85/300 - Train Loss: 0.0883, Val Loss: 0.0770\n",
      "Epoch 86/300 - Train Loss: 0.0895, Val Loss: 0.0762\n",
      "Epoch 87/300 - Train Loss: 0.0901, Val Loss: 0.0753\n",
      "Epoch 88/300 - Train Loss: 0.0901, Val Loss: 0.0752\n",
      "Epoch 89/300 - Train Loss: 0.0896, Val Loss: 0.0748\n",
      "Epoch 90/300 - Train Loss: 0.0899, Val Loss: 0.0758\n",
      "Epoch 91/300 - Train Loss: 0.0891, Val Loss: 0.0755\n",
      "Epoch 92/300 - Train Loss: 0.0916, Val Loss: 0.0769\n",
      "Epoch 93/300 - Train Loss: 0.0893, Val Loss: 0.0752\n",
      "Epoch 94/300 - Train Loss: 0.0908, Val Loss: 0.0762\n",
      "Epoch 95/300 - Train Loss: 0.0891, Val Loss: 0.0755\n",
      "Epoch 96/300 - Train Loss: 0.0896, Val Loss: 0.0746\n",
      "Epoch 97/300 - Train Loss: 0.0903, Val Loss: 0.0743\n",
      "Epoch 98/300 - Train Loss: 0.0912, Val Loss: 0.0745\n",
      "Epoch 99/300 - Train Loss: 0.0891, Val Loss: 0.0752\n",
      "Epoch 100/300 - Train Loss: 0.0904, Val Loss: 0.0755\n",
      "Epoch 101/300 - Train Loss: 0.0910, Val Loss: 0.0731\n",
      "Epoch 102/300 - Train Loss: 0.0908, Val Loss: 0.0757\n",
      "Epoch 103/300 - Train Loss: 0.0912, Val Loss: 0.0749\n",
      "Epoch 104/300 - Train Loss: 0.0904, Val Loss: 0.0734\n",
      "Epoch 105/300 - Train Loss: 0.0897, Val Loss: 0.0741\n",
      "Epoch 106/300 - Train Loss: 0.0905, Val Loss: 0.0736\n",
      "Epoch 107/300 - Train Loss: 0.0905, Val Loss: 0.0763\n",
      "Epoch 108/300 - Train Loss: 0.0905, Val Loss: 0.0742\n",
      "Epoch 109/300 - Train Loss: 0.0898, Val Loss: 0.0747\n",
      "Epoch 110/300 - Train Loss: 0.0900, Val Loss: 0.0758\n",
      "Epoch 111/300 - Train Loss: 0.0895, Val Loss: 0.0746\n",
      "Epoch 112/300 - Train Loss: 0.0901, Val Loss: 0.0742\n",
      "Epoch 113/300 - Train Loss: 0.0907, Val Loss: 0.0752\n",
      "Epoch 114/300 - Train Loss: 0.0902, Val Loss: 0.0750\n",
      "Epoch 115/300 - Train Loss: 0.0895, Val Loss: 0.0760\n",
      "Epoch 116/300 - Train Loss: 0.0881, Val Loss: 0.0741\n",
      "Epoch 117/300 - Train Loss: 0.0920, Val Loss: 0.0752\n",
      "Epoch 118/300 - Train Loss: 0.0892, Val Loss: 0.0758\n",
      "Epoch 119/300 - Train Loss: 0.0874, Val Loss: 0.0746\n",
      "Epoch 120/300 - Train Loss: 0.0889, Val Loss: 0.0742\n",
      "Epoch 121/300 - Train Loss: 0.0923, Val Loss: 0.0749\n",
      "Epoch 122/300 - Train Loss: 0.0905, Val Loss: 0.0742\n",
      "Epoch 123/300 - Train Loss: 0.0914, Val Loss: 0.0759\n",
      "Epoch 124/300 - Train Loss: 0.0909, Val Loss: 0.0742\n",
      "Epoch 125/300 - Train Loss: 0.0904, Val Loss: 0.0761\n",
      "Epoch 126/300 - Train Loss: 0.0898, Val Loss: 0.0756\n",
      "Epoch 127/300 - Train Loss: 0.0894, Val Loss: 0.0746\n",
      "Epoch 128/300 - Train Loss: 0.0894, Val Loss: 0.0749\n",
      "Epoch 129/300 - Train Loss: 0.0904, Val Loss: 0.0745\n",
      "Epoch 130/300 - Train Loss: 0.0882, Val Loss: 0.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:00:39,691] Trial 43 finished with value: 0.9644985011430262 and parameters: {'F1': 8, 'F2': 16, 'D': 8, 'dropout': 0.40897987153051607, 'learning_rate': 2.4306564976986812e-05, 'batch_size': 64, 'weight_decay': 0.0035241282826506287}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/300 - Train Loss: 0.0918, Val Loss: 0.0749\n",
      "Early stopping at epoch 131\n",
      "Macro F1 Score: 0.9645, Macro Precision: 0.9589, Macro Recall: 0.9705\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 45\n",
      "Training with F1=8, F2=16, D=8, dropout=0.4669409062367185, LR=4.884182876863905e-05, BS=64, WD=1.000429687236434e-05\n",
      "Epoch 1/300 - Train Loss: 0.6424, Val Loss: 0.3693\n",
      "Epoch 2/300 - Train Loss: 0.2960, Val Loss: 0.2686\n",
      "Epoch 3/300 - Train Loss: 0.2353, Val Loss: 0.1958\n",
      "Epoch 4/300 - Train Loss: 0.1937, Val Loss: 0.1560\n",
      "Epoch 5/300 - Train Loss: 0.1625, Val Loss: 0.1349\n",
      "Epoch 6/300 - Train Loss: 0.1485, Val Loss: 0.1200\n",
      "Epoch 7/300 - Train Loss: 0.1389, Val Loss: 0.1109\n",
      "Epoch 8/300 - Train Loss: 0.1325, Val Loss: 0.1065\n",
      "Epoch 9/300 - Train Loss: 0.1273, Val Loss: 0.1014\n",
      "Epoch 10/300 - Train Loss: 0.1223, Val Loss: 0.0979\n",
      "Epoch 11/300 - Train Loss: 0.1175, Val Loss: 0.0966\n",
      "Epoch 12/300 - Train Loss: 0.1137, Val Loss: 0.0928\n",
      "Epoch 13/300 - Train Loss: 0.1069, Val Loss: 0.0896\n",
      "Epoch 14/300 - Train Loss: 0.1060, Val Loss: 0.0941\n",
      "Epoch 15/300 - Train Loss: 0.1022, Val Loss: 0.0862\n",
      "Epoch 16/300 - Train Loss: 0.1005, Val Loss: 0.0876\n",
      "Epoch 17/300 - Train Loss: 0.1013, Val Loss: 0.0849\n",
      "Epoch 18/300 - Train Loss: 0.0991, Val Loss: 0.0854\n",
      "Epoch 19/300 - Train Loss: 0.1005, Val Loss: 0.0820\n",
      "Epoch 20/300 - Train Loss: 0.0986, Val Loss: 0.0847\n",
      "Epoch 21/300 - Train Loss: 0.0977, Val Loss: 0.0844\n",
      "Epoch 22/300 - Train Loss: 0.0959, Val Loss: 0.0822\n",
      "Epoch 23/300 - Train Loss: 0.0948, Val Loss: 0.0825\n",
      "Epoch 24/300 - Train Loss: 0.0943, Val Loss: 0.0817\n",
      "Epoch 25/300 - Train Loss: 0.0946, Val Loss: 0.0847\n",
      "Epoch 26/300 - Train Loss: 0.0942, Val Loss: 0.0803\n",
      "Epoch 27/300 - Train Loss: 0.0942, Val Loss: 0.0794\n",
      "Epoch 28/300 - Train Loss: 0.0925, Val Loss: 0.0797\n",
      "Epoch 29/300 - Train Loss: 0.0923, Val Loss: 0.0798\n",
      "Epoch 30/300 - Train Loss: 0.0929, Val Loss: 0.0796\n",
      "Epoch 31/300 - Train Loss: 0.0937, Val Loss: 0.0782\n",
      "Epoch 32/300 - Train Loss: 0.0905, Val Loss: 0.0765\n",
      "Epoch 33/300 - Train Loss: 0.0912, Val Loss: 0.0813\n",
      "Epoch 34/300 - Train Loss: 0.0904, Val Loss: 0.0790\n",
      "Epoch 35/300 - Train Loss: 0.0902, Val Loss: 0.0822\n",
      "Epoch 36/300 - Train Loss: 0.0889, Val Loss: 0.0789\n",
      "Epoch 37/300 - Train Loss: 0.0883, Val Loss: 0.0799\n",
      "Epoch 38/300 - Train Loss: 0.0895, Val Loss: 0.0781\n",
      "Epoch 39/300 - Train Loss: 0.0884, Val Loss: 0.0771\n",
      "Epoch 40/300 - Train Loss: 0.0880, Val Loss: 0.0799\n",
      "Epoch 41/300 - Train Loss: 0.0889, Val Loss: 0.0751\n",
      "Epoch 42/300 - Train Loss: 0.0886, Val Loss: 0.0784\n",
      "Epoch 43/300 - Train Loss: 0.0876, Val Loss: 0.0768\n",
      "Epoch 44/300 - Train Loss: 0.0886, Val Loss: 0.0771\n",
      "Epoch 45/300 - Train Loss: 0.0868, Val Loss: 0.0769\n",
      "Epoch 46/300 - Train Loss: 0.0862, Val Loss: 0.0789\n",
      "Epoch 47/300 - Train Loss: 0.0878, Val Loss: 0.0754\n",
      "Epoch 48/300 - Train Loss: 0.0854, Val Loss: 0.0754\n",
      "Epoch 49/300 - Train Loss: 0.0855, Val Loss: 0.0770\n",
      "Epoch 50/300 - Train Loss: 0.0869, Val Loss: 0.0744\n",
      "Epoch 51/300 - Train Loss: 0.0869, Val Loss: 0.0741\n",
      "Epoch 52/300 - Train Loss: 0.0855, Val Loss: 0.0743\n",
      "Epoch 53/300 - Train Loss: 0.0843, Val Loss: 0.0738\n",
      "Epoch 54/300 - Train Loss: 0.0856, Val Loss: 0.0739\n",
      "Epoch 55/300 - Train Loss: 0.0857, Val Loss: 0.0761\n",
      "Epoch 56/300 - Train Loss: 0.0831, Val Loss: 0.0739\n",
      "Epoch 57/300 - Train Loss: 0.0846, Val Loss: 0.0751\n",
      "Epoch 58/300 - Train Loss: 0.0831, Val Loss: 0.0757\n",
      "Epoch 59/300 - Train Loss: 0.0842, Val Loss: 0.0748\n",
      "Epoch 60/300 - Train Loss: 0.0824, Val Loss: 0.0747\n",
      "Epoch 61/300 - Train Loss: 0.0841, Val Loss: 0.0737\n",
      "Epoch 62/300 - Train Loss: 0.0847, Val Loss: 0.0731\n",
      "Epoch 63/300 - Train Loss: 0.0857, Val Loss: 0.0746\n",
      "Epoch 64/300 - Train Loss: 0.0818, Val Loss: 0.0751\n",
      "Epoch 65/300 - Train Loss: 0.0830, Val Loss: 0.0744\n",
      "Epoch 66/300 - Train Loss: 0.0821, Val Loss: 0.0745\n",
      "Epoch 67/300 - Train Loss: 0.0819, Val Loss: 0.0749\n",
      "Epoch 68/300 - Train Loss: 0.0816, Val Loss: 0.0739\n",
      "Epoch 69/300 - Train Loss: 0.0816, Val Loss: 0.0754\n",
      "Epoch 70/300 - Train Loss: 0.0830, Val Loss: 0.0762\n",
      "Epoch 71/300 - Train Loss: 0.0833, Val Loss: 0.0743\n",
      "Epoch 72/300 - Train Loss: 0.0837, Val Loss: 0.0754\n",
      "Epoch 73/300 - Train Loss: 0.0835, Val Loss: 0.0732\n",
      "Epoch 74/300 - Train Loss: 0.0827, Val Loss: 0.0768\n",
      "Epoch 75/300 - Train Loss: 0.0819, Val Loss: 0.0763\n",
      "Epoch 76/300 - Train Loss: 0.0833, Val Loss: 0.0743\n",
      "Epoch 77/300 - Train Loss: 0.0837, Val Loss: 0.0769\n",
      "Epoch 78/300 - Train Loss: 0.0802, Val Loss: 0.0767\n",
      "Epoch 79/300 - Train Loss: 0.0810, Val Loss: 0.0728\n",
      "Epoch 80/300 - Train Loss: 0.0816, Val Loss: 0.0734\n",
      "Epoch 81/300 - Train Loss: 0.0809, Val Loss: 0.0731\n",
      "Epoch 82/300 - Train Loss: 0.0808, Val Loss: 0.0733\n",
      "Epoch 83/300 - Train Loss: 0.0815, Val Loss: 0.0730\n",
      "Epoch 84/300 - Train Loss: 0.0808, Val Loss: 0.0751\n",
      "Epoch 85/300 - Train Loss: 0.0822, Val Loss: 0.0746\n",
      "Epoch 86/300 - Train Loss: 0.0817, Val Loss: 0.0738\n",
      "Epoch 87/300 - Train Loss: 0.0808, Val Loss: 0.0740\n",
      "Epoch 88/300 - Train Loss: 0.0814, Val Loss: 0.0758\n",
      "Epoch 89/300 - Train Loss: 0.0808, Val Loss: 0.0736\n",
      "Epoch 90/300 - Train Loss: 0.0799, Val Loss: 0.0732\n",
      "Epoch 91/300 - Train Loss: 0.0801, Val Loss: 0.0723\n",
      "Epoch 92/300 - Train Loss: 0.0795, Val Loss: 0.0731\n",
      "Epoch 93/300 - Train Loss: 0.0811, Val Loss: 0.0730\n",
      "Epoch 94/300 - Train Loss: 0.0791, Val Loss: 0.0737\n",
      "Epoch 95/300 - Train Loss: 0.0788, Val Loss: 0.0740\n",
      "Epoch 96/300 - Train Loss: 0.0795, Val Loss: 0.0732\n",
      "Epoch 97/300 - Train Loss: 0.0787, Val Loss: 0.0751\n",
      "Epoch 98/300 - Train Loss: 0.0806, Val Loss: 0.0759\n",
      "Epoch 99/300 - Train Loss: 0.0805, Val Loss: 0.0757\n",
      "Epoch 100/300 - Train Loss: 0.0794, Val Loss: 0.0738\n",
      "Epoch 101/300 - Train Loss: 0.0792, Val Loss: 0.0725\n",
      "Epoch 102/300 - Train Loss: 0.0804, Val Loss: 0.0717\n",
      "Epoch 103/300 - Train Loss: 0.0797, Val Loss: 0.0745\n",
      "Epoch 104/300 - Train Loss: 0.0788, Val Loss: 0.0728\n",
      "Epoch 105/300 - Train Loss: 0.0792, Val Loss: 0.0727\n",
      "Epoch 106/300 - Train Loss: 0.0780, Val Loss: 0.0739\n",
      "Epoch 107/300 - Train Loss: 0.0793, Val Loss: 0.0735\n",
      "Epoch 108/300 - Train Loss: 0.0770, Val Loss: 0.0718\n",
      "Epoch 109/300 - Train Loss: 0.0766, Val Loss: 0.0741\n",
      "Epoch 110/300 - Train Loss: 0.0778, Val Loss: 0.0727\n",
      "Epoch 111/300 - Train Loss: 0.0806, Val Loss: 0.0742\n",
      "Epoch 112/300 - Train Loss: 0.0786, Val Loss: 0.0747\n",
      "Epoch 113/300 - Train Loss: 0.0784, Val Loss: 0.0729\n",
      "Epoch 114/300 - Train Loss: 0.0772, Val Loss: 0.0756\n",
      "Epoch 115/300 - Train Loss: 0.0781, Val Loss: 0.0722\n",
      "Epoch 116/300 - Train Loss: 0.0772, Val Loss: 0.0726\n",
      "Epoch 117/300 - Train Loss: 0.0769, Val Loss: 0.0727\n",
      "Epoch 118/300 - Train Loss: 0.0779, Val Loss: 0.0736\n",
      "Epoch 119/300 - Train Loss: 0.0798, Val Loss: 0.0730\n",
      "Epoch 120/300 - Train Loss: 0.0783, Val Loss: 0.0720\n",
      "Epoch 121/300 - Train Loss: 0.0775, Val Loss: 0.0730\n",
      "Epoch 122/300 - Train Loss: 0.0782, Val Loss: 0.0741\n",
      "Epoch 123/300 - Train Loss: 0.0789, Val Loss: 0.0732\n",
      "Epoch 124/300 - Train Loss: 0.0785, Val Loss: 0.0742\n",
      "Epoch 125/300 - Train Loss: 0.0776, Val Loss: 0.0732\n",
      "Epoch 126/300 - Train Loss: 0.0763, Val Loss: 0.0703\n",
      "Epoch 127/300 - Train Loss: 0.0757, Val Loss: 0.0745\n",
      "Epoch 128/300 - Train Loss: 0.0792, Val Loss: 0.0713\n",
      "Epoch 129/300 - Train Loss: 0.0753, Val Loss: 0.0746\n",
      "Epoch 130/300 - Train Loss: 0.0788, Val Loss: 0.0745\n",
      "Epoch 131/300 - Train Loss: 0.0797, Val Loss: 0.0722\n",
      "Epoch 132/300 - Train Loss: 0.0775, Val Loss: 0.0750\n",
      "Epoch 133/300 - Train Loss: 0.0760, Val Loss: 0.0731\n",
      "Epoch 134/300 - Train Loss: 0.0751, Val Loss: 0.0724\n",
      "Epoch 135/300 - Train Loss: 0.0770, Val Loss: 0.0725\n",
      "Epoch 136/300 - Train Loss: 0.0752, Val Loss: 0.0733\n",
      "Epoch 137/300 - Train Loss: 0.0753, Val Loss: 0.0723\n",
      "Epoch 138/300 - Train Loss: 0.0770, Val Loss: 0.0713\n",
      "Epoch 139/300 - Train Loss: 0.0762, Val Loss: 0.0723\n",
      "Epoch 140/300 - Train Loss: 0.0751, Val Loss: 0.0708\n",
      "Epoch 141/300 - Train Loss: 0.0764, Val Loss: 0.0719\n",
      "Epoch 142/300 - Train Loss: 0.0773, Val Loss: 0.0719\n",
      "Epoch 143/300 - Train Loss: 0.0761, Val Loss: 0.0720\n",
      "Epoch 144/300 - Train Loss: 0.0784, Val Loss: 0.0737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0764, Val Loss: 0.0724\n",
      "Epoch 146/300 - Train Loss: 0.0762, Val Loss: 0.0732\n",
      "Epoch 147/300 - Train Loss: 0.0774, Val Loss: 0.0716\n",
      "Epoch 148/300 - Train Loss: 0.0731, Val Loss: 0.0714\n",
      "Epoch 149/300 - Train Loss: 0.0766, Val Loss: 0.0713\n",
      "Epoch 150/300 - Train Loss: 0.0752, Val Loss: 0.0738\n",
      "Epoch 151/300 - Train Loss: 0.0766, Val Loss: 0.0726\n",
      "Epoch 152/300 - Train Loss: 0.0756, Val Loss: 0.0724\n",
      "Epoch 153/300 - Train Loss: 0.0754, Val Loss: 0.0727\n",
      "Epoch 154/300 - Train Loss: 0.0755, Val Loss: 0.0718\n",
      "Epoch 155/300 - Train Loss: 0.0762, Val Loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:05:45,981] Trial 44 finished with value: 0.9648698447136711 and parameters: {'F1': 8, 'F2': 16, 'D': 8, 'dropout': 0.4669409062367185, 'learning_rate': 4.884182876863905e-05, 'batch_size': 64, 'weight_decay': 1.000429687236434e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 156/300 - Train Loss: 0.0759, Val Loss: 0.0728\n",
      "Early stopping at epoch 156\n",
      "Macro F1 Score: 0.9649, Macro Precision: 0.9552, Macro Recall: 0.9756\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 46\n",
      "Training with F1=8, F2=16, D=8, dropout=0.1161254071231082, LR=1.8884040394201456e-05, BS=64, WD=1.4693356953562716e-05\n",
      "Epoch 1/300 - Train Loss: 0.8032, Val Loss: 0.5375\n",
      "Epoch 2/300 - Train Loss: 0.4435, Val Loss: 0.3604\n",
      "Epoch 3/300 - Train Loss: 0.3216, Val Loss: 0.2670\n",
      "Epoch 4/300 - Train Loss: 0.2523, Val Loss: 0.2145\n",
      "Epoch 5/300 - Train Loss: 0.2165, Val Loss: 0.1827\n",
      "Epoch 6/300 - Train Loss: 0.1910, Val Loss: 0.1644\n",
      "Epoch 7/300 - Train Loss: 0.1674, Val Loss: 0.1489\n",
      "Epoch 8/300 - Train Loss: 0.1522, Val Loss: 0.1309\n",
      "Epoch 9/300 - Train Loss: 0.1392, Val Loss: 0.1213\n",
      "Epoch 10/300 - Train Loss: 0.1293, Val Loss: 0.1126\n",
      "Epoch 11/300 - Train Loss: 0.1215, Val Loss: 0.1056\n",
      "Epoch 12/300 - Train Loss: 0.1169, Val Loss: 0.1012\n",
      "Epoch 13/300 - Train Loss: 0.1130, Val Loss: 0.0994\n",
      "Epoch 14/300 - Train Loss: 0.1087, Val Loss: 0.0973\n",
      "Epoch 15/300 - Train Loss: 0.1071, Val Loss: 0.0969\n",
      "Epoch 16/300 - Train Loss: 0.1033, Val Loss: 0.0947\n",
      "Epoch 17/300 - Train Loss: 0.1024, Val Loss: 0.0908\n",
      "Epoch 18/300 - Train Loss: 0.1015, Val Loss: 0.0904\n",
      "Epoch 19/300 - Train Loss: 0.0999, Val Loss: 0.0913\n",
      "Epoch 20/300 - Train Loss: 0.0982, Val Loss: 0.0858\n",
      "Epoch 21/300 - Train Loss: 0.0969, Val Loss: 0.0864\n",
      "Epoch 22/300 - Train Loss: 0.0947, Val Loss: 0.0852\n",
      "Epoch 23/300 - Train Loss: 0.0963, Val Loss: 0.0878\n",
      "Epoch 24/300 - Train Loss: 0.0940, Val Loss: 0.0855\n",
      "Epoch 25/300 - Train Loss: 0.0933, Val Loss: 0.0873\n",
      "Epoch 26/300 - Train Loss: 0.0944, Val Loss: 0.0854\n",
      "Epoch 27/300 - Train Loss: 0.0928, Val Loss: 0.0845\n",
      "Epoch 28/300 - Train Loss: 0.0923, Val Loss: 0.0826\n",
      "Epoch 29/300 - Train Loss: 0.0927, Val Loss: 0.0813\n",
      "Epoch 30/300 - Train Loss: 0.0908, Val Loss: 0.0840\n",
      "Epoch 31/300 - Train Loss: 0.0896, Val Loss: 0.0821\n",
      "Epoch 32/300 - Train Loss: 0.0895, Val Loss: 0.0822\n",
      "Epoch 33/300 - Train Loss: 0.0890, Val Loss: 0.0816\n",
      "Epoch 34/300 - Train Loss: 0.0881, Val Loss: 0.0800\n",
      "Epoch 35/300 - Train Loss: 0.0882, Val Loss: 0.0812\n",
      "Epoch 36/300 - Train Loss: 0.0881, Val Loss: 0.0802\n",
      "Epoch 37/300 - Train Loss: 0.0878, Val Loss: 0.0796\n",
      "Epoch 38/300 - Train Loss: 0.0869, Val Loss: 0.0811\n",
      "Epoch 39/300 - Train Loss: 0.0870, Val Loss: 0.0824\n",
      "Epoch 40/300 - Train Loss: 0.0864, Val Loss: 0.0792\n",
      "Epoch 41/300 - Train Loss: 0.0861, Val Loss: 0.0808\n",
      "Epoch 42/300 - Train Loss: 0.0853, Val Loss: 0.0798\n",
      "Epoch 43/300 - Train Loss: 0.0846, Val Loss: 0.0792\n",
      "Epoch 44/300 - Train Loss: 0.0855, Val Loss: 0.0780\n",
      "Epoch 45/300 - Train Loss: 0.0848, Val Loss: 0.0803\n",
      "Epoch 46/300 - Train Loss: 0.0842, Val Loss: 0.0788\n",
      "Epoch 47/300 - Train Loss: 0.0845, Val Loss: 0.0773\n",
      "Epoch 48/300 - Train Loss: 0.0856, Val Loss: 0.0789\n",
      "Epoch 49/300 - Train Loss: 0.0840, Val Loss: 0.0780\n",
      "Epoch 50/300 - Train Loss: 0.0817, Val Loss: 0.0814\n",
      "Epoch 51/300 - Train Loss: 0.0821, Val Loss: 0.0776\n",
      "Epoch 52/300 - Train Loss: 0.0822, Val Loss: 0.0753\n",
      "Epoch 53/300 - Train Loss: 0.0829, Val Loss: 0.0783\n",
      "Epoch 54/300 - Train Loss: 0.0804, Val Loss: 0.0771\n",
      "Epoch 55/300 - Train Loss: 0.0819, Val Loss: 0.0785\n",
      "Epoch 56/300 - Train Loss: 0.0807, Val Loss: 0.0753\n",
      "Epoch 57/300 - Train Loss: 0.0810, Val Loss: 0.0769\n",
      "Epoch 58/300 - Train Loss: 0.0810, Val Loss: 0.0758\n",
      "Epoch 59/300 - Train Loss: 0.0819, Val Loss: 0.0773\n",
      "Epoch 60/300 - Train Loss: 0.0797, Val Loss: 0.0788\n",
      "Epoch 61/300 - Train Loss: 0.0809, Val Loss: 0.0762\n",
      "Epoch 62/300 - Train Loss: 0.0806, Val Loss: 0.0785\n",
      "Epoch 63/300 - Train Loss: 0.0800, Val Loss: 0.0771\n",
      "Epoch 64/300 - Train Loss: 0.0808, Val Loss: 0.0761\n",
      "Epoch 65/300 - Train Loss: 0.0802, Val Loss: 0.0752\n",
      "Epoch 66/300 - Train Loss: 0.0791, Val Loss: 0.0754\n",
      "Epoch 67/300 - Train Loss: 0.0797, Val Loss: 0.0754\n",
      "Epoch 68/300 - Train Loss: 0.0784, Val Loss: 0.0755\n",
      "Epoch 69/300 - Train Loss: 0.0801, Val Loss: 0.0760\n",
      "Epoch 70/300 - Train Loss: 0.0779, Val Loss: 0.0745\n",
      "Epoch 71/300 - Train Loss: 0.0776, Val Loss: 0.0772\n",
      "Epoch 72/300 - Train Loss: 0.0787, Val Loss: 0.0754\n",
      "Epoch 73/300 - Train Loss: 0.0789, Val Loss: 0.0738\n",
      "Epoch 74/300 - Train Loss: 0.0782, Val Loss: 0.0743\n",
      "Epoch 75/300 - Train Loss: 0.0785, Val Loss: 0.0753\n",
      "Epoch 76/300 - Train Loss: 0.0760, Val Loss: 0.0734\n",
      "Epoch 77/300 - Train Loss: 0.0763, Val Loss: 0.0741\n",
      "Epoch 78/300 - Train Loss: 0.0766, Val Loss: 0.0727\n",
      "Epoch 79/300 - Train Loss: 0.0754, Val Loss: 0.0726\n",
      "Epoch 80/300 - Train Loss: 0.0759, Val Loss: 0.0734\n",
      "Epoch 81/300 - Train Loss: 0.0756, Val Loss: 0.0740\n",
      "Epoch 82/300 - Train Loss: 0.0769, Val Loss: 0.0713\n",
      "Epoch 83/300 - Train Loss: 0.0761, Val Loss: 0.0734\n",
      "Epoch 84/300 - Train Loss: 0.0759, Val Loss: 0.0746\n",
      "Epoch 85/300 - Train Loss: 0.0753, Val Loss: 0.0733\n",
      "Epoch 86/300 - Train Loss: 0.0759, Val Loss: 0.0735\n",
      "Epoch 87/300 - Train Loss: 0.0754, Val Loss: 0.0723\n",
      "Epoch 88/300 - Train Loss: 0.0745, Val Loss: 0.0737\n",
      "Epoch 89/300 - Train Loss: 0.0751, Val Loss: 0.0730\n",
      "Epoch 90/300 - Train Loss: 0.0746, Val Loss: 0.0733\n",
      "Epoch 91/300 - Train Loss: 0.0737, Val Loss: 0.0749\n",
      "Epoch 92/300 - Train Loss: 0.0749, Val Loss: 0.0717\n",
      "Epoch 93/300 - Train Loss: 0.0750, Val Loss: 0.0719\n",
      "Epoch 94/300 - Train Loss: 0.0735, Val Loss: 0.0726\n",
      "Epoch 95/300 - Train Loss: 0.0747, Val Loss: 0.0730\n",
      "Epoch 96/300 - Train Loss: 0.0732, Val Loss: 0.0725\n",
      "Epoch 97/300 - Train Loss: 0.0722, Val Loss: 0.0715\n",
      "Epoch 98/300 - Train Loss: 0.0726, Val Loss: 0.0736\n",
      "Epoch 99/300 - Train Loss: 0.0731, Val Loss: 0.0710\n",
      "Epoch 100/300 - Train Loss: 0.0728, Val Loss: 0.0723\n",
      "Epoch 101/300 - Train Loss: 0.0733, Val Loss: 0.0723\n",
      "Epoch 102/300 - Train Loss: 0.0723, Val Loss: 0.0719\n",
      "Epoch 103/300 - Train Loss: 0.0722, Val Loss: 0.0717\n",
      "Epoch 104/300 - Train Loss: 0.0731, Val Loss: 0.0732\n",
      "Epoch 105/300 - Train Loss: 0.0736, Val Loss: 0.0720\n",
      "Epoch 106/300 - Train Loss: 0.0716, Val Loss: 0.0725\n",
      "Epoch 107/300 - Train Loss: 0.0710, Val Loss: 0.0707\n",
      "Epoch 108/300 - Train Loss: 0.0722, Val Loss: 0.0698\n",
      "Epoch 109/300 - Train Loss: 0.0707, Val Loss: 0.0712\n",
      "Epoch 110/300 - Train Loss: 0.0718, Val Loss: 0.0698\n",
      "Epoch 111/300 - Train Loss: 0.0714, Val Loss: 0.0727\n",
      "Epoch 112/300 - Train Loss: 0.0720, Val Loss: 0.0723\n",
      "Epoch 113/300 - Train Loss: 0.0713, Val Loss: 0.0699\n",
      "Epoch 114/300 - Train Loss: 0.0712, Val Loss: 0.0700\n",
      "Epoch 115/300 - Train Loss: 0.0706, Val Loss: 0.0702\n",
      "Epoch 116/300 - Train Loss: 0.0709, Val Loss: 0.0714\n",
      "Epoch 117/300 - Train Loss: 0.0693, Val Loss: 0.0723\n",
      "Epoch 118/300 - Train Loss: 0.0714, Val Loss: 0.0689\n",
      "Epoch 119/300 - Train Loss: 0.0712, Val Loss: 0.0717\n",
      "Epoch 120/300 - Train Loss: 0.0717, Val Loss: 0.0691\n",
      "Epoch 121/300 - Train Loss: 0.0702, Val Loss: 0.0714\n",
      "Epoch 122/300 - Train Loss: 0.0694, Val Loss: 0.0720\n",
      "Epoch 123/300 - Train Loss: 0.0708, Val Loss: 0.0705\n",
      "Epoch 124/300 - Train Loss: 0.0692, Val Loss: 0.0716\n",
      "Epoch 125/300 - Train Loss: 0.0703, Val Loss: 0.0694\n",
      "Epoch 126/300 - Train Loss: 0.0689, Val Loss: 0.0697\n",
      "Epoch 127/300 - Train Loss: 0.0695, Val Loss: 0.0716\n",
      "Epoch 128/300 - Train Loss: 0.0698, Val Loss: 0.0710\n",
      "Epoch 129/300 - Train Loss: 0.0695, Val Loss: 0.0706\n",
      "Epoch 130/300 - Train Loss: 0.0685, Val Loss: 0.0700\n",
      "Epoch 131/300 - Train Loss: 0.0695, Val Loss: 0.0711\n",
      "Epoch 132/300 - Train Loss: 0.0685, Val Loss: 0.0701\n",
      "Epoch 133/300 - Train Loss: 0.0697, Val Loss: 0.0692\n",
      "Epoch 134/300 - Train Loss: 0.0697, Val Loss: 0.0694\n",
      "Epoch 135/300 - Train Loss: 0.0695, Val Loss: 0.0703\n",
      "Epoch 136/300 - Train Loss: 0.0678, Val Loss: 0.0709\n",
      "Epoch 137/300 - Train Loss: 0.0674, Val Loss: 0.0710\n",
      "Epoch 138/300 - Train Loss: 0.0686, Val Loss: 0.0713\n",
      "Epoch 139/300 - Train Loss: 0.0674, Val Loss: 0.0713\n",
      "Epoch 140/300 - Train Loss: 0.0682, Val Loss: 0.0698\n",
      "Epoch 141/300 - Train Loss: 0.0679, Val Loss: 0.0695\n",
      "Epoch 142/300 - Train Loss: 0.0678, Val Loss: 0.0695\n",
      "Epoch 143/300 - Train Loss: 0.0683, Val Loss: 0.0677\n",
      "Epoch 144/300 - Train Loss: 0.0690, Val Loss: 0.0696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0678, Val Loss: 0.0679\n",
      "Epoch 146/300 - Train Loss: 0.0669, Val Loss: 0.0702\n",
      "Epoch 147/300 - Train Loss: 0.0668, Val Loss: 0.0680\n",
      "Epoch 148/300 - Train Loss: 0.0684, Val Loss: 0.0681\n",
      "Epoch 149/300 - Train Loss: 0.0663, Val Loss: 0.0683\n",
      "Epoch 150/300 - Train Loss: 0.0683, Val Loss: 0.0684\n",
      "Epoch 151/300 - Train Loss: 0.0673, Val Loss: 0.0681\n",
      "Epoch 152/300 - Train Loss: 0.0679, Val Loss: 0.0682\n",
      "Epoch 153/300 - Train Loss: 0.0670, Val Loss: 0.0680\n",
      "Epoch 154/300 - Train Loss: 0.0678, Val Loss: 0.0689\n",
      "Epoch 155/300 - Train Loss: 0.0659, Val Loss: 0.0683\n",
      "Epoch 156/300 - Train Loss: 0.0671, Val Loss: 0.0704\n",
      "Epoch 157/300 - Train Loss: 0.0665, Val Loss: 0.0715\n",
      "Epoch 158/300 - Train Loss: 0.0667, Val Loss: 0.0706\n",
      "Epoch 159/300 - Train Loss: 0.0661, Val Loss: 0.0678\n",
      "Epoch 160/300 - Train Loss: 0.0662, Val Loss: 0.0696\n",
      "Epoch 161/300 - Train Loss: 0.0661, Val Loss: 0.0693\n",
      "Epoch 162/300 - Train Loss: 0.0656, Val Loss: 0.0696\n",
      "Epoch 163/300 - Train Loss: 0.0663, Val Loss: 0.0697\n",
      "Epoch 164/300 - Train Loss: 0.0663, Val Loss: 0.0674\n",
      "Epoch 165/300 - Train Loss: 0.0664, Val Loss: 0.0689\n",
      "Epoch 166/300 - Train Loss: 0.0652, Val Loss: 0.0687\n",
      "Epoch 167/300 - Train Loss: 0.0653, Val Loss: 0.0688\n",
      "Epoch 168/300 - Train Loss: 0.0658, Val Loss: 0.0675\n",
      "Epoch 169/300 - Train Loss: 0.0650, Val Loss: 0.0697\n",
      "Epoch 170/300 - Train Loss: 0.0648, Val Loss: 0.0709\n",
      "Epoch 171/300 - Train Loss: 0.0644, Val Loss: 0.0668\n",
      "Epoch 172/300 - Train Loss: 0.0651, Val Loss: 0.0667\n",
      "Epoch 173/300 - Train Loss: 0.0657, Val Loss: 0.0678\n",
      "Epoch 174/300 - Train Loss: 0.0663, Val Loss: 0.0684\n",
      "Epoch 175/300 - Train Loss: 0.0649, Val Loss: 0.0700\n",
      "Epoch 176/300 - Train Loss: 0.0645, Val Loss: 0.0673\n",
      "Epoch 177/300 - Train Loss: 0.0661, Val Loss: 0.0671\n",
      "Epoch 178/300 - Train Loss: 0.0656, Val Loss: 0.0698\n",
      "Epoch 179/300 - Train Loss: 0.0647, Val Loss: 0.0675\n",
      "Epoch 180/300 - Train Loss: 0.0647, Val Loss: 0.0683\n",
      "Epoch 181/300 - Train Loss: 0.0640, Val Loss: 0.0687\n",
      "Epoch 182/300 - Train Loss: 0.0635, Val Loss: 0.0684\n",
      "Epoch 183/300 - Train Loss: 0.0662, Val Loss: 0.0695\n",
      "Epoch 184/300 - Train Loss: 0.0636, Val Loss: 0.0682\n",
      "Epoch 185/300 - Train Loss: 0.0642, Val Loss: 0.0676\n",
      "Epoch 186/300 - Train Loss: 0.0659, Val Loss: 0.0680\n",
      "Epoch 187/300 - Train Loss: 0.0641, Val Loss: 0.0691\n",
      "Epoch 188/300 - Train Loss: 0.0648, Val Loss: 0.0663\n",
      "Epoch 189/300 - Train Loss: 0.0642, Val Loss: 0.0688\n",
      "Epoch 190/300 - Train Loss: 0.0633, Val Loss: 0.0682\n",
      "Epoch 191/300 - Train Loss: 0.0629, Val Loss: 0.0695\n",
      "Epoch 192/300 - Train Loss: 0.0645, Val Loss: 0.0665\n",
      "Epoch 193/300 - Train Loss: 0.0644, Val Loss: 0.0677\n",
      "Epoch 194/300 - Train Loss: 0.0637, Val Loss: 0.0677\n",
      "Epoch 195/300 - Train Loss: 0.0634, Val Loss: 0.0682\n",
      "Epoch 196/300 - Train Loss: 0.0629, Val Loss: 0.0681\n",
      "Epoch 197/300 - Train Loss: 0.0633, Val Loss: 0.0701\n",
      "Epoch 198/300 - Train Loss: 0.0635, Val Loss: 0.0688\n",
      "Epoch 199/300 - Train Loss: 0.0626, Val Loss: 0.0682\n",
      "Epoch 200/300 - Train Loss: 0.0634, Val Loss: 0.0676\n",
      "Epoch 201/300 - Train Loss: 0.0626, Val Loss: 0.0671\n",
      "Epoch 202/300 - Train Loss: 0.0625, Val Loss: 0.0665\n",
      "Epoch 203/300 - Train Loss: 0.0619, Val Loss: 0.0711\n",
      "Epoch 204/300 - Train Loss: 0.0637, Val Loss: 0.0689\n",
      "Epoch 205/300 - Train Loss: 0.0641, Val Loss: 0.0706\n",
      "Epoch 206/300 - Train Loss: 0.0641, Val Loss: 0.0675\n",
      "Epoch 207/300 - Train Loss: 0.0625, Val Loss: 0.0684\n",
      "Epoch 208/300 - Train Loss: 0.0635, Val Loss: 0.0676\n",
      "Epoch 209/300 - Train Loss: 0.0632, Val Loss: 0.0658\n",
      "Epoch 210/300 - Train Loss: 0.0634, Val Loss: 0.0676\n",
      "Epoch 211/300 - Train Loss: 0.0629, Val Loss: 0.0659\n",
      "Epoch 212/300 - Train Loss: 0.0610, Val Loss: 0.0669\n",
      "Epoch 213/300 - Train Loss: 0.0615, Val Loss: 0.0674\n",
      "Epoch 214/300 - Train Loss: 0.0616, Val Loss: 0.0687\n",
      "Epoch 215/300 - Train Loss: 0.0626, Val Loss: 0.0678\n",
      "Epoch 216/300 - Train Loss: 0.0616, Val Loss: 0.0667\n",
      "Epoch 217/300 - Train Loss: 0.0613, Val Loss: 0.0678\n",
      "Epoch 218/300 - Train Loss: 0.0628, Val Loss: 0.0660\n",
      "Epoch 219/300 - Train Loss: 0.0625, Val Loss: 0.0666\n",
      "Epoch 220/300 - Train Loss: 0.0604, Val Loss: 0.0673\n",
      "Epoch 221/300 - Train Loss: 0.0613, Val Loss: 0.0674\n",
      "Epoch 222/300 - Train Loss: 0.0611, Val Loss: 0.0673\n",
      "Epoch 223/300 - Train Loss: 0.0619, Val Loss: 0.0681\n",
      "Epoch 224/300 - Train Loss: 0.0614, Val Loss: 0.0679\n",
      "Epoch 225/300 - Train Loss: 0.0625, Val Loss: 0.0672\n",
      "Epoch 226/300 - Train Loss: 0.0605, Val Loss: 0.0680\n",
      "Epoch 227/300 - Train Loss: 0.0610, Val Loss: 0.0660\n",
      "Epoch 228/300 - Train Loss: 0.0632, Val Loss: 0.0677\n",
      "Epoch 229/300 - Train Loss: 0.0616, Val Loss: 0.0689\n",
      "Epoch 230/300 - Train Loss: 0.0608, Val Loss: 0.0674\n",
      "Epoch 231/300 - Train Loss: 0.0621, Val Loss: 0.0663\n",
      "Epoch 232/300 - Train Loss: 0.0618, Val Loss: 0.0672\n",
      "Epoch 233/300 - Train Loss: 0.0625, Val Loss: 0.0690\n",
      "Epoch 234/300 - Train Loss: 0.0616, Val Loss: 0.0684\n",
      "Epoch 235/300 - Train Loss: 0.0615, Val Loss: 0.0687\n",
      "Epoch 236/300 - Train Loss: 0.0599, Val Loss: 0.0691\n",
      "Epoch 237/300 - Train Loss: 0.0599, Val Loss: 0.0703\n",
      "Epoch 238/300 - Train Loss: 0.0608, Val Loss: 0.0659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:13:35,366] Trial 45 finished with value: 0.9596511768631912 and parameters: {'F1': 8, 'F2': 16, 'D': 8, 'dropout': 0.1161254071231082, 'learning_rate': 1.8884040394201456e-05, 'batch_size': 64, 'weight_decay': 1.4693356953562716e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 239/300 - Train Loss: 0.0608, Val Loss: 0.0674\n",
      "Early stopping at epoch 239\n",
      "Macro F1 Score: 0.9597, Macro Precision: 0.9460, Macro Recall: 0.9751\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.87      0.97      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 47\n",
      "Training with F1=8, F2=16, D=8, dropout=0.36522739093980694, LR=1.1853905218092865e-05, BS=64, WD=2.5841331391839062e-05\n",
      "Epoch 1/300 - Train Loss: 1.0026, Val Loss: 0.8365\n",
      "Epoch 2/300 - Train Loss: 0.6904, Val Loss: 0.5680\n",
      "Epoch 3/300 - Train Loss: 0.4944, Val Loss: 0.4324\n",
      "Epoch 4/300 - Train Loss: 0.3993, Val Loss: 0.3657\n",
      "Epoch 5/300 - Train Loss: 0.3411, Val Loss: 0.3226\n",
      "Epoch 6/300 - Train Loss: 0.2980, Val Loss: 0.2756\n",
      "Epoch 7/300 - Train Loss: 0.2659, Val Loss: 0.2391\n",
      "Epoch 8/300 - Train Loss: 0.2419, Val Loss: 0.2193\n",
      "Epoch 9/300 - Train Loss: 0.2227, Val Loss: 0.1968\n",
      "Epoch 10/300 - Train Loss: 0.2049, Val Loss: 0.1828\n",
      "Epoch 11/300 - Train Loss: 0.1906, Val Loss: 0.1663\n",
      "Epoch 12/300 - Train Loss: 0.1762, Val Loss: 0.1496\n",
      "Epoch 13/300 - Train Loss: 0.1642, Val Loss: 0.1429\n",
      "Epoch 14/300 - Train Loss: 0.1524, Val Loss: 0.1313\n",
      "Epoch 15/300 - Train Loss: 0.1420, Val Loss: 0.1241\n",
      "Epoch 16/300 - Train Loss: 0.1368, Val Loss: 0.1158\n",
      "Epoch 17/300 - Train Loss: 0.1306, Val Loss: 0.1119\n",
      "Epoch 18/300 - Train Loss: 0.1234, Val Loss: 0.1113\n",
      "Epoch 19/300 - Train Loss: 0.1211, Val Loss: 0.1059\n",
      "Epoch 20/300 - Train Loss: 0.1174, Val Loss: 0.1048\n",
      "Epoch 21/300 - Train Loss: 0.1141, Val Loss: 0.1024\n",
      "Epoch 22/300 - Train Loss: 0.1152, Val Loss: 0.0987\n",
      "Epoch 23/300 - Train Loss: 0.1113, Val Loss: 0.1051\n",
      "Epoch 24/300 - Train Loss: 0.1108, Val Loss: 0.0991\n",
      "Epoch 25/300 - Train Loss: 0.1089, Val Loss: 0.0949\n",
      "Epoch 26/300 - Train Loss: 0.1086, Val Loss: 0.0963\n",
      "Epoch 27/300 - Train Loss: 0.1065, Val Loss: 0.0936\n",
      "Epoch 28/300 - Train Loss: 0.1048, Val Loss: 0.0942\n",
      "Epoch 29/300 - Train Loss: 0.1057, Val Loss: 0.0907\n",
      "Epoch 30/300 - Train Loss: 0.1024, Val Loss: 0.0918\n",
      "Epoch 31/300 - Train Loss: 0.1010, Val Loss: 0.0922\n",
      "Epoch 32/300 - Train Loss: 0.1034, Val Loss: 0.0895\n",
      "Epoch 33/300 - Train Loss: 0.1020, Val Loss: 0.0893\n",
      "Epoch 34/300 - Train Loss: 0.1004, Val Loss: 0.0920\n",
      "Epoch 35/300 - Train Loss: 0.1014, Val Loss: 0.0873\n",
      "Epoch 36/300 - Train Loss: 0.1017, Val Loss: 0.0917\n",
      "Epoch 37/300 - Train Loss: 0.0992, Val Loss: 0.0857\n",
      "Epoch 38/300 - Train Loss: 0.1020, Val Loss: 0.0862\n",
      "Epoch 39/300 - Train Loss: 0.0995, Val Loss: 0.0858\n",
      "Epoch 40/300 - Train Loss: 0.0980, Val Loss: 0.0882\n",
      "Epoch 41/300 - Train Loss: 0.0994, Val Loss: 0.0865\n",
      "Epoch 42/300 - Train Loss: 0.0974, Val Loss: 0.0843\n",
      "Epoch 43/300 - Train Loss: 0.0993, Val Loss: 0.0850\n",
      "Epoch 44/300 - Train Loss: 0.0978, Val Loss: 0.0844\n",
      "Epoch 45/300 - Train Loss: 0.0970, Val Loss: 0.0841\n",
      "Epoch 46/300 - Train Loss: 0.0970, Val Loss: 0.0837\n",
      "Epoch 47/300 - Train Loss: 0.0968, Val Loss: 0.0827\n",
      "Epoch 48/300 - Train Loss: 0.0950, Val Loss: 0.0831\n",
      "Epoch 49/300 - Train Loss: 0.0968, Val Loss: 0.0851\n",
      "Epoch 50/300 - Train Loss: 0.0957, Val Loss: 0.0814\n",
      "Epoch 51/300 - Train Loss: 0.0939, Val Loss: 0.0822\n",
      "Epoch 52/300 - Train Loss: 0.0958, Val Loss: 0.0809\n",
      "Epoch 53/300 - Train Loss: 0.0953, Val Loss: 0.0827\n",
      "Epoch 54/300 - Train Loss: 0.0937, Val Loss: 0.0822\n",
      "Epoch 55/300 - Train Loss: 0.0946, Val Loss: 0.0818\n",
      "Epoch 56/300 - Train Loss: 0.0943, Val Loss: 0.0819\n",
      "Epoch 57/300 - Train Loss: 0.0948, Val Loss: 0.0809\n",
      "Epoch 58/300 - Train Loss: 0.0957, Val Loss: 0.0798\n",
      "Epoch 59/300 - Train Loss: 0.0933, Val Loss: 0.0803\n",
      "Epoch 60/300 - Train Loss: 0.0942, Val Loss: 0.0811\n",
      "Epoch 61/300 - Train Loss: 0.0934, Val Loss: 0.0795\n",
      "Epoch 62/300 - Train Loss: 0.0932, Val Loss: 0.0804\n",
      "Epoch 63/300 - Train Loss: 0.0924, Val Loss: 0.0803\n",
      "Epoch 64/300 - Train Loss: 0.0939, Val Loss: 0.0805\n",
      "Epoch 65/300 - Train Loss: 0.0927, Val Loss: 0.0796\n",
      "Epoch 66/300 - Train Loss: 0.0918, Val Loss: 0.0798\n",
      "Epoch 67/300 - Train Loss: 0.0919, Val Loss: 0.0806\n",
      "Epoch 68/300 - Train Loss: 0.0910, Val Loss: 0.0788\n",
      "Epoch 69/300 - Train Loss: 0.0919, Val Loss: 0.0784\n",
      "Epoch 70/300 - Train Loss: 0.0902, Val Loss: 0.0807\n",
      "Epoch 71/300 - Train Loss: 0.0900, Val Loss: 0.0803\n",
      "Epoch 72/300 - Train Loss: 0.0900, Val Loss: 0.0797\n",
      "Epoch 73/300 - Train Loss: 0.0908, Val Loss: 0.0778\n",
      "Epoch 74/300 - Train Loss: 0.0908, Val Loss: 0.0785\n",
      "Epoch 75/300 - Train Loss: 0.0924, Val Loss: 0.0794\n",
      "Epoch 76/300 - Train Loss: 0.0895, Val Loss: 0.0784\n",
      "Epoch 77/300 - Train Loss: 0.0895, Val Loss: 0.0796\n",
      "Epoch 78/300 - Train Loss: 0.0893, Val Loss: 0.0798\n",
      "Epoch 79/300 - Train Loss: 0.0907, Val Loss: 0.0781\n",
      "Epoch 80/300 - Train Loss: 0.0893, Val Loss: 0.0769\n",
      "Epoch 81/300 - Train Loss: 0.0907, Val Loss: 0.0781\n",
      "Epoch 82/300 - Train Loss: 0.0888, Val Loss: 0.0776\n",
      "Epoch 83/300 - Train Loss: 0.0898, Val Loss: 0.0779\n",
      "Epoch 84/300 - Train Loss: 0.0886, Val Loss: 0.0788\n",
      "Epoch 85/300 - Train Loss: 0.0876, Val Loss: 0.0779\n",
      "Epoch 86/300 - Train Loss: 0.0903, Val Loss: 0.0783\n",
      "Epoch 87/300 - Train Loss: 0.0897, Val Loss: 0.0783\n",
      "Epoch 88/300 - Train Loss: 0.0886, Val Loss: 0.0795\n",
      "Epoch 89/300 - Train Loss: 0.0896, Val Loss: 0.0785\n",
      "Epoch 90/300 - Train Loss: 0.0880, Val Loss: 0.0777\n",
      "Epoch 91/300 - Train Loss: 0.0876, Val Loss: 0.0776\n",
      "Epoch 92/300 - Train Loss: 0.0873, Val Loss: 0.0772\n",
      "Epoch 93/300 - Train Loss: 0.0889, Val Loss: 0.0778\n",
      "Epoch 94/300 - Train Loss: 0.0893, Val Loss: 0.0759\n",
      "Epoch 95/300 - Train Loss: 0.0893, Val Loss: 0.0750\n",
      "Epoch 96/300 - Train Loss: 0.0892, Val Loss: 0.0782\n",
      "Epoch 97/300 - Train Loss: 0.0867, Val Loss: 0.0768\n",
      "Epoch 98/300 - Train Loss: 0.0877, Val Loss: 0.0767\n",
      "Epoch 99/300 - Train Loss: 0.0894, Val Loss: 0.0773\n",
      "Epoch 100/300 - Train Loss: 0.0873, Val Loss: 0.0773\n",
      "Epoch 101/300 - Train Loss: 0.0889, Val Loss: 0.0759\n",
      "Epoch 102/300 - Train Loss: 0.0875, Val Loss: 0.0769\n",
      "Epoch 103/300 - Train Loss: 0.0882, Val Loss: 0.0756\n",
      "Epoch 104/300 - Train Loss: 0.0891, Val Loss: 0.0769\n",
      "Epoch 105/300 - Train Loss: 0.0873, Val Loss: 0.0768\n",
      "Epoch 106/300 - Train Loss: 0.0858, Val Loss: 0.0759\n",
      "Epoch 107/300 - Train Loss: 0.0879, Val Loss: 0.0767\n",
      "Epoch 108/300 - Train Loss: 0.0873, Val Loss: 0.0782\n",
      "Epoch 109/300 - Train Loss: 0.0850, Val Loss: 0.0767\n",
      "Epoch 110/300 - Train Loss: 0.0866, Val Loss: 0.0770\n",
      "Epoch 111/300 - Train Loss: 0.0857, Val Loss: 0.0787\n",
      "Epoch 112/300 - Train Loss: 0.0867, Val Loss: 0.0762\n",
      "Epoch 113/300 - Train Loss: 0.0849, Val Loss: 0.0782\n",
      "Epoch 114/300 - Train Loss: 0.0855, Val Loss: 0.0768\n",
      "Epoch 115/300 - Train Loss: 0.0856, Val Loss: 0.0760\n",
      "Epoch 116/300 - Train Loss: 0.0867, Val Loss: 0.0781\n",
      "Epoch 117/300 - Train Loss: 0.0864, Val Loss: 0.0751\n",
      "Epoch 118/300 - Train Loss: 0.0862, Val Loss: 0.0774\n",
      "Epoch 119/300 - Train Loss: 0.0853, Val Loss: 0.0758\n",
      "Epoch 120/300 - Train Loss: 0.0870, Val Loss: 0.0759\n",
      "Epoch 121/300 - Train Loss: 0.0850, Val Loss: 0.0766\n",
      "Epoch 122/300 - Train Loss: 0.0849, Val Loss: 0.0776\n",
      "Epoch 123/300 - Train Loss: 0.0846, Val Loss: 0.0755\n",
      "Epoch 124/300 - Train Loss: 0.0847, Val Loss: 0.0757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:17:41,422] Trial 46 finished with value: 0.9582075114212892 and parameters: {'F1': 8, 'F2': 16, 'D': 8, 'dropout': 0.36522739093980694, 'learning_rate': 1.1853905218092865e-05, 'batch_size': 64, 'weight_decay': 2.5841331391839062e-05}. Best is trial 22 with value: 0.9700424024810431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/300 - Train Loss: 0.0863, Val Loss: 0.0762\n",
      "Early stopping at epoch 125\n",
      "Macro F1 Score: 0.9582, Macro Precision: 0.9490, Macro Recall: 0.9686\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.88      0.95      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 48\n",
      "Training with F1=8, F2=32, D=8, dropout=0.5327538193366854, LR=7.545287053034844e-05, BS=64, WD=3.760613019604944e-05\n",
      "Epoch 1/300 - Train Loss: 0.5239, Val Loss: 0.2790\n",
      "Epoch 2/300 - Train Loss: 0.2317, Val Loss: 0.1989\n",
      "Epoch 3/300 - Train Loss: 0.1877, Val Loss: 0.1466\n",
      "Epoch 4/300 - Train Loss: 0.1516, Val Loss: 0.1317\n",
      "Epoch 5/300 - Train Loss: 0.1289, Val Loss: 0.1098\n",
      "Epoch 6/300 - Train Loss: 0.1166, Val Loss: 0.0988\n",
      "Epoch 7/300 - Train Loss: 0.1104, Val Loss: 0.0963\n",
      "Epoch 8/300 - Train Loss: 0.1078, Val Loss: 0.0978\n",
      "Epoch 9/300 - Train Loss: 0.1049, Val Loss: 0.0973\n",
      "Epoch 10/300 - Train Loss: 0.1012, Val Loss: 0.0896\n",
      "Epoch 11/300 - Train Loss: 0.1012, Val Loss: 0.0897\n",
      "Epoch 12/300 - Train Loss: 0.0997, Val Loss: 0.0896\n",
      "Epoch 13/300 - Train Loss: 0.0980, Val Loss: 0.0903\n",
      "Epoch 14/300 - Train Loss: 0.0972, Val Loss: 0.0842\n",
      "Epoch 15/300 - Train Loss: 0.0964, Val Loss: 0.0817\n",
      "Epoch 16/300 - Train Loss: 0.0932, Val Loss: 0.0840\n",
      "Epoch 17/300 - Train Loss: 0.0930, Val Loss: 0.0864\n",
      "Epoch 18/300 - Train Loss: 0.0926, Val Loss: 0.0811\n",
      "Epoch 19/300 - Train Loss: 0.0913, Val Loss: 0.0848\n",
      "Epoch 20/300 - Train Loss: 0.0925, Val Loss: 0.0812\n",
      "Epoch 21/300 - Train Loss: 0.0910, Val Loss: 0.0795\n",
      "Epoch 22/300 - Train Loss: 0.0907, Val Loss: 0.0787\n",
      "Epoch 23/300 - Train Loss: 0.0904, Val Loss: 0.0790\n",
      "Epoch 24/300 - Train Loss: 0.0896, Val Loss: 0.0808\n",
      "Epoch 25/300 - Train Loss: 0.0907, Val Loss: 0.0831\n",
      "Epoch 26/300 - Train Loss: 0.0891, Val Loss: 0.0812\n",
      "Epoch 27/300 - Train Loss: 0.0885, Val Loss: 0.0784\n",
      "Epoch 28/300 - Train Loss: 0.0879, Val Loss: 0.0767\n",
      "Epoch 29/300 - Train Loss: 0.0861, Val Loss: 0.0761\n",
      "Epoch 30/300 - Train Loss: 0.0866, Val Loss: 0.0807\n",
      "Epoch 31/300 - Train Loss: 0.0857, Val Loss: 0.0744\n",
      "Epoch 32/300 - Train Loss: 0.0861, Val Loss: 0.0777\n",
      "Epoch 33/300 - Train Loss: 0.0879, Val Loss: 0.0805\n",
      "Epoch 34/300 - Train Loss: 0.0849, Val Loss: 0.0759\n",
      "Epoch 35/300 - Train Loss: 0.0844, Val Loss: 0.0742\n",
      "Epoch 36/300 - Train Loss: 0.0835, Val Loss: 0.0794\n",
      "Epoch 37/300 - Train Loss: 0.0846, Val Loss: 0.0769\n",
      "Epoch 38/300 - Train Loss: 0.0829, Val Loss: 0.0758\n",
      "Epoch 39/300 - Train Loss: 0.0827, Val Loss: 0.0780\n",
      "Epoch 40/300 - Train Loss: 0.0819, Val Loss: 0.0734\n",
      "Epoch 41/300 - Train Loss: 0.0832, Val Loss: 0.0747\n",
      "Epoch 42/300 - Train Loss: 0.0820, Val Loss: 0.0744\n",
      "Epoch 43/300 - Train Loss: 0.0813, Val Loss: 0.0734\n",
      "Epoch 44/300 - Train Loss: 0.0825, Val Loss: 0.0727\n",
      "Epoch 45/300 - Train Loss: 0.0822, Val Loss: 0.0754\n",
      "Epoch 46/300 - Train Loss: 0.0815, Val Loss: 0.0740\n",
      "Epoch 47/300 - Train Loss: 0.0808, Val Loss: 0.0730\n",
      "Epoch 48/300 - Train Loss: 0.0817, Val Loss: 0.0744\n",
      "Epoch 49/300 - Train Loss: 0.0792, Val Loss: 0.0730\n",
      "Epoch 50/300 - Train Loss: 0.0797, Val Loss: 0.0774\n",
      "Epoch 51/300 - Train Loss: 0.0809, Val Loss: 0.0706\n",
      "Epoch 52/300 - Train Loss: 0.0799, Val Loss: 0.0734\n",
      "Epoch 53/300 - Train Loss: 0.0797, Val Loss: 0.0743\n",
      "Epoch 54/300 - Train Loss: 0.0808, Val Loss: 0.0754\n",
      "Epoch 55/300 - Train Loss: 0.0786, Val Loss: 0.0722\n",
      "Epoch 56/300 - Train Loss: 0.0797, Val Loss: 0.0722\n",
      "Epoch 57/300 - Train Loss: 0.0781, Val Loss: 0.0693\n",
      "Epoch 58/300 - Train Loss: 0.0803, Val Loss: 0.0726\n",
      "Epoch 59/300 - Train Loss: 0.0775, Val Loss: 0.0726\n",
      "Epoch 60/300 - Train Loss: 0.0786, Val Loss: 0.0709\n",
      "Epoch 61/300 - Train Loss: 0.0771, Val Loss: 0.0710\n",
      "Epoch 62/300 - Train Loss: 0.0779, Val Loss: 0.0706\n",
      "Epoch 63/300 - Train Loss: 0.0787, Val Loss: 0.0743\n",
      "Epoch 64/300 - Train Loss: 0.0787, Val Loss: 0.0724\n",
      "Epoch 65/300 - Train Loss: 0.0760, Val Loss: 0.0737\n",
      "Epoch 66/300 - Train Loss: 0.0766, Val Loss: 0.0723\n",
      "Epoch 67/300 - Train Loss: 0.0758, Val Loss: 0.0738\n",
      "Epoch 68/300 - Train Loss: 0.0782, Val Loss: 0.0721\n",
      "Epoch 69/300 - Train Loss: 0.0788, Val Loss: 0.0711\n",
      "Epoch 70/300 - Train Loss: 0.0765, Val Loss: 0.0691\n",
      "Epoch 71/300 - Train Loss: 0.0763, Val Loss: 0.0702\n",
      "Epoch 72/300 - Train Loss: 0.0763, Val Loss: 0.0690\n",
      "Epoch 73/300 - Train Loss: 0.0749, Val Loss: 0.0706\n",
      "Epoch 74/300 - Train Loss: 0.0775, Val Loss: 0.0701\n",
      "Epoch 75/300 - Train Loss: 0.0749, Val Loss: 0.0705\n",
      "Epoch 76/300 - Train Loss: 0.0753, Val Loss: 0.0724\n",
      "Epoch 77/300 - Train Loss: 0.0761, Val Loss: 0.0726\n",
      "Epoch 78/300 - Train Loss: 0.0764, Val Loss: 0.0720\n",
      "Epoch 79/300 - Train Loss: 0.0755, Val Loss: 0.0696\n",
      "Epoch 80/300 - Train Loss: 0.0749, Val Loss: 0.0702\n",
      "Epoch 81/300 - Train Loss: 0.0742, Val Loss: 0.0721\n",
      "Epoch 82/300 - Train Loss: 0.0742, Val Loss: 0.0675\n",
      "Epoch 83/300 - Train Loss: 0.0752, Val Loss: 0.0713\n",
      "Epoch 84/300 - Train Loss: 0.0740, Val Loss: 0.0683\n",
      "Epoch 85/300 - Train Loss: 0.0760, Val Loss: 0.0700\n",
      "Epoch 86/300 - Train Loss: 0.0748, Val Loss: 0.0704\n",
      "Epoch 87/300 - Train Loss: 0.0749, Val Loss: 0.0700\n",
      "Epoch 88/300 - Train Loss: 0.0734, Val Loss: 0.0713\n",
      "Epoch 89/300 - Train Loss: 0.0742, Val Loss: 0.0675\n",
      "Epoch 90/300 - Train Loss: 0.0741, Val Loss: 0.0721\n",
      "Epoch 91/300 - Train Loss: 0.0728, Val Loss: 0.0670\n",
      "Epoch 92/300 - Train Loss: 0.0729, Val Loss: 0.0710\n",
      "Epoch 93/300 - Train Loss: 0.0738, Val Loss: 0.0709\n",
      "Epoch 94/300 - Train Loss: 0.0743, Val Loss: 0.0705\n",
      "Epoch 95/300 - Train Loss: 0.0719, Val Loss: 0.0721\n",
      "Epoch 96/300 - Train Loss: 0.0730, Val Loss: 0.0696\n",
      "Epoch 97/300 - Train Loss: 0.0731, Val Loss: 0.0707\n",
      "Epoch 98/300 - Train Loss: 0.0714, Val Loss: 0.0706\n",
      "Epoch 99/300 - Train Loss: 0.0719, Val Loss: 0.0718\n",
      "Epoch 100/300 - Train Loss: 0.0717, Val Loss: 0.0698\n",
      "Epoch 101/300 - Train Loss: 0.0734, Val Loss: 0.0688\n",
      "Epoch 102/300 - Train Loss: 0.0717, Val Loss: 0.0687\n",
      "Epoch 103/300 - Train Loss: 0.0748, Val Loss: 0.0682\n",
      "Epoch 104/300 - Train Loss: 0.0751, Val Loss: 0.0727\n",
      "Epoch 105/300 - Train Loss: 0.0726, Val Loss: 0.0718\n",
      "Epoch 106/300 - Train Loss: 0.0727, Val Loss: 0.0697\n",
      "Epoch 107/300 - Train Loss: 0.0709, Val Loss: 0.0670\n",
      "Epoch 108/300 - Train Loss: 0.0718, Val Loss: 0.0720\n",
      "Epoch 109/300 - Train Loss: 0.0716, Val Loss: 0.0697\n",
      "Epoch 110/300 - Train Loss: 0.0718, Val Loss: 0.0676\n",
      "Epoch 111/300 - Train Loss: 0.0716, Val Loss: 0.0690\n",
      "Epoch 112/300 - Train Loss: 0.0726, Val Loss: 0.0699\n",
      "Epoch 113/300 - Train Loss: 0.0731, Val Loss: 0.0689\n",
      "Epoch 114/300 - Train Loss: 0.0726, Val Loss: 0.0697\n",
      "Epoch 115/300 - Train Loss: 0.0705, Val Loss: 0.0689\n",
      "Epoch 116/300 - Train Loss: 0.0695, Val Loss: 0.0715\n",
      "Epoch 117/300 - Train Loss: 0.0736, Val Loss: 0.0699\n",
      "Epoch 118/300 - Train Loss: 0.0730, Val Loss: 0.0683\n",
      "Epoch 119/300 - Train Loss: 0.0725, Val Loss: 0.0700\n",
      "Epoch 120/300 - Train Loss: 0.0722, Val Loss: 0.0691\n",
      "Epoch 121/300 - Train Loss: 0.0718, Val Loss: 0.0702\n",
      "Epoch 122/300 - Train Loss: 0.0708, Val Loss: 0.0674\n",
      "Epoch 123/300 - Train Loss: 0.0698, Val Loss: 0.0672\n",
      "Epoch 124/300 - Train Loss: 0.0728, Val Loss: 0.0717\n",
      "Epoch 125/300 - Train Loss: 0.0708, Val Loss: 0.0700\n",
      "Epoch 126/300 - Train Loss: 0.0700, Val Loss: 0.0704\n",
      "Epoch 127/300 - Train Loss: 0.0708, Val Loss: 0.0687\n",
      "Epoch 128/300 - Train Loss: 0.0688, Val Loss: 0.0700\n",
      "Epoch 129/300 - Train Loss: 0.0684, Val Loss: 0.0690\n",
      "Epoch 130/300 - Train Loss: 0.0707, Val Loss: 0.0698\n",
      "Epoch 131/300 - Train Loss: 0.0695, Val Loss: 0.0690\n",
      "Epoch 132/300 - Train Loss: 0.0682, Val Loss: 0.0695\n",
      "Epoch 133/300 - Train Loss: 0.0690, Val Loss: 0.0716\n",
      "Epoch 134/300 - Train Loss: 0.0697, Val Loss: 0.0692\n",
      "Epoch 135/300 - Train Loss: 0.0710, Val Loss: 0.0727\n",
      "Epoch 136/300 - Train Loss: 0.0714, Val Loss: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:22:22,815] Trial 47 finished with value: 0.9720323096544233 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.5327538193366854, 'learning_rate': 7.545287053034844e-05, 'batch_size': 64, 'weight_decay': 3.760613019604944e-05}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/300 - Train Loss: 0.0692, Val Loss: 0.0681\n",
      "Early stopping at epoch 137\n",
      "Macro F1 Score: 0.9720, Macro Precision: 0.9618, Macro Recall: 0.9833\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 49\n",
      "Training with F1=4, F2=32, D=4, dropout=0.6668827240806698, LR=0.0001211958140115106, BS=256, WD=3.7628062797290926e-05\n",
      "Epoch 1/300 - Train Loss: 0.7848, Val Loss: 0.5987\n",
      "Epoch 2/300 - Train Loss: 0.4555, Val Loss: 0.4141\n",
      "Epoch 3/300 - Train Loss: 0.3736, Val Loss: 0.3724\n",
      "Epoch 4/300 - Train Loss: 0.3371, Val Loss: 0.3353\n",
      "Epoch 5/300 - Train Loss: 0.3064, Val Loss: 0.3027\n",
      "Epoch 6/300 - Train Loss: 0.2796, Val Loss: 0.2689\n",
      "Epoch 7/300 - Train Loss: 0.2601, Val Loss: 0.2464\n",
      "Epoch 8/300 - Train Loss: 0.2479, Val Loss: 0.2221\n",
      "Epoch 9/300 - Train Loss: 0.2263, Val Loss: 0.1999\n",
      "Epoch 10/300 - Train Loss: 0.1983, Val Loss: 0.1609\n",
      "Epoch 11/300 - Train Loss: 0.1668, Val Loss: 0.1446\n",
      "Epoch 12/300 - Train Loss: 0.1517, Val Loss: 0.1333\n",
      "Epoch 13/300 - Train Loss: 0.1473, Val Loss: 0.1295\n",
      "Epoch 14/300 - Train Loss: 0.1433, Val Loss: 0.1236\n",
      "Epoch 15/300 - Train Loss: 0.1381, Val Loss: 0.1244\n",
      "Epoch 16/300 - Train Loss: 0.1371, Val Loss: 0.1195\n",
      "Epoch 17/300 - Train Loss: 0.1324, Val Loss: 0.1160\n",
      "Epoch 18/300 - Train Loss: 0.1314, Val Loss: 0.1163\n",
      "Epoch 19/300 - Train Loss: 0.1302, Val Loss: 0.1165\n",
      "Epoch 20/300 - Train Loss: 0.1293, Val Loss: 0.1210\n",
      "Epoch 21/300 - Train Loss: 0.1272, Val Loss: 0.1149\n",
      "Epoch 22/300 - Train Loss: 0.1249, Val Loss: 0.1134\n",
      "Epoch 23/300 - Train Loss: 0.1262, Val Loss: 0.1095\n",
      "Epoch 24/300 - Train Loss: 0.1251, Val Loss: 0.1123\n",
      "Epoch 25/300 - Train Loss: 0.1240, Val Loss: 0.1072\n",
      "Epoch 26/300 - Train Loss: 0.1235, Val Loss: 0.1072\n",
      "Epoch 27/300 - Train Loss: 0.1236, Val Loss: 0.1098\n",
      "Epoch 28/300 - Train Loss: 0.1207, Val Loss: 0.1096\n",
      "Epoch 29/300 - Train Loss: 0.1216, Val Loss: 0.1026\n",
      "Epoch 30/300 - Train Loss: 0.1230, Val Loss: 0.1056\n",
      "Epoch 31/300 - Train Loss: 0.1210, Val Loss: 0.1045\n",
      "Epoch 32/300 - Train Loss: 0.1195, Val Loss: 0.1042\n",
      "Epoch 33/300 - Train Loss: 0.1182, Val Loss: 0.1032\n",
      "Epoch 34/300 - Train Loss: 0.1193, Val Loss: 0.1018\n",
      "Epoch 35/300 - Train Loss: 0.1189, Val Loss: 0.1018\n",
      "Epoch 36/300 - Train Loss: 0.1193, Val Loss: 0.1067\n",
      "Epoch 37/300 - Train Loss: 0.1155, Val Loss: 0.1043\n",
      "Epoch 38/300 - Train Loss: 0.1156, Val Loss: 0.1082\n",
      "Epoch 39/300 - Train Loss: 0.1146, Val Loss: 0.1020\n",
      "Epoch 40/300 - Train Loss: 0.1133, Val Loss: 0.1048\n",
      "Epoch 41/300 - Train Loss: 0.1149, Val Loss: 0.1026\n",
      "Epoch 42/300 - Train Loss: 0.1128, Val Loss: 0.1060\n",
      "Epoch 43/300 - Train Loss: 0.1117, Val Loss: 0.1040\n",
      "Epoch 44/300 - Train Loss: 0.1123, Val Loss: 0.1011\n",
      "Epoch 45/300 - Train Loss: 0.1088, Val Loss: 0.0969\n",
      "Epoch 46/300 - Train Loss: 0.1097, Val Loss: 0.0980\n",
      "Epoch 47/300 - Train Loss: 0.1070, Val Loss: 0.1012\n",
      "Epoch 48/300 - Train Loss: 0.1080, Val Loss: 0.0949\n",
      "Epoch 49/300 - Train Loss: 0.1076, Val Loss: 0.1021\n",
      "Epoch 50/300 - Train Loss: 0.1050, Val Loss: 0.0974\n",
      "Epoch 51/300 - Train Loss: 0.1054, Val Loss: 0.1012\n",
      "Epoch 52/300 - Train Loss: 0.1061, Val Loss: 0.0978\n",
      "Epoch 53/300 - Train Loss: 0.1050, Val Loss: 0.0973\n",
      "Epoch 54/300 - Train Loss: 0.1038, Val Loss: 0.0980\n",
      "Epoch 55/300 - Train Loss: 0.1035, Val Loss: 0.1006\n",
      "Epoch 56/300 - Train Loss: 0.1073, Val Loss: 0.0989\n",
      "Epoch 57/300 - Train Loss: 0.1060, Val Loss: 0.1019\n",
      "Epoch 58/300 - Train Loss: 0.1030, Val Loss: 0.0997\n",
      "Epoch 59/300 - Train Loss: 0.1033, Val Loss: 0.0989\n",
      "Epoch 60/300 - Train Loss: 0.1045, Val Loss: 0.0971\n",
      "Epoch 61/300 - Train Loss: 0.1047, Val Loss: 0.0941\n",
      "Epoch 62/300 - Train Loss: 0.1031, Val Loss: 0.0940\n",
      "Epoch 63/300 - Train Loss: 0.1019, Val Loss: 0.1007\n",
      "Epoch 64/300 - Train Loss: 0.1028, Val Loss: 0.0987\n",
      "Epoch 65/300 - Train Loss: 0.1033, Val Loss: 0.0947\n",
      "Epoch 66/300 - Train Loss: 0.1037, Val Loss: 0.0951\n",
      "Epoch 67/300 - Train Loss: 0.1019, Val Loss: 0.0946\n",
      "Epoch 68/300 - Train Loss: 0.1001, Val Loss: 0.0936\n",
      "Epoch 69/300 - Train Loss: 0.1010, Val Loss: 0.0980\n",
      "Epoch 70/300 - Train Loss: 0.1028, Val Loss: 0.0937\n",
      "Epoch 71/300 - Train Loss: 0.1012, Val Loss: 0.0955\n",
      "Epoch 72/300 - Train Loss: 0.1049, Val Loss: 0.0984\n",
      "Epoch 73/300 - Train Loss: 0.1015, Val Loss: 0.0957\n",
      "Epoch 74/300 - Train Loss: 0.1011, Val Loss: 0.0937\n",
      "Epoch 75/300 - Train Loss: 0.0998, Val Loss: 0.0978\n",
      "Epoch 76/300 - Train Loss: 0.1003, Val Loss: 0.0922\n",
      "Epoch 77/300 - Train Loss: 0.1004, Val Loss: 0.0929\n",
      "Epoch 78/300 - Train Loss: 0.0991, Val Loss: 0.0957\n",
      "Epoch 79/300 - Train Loss: 0.1029, Val Loss: 0.0924\n",
      "Epoch 80/300 - Train Loss: 0.0997, Val Loss: 0.0938\n",
      "Epoch 81/300 - Train Loss: 0.1006, Val Loss: 0.0932\n",
      "Epoch 82/300 - Train Loss: 0.1018, Val Loss: 0.0921\n",
      "Epoch 83/300 - Train Loss: 0.0995, Val Loss: 0.0930\n",
      "Epoch 84/300 - Train Loss: 0.1010, Val Loss: 0.0944\n",
      "Epoch 85/300 - Train Loss: 0.1018, Val Loss: 0.0967\n",
      "Epoch 86/300 - Train Loss: 0.1047, Val Loss: 0.0927\n",
      "Epoch 87/300 - Train Loss: 0.1005, Val Loss: 0.0955\n",
      "Epoch 88/300 - Train Loss: 0.1004, Val Loss: 0.0934\n",
      "Epoch 89/300 - Train Loss: 0.1027, Val Loss: 0.0948\n",
      "Epoch 90/300 - Train Loss: 0.1006, Val Loss: 0.0996\n",
      "Epoch 91/300 - Train Loss: 0.0977, Val Loss: 0.0930\n",
      "Epoch 92/300 - Train Loss: 0.0991, Val Loss: 0.0963\n",
      "Epoch 93/300 - Train Loss: 0.0982, Val Loss: 0.0913\n",
      "Epoch 94/300 - Train Loss: 0.0989, Val Loss: 0.0926\n",
      "Epoch 95/300 - Train Loss: 0.1008, Val Loss: 0.0931\n",
      "Epoch 96/300 - Train Loss: 0.0987, Val Loss: 0.0925\n",
      "Epoch 97/300 - Train Loss: 0.0995, Val Loss: 0.0934\n",
      "Epoch 98/300 - Train Loss: 0.0987, Val Loss: 0.0890\n",
      "Epoch 99/300 - Train Loss: 0.0982, Val Loss: 0.0962\n",
      "Epoch 100/300 - Train Loss: 0.1005, Val Loss: 0.0948\n",
      "Epoch 101/300 - Train Loss: 0.0966, Val Loss: 0.0928\n",
      "Epoch 102/300 - Train Loss: 0.0962, Val Loss: 0.0955\n",
      "Epoch 103/300 - Train Loss: 0.0971, Val Loss: 0.0880\n",
      "Epoch 104/300 - Train Loss: 0.0973, Val Loss: 0.0955\n",
      "Epoch 105/300 - Train Loss: 0.0980, Val Loss: 0.0935\n",
      "Epoch 106/300 - Train Loss: 0.0968, Val Loss: 0.0949\n",
      "Epoch 107/300 - Train Loss: 0.0963, Val Loss: 0.0945\n",
      "Epoch 108/300 - Train Loss: 0.0985, Val Loss: 0.0987\n",
      "Epoch 109/300 - Train Loss: 0.0975, Val Loss: 0.0895\n",
      "Epoch 110/300 - Train Loss: 0.0962, Val Loss: 0.0952\n",
      "Epoch 111/300 - Train Loss: 0.0985, Val Loss: 0.0930\n",
      "Epoch 112/300 - Train Loss: 0.0999, Val Loss: 0.0926\n",
      "Epoch 113/300 - Train Loss: 0.0971, Val Loss: 0.0985\n",
      "Epoch 114/300 - Train Loss: 0.0968, Val Loss: 0.0933\n",
      "Epoch 115/300 - Train Loss: 0.0964, Val Loss: 0.0897\n",
      "Epoch 116/300 - Train Loss: 0.0966, Val Loss: 0.0903\n",
      "Epoch 117/300 - Train Loss: 0.0956, Val Loss: 0.0971\n",
      "Epoch 118/300 - Train Loss: 0.0948, Val Loss: 0.0965\n",
      "Epoch 119/300 - Train Loss: 0.0975, Val Loss: 0.0918\n",
      "Epoch 120/300 - Train Loss: 0.0972, Val Loss: 0.0923\n",
      "Epoch 121/300 - Train Loss: 0.0952, Val Loss: 0.0956\n",
      "Epoch 122/300 - Train Loss: 0.0975, Val Loss: 0.0934\n",
      "Epoch 123/300 - Train Loss: 0.0965, Val Loss: 0.0941\n",
      "Epoch 124/300 - Train Loss: 0.0952, Val Loss: 0.0948\n",
      "Epoch 125/300 - Train Loss: 0.0960, Val Loss: 0.0925\n",
      "Epoch 126/300 - Train Loss: 0.0970, Val Loss: 0.0891\n",
      "Epoch 127/300 - Train Loss: 0.0966, Val Loss: 0.0979\n",
      "Epoch 128/300 - Train Loss: 0.0955, Val Loss: 0.0951\n",
      "Epoch 129/300 - Train Loss: 0.0988, Val Loss: 0.0920\n",
      "Epoch 130/300 - Train Loss: 0.0956, Val Loss: 0.0934\n",
      "Epoch 131/300 - Train Loss: 0.0964, Val Loss: 0.0923\n",
      "Epoch 132/300 - Train Loss: 0.0941, Val Loss: 0.0952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:24:29,506] Trial 48 finished with value: 0.940727557843358 and parameters: {'F1': 4, 'F2': 32, 'D': 4, 'dropout': 0.6668827240806698, 'learning_rate': 0.0001211958140115106, 'batch_size': 256, 'weight_decay': 3.7628062797290926e-05}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/300 - Train Loss: 0.0980, Val Loss: 0.0950\n",
      "Early stopping at epoch 133\n",
      "Macro F1 Score: 0.9407, Macro Precision: 0.9180, Macro Recall: 0.9698\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.79      0.97      0.87        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.97      1443\n",
      "   macro avg       0.92      0.97      0.94      1443\n",
      "weighted avg       0.97      0.97      0.97      1443\n",
      "\n",
      "\n",
      "Trial 50\n",
      "Training with F1=8, F2=32, D=4, dropout=0.2266356571533995, LR=0.0004479669414237251, BS=64, WD=4.838459174809915e-05\n",
      "Epoch 1/300 - Train Loss: 0.1937, Val Loss: 0.0940\n",
      "Epoch 2/300 - Train Loss: 0.1031, Val Loss: 0.0835\n",
      "Epoch 3/300 - Train Loss: 0.0930, Val Loss: 0.0837\n",
      "Epoch 4/300 - Train Loss: 0.0916, Val Loss: 0.0845\n",
      "Epoch 5/300 - Train Loss: 0.0892, Val Loss: 0.0844\n",
      "Epoch 6/300 - Train Loss: 0.0872, Val Loss: 0.0793\n",
      "Epoch 7/300 - Train Loss: 0.0862, Val Loss: 0.0763\n",
      "Epoch 8/300 - Train Loss: 0.0865, Val Loss: 0.0882\n",
      "Epoch 9/300 - Train Loss: 0.0841, Val Loss: 0.0831\n",
      "Epoch 10/300 - Train Loss: 0.0824, Val Loss: 0.0811\n",
      "Epoch 11/300 - Train Loss: 0.0831, Val Loss: 0.0734\n",
      "Epoch 12/300 - Train Loss: 0.0817, Val Loss: 0.0765\n",
      "Epoch 13/300 - Train Loss: 0.0786, Val Loss: 0.0753\n",
      "Epoch 14/300 - Train Loss: 0.0777, Val Loss: 0.0714\n",
      "Epoch 15/300 - Train Loss: 0.0788, Val Loss: 0.0802\n",
      "Epoch 16/300 - Train Loss: 0.0782, Val Loss: 0.0720\n",
      "Epoch 17/300 - Train Loss: 0.0769, Val Loss: 0.0762\n",
      "Epoch 18/300 - Train Loss: 0.0758, Val Loss: 0.0757\n",
      "Epoch 19/300 - Train Loss: 0.0734, Val Loss: 0.0783\n",
      "Epoch 20/300 - Train Loss: 0.0732, Val Loss: 0.0710\n",
      "Epoch 21/300 - Train Loss: 0.0741, Val Loss: 0.0786\n",
      "Epoch 22/300 - Train Loss: 0.0727, Val Loss: 0.0758\n",
      "Epoch 23/300 - Train Loss: 0.0716, Val Loss: 0.0778\n",
      "Epoch 24/300 - Train Loss: 0.0704, Val Loss: 0.0803\n",
      "Epoch 25/300 - Train Loss: 0.0715, Val Loss: 0.0734\n",
      "Epoch 26/300 - Train Loss: 0.0708, Val Loss: 0.0767\n",
      "Epoch 27/300 - Train Loss: 0.0687, Val Loss: 0.0739\n",
      "Epoch 28/300 - Train Loss: 0.0686, Val Loss: 0.0767\n",
      "Epoch 29/300 - Train Loss: 0.0672, Val Loss: 0.0730\n",
      "Epoch 30/300 - Train Loss: 0.0659, Val Loss: 0.0720\n",
      "Epoch 31/300 - Train Loss: 0.0664, Val Loss: 0.0756\n",
      "Epoch 32/300 - Train Loss: 0.0670, Val Loss: 0.0718\n",
      "Epoch 33/300 - Train Loss: 0.0689, Val Loss: 0.0698\n",
      "Epoch 34/300 - Train Loss: 0.0633, Val Loss: 0.0711\n",
      "Epoch 35/300 - Train Loss: 0.0651, Val Loss: 0.0755\n",
      "Epoch 36/300 - Train Loss: 0.0662, Val Loss: 0.0727\n",
      "Epoch 37/300 - Train Loss: 0.0647, Val Loss: 0.0708\n",
      "Epoch 38/300 - Train Loss: 0.0625, Val Loss: 0.0730\n",
      "Epoch 39/300 - Train Loss: 0.0632, Val Loss: 0.0734\n",
      "Epoch 40/300 - Train Loss: 0.0618, Val Loss: 0.0739\n",
      "Epoch 41/300 - Train Loss: 0.0623, Val Loss: 0.0762\n",
      "Epoch 42/300 - Train Loss: 0.0600, Val Loss: 0.0731\n",
      "Epoch 43/300 - Train Loss: 0.0620, Val Loss: 0.0743\n",
      "Epoch 44/300 - Train Loss: 0.0595, Val Loss: 0.0771\n",
      "Epoch 45/300 - Train Loss: 0.0611, Val Loss: 0.0761\n",
      "Epoch 46/300 - Train Loss: 0.0584, Val Loss: 0.0703\n",
      "Epoch 47/300 - Train Loss: 0.0593, Val Loss: 0.0732\n",
      "Epoch 48/300 - Train Loss: 0.0563, Val Loss: 0.0799\n",
      "Epoch 49/300 - Train Loss: 0.0595, Val Loss: 0.0748\n",
      "Epoch 50/300 - Train Loss: 0.0575, Val Loss: 0.0742\n",
      "Epoch 51/300 - Train Loss: 0.0575, Val Loss: 0.0744\n",
      "Epoch 52/300 - Train Loss: 0.0574, Val Loss: 0.0761\n",
      "Epoch 53/300 - Train Loss: 0.0551, Val Loss: 0.0768\n",
      "Epoch 54/300 - Train Loss: 0.0575, Val Loss: 0.0743\n",
      "Epoch 55/300 - Train Loss: 0.0558, Val Loss: 0.0742\n",
      "Epoch 56/300 - Train Loss: 0.0567, Val Loss: 0.0713\n",
      "Epoch 57/300 - Train Loss: 0.0565, Val Loss: 0.0709\n",
      "Epoch 58/300 - Train Loss: 0.0556, Val Loss: 0.0730\n",
      "Epoch 59/300 - Train Loss: 0.0558, Val Loss: 0.0697\n",
      "Epoch 60/300 - Train Loss: 0.0556, Val Loss: 0.0763\n",
      "Epoch 61/300 - Train Loss: 0.0550, Val Loss: 0.0774\n",
      "Epoch 62/300 - Train Loss: 0.0551, Val Loss: 0.0716\n",
      "Epoch 63/300 - Train Loss: 0.0532, Val Loss: 0.0731\n",
      "Epoch 64/300 - Train Loss: 0.0537, Val Loss: 0.0835\n",
      "Epoch 65/300 - Train Loss: 0.0544, Val Loss: 0.0762\n",
      "Epoch 66/300 - Train Loss: 0.0520, Val Loss: 0.0765\n",
      "Epoch 67/300 - Train Loss: 0.0523, Val Loss: 0.0775\n",
      "Epoch 68/300 - Train Loss: 0.0533, Val Loss: 0.0730\n",
      "Epoch 69/300 - Train Loss: 0.0522, Val Loss: 0.0744\n",
      "Epoch 70/300 - Train Loss: 0.0515, Val Loss: 0.0721\n",
      "Epoch 71/300 - Train Loss: 0.0497, Val Loss: 0.0801\n",
      "Epoch 72/300 - Train Loss: 0.0520, Val Loss: 0.0787\n",
      "Epoch 73/300 - Train Loss: 0.0491, Val Loss: 0.0773\n",
      "Epoch 74/300 - Train Loss: 0.0500, Val Loss: 0.0762\n",
      "Epoch 75/300 - Train Loss: 0.0498, Val Loss: 0.0745\n",
      "Epoch 76/300 - Train Loss: 0.0500, Val Loss: 0.0797\n",
      "Epoch 77/300 - Train Loss: 0.0509, Val Loss: 0.0734\n",
      "Epoch 78/300 - Train Loss: 0.0498, Val Loss: 0.0800\n",
      "Epoch 79/300 - Train Loss: 0.0483, Val Loss: 0.0735\n",
      "Epoch 80/300 - Train Loss: 0.0492, Val Loss: 0.0740\n",
      "Epoch 81/300 - Train Loss: 0.0475, Val Loss: 0.0794\n",
      "Epoch 82/300 - Train Loss: 0.0494, Val Loss: 0.0761\n",
      "Epoch 83/300 - Train Loss: 0.0486, Val Loss: 0.0792\n",
      "Epoch 84/300 - Train Loss: 0.0470, Val Loss: 0.0813\n",
      "Epoch 85/300 - Train Loss: 0.0483, Val Loss: 0.0835\n",
      "Epoch 86/300 - Train Loss: 0.0476, Val Loss: 0.0731\n",
      "Epoch 87/300 - Train Loss: 0.0467, Val Loss: 0.0768\n",
      "Epoch 88/300 - Train Loss: 0.0459, Val Loss: 0.0737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:26:39,221] Trial 49 finished with value: 0.9688584453277809 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.2266356571533995, 'learning_rate': 0.0004479669414237251, 'batch_size': 64, 'weight_decay': 4.838459174809915e-05}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/300 - Train Loss: 0.0469, Val Loss: 0.0763\n",
      "Early stopping at epoch 89\n",
      "Macro F1 Score: 0.9689, Macro Precision: 0.9570, Macro Recall: 0.9822\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 51\n",
      "Training with F1=32, F2=32, D=8, dropout=0.549505767618878, LR=0.0007587412002943905, BS=64, WD=0.00013879772724210708\n",
      "Epoch 1/300 - Train Loss: 0.1667, Val Loss: 0.0818\n",
      "Epoch 2/300 - Train Loss: 0.0993, Val Loss: 0.0780\n",
      "Epoch 3/300 - Train Loss: 0.0929, Val Loss: 0.0808\n",
      "Epoch 4/300 - Train Loss: 0.0902, Val Loss: 0.0750\n",
      "Epoch 5/300 - Train Loss: 0.0866, Val Loss: 0.0760\n",
      "Epoch 6/300 - Train Loss: 0.0851, Val Loss: 0.0764\n",
      "Epoch 7/300 - Train Loss: 0.0873, Val Loss: 0.0729\n",
      "Epoch 8/300 - Train Loss: 0.0851, Val Loss: 0.0772\n",
      "Epoch 9/300 - Train Loss: 0.0815, Val Loss: 0.0728\n",
      "Epoch 10/300 - Train Loss: 0.0833, Val Loss: 0.0728\n",
      "Epoch 11/300 - Train Loss: 0.0818, Val Loss: 0.0736\n",
      "Epoch 12/300 - Train Loss: 0.0814, Val Loss: 0.0706\n",
      "Epoch 13/300 - Train Loss: 0.0808, Val Loss: 0.0709\n",
      "Epoch 14/300 - Train Loss: 0.0808, Val Loss: 0.0799\n",
      "Epoch 15/300 - Train Loss: 0.0813, Val Loss: 0.0733\n",
      "Epoch 16/300 - Train Loss: 0.0808, Val Loss: 0.0706\n",
      "Epoch 17/300 - Train Loss: 0.0811, Val Loss: 0.0707\n",
      "Epoch 18/300 - Train Loss: 0.0764, Val Loss: 0.0692\n",
      "Epoch 19/300 - Train Loss: 0.0790, Val Loss: 0.0695\n",
      "Epoch 20/300 - Train Loss: 0.0786, Val Loss: 0.0696\n",
      "Epoch 21/300 - Train Loss: 0.0786, Val Loss: 0.0701\n",
      "Epoch 22/300 - Train Loss: 0.0794, Val Loss: 0.0739\n",
      "Epoch 23/300 - Train Loss: 0.0775, Val Loss: 0.0744\n",
      "Epoch 24/300 - Train Loss: 0.0778, Val Loss: 0.0723\n",
      "Epoch 25/300 - Train Loss: 0.0776, Val Loss: 0.0726\n",
      "Epoch 26/300 - Train Loss: 0.0775, Val Loss: 0.0725\n",
      "Epoch 27/300 - Train Loss: 0.0765, Val Loss: 0.0709\n",
      "Epoch 28/300 - Train Loss: 0.0774, Val Loss: 0.0722\n",
      "Epoch 29/300 - Train Loss: 0.0778, Val Loss: 0.0697\n",
      "Epoch 30/300 - Train Loss: 0.0766, Val Loss: 0.0702\n",
      "Epoch 31/300 - Train Loss: 0.0773, Val Loss: 0.0718\n",
      "Epoch 32/300 - Train Loss: 0.0791, Val Loss: 0.0777\n",
      "Epoch 33/300 - Train Loss: 0.0764, Val Loss: 0.0677\n",
      "Epoch 34/300 - Train Loss: 0.0780, Val Loss: 0.0666\n",
      "Epoch 35/300 - Train Loss: 0.0757, Val Loss: 0.0712\n",
      "Epoch 36/300 - Train Loss: 0.0751, Val Loss: 0.0724\n",
      "Epoch 37/300 - Train Loss: 0.0764, Val Loss: 0.0721\n",
      "Epoch 38/300 - Train Loss: 0.0778, Val Loss: 0.0717\n",
      "Epoch 39/300 - Train Loss: 0.0754, Val Loss: 0.0724\n",
      "Epoch 40/300 - Train Loss: 0.0763, Val Loss: 0.0684\n",
      "Epoch 41/300 - Train Loss: 0.0759, Val Loss: 0.0765\n",
      "Epoch 42/300 - Train Loss: 0.0759, Val Loss: 0.0714\n",
      "Epoch 43/300 - Train Loss: 0.0753, Val Loss: 0.0690\n",
      "Epoch 44/300 - Train Loss: 0.0772, Val Loss: 0.0829\n",
      "Epoch 45/300 - Train Loss: 0.0763, Val Loss: 0.0688\n",
      "Epoch 46/300 - Train Loss: 0.0774, Val Loss: 0.0716\n",
      "Epoch 47/300 - Train Loss: 0.0744, Val Loss: 0.0718\n",
      "Epoch 48/300 - Train Loss: 0.0770, Val Loss: 0.0729\n",
      "Epoch 49/300 - Train Loss: 0.0761, Val Loss: 0.0699\n",
      "Epoch 50/300 - Train Loss: 0.0758, Val Loss: 0.0725\n",
      "Epoch 51/300 - Train Loss: 0.0758, Val Loss: 0.0732\n",
      "Epoch 52/300 - Train Loss: 0.0742, Val Loss: 0.0718\n",
      "Epoch 53/300 - Train Loss: 0.0761, Val Loss: 0.0703\n",
      "Epoch 54/300 - Train Loss: 0.0745, Val Loss: 0.0724\n",
      "Epoch 55/300 - Train Loss: 0.0760, Val Loss: 0.0819\n",
      "Epoch 56/300 - Train Loss: 0.0722, Val Loss: 0.0744\n",
      "Epoch 57/300 - Train Loss: 0.0752, Val Loss: 0.0696\n",
      "Epoch 58/300 - Train Loss: 0.0737, Val Loss: 0.0658\n",
      "Epoch 59/300 - Train Loss: 0.0732, Val Loss: 0.0892\n",
      "Epoch 60/300 - Train Loss: 0.0760, Val Loss: 0.0686\n",
      "Epoch 61/300 - Train Loss: 0.0736, Val Loss: 0.0725\n",
      "Epoch 62/300 - Train Loss: 0.0752, Val Loss: 0.0769\n",
      "Epoch 63/300 - Train Loss: 0.0751, Val Loss: 0.0712\n",
      "Epoch 64/300 - Train Loss: 0.0752, Val Loss: 0.0734\n",
      "Epoch 65/300 - Train Loss: 0.0748, Val Loss: 0.0714\n",
      "Epoch 66/300 - Train Loss: 0.0762, Val Loss: 0.0714\n",
      "Epoch 67/300 - Train Loss: 0.0742, Val Loss: 0.0717\n",
      "Epoch 68/300 - Train Loss: 0.0751, Val Loss: 0.0722\n",
      "Epoch 69/300 - Train Loss: 0.0771, Val Loss: 0.0810\n",
      "Epoch 70/300 - Train Loss: 0.0760, Val Loss: 0.0717\n",
      "Epoch 71/300 - Train Loss: 0.0754, Val Loss: 0.0697\n",
      "Epoch 72/300 - Train Loss: 0.0730, Val Loss: 0.0716\n",
      "Epoch 73/300 - Train Loss: 0.0736, Val Loss: 0.0696\n",
      "Epoch 74/300 - Train Loss: 0.0735, Val Loss: 0.0738\n",
      "Epoch 75/300 - Train Loss: 0.0731, Val Loss: 0.0722\n",
      "Epoch 76/300 - Train Loss: 0.0734, Val Loss: 0.0716\n",
      "Epoch 77/300 - Train Loss: 0.0735, Val Loss: 0.0706\n",
      "Epoch 78/300 - Train Loss: 0.0744, Val Loss: 0.0696\n",
      "Epoch 79/300 - Train Loss: 0.0731, Val Loss: 0.0822\n",
      "Epoch 80/300 - Train Loss: 0.0718, Val Loss: 0.0768\n",
      "Epoch 81/300 - Train Loss: 0.0758, Val Loss: 0.0683\n",
      "Epoch 82/300 - Train Loss: 0.0743, Val Loss: 0.0685\n",
      "Epoch 83/300 - Train Loss: 0.0725, Val Loss: 0.0743\n",
      "Epoch 84/300 - Train Loss: 0.0747, Val Loss: 0.0706\n",
      "Epoch 85/300 - Train Loss: 0.0742, Val Loss: 0.0692\n",
      "Epoch 86/300 - Train Loss: 0.0732, Val Loss: 0.0701\n",
      "Epoch 87/300 - Train Loss: 0.0737, Val Loss: 0.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:35:00,654] Trial 50 finished with value: 0.9673263030138441 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.549505767618878, 'learning_rate': 0.0007587412002943905, 'batch_size': 64, 'weight_decay': 0.00013879772724210708}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/300 - Train Loss: 0.0740, Val Loss: 0.0699\n",
      "Early stopping at epoch 88\n",
      "Macro F1 Score: 0.9673, Macro Precision: 0.9709, Macro Recall: 0.9639\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 52\n",
      "Training with F1=8, F2=32, D=4, dropout=0.19572480155952365, LR=0.0006320773632376858, BS=64, WD=5.456249718579339e-05\n",
      "Epoch 1/300 - Train Loss: 0.1731, Val Loss: 0.0788\n",
      "Epoch 2/300 - Train Loss: 0.0972, Val Loss: 0.0858\n",
      "Epoch 3/300 - Train Loss: 0.0898, Val Loss: 0.0815\n",
      "Epoch 4/300 - Train Loss: 0.0864, Val Loss: 0.0810\n",
      "Epoch 5/300 - Train Loss: 0.0880, Val Loss: 0.0760\n",
      "Epoch 6/300 - Train Loss: 0.0842, Val Loss: 0.0758\n",
      "Epoch 7/300 - Train Loss: 0.0824, Val Loss: 0.0744\n",
      "Epoch 8/300 - Train Loss: 0.0814, Val Loss: 0.0743\n",
      "Epoch 9/300 - Train Loss: 0.0806, Val Loss: 0.0803\n",
      "Epoch 10/300 - Train Loss: 0.0784, Val Loss: 0.0733\n",
      "Epoch 11/300 - Train Loss: 0.0771, Val Loss: 0.0744\n",
      "Epoch 12/300 - Train Loss: 0.0789, Val Loss: 0.0708\n",
      "Epoch 13/300 - Train Loss: 0.0754, Val Loss: 0.0727\n",
      "Epoch 14/300 - Train Loss: 0.0732, Val Loss: 0.0733\n",
      "Epoch 15/300 - Train Loss: 0.0727, Val Loss: 0.0758\n",
      "Epoch 16/300 - Train Loss: 0.0721, Val Loss: 0.0722\n",
      "Epoch 17/300 - Train Loss: 0.0712, Val Loss: 0.0735\n",
      "Epoch 18/300 - Train Loss: 0.0705, Val Loss: 0.0761\n",
      "Epoch 19/300 - Train Loss: 0.0696, Val Loss: 0.0745\n",
      "Epoch 20/300 - Train Loss: 0.0700, Val Loss: 0.0734\n",
      "Epoch 21/300 - Train Loss: 0.0688, Val Loss: 0.0801\n",
      "Epoch 22/300 - Train Loss: 0.0662, Val Loss: 0.0772\n",
      "Epoch 23/300 - Train Loss: 0.0681, Val Loss: 0.0742\n",
      "Epoch 24/300 - Train Loss: 0.0643, Val Loss: 0.0784\n",
      "Epoch 25/300 - Train Loss: 0.0648, Val Loss: 0.0886\n",
      "Epoch 26/300 - Train Loss: 0.0642, Val Loss: 0.0739\n",
      "Epoch 27/300 - Train Loss: 0.0633, Val Loss: 0.0764\n",
      "Epoch 28/300 - Train Loss: 0.0634, Val Loss: 0.0717\n",
      "Epoch 29/300 - Train Loss: 0.0621, Val Loss: 0.0808\n",
      "Epoch 30/300 - Train Loss: 0.0622, Val Loss: 0.0744\n",
      "Epoch 31/300 - Train Loss: 0.0589, Val Loss: 0.0799\n",
      "Epoch 32/300 - Train Loss: 0.0618, Val Loss: 0.0762\n",
      "Epoch 33/300 - Train Loss: 0.0589, Val Loss: 0.0751\n",
      "Epoch 34/300 - Train Loss: 0.0595, Val Loss: 0.0765\n",
      "Epoch 35/300 - Train Loss: 0.0587, Val Loss: 0.0813\n",
      "Epoch 36/300 - Train Loss: 0.0583, Val Loss: 0.0748\n",
      "Epoch 37/300 - Train Loss: 0.0595, Val Loss: 0.0800\n",
      "Epoch 38/300 - Train Loss: 0.0572, Val Loss: 0.0789\n",
      "Epoch 39/300 - Train Loss: 0.0582, Val Loss: 0.0790\n",
      "Epoch 40/300 - Train Loss: 0.0577, Val Loss: 0.0755\n",
      "Epoch 41/300 - Train Loss: 0.0557, Val Loss: 0.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:36:01,813] Trial 51 finished with value: 0.9664846378167006 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.19572480155952365, 'learning_rate': 0.0006320773632376858, 'batch_size': 64, 'weight_decay': 5.456249718579339e-05}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300 - Train Loss: 0.0563, Val Loss: 0.0858\n",
      "Early stopping at epoch 42\n",
      "Macro F1 Score: 0.9665, Macro Precision: 0.9604, Macro Recall: 0.9729\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 53\n",
      "Training with F1=8, F2=32, D=4, dropout=0.29163263059178224, LR=0.0004622251466348368, BS=64, WD=4.3440811998475875e-05\n",
      "Epoch 1/300 - Train Loss: 0.2161, Val Loss: 0.0945\n",
      "Epoch 2/300 - Train Loss: 0.1012, Val Loss: 0.0841\n",
      "Epoch 3/300 - Train Loss: 0.0950, Val Loss: 0.0859\n",
      "Epoch 4/300 - Train Loss: 0.0922, Val Loss: 0.0750\n",
      "Epoch 5/300 - Train Loss: 0.0904, Val Loss: 0.0757\n",
      "Epoch 6/300 - Train Loss: 0.0894, Val Loss: 0.0777\n",
      "Epoch 7/300 - Train Loss: 0.0854, Val Loss: 0.0754\n",
      "Epoch 8/300 - Train Loss: 0.0858, Val Loss: 0.0799\n",
      "Epoch 9/300 - Train Loss: 0.0833, Val Loss: 0.0752\n",
      "Epoch 10/300 - Train Loss: 0.0817, Val Loss: 0.0784\n",
      "Epoch 11/300 - Train Loss: 0.0800, Val Loss: 0.0759\n",
      "Epoch 12/300 - Train Loss: 0.0804, Val Loss: 0.0748\n",
      "Epoch 13/300 - Train Loss: 0.0797, Val Loss: 0.0774\n",
      "Epoch 14/300 - Train Loss: 0.0799, Val Loss: 0.0809\n",
      "Epoch 15/300 - Train Loss: 0.0780, Val Loss: 0.0762\n",
      "Epoch 16/300 - Train Loss: 0.0777, Val Loss: 0.0780\n",
      "Epoch 17/300 - Train Loss: 0.0754, Val Loss: 0.0723\n",
      "Epoch 18/300 - Train Loss: 0.0761, Val Loss: 0.0775\n",
      "Epoch 19/300 - Train Loss: 0.0765, Val Loss: 0.0780\n",
      "Epoch 20/300 - Train Loss: 0.0743, Val Loss: 0.0738\n",
      "Epoch 21/300 - Train Loss: 0.0769, Val Loss: 0.0791\n",
      "Epoch 22/300 - Train Loss: 0.0744, Val Loss: 0.0796\n",
      "Epoch 23/300 - Train Loss: 0.0702, Val Loss: 0.0734\n",
      "Epoch 24/300 - Train Loss: 0.0732, Val Loss: 0.0760\n",
      "Epoch 25/300 - Train Loss: 0.0718, Val Loss: 0.0768\n",
      "Epoch 26/300 - Train Loss: 0.0724, Val Loss: 0.0747\n",
      "Epoch 27/300 - Train Loss: 0.0701, Val Loss: 0.0808\n",
      "Epoch 28/300 - Train Loss: 0.0710, Val Loss: 0.0764\n",
      "Epoch 29/300 - Train Loss: 0.0699, Val Loss: 0.0769\n",
      "Epoch 30/300 - Train Loss: 0.0687, Val Loss: 0.0785\n",
      "Epoch 31/300 - Train Loss: 0.0695, Val Loss: 0.0756\n",
      "Epoch 32/300 - Train Loss: 0.0679, Val Loss: 0.0749\n",
      "Epoch 33/300 - Train Loss: 0.0674, Val Loss: 0.0741\n",
      "Epoch 34/300 - Train Loss: 0.0678, Val Loss: 0.0731\n",
      "Epoch 35/300 - Train Loss: 0.0674, Val Loss: 0.0751\n",
      "Epoch 36/300 - Train Loss: 0.0679, Val Loss: 0.0790\n",
      "Epoch 37/300 - Train Loss: 0.0673, Val Loss: 0.0799\n",
      "Epoch 38/300 - Train Loss: 0.0657, Val Loss: 0.0826\n",
      "Epoch 39/300 - Train Loss: 0.0678, Val Loss: 0.0805\n",
      "Epoch 40/300 - Train Loss: 0.0654, Val Loss: 0.0792\n",
      "Epoch 41/300 - Train Loss: 0.0660, Val Loss: 0.0762\n",
      "Epoch 42/300 - Train Loss: 0.0671, Val Loss: 0.0771\n",
      "Epoch 43/300 - Train Loss: 0.0641, Val Loss: 0.0738\n",
      "Epoch 44/300 - Train Loss: 0.0651, Val Loss: 0.0843\n",
      "Epoch 45/300 - Train Loss: 0.0666, Val Loss: 0.0785\n",
      "Epoch 46/300 - Train Loss: 0.0641, Val Loss: 0.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:37:10,199] Trial 52 finished with value: 0.9667905741191412 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.29163263059178224, 'learning_rate': 0.0004622251466348368, 'batch_size': 64, 'weight_decay': 4.3440811998475875e-05}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300 - Train Loss: 0.0657, Val Loss: 0.0766\n",
      "Early stopping at epoch 47\n",
      "Macro F1 Score: 0.9668, Macro Precision: 0.9632, Macro Recall: 0.9707\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 54\n",
      "Training with F1=8, F2=32, D=4, dropout=0.23637048948898945, LR=3.403427234183219e-05, BS=64, WD=7.923261013943603e-05\n",
      "Epoch 1/300 - Train Loss: 0.5959, Val Loss: 0.3334\n",
      "Epoch 2/300 - Train Loss: 0.2766, Val Loss: 0.2238\n",
      "Epoch 3/300 - Train Loss: 0.2094, Val Loss: 0.1686\n",
      "Epoch 4/300 - Train Loss: 0.1746, Val Loss: 0.1425\n",
      "Epoch 5/300 - Train Loss: 0.1576, Val Loss: 0.1242\n",
      "Epoch 6/300 - Train Loss: 0.1426, Val Loss: 0.1138\n",
      "Epoch 7/300 - Train Loss: 0.1308, Val Loss: 0.1083\n",
      "Epoch 8/300 - Train Loss: 0.1233, Val Loss: 0.1012\n",
      "Epoch 9/300 - Train Loss: 0.1144, Val Loss: 0.0980\n",
      "Epoch 10/300 - Train Loss: 0.1086, Val Loss: 0.0986\n",
      "Epoch 11/300 - Train Loss: 0.1053, Val Loss: 0.0940\n",
      "Epoch 12/300 - Train Loss: 0.1015, Val Loss: 0.0874\n",
      "Epoch 13/300 - Train Loss: 0.0987, Val Loss: 0.0902\n",
      "Epoch 14/300 - Train Loss: 0.0966, Val Loss: 0.0871\n",
      "Epoch 15/300 - Train Loss: 0.0962, Val Loss: 0.0841\n",
      "Epoch 16/300 - Train Loss: 0.0946, Val Loss: 0.0895\n",
      "Epoch 17/300 - Train Loss: 0.0933, Val Loss: 0.0837\n",
      "Epoch 18/300 - Train Loss: 0.0916, Val Loss: 0.0874\n",
      "Epoch 19/300 - Train Loss: 0.0928, Val Loss: 0.0830\n",
      "Epoch 20/300 - Train Loss: 0.0917, Val Loss: 0.0888\n",
      "Epoch 21/300 - Train Loss: 0.0903, Val Loss: 0.0818\n",
      "Epoch 22/300 - Train Loss: 0.0909, Val Loss: 0.0816\n",
      "Epoch 23/300 - Train Loss: 0.0880, Val Loss: 0.0834\n",
      "Epoch 24/300 - Train Loss: 0.0889, Val Loss: 0.0830\n",
      "Epoch 25/300 - Train Loss: 0.0867, Val Loss: 0.0822\n",
      "Epoch 26/300 - Train Loss: 0.0867, Val Loss: 0.0836\n",
      "Epoch 27/300 - Train Loss: 0.0879, Val Loss: 0.0842\n",
      "Epoch 28/300 - Train Loss: 0.0885, Val Loss: 0.0787\n",
      "Epoch 29/300 - Train Loss: 0.0851, Val Loss: 0.0796\n",
      "Epoch 30/300 - Train Loss: 0.0849, Val Loss: 0.0839\n",
      "Epoch 31/300 - Train Loss: 0.0853, Val Loss: 0.0781\n",
      "Epoch 32/300 - Train Loss: 0.0859, Val Loss: 0.0814\n",
      "Epoch 33/300 - Train Loss: 0.0849, Val Loss: 0.0797\n",
      "Epoch 34/300 - Train Loss: 0.0844, Val Loss: 0.0802\n",
      "Epoch 35/300 - Train Loss: 0.0832, Val Loss: 0.0810\n",
      "Epoch 36/300 - Train Loss: 0.0832, Val Loss: 0.0812\n",
      "Epoch 37/300 - Train Loss: 0.0824, Val Loss: 0.0839\n",
      "Epoch 38/300 - Train Loss: 0.0827, Val Loss: 0.0788\n",
      "Epoch 39/300 - Train Loss: 0.0832, Val Loss: 0.0807\n",
      "Epoch 40/300 - Train Loss: 0.0812, Val Loss: 0.0806\n",
      "Epoch 41/300 - Train Loss: 0.0826, Val Loss: 0.0783\n",
      "Epoch 42/300 - Train Loss: 0.0819, Val Loss: 0.0798\n",
      "Epoch 43/300 - Train Loss: 0.0803, Val Loss: 0.0806\n",
      "Epoch 44/300 - Train Loss: 0.0803, Val Loss: 0.0794\n",
      "Epoch 45/300 - Train Loss: 0.0802, Val Loss: 0.0790\n",
      "Epoch 46/300 - Train Loss: 0.0799, Val Loss: 0.0798\n",
      "Epoch 47/300 - Train Loss: 0.0798, Val Loss: 0.0803\n",
      "Epoch 48/300 - Train Loss: 0.0798, Val Loss: 0.0795\n",
      "Epoch 49/300 - Train Loss: 0.0815, Val Loss: 0.0798\n",
      "Epoch 50/300 - Train Loss: 0.0794, Val Loss: 0.0799\n",
      "Epoch 51/300 - Train Loss: 0.0782, Val Loss: 0.0835\n",
      "Epoch 52/300 - Train Loss: 0.0789, Val Loss: 0.0800\n",
      "Epoch 53/300 - Train Loss: 0.0791, Val Loss: 0.0802\n",
      "Epoch 54/300 - Train Loss: 0.0777, Val Loss: 0.0821\n",
      "Epoch 55/300 - Train Loss: 0.0779, Val Loss: 0.0790\n",
      "Epoch 56/300 - Train Loss: 0.0781, Val Loss: 0.0781\n",
      "Epoch 57/300 - Train Loss: 0.0780, Val Loss: 0.0790\n",
      "Epoch 58/300 - Train Loss: 0.0780, Val Loss: 0.0821\n",
      "Epoch 59/300 - Train Loss: 0.0763, Val Loss: 0.0781\n",
      "Epoch 60/300 - Train Loss: 0.0771, Val Loss: 0.0756\n",
      "Epoch 61/300 - Train Loss: 0.0767, Val Loss: 0.0768\n",
      "Epoch 62/300 - Train Loss: 0.0768, Val Loss: 0.0784\n",
      "Epoch 63/300 - Train Loss: 0.0776, Val Loss: 0.0776\n",
      "Epoch 64/300 - Train Loss: 0.0767, Val Loss: 0.0774\n",
      "Epoch 65/300 - Train Loss: 0.0762, Val Loss: 0.0782\n",
      "Epoch 66/300 - Train Loss: 0.0776, Val Loss: 0.0789\n",
      "Epoch 67/300 - Train Loss: 0.0752, Val Loss: 0.0787\n",
      "Epoch 68/300 - Train Loss: 0.0756, Val Loss: 0.0782\n",
      "Epoch 69/300 - Train Loss: 0.0746, Val Loss: 0.0796\n",
      "Epoch 70/300 - Train Loss: 0.0766, Val Loss: 0.0769\n",
      "Epoch 71/300 - Train Loss: 0.0775, Val Loss: 0.0808\n",
      "Epoch 72/300 - Train Loss: 0.0739, Val Loss: 0.0779\n",
      "Epoch 73/300 - Train Loss: 0.0747, Val Loss: 0.0768\n",
      "Epoch 74/300 - Train Loss: 0.0768, Val Loss: 0.0786\n",
      "Epoch 75/300 - Train Loss: 0.0761, Val Loss: 0.0789\n",
      "Epoch 76/300 - Train Loss: 0.0745, Val Loss: 0.0760\n",
      "Epoch 77/300 - Train Loss: 0.0741, Val Loss: 0.0798\n",
      "Epoch 78/300 - Train Loss: 0.0741, Val Loss: 0.0763\n",
      "Epoch 79/300 - Train Loss: 0.0751, Val Loss: 0.0760\n",
      "Epoch 80/300 - Train Loss: 0.0751, Val Loss: 0.0760\n",
      "Epoch 81/300 - Train Loss: 0.0726, Val Loss: 0.0776\n",
      "Epoch 82/300 - Train Loss: 0.0749, Val Loss: 0.0781\n",
      "Epoch 83/300 - Train Loss: 0.0738, Val Loss: 0.0769\n",
      "Epoch 84/300 - Train Loss: 0.0741, Val Loss: 0.0773\n",
      "Epoch 85/300 - Train Loss: 0.0748, Val Loss: 0.0789\n",
      "Epoch 86/300 - Train Loss: 0.0745, Val Loss: 0.0806\n",
      "Epoch 87/300 - Train Loss: 0.0731, Val Loss: 0.0785\n",
      "Epoch 88/300 - Train Loss: 0.0722, Val Loss: 0.0765\n",
      "Epoch 89/300 - Train Loss: 0.0722, Val Loss: 0.0779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:39:21,286] Trial 53 finished with value: 0.9576000278193634 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.23637048948898945, 'learning_rate': 3.403427234183219e-05, 'batch_size': 64, 'weight_decay': 7.923261013943603e-05}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/300 - Train Loss: 0.0720, Val Loss: 0.0775\n",
      "Early stopping at epoch 90\n",
      "Macro F1 Score: 0.9576, Macro Precision: 0.9462, Macro Recall: 0.9704\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.87      0.95      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 55\n",
      "Training with F1=8, F2=32, D=4, dropout=0.5862520438091159, LR=0.00019639761099335694, BS=64, WD=0.00010596274966975348\n",
      "Epoch 1/300 - Train Loss: 0.3546, Val Loss: 0.1873\n",
      "Epoch 2/300 - Train Loss: 0.1610, Val Loss: 0.1228\n",
      "Epoch 3/300 - Train Loss: 0.1347, Val Loss: 0.1081\n",
      "Epoch 4/300 - Train Loss: 0.1231, Val Loss: 0.1142\n",
      "Epoch 5/300 - Train Loss: 0.1091, Val Loss: 0.1011\n",
      "Epoch 6/300 - Train Loss: 0.1064, Val Loss: 0.0913\n",
      "Epoch 7/300 - Train Loss: 0.1033, Val Loss: 0.0918\n",
      "Epoch 8/300 - Train Loss: 0.1025, Val Loss: 0.0935\n",
      "Epoch 9/300 - Train Loss: 0.1007, Val Loss: 0.0866\n",
      "Epoch 10/300 - Train Loss: 0.1002, Val Loss: 0.0878\n",
      "Epoch 11/300 - Train Loss: 0.0985, Val Loss: 0.0836\n",
      "Epoch 12/300 - Train Loss: 0.0980, Val Loss: 0.0910\n",
      "Epoch 13/300 - Train Loss: 0.0967, Val Loss: 0.0826\n",
      "Epoch 14/300 - Train Loss: 0.0962, Val Loss: 0.0864\n",
      "Epoch 15/300 - Train Loss: 0.0952, Val Loss: 0.0828\n",
      "Epoch 16/300 - Train Loss: 0.0952, Val Loss: 0.0843\n",
      "Epoch 17/300 - Train Loss: 0.0935, Val Loss: 0.0845\n",
      "Epoch 18/300 - Train Loss: 0.0927, Val Loss: 0.0899\n",
      "Epoch 19/300 - Train Loss: 0.0925, Val Loss: 0.0821\n",
      "Epoch 20/300 - Train Loss: 0.0917, Val Loss: 0.0849\n",
      "Epoch 21/300 - Train Loss: 0.0924, Val Loss: 0.0830\n",
      "Epoch 22/300 - Train Loss: 0.0933, Val Loss: 0.0824\n",
      "Epoch 23/300 - Train Loss: 0.0908, Val Loss: 0.0857\n",
      "Epoch 24/300 - Train Loss: 0.0933, Val Loss: 0.0816\n",
      "Epoch 25/300 - Train Loss: 0.0935, Val Loss: 0.0839\n",
      "Epoch 26/300 - Train Loss: 0.0915, Val Loss: 0.0817\n",
      "Epoch 27/300 - Train Loss: 0.0900, Val Loss: 0.0850\n",
      "Epoch 28/300 - Train Loss: 0.0910, Val Loss: 0.0814\n",
      "Epoch 29/300 - Train Loss: 0.0885, Val Loss: 0.0811\n",
      "Epoch 30/300 - Train Loss: 0.0904, Val Loss: 0.0851\n",
      "Epoch 31/300 - Train Loss: 0.0880, Val Loss: 0.0815\n",
      "Epoch 32/300 - Train Loss: 0.0872, Val Loss: 0.0866\n",
      "Epoch 33/300 - Train Loss: 0.0874, Val Loss: 0.0841\n",
      "Epoch 34/300 - Train Loss: 0.0878, Val Loss: 0.0825\n",
      "Epoch 35/300 - Train Loss: 0.0875, Val Loss: 0.0790\n",
      "Epoch 36/300 - Train Loss: 0.0870, Val Loss: 0.0774\n",
      "Epoch 37/300 - Train Loss: 0.0863, Val Loss: 0.0812\n",
      "Epoch 38/300 - Train Loss: 0.0876, Val Loss: 0.0769\n",
      "Epoch 39/300 - Train Loss: 0.0872, Val Loss: 0.0754\n",
      "Epoch 40/300 - Train Loss: 0.0870, Val Loss: 0.0771\n",
      "Epoch 41/300 - Train Loss: 0.0860, Val Loss: 0.0791\n",
      "Epoch 42/300 - Train Loss: 0.0856, Val Loss: 0.0794\n",
      "Epoch 43/300 - Train Loss: 0.0869, Val Loss: 0.0766\n",
      "Epoch 44/300 - Train Loss: 0.0829, Val Loss: 0.0765\n",
      "Epoch 45/300 - Train Loss: 0.0856, Val Loss: 0.0830\n",
      "Epoch 46/300 - Train Loss: 0.0854, Val Loss: 0.0792\n",
      "Epoch 47/300 - Train Loss: 0.0840, Val Loss: 0.0800\n",
      "Epoch 48/300 - Train Loss: 0.0856, Val Loss: 0.0815\n",
      "Epoch 49/300 - Train Loss: 0.0854, Val Loss: 0.0835\n",
      "Epoch 50/300 - Train Loss: 0.0829, Val Loss: 0.0791\n",
      "Epoch 51/300 - Train Loss: 0.0846, Val Loss: 0.0789\n",
      "Epoch 52/300 - Train Loss: 0.0816, Val Loss: 0.0797\n",
      "Epoch 53/300 - Train Loss: 0.0864, Val Loss: 0.0763\n",
      "Epoch 54/300 - Train Loss: 0.0844, Val Loss: 0.0765\n",
      "Epoch 55/300 - Train Loss: 0.0824, Val Loss: 0.0756\n",
      "Epoch 56/300 - Train Loss: 0.0842, Val Loss: 0.0758\n",
      "Epoch 57/300 - Train Loss: 0.0827, Val Loss: 0.0775\n",
      "Epoch 58/300 - Train Loss: 0.0823, Val Loss: 0.0796\n",
      "Epoch 59/300 - Train Loss: 0.0827, Val Loss: 0.0791\n",
      "Epoch 60/300 - Train Loss: 0.0836, Val Loss: 0.0767\n",
      "Epoch 61/300 - Train Loss: 0.0839, Val Loss: 0.0771\n",
      "Epoch 62/300 - Train Loss: 0.0836, Val Loss: 0.0791\n",
      "Epoch 63/300 - Train Loss: 0.0827, Val Loss: 0.0768\n",
      "Epoch 64/300 - Train Loss: 0.0828, Val Loss: 0.0807\n",
      "Epoch 65/300 - Train Loss: 0.0831, Val Loss: 0.0802\n",
      "Epoch 66/300 - Train Loss: 0.0825, Val Loss: 0.0790\n",
      "Epoch 67/300 - Train Loss: 0.0831, Val Loss: 0.0768\n",
      "Epoch 68/300 - Train Loss: 0.0802, Val Loss: 0.0782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:41:01,787] Trial 54 finished with value: 0.961834041790475 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.5862520438091159, 'learning_rate': 0.00019639761099335694, 'batch_size': 64, 'weight_decay': 0.00010596274966975348}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.0832, Val Loss: 0.0795\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9618, Macro Precision: 0.9541, Macro Recall: 0.9702\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 56\n",
      "Training with F1=8, F2=32, D=4, dropout=0.14093004091516453, LR=7.95674104844265e-05, BS=128, WD=5.025845058108413e-05\n",
      "Epoch 1/300 - Train Loss: 0.5421, Val Loss: 0.3158\n",
      "Epoch 2/300 - Train Loss: 0.2679, Val Loss: 0.2027\n",
      "Epoch 3/300 - Train Loss: 0.1980, Val Loss: 0.1653\n",
      "Epoch 4/300 - Train Loss: 0.1599, Val Loss: 0.1253\n",
      "Epoch 5/300 - Train Loss: 0.1296, Val Loss: 0.1131\n",
      "Epoch 6/300 - Train Loss: 0.1120, Val Loss: 0.1012\n",
      "Epoch 7/300 - Train Loss: 0.1033, Val Loss: 0.0931\n",
      "Epoch 8/300 - Train Loss: 0.0949, Val Loss: 0.0892\n",
      "Epoch 9/300 - Train Loss: 0.0930, Val Loss: 0.0871\n",
      "Epoch 10/300 - Train Loss: 0.0913, Val Loss: 0.0849\n",
      "Epoch 11/300 - Train Loss: 0.0879, Val Loss: 0.0830\n",
      "Epoch 12/300 - Train Loss: 0.0867, Val Loss: 0.0834\n",
      "Epoch 13/300 - Train Loss: 0.0865, Val Loss: 0.0830\n",
      "Epoch 14/300 - Train Loss: 0.0863, Val Loss: 0.0835\n",
      "Epoch 15/300 - Train Loss: 0.0845, Val Loss: 0.0803\n",
      "Epoch 16/300 - Train Loss: 0.0841, Val Loss: 0.0802\n",
      "Epoch 17/300 - Train Loss: 0.0832, Val Loss: 0.0799\n",
      "Epoch 18/300 - Train Loss: 0.0839, Val Loss: 0.0800\n",
      "Epoch 19/300 - Train Loss: 0.0813, Val Loss: 0.0781\n",
      "Epoch 20/300 - Train Loss: 0.0807, Val Loss: 0.0780\n",
      "Epoch 21/300 - Train Loss: 0.0795, Val Loss: 0.0782\n",
      "Epoch 22/300 - Train Loss: 0.0801, Val Loss: 0.0772\n",
      "Epoch 23/300 - Train Loss: 0.0771, Val Loss: 0.0757\n",
      "Epoch 24/300 - Train Loss: 0.0802, Val Loss: 0.0770\n",
      "Epoch 25/300 - Train Loss: 0.0793, Val Loss: 0.0779\n",
      "Epoch 26/300 - Train Loss: 0.0790, Val Loss: 0.0780\n",
      "Epoch 27/300 - Train Loss: 0.0773, Val Loss: 0.0754\n",
      "Epoch 28/300 - Train Loss: 0.0788, Val Loss: 0.0774\n",
      "Epoch 29/300 - Train Loss: 0.0765, Val Loss: 0.0767\n",
      "Epoch 30/300 - Train Loss: 0.0758, Val Loss: 0.0762\n",
      "Epoch 31/300 - Train Loss: 0.0761, Val Loss: 0.0763\n",
      "Epoch 32/300 - Train Loss: 0.0769, Val Loss: 0.0759\n",
      "Epoch 33/300 - Train Loss: 0.0747, Val Loss: 0.0762\n",
      "Epoch 34/300 - Train Loss: 0.0747, Val Loss: 0.0766\n",
      "Epoch 35/300 - Train Loss: 0.0747, Val Loss: 0.0766\n",
      "Epoch 36/300 - Train Loss: 0.0750, Val Loss: 0.0753\n",
      "Epoch 37/300 - Train Loss: 0.0736, Val Loss: 0.0760\n",
      "Epoch 38/300 - Train Loss: 0.0733, Val Loss: 0.0766\n",
      "Epoch 39/300 - Train Loss: 0.0717, Val Loss: 0.0783\n",
      "Epoch 40/300 - Train Loss: 0.0725, Val Loss: 0.0778\n",
      "Epoch 41/300 - Train Loss: 0.0733, Val Loss: 0.0774\n",
      "Epoch 42/300 - Train Loss: 0.0717, Val Loss: 0.0775\n",
      "Epoch 43/300 - Train Loss: 0.0723, Val Loss: 0.0781\n",
      "Epoch 44/300 - Train Loss: 0.0722, Val Loss: 0.0758\n",
      "Epoch 45/300 - Train Loss: 0.0721, Val Loss: 0.0755\n",
      "Epoch 46/300 - Train Loss: 0.0705, Val Loss: 0.0759\n",
      "Epoch 47/300 - Train Loss: 0.0718, Val Loss: 0.0755\n",
      "Epoch 48/300 - Train Loss: 0.0707, Val Loss: 0.0768\n",
      "Epoch 49/300 - Train Loss: 0.0702, Val Loss: 0.0755\n",
      "Epoch 50/300 - Train Loss: 0.0703, Val Loss: 0.0765\n",
      "Epoch 51/300 - Train Loss: 0.0701, Val Loss: 0.0744\n",
      "Epoch 52/300 - Train Loss: 0.0693, Val Loss: 0.0753\n",
      "Epoch 53/300 - Train Loss: 0.0699, Val Loss: 0.0756\n",
      "Epoch 54/300 - Train Loss: 0.0692, Val Loss: 0.0766\n",
      "Epoch 55/300 - Train Loss: 0.0698, Val Loss: 0.0765\n",
      "Epoch 56/300 - Train Loss: 0.0687, Val Loss: 0.0756\n",
      "Epoch 57/300 - Train Loss: 0.0684, Val Loss: 0.0756\n",
      "Epoch 58/300 - Train Loss: 0.0681, Val Loss: 0.0773\n",
      "Epoch 59/300 - Train Loss: 0.0687, Val Loss: 0.0753\n",
      "Epoch 60/300 - Train Loss: 0.0685, Val Loss: 0.0749\n",
      "Epoch 61/300 - Train Loss: 0.0674, Val Loss: 0.0770\n",
      "Epoch 62/300 - Train Loss: 0.0685, Val Loss: 0.0770\n",
      "Epoch 63/300 - Train Loss: 0.0680, Val Loss: 0.0769\n",
      "Epoch 64/300 - Train Loss: 0.0666, Val Loss: 0.0778\n",
      "Epoch 65/300 - Train Loss: 0.0662, Val Loss: 0.0750\n",
      "Epoch 66/300 - Train Loss: 0.0664, Val Loss: 0.0757\n",
      "Epoch 67/300 - Train Loss: 0.0660, Val Loss: 0.0751\n",
      "Epoch 68/300 - Train Loss: 0.0660, Val Loss: 0.0752\n",
      "Epoch 69/300 - Train Loss: 0.0653, Val Loss: 0.0759\n",
      "Epoch 70/300 - Train Loss: 0.0661, Val Loss: 0.0764\n",
      "Epoch 71/300 - Train Loss: 0.0652, Val Loss: 0.0759\n",
      "Epoch 72/300 - Train Loss: 0.0644, Val Loss: 0.0770\n",
      "Epoch 73/300 - Train Loss: 0.0640, Val Loss: 0.0749\n",
      "Epoch 74/300 - Train Loss: 0.0642, Val Loss: 0.0760\n",
      "Epoch 75/300 - Train Loss: 0.0637, Val Loss: 0.0771\n",
      "Epoch 76/300 - Train Loss: 0.0647, Val Loss: 0.0749\n",
      "Epoch 77/300 - Train Loss: 0.0633, Val Loss: 0.0771\n",
      "Epoch 78/300 - Train Loss: 0.0640, Val Loss: 0.0749\n",
      "Epoch 79/300 - Train Loss: 0.0641, Val Loss: 0.0753\n",
      "Epoch 80/300 - Train Loss: 0.0630, Val Loss: 0.0775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:42:47,427] Trial 55 finished with value: 0.9580711066205946 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.14093004091516453, 'learning_rate': 7.95674104844265e-05, 'batch_size': 128, 'weight_decay': 5.025845058108413e-05}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300 - Train Loss: 0.0637, Val Loss: 0.0779\n",
      "Early stopping at epoch 81\n",
      "Macro F1 Score: 0.9581, Macro Precision: 0.9433, Macro Recall: 0.9753\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.86      0.97      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.94      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 57\n",
      "Training with F1=8, F2=32, D=4, dropout=0.22876420359671873, LR=0.0009856478063345788, BS=32, WD=0.00037446548671103295\n",
      "Epoch 1/300 - Train Loss: 0.1495, Val Loss: 0.0807\n",
      "Epoch 2/300 - Train Loss: 0.1067, Val Loss: 0.0851\n",
      "Epoch 3/300 - Train Loss: 0.1023, Val Loss: 0.0785\n",
      "Epoch 4/300 - Train Loss: 0.0980, Val Loss: 0.0767\n",
      "Epoch 5/300 - Train Loss: 0.0972, Val Loss: 0.0765\n",
      "Epoch 6/300 - Train Loss: 0.0946, Val Loss: 0.0768\n",
      "Epoch 7/300 - Train Loss: 0.0926, Val Loss: 0.0769\n",
      "Epoch 8/300 - Train Loss: 0.0912, Val Loss: 0.0818\n",
      "Epoch 9/300 - Train Loss: 0.0923, Val Loss: 0.0721\n",
      "Epoch 10/300 - Train Loss: 0.0913, Val Loss: 0.0733\n",
      "Epoch 11/300 - Train Loss: 0.0901, Val Loss: 0.0779\n",
      "Epoch 12/300 - Train Loss: 0.0895, Val Loss: 0.0698\n",
      "Epoch 13/300 - Train Loss: 0.0896, Val Loss: 0.0727\n",
      "Epoch 14/300 - Train Loss: 0.0894, Val Loss: 0.0725\n",
      "Epoch 15/300 - Train Loss: 0.0901, Val Loss: 0.0881\n",
      "Epoch 16/300 - Train Loss: 0.0864, Val Loss: 0.0776\n",
      "Epoch 17/300 - Train Loss: 0.0878, Val Loss: 0.0731\n",
      "Epoch 18/300 - Train Loss: 0.0861, Val Loss: 0.0703\n",
      "Epoch 19/300 - Train Loss: 0.0856, Val Loss: 0.0705\n",
      "Epoch 20/300 - Train Loss: 0.0888, Val Loss: 0.0847\n",
      "Epoch 21/300 - Train Loss: 0.0868, Val Loss: 0.0782\n",
      "Epoch 22/300 - Train Loss: 0.0867, Val Loss: 0.0749\n",
      "Epoch 23/300 - Train Loss: 0.0861, Val Loss: 0.0753\n",
      "Epoch 24/300 - Train Loss: 0.0845, Val Loss: 0.0727\n",
      "Epoch 25/300 - Train Loss: 0.0884, Val Loss: 0.0690\n",
      "Epoch 26/300 - Train Loss: 0.0854, Val Loss: 0.0762\n",
      "Epoch 27/300 - Train Loss: 0.0841, Val Loss: 0.0786\n",
      "Epoch 28/300 - Train Loss: 0.0875, Val Loss: 0.0753\n",
      "Epoch 29/300 - Train Loss: 0.0879, Val Loss: 0.0821\n",
      "Epoch 30/300 - Train Loss: 0.0860, Val Loss: 0.0796\n",
      "Epoch 31/300 - Train Loss: 0.0850, Val Loss: 0.0735\n",
      "Epoch 32/300 - Train Loss: 0.0851, Val Loss: 0.0804\n",
      "Epoch 33/300 - Train Loss: 0.0841, Val Loss: 0.0916\n",
      "Epoch 34/300 - Train Loss: 0.0869, Val Loss: 0.0753\n",
      "Epoch 35/300 - Train Loss: 0.0883, Val Loss: 0.0812\n",
      "Epoch 36/300 - Train Loss: 0.0855, Val Loss: 0.0717\n",
      "Epoch 37/300 - Train Loss: 0.0805, Val Loss: 0.0822\n",
      "Epoch 38/300 - Train Loss: 0.0828, Val Loss: 0.0755\n",
      "Epoch 39/300 - Train Loss: 0.0835, Val Loss: 0.0760\n",
      "Epoch 40/300 - Train Loss: 0.0857, Val Loss: 0.0741\n",
      "Epoch 41/300 - Train Loss: 0.0820, Val Loss: 0.0838\n",
      "Epoch 42/300 - Train Loss: 0.0810, Val Loss: 0.0653\n",
      "Epoch 43/300 - Train Loss: 0.0826, Val Loss: 0.0732\n",
      "Epoch 44/300 - Train Loss: 0.0824, Val Loss: 0.0703\n",
      "Epoch 45/300 - Train Loss: 0.0806, Val Loss: 0.0767\n",
      "Epoch 46/300 - Train Loss: 0.0832, Val Loss: 0.0782\n",
      "Epoch 47/300 - Train Loss: 0.0844, Val Loss: 0.0698\n",
      "Epoch 48/300 - Train Loss: 0.0836, Val Loss: 0.0699\n",
      "Epoch 49/300 - Train Loss: 0.0810, Val Loss: 0.0708\n",
      "Epoch 50/300 - Train Loss: 0.0827, Val Loss: 0.0724\n",
      "Epoch 51/300 - Train Loss: 0.0815, Val Loss: 0.0781\n",
      "Epoch 52/300 - Train Loss: 0.0844, Val Loss: 0.0769\n",
      "Epoch 53/300 - Train Loss: 0.0814, Val Loss: 0.0748\n",
      "Epoch 54/300 - Train Loss: 0.0812, Val Loss: 0.0761\n",
      "Epoch 55/300 - Train Loss: 0.0790, Val Loss: 0.0692\n",
      "Epoch 56/300 - Train Loss: 0.0810, Val Loss: 0.0756\n",
      "Epoch 57/300 - Train Loss: 0.0824, Val Loss: 0.0760\n",
      "Epoch 58/300 - Train Loss: 0.0817, Val Loss: 0.0704\n",
      "Epoch 59/300 - Train Loss: 0.0801, Val Loss: 0.0735\n",
      "Epoch 60/300 - Train Loss: 0.0797, Val Loss: 0.0753\n",
      "Epoch 61/300 - Train Loss: 0.0792, Val Loss: 0.0776\n",
      "Epoch 62/300 - Train Loss: 0.0812, Val Loss: 0.0739\n",
      "Epoch 63/300 - Train Loss: 0.0810, Val Loss: 0.0786\n",
      "Epoch 64/300 - Train Loss: 0.0805, Val Loss: 0.0738\n",
      "Epoch 65/300 - Train Loss: 0.0845, Val Loss: 0.0731\n",
      "Epoch 66/300 - Train Loss: 0.0812, Val Loss: 0.0790\n",
      "Epoch 67/300 - Train Loss: 0.0810, Val Loss: 0.0730\n",
      "Epoch 68/300 - Train Loss: 0.0789, Val Loss: 0.0784\n",
      "Epoch 69/300 - Train Loss: 0.0831, Val Loss: 0.0796\n",
      "Epoch 70/300 - Train Loss: 0.0801, Val Loss: 0.0801\n",
      "Epoch 71/300 - Train Loss: 0.0807, Val Loss: 0.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:45:14,945] Trial 56 finished with value: 0.9713367327133439 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.22876420359671873, 'learning_rate': 0.0009856478063345788, 'batch_size': 32, 'weight_decay': 0.00037446548671103295}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300 - Train Loss: 0.0791, Val Loss: 0.0745\n",
      "Early stopping at epoch 72\n",
      "Macro F1 Score: 0.9713, Macro Precision: 0.9724, Macro Recall: 0.9704\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 58\n",
      "Training with F1=16, F2=32, D=2, dropout=0.23746789966003345, LR=0.0005077051612577749, BS=32, WD=0.0007310831458959879\n",
      "Epoch 1/300 - Train Loss: 0.1682, Val Loss: 0.1028\n",
      "Epoch 2/300 - Train Loss: 0.1032, Val Loss: 0.0834\n",
      "Epoch 3/300 - Train Loss: 0.0995, Val Loss: 0.0793\n",
      "Epoch 4/300 - Train Loss: 0.0965, Val Loss: 0.0660\n",
      "Epoch 5/300 - Train Loss: 0.0902, Val Loss: 0.0778\n",
      "Epoch 6/300 - Train Loss: 0.0898, Val Loss: 0.0756\n",
      "Epoch 7/300 - Train Loss: 0.0931, Val Loss: 0.0722\n",
      "Epoch 8/300 - Train Loss: 0.0902, Val Loss: 0.0728\n",
      "Epoch 9/300 - Train Loss: 0.0882, Val Loss: 0.0685\n",
      "Epoch 10/300 - Train Loss: 0.0871, Val Loss: 0.0764\n",
      "Epoch 11/300 - Train Loss: 0.0863, Val Loss: 0.0725\n",
      "Epoch 12/300 - Train Loss: 0.0874, Val Loss: 0.0730\n",
      "Epoch 13/300 - Train Loss: 0.0856, Val Loss: 0.0741\n",
      "Epoch 14/300 - Train Loss: 0.0859, Val Loss: 0.0684\n",
      "Epoch 15/300 - Train Loss: 0.0861, Val Loss: 0.0782\n",
      "Epoch 16/300 - Train Loss: 0.0873, Val Loss: 0.0776\n",
      "Epoch 17/300 - Train Loss: 0.0864, Val Loss: 0.0790\n",
      "Epoch 18/300 - Train Loss: 0.0833, Val Loss: 0.0717\n",
      "Epoch 19/300 - Train Loss: 0.0853, Val Loss: 0.0714\n",
      "Epoch 20/300 - Train Loss: 0.0860, Val Loss: 0.0728\n",
      "Epoch 21/300 - Train Loss: 0.0856, Val Loss: 0.0809\n",
      "Epoch 22/300 - Train Loss: 0.0832, Val Loss: 0.0710\n",
      "Epoch 23/300 - Train Loss: 0.0837, Val Loss: 0.0800\n",
      "Epoch 24/300 - Train Loss: 0.0845, Val Loss: 0.0842\n",
      "Epoch 25/300 - Train Loss: 0.0846, Val Loss: 0.0851\n",
      "Epoch 26/300 - Train Loss: 0.0836, Val Loss: 0.0739\n",
      "Epoch 27/300 - Train Loss: 0.0859, Val Loss: 0.0752\n",
      "Epoch 28/300 - Train Loss: 0.0854, Val Loss: 0.0702\n",
      "Epoch 29/300 - Train Loss: 0.0848, Val Loss: 0.0723\n",
      "Epoch 30/300 - Train Loss: 0.0856, Val Loss: 0.0758\n",
      "Epoch 31/300 - Train Loss: 0.0841, Val Loss: 0.0681\n",
      "Epoch 32/300 - Train Loss: 0.0841, Val Loss: 0.0705\n",
      "Epoch 33/300 - Train Loss: 0.0853, Val Loss: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:46:26,372] Trial 57 finished with value: 0.961395359647601 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.23746789966003345, 'learning_rate': 0.0005077051612577749, 'batch_size': 32, 'weight_decay': 0.0007310831458959879}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300 - Train Loss: 0.0853, Val Loss: 0.0702\n",
      "Early stopping at epoch 34\n",
      "Macro F1 Score: 0.9614, Macro Precision: 0.9625, Macro Recall: 0.9604\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.92      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 59\n",
      "Training with F1=32, F2=32, D=4, dropout=0.2702398463210339, LR=0.00030356831860029485, BS=32, WD=0.0003697651399837232\n",
      "Epoch 1/300 - Train Loss: 0.1808, Val Loss: 0.0896\n",
      "Epoch 2/300 - Train Loss: 0.1041, Val Loss: 0.0795\n",
      "Epoch 3/300 - Train Loss: 0.0971, Val Loss: 0.0799\n",
      "Epoch 4/300 - Train Loss: 0.0902, Val Loss: 0.0777\n",
      "Epoch 5/300 - Train Loss: 0.0864, Val Loss: 0.0795\n",
      "Epoch 6/300 - Train Loss: 0.0874, Val Loss: 0.0703\n",
      "Epoch 7/300 - Train Loss: 0.0860, Val Loss: 0.0715\n",
      "Epoch 8/300 - Train Loss: 0.0828, Val Loss: 0.0751\n",
      "Epoch 9/300 - Train Loss: 0.0817, Val Loss: 0.0685\n",
      "Epoch 10/300 - Train Loss: 0.0801, Val Loss: 0.0665\n",
      "Epoch 11/300 - Train Loss: 0.0799, Val Loss: 0.0728\n",
      "Epoch 12/300 - Train Loss: 0.0783, Val Loss: 0.0695\n",
      "Epoch 13/300 - Train Loss: 0.0786, Val Loss: 0.0705\n",
      "Epoch 14/300 - Train Loss: 0.0798, Val Loss: 0.0760\n",
      "Epoch 15/300 - Train Loss: 0.0783, Val Loss: 0.0702\n",
      "Epoch 16/300 - Train Loss: 0.0769, Val Loss: 0.0836\n",
      "Epoch 17/300 - Train Loss: 0.0772, Val Loss: 0.0723\n",
      "Epoch 18/300 - Train Loss: 0.0756, Val Loss: 0.0781\n",
      "Epoch 19/300 - Train Loss: 0.0741, Val Loss: 0.0677\n",
      "Epoch 20/300 - Train Loss: 0.0758, Val Loss: 0.0788\n",
      "Epoch 21/300 - Train Loss: 0.0744, Val Loss: 0.0697\n",
      "Epoch 22/300 - Train Loss: 0.0738, Val Loss: 0.0701\n",
      "Epoch 23/300 - Train Loss: 0.0731, Val Loss: 0.0688\n",
      "Epoch 24/300 - Train Loss: 0.0755, Val Loss: 0.0751\n",
      "Epoch 25/300 - Train Loss: 0.0739, Val Loss: 0.0728\n",
      "Epoch 26/300 - Train Loss: 0.0728, Val Loss: 0.0648\n",
      "Epoch 27/300 - Train Loss: 0.0725, Val Loss: 0.0728\n",
      "Epoch 28/300 - Train Loss: 0.0713, Val Loss: 0.0710\n",
      "Epoch 29/300 - Train Loss: 0.0735, Val Loss: 0.0692\n",
      "Epoch 30/300 - Train Loss: 0.0713, Val Loss: 0.0682\n",
      "Epoch 31/300 - Train Loss: 0.0699, Val Loss: 0.0739\n",
      "Epoch 32/300 - Train Loss: 0.0706, Val Loss: 0.0872\n",
      "Epoch 33/300 - Train Loss: 0.0716, Val Loss: 0.0711\n",
      "Epoch 34/300 - Train Loss: 0.0705, Val Loss: 0.0751\n",
      "Epoch 35/300 - Train Loss: 0.0695, Val Loss: 0.0737\n",
      "Epoch 36/300 - Train Loss: 0.0722, Val Loss: 0.0669\n",
      "Epoch 37/300 - Train Loss: 0.0714, Val Loss: 0.0738\n",
      "Epoch 38/300 - Train Loss: 0.0690, Val Loss: 0.0725\n",
      "Epoch 39/300 - Train Loss: 0.0690, Val Loss: 0.0738\n",
      "Epoch 40/300 - Train Loss: 0.0696, Val Loss: 0.0707\n",
      "Epoch 41/300 - Train Loss: 0.0702, Val Loss: 0.0782\n",
      "Epoch 42/300 - Train Loss: 0.0693, Val Loss: 0.0814\n",
      "Epoch 43/300 - Train Loss: 0.0715, Val Loss: 0.0704\n",
      "Epoch 44/300 - Train Loss: 0.0695, Val Loss: 0.0735\n",
      "Epoch 45/300 - Train Loss: 0.0694, Val Loss: 0.0695\n",
      "Epoch 46/300 - Train Loss: 0.0697, Val Loss: 0.0691\n",
      "Epoch 47/300 - Train Loss: 0.0686, Val Loss: 0.0681\n",
      "Epoch 48/300 - Train Loss: 0.0668, Val Loss: 0.0711\n",
      "Epoch 49/300 - Train Loss: 0.0699, Val Loss: 0.0682\n",
      "Epoch 50/300 - Train Loss: 0.0683, Val Loss: 0.0778\n",
      "Epoch 51/300 - Train Loss: 0.0688, Val Loss: 0.0700\n",
      "Epoch 52/300 - Train Loss: 0.0689, Val Loss: 0.0755\n",
      "Epoch 53/300 - Train Loss: 0.0676, Val Loss: 0.0663\n",
      "Epoch 54/300 - Train Loss: 0.0698, Val Loss: 0.0885\n",
      "Epoch 55/300 - Train Loss: 0.0702, Val Loss: 0.0771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:50:14,088] Trial 58 finished with value: 0.9668016368766814 and parameters: {'F1': 32, 'F2': 32, 'D': 4, 'dropout': 0.2702398463210339, 'learning_rate': 0.00030356831860029485, 'batch_size': 32, 'weight_decay': 0.0003697651399837232}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300 - Train Loss: 0.0656, Val Loss: 0.0688\n",
      "Early stopping at epoch 56\n",
      "Macro F1 Score: 0.9668, Macro Precision: 0.9627, Macro Recall: 0.9711\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 60\n",
      "Training with F1=4, F2=32, D=2, dropout=0.30381736838262646, LR=0.0009627952113430888, BS=32, WD=0.0019706360508353962\n",
      "Epoch 1/300 - Train Loss: 0.1859, Val Loss: 0.0967\n",
      "Epoch 2/300 - Train Loss: 0.1263, Val Loss: 0.0923\n",
      "Epoch 3/300 - Train Loss: 0.1170, Val Loss: 0.0895\n",
      "Epoch 4/300 - Train Loss: 0.1169, Val Loss: 0.0840\n",
      "Epoch 5/300 - Train Loss: 0.1151, Val Loss: 0.0824\n",
      "Epoch 6/300 - Train Loss: 0.1117, Val Loss: 0.0827\n",
      "Epoch 7/300 - Train Loss: 0.1117, Val Loss: 0.0930\n",
      "Epoch 8/300 - Train Loss: 0.1153, Val Loss: 0.0852\n",
      "Epoch 9/300 - Train Loss: 0.1145, Val Loss: 0.0820\n",
      "Epoch 10/300 - Train Loss: 0.1150, Val Loss: 0.0740\n",
      "Epoch 11/300 - Train Loss: 0.1127, Val Loss: 0.0821\n",
      "Epoch 12/300 - Train Loss: 0.1126, Val Loss: 0.0813\n",
      "Epoch 13/300 - Train Loss: 0.1130, Val Loss: 0.0744\n",
      "Epoch 14/300 - Train Loss: 0.1107, Val Loss: 0.0848\n",
      "Epoch 15/300 - Train Loss: 0.1125, Val Loss: 0.0771\n",
      "Epoch 16/300 - Train Loss: 0.1124, Val Loss: 0.0898\n",
      "Epoch 17/300 - Train Loss: 0.1108, Val Loss: 0.0789\n",
      "Epoch 18/300 - Train Loss: 0.1113, Val Loss: 0.0822\n",
      "Epoch 19/300 - Train Loss: 0.1140, Val Loss: 0.0778\n",
      "Epoch 20/300 - Train Loss: 0.1098, Val Loss: 0.0804\n",
      "Epoch 21/300 - Train Loss: 0.1103, Val Loss: 0.0849\n",
      "Epoch 22/300 - Train Loss: 0.1116, Val Loss: 0.0840\n",
      "Epoch 23/300 - Train Loss: 0.1166, Val Loss: 0.0774\n",
      "Epoch 24/300 - Train Loss: 0.1127, Val Loss: 0.0935\n",
      "Epoch 25/300 - Train Loss: 0.1109, Val Loss: 0.0797\n",
      "Epoch 26/300 - Train Loss: 0.1122, Val Loss: 0.0868\n",
      "Epoch 27/300 - Train Loss: 0.1128, Val Loss: 0.0791\n",
      "Epoch 28/300 - Train Loss: 0.1133, Val Loss: 0.0853\n",
      "Epoch 29/300 - Train Loss: 0.1122, Val Loss: 0.0785\n",
      "Epoch 30/300 - Train Loss: 0.1108, Val Loss: 0.0764\n",
      "Epoch 31/300 - Train Loss: 0.1109, Val Loss: 0.0807\n",
      "Epoch 32/300 - Train Loss: 0.1098, Val Loss: 0.0848\n",
      "Epoch 33/300 - Train Loss: 0.1098, Val Loss: 0.0919\n",
      "Epoch 34/300 - Train Loss: 0.1102, Val Loss: 0.0964\n",
      "Epoch 35/300 - Train Loss: 0.1119, Val Loss: 0.0840\n",
      "Epoch 36/300 - Train Loss: 0.1093, Val Loss: 0.0782\n",
      "Epoch 37/300 - Train Loss: 0.1117, Val Loss: 0.0835\n",
      "Epoch 38/300 - Train Loss: 0.1119, Val Loss: 0.0909\n",
      "Epoch 39/300 - Train Loss: 0.1118, Val Loss: 0.0831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:51:28,376] Trial 59 finished with value: 0.9630441005293283 and parameters: {'F1': 4, 'F2': 32, 'D': 2, 'dropout': 0.30381736838262646, 'learning_rate': 0.0009627952113430888, 'batch_size': 32, 'weight_decay': 0.0019706360508353962}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300 - Train Loss: 0.1126, Val Loss: 0.0857\n",
      "Early stopping at epoch 40\n",
      "Macro F1 Score: 0.9630, Macro Precision: 0.9621, Macro Recall: 0.9642\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.92      0.93      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 61\n",
      "Training with F1=8, F2=32, D=4, dropout=0.3296282788664616, LR=0.0004244449249013814, BS=32, WD=0.00018135199384738122\n",
      "Epoch 1/300 - Train Loss: 0.1791, Val Loss: 0.0945\n",
      "Epoch 2/300 - Train Loss: 0.1106, Val Loss: 0.0824\n",
      "Epoch 3/300 - Train Loss: 0.1051, Val Loss: 0.0778\n",
      "Epoch 4/300 - Train Loss: 0.0999, Val Loss: 0.0870\n",
      "Epoch 5/300 - Train Loss: 0.0964, Val Loss: 0.0833\n",
      "Epoch 6/300 - Train Loss: 0.0921, Val Loss: 0.0778\n",
      "Epoch 7/300 - Train Loss: 0.0912, Val Loss: 0.0763\n",
      "Epoch 8/300 - Train Loss: 0.0907, Val Loss: 0.0761\n",
      "Epoch 9/300 - Train Loss: 0.0864, Val Loss: 0.0797\n",
      "Epoch 10/300 - Train Loss: 0.0907, Val Loss: 0.0746\n",
      "Epoch 11/300 - Train Loss: 0.0880, Val Loss: 0.0753\n",
      "Epoch 12/300 - Train Loss: 0.0883, Val Loss: 0.0836\n",
      "Epoch 13/300 - Train Loss: 0.0887, Val Loss: 0.0800\n",
      "Epoch 14/300 - Train Loss: 0.0836, Val Loss: 0.0771\n",
      "Epoch 15/300 - Train Loss: 0.0841, Val Loss: 0.0770\n",
      "Epoch 16/300 - Train Loss: 0.0863, Val Loss: 0.0763\n",
      "Epoch 17/300 - Train Loss: 0.0852, Val Loss: 0.0738\n",
      "Epoch 18/300 - Train Loss: 0.0847, Val Loss: 0.0782\n",
      "Epoch 19/300 - Train Loss: 0.0828, Val Loss: 0.0772\n",
      "Epoch 20/300 - Train Loss: 0.0828, Val Loss: 0.0790\n",
      "Epoch 21/300 - Train Loss: 0.0817, Val Loss: 0.0695\n",
      "Epoch 22/300 - Train Loss: 0.0789, Val Loss: 0.0761\n",
      "Epoch 23/300 - Train Loss: 0.0823, Val Loss: 0.0818\n",
      "Epoch 24/300 - Train Loss: 0.0793, Val Loss: 0.0758\n",
      "Epoch 25/300 - Train Loss: 0.0804, Val Loss: 0.0806\n",
      "Epoch 26/300 - Train Loss: 0.0791, Val Loss: 0.0766\n",
      "Epoch 27/300 - Train Loss: 0.0809, Val Loss: 0.0812\n",
      "Epoch 28/300 - Train Loss: 0.0787, Val Loss: 0.0729\n",
      "Epoch 29/300 - Train Loss: 0.0804, Val Loss: 0.0782\n",
      "Epoch 30/300 - Train Loss: 0.0819, Val Loss: 0.0756\n",
      "Epoch 31/300 - Train Loss: 0.0802, Val Loss: 0.0755\n",
      "Epoch 32/300 - Train Loss: 0.0807, Val Loss: 0.0755\n",
      "Epoch 33/300 - Train Loss: 0.0775, Val Loss: 0.0732\n",
      "Epoch 34/300 - Train Loss: 0.0811, Val Loss: 0.0800\n",
      "Epoch 35/300 - Train Loss: 0.0773, Val Loss: 0.0766\n",
      "Epoch 36/300 - Train Loss: 0.0784, Val Loss: 0.0735\n",
      "Epoch 37/300 - Train Loss: 0.0768, Val Loss: 0.0738\n",
      "Epoch 38/300 - Train Loss: 0.0745, Val Loss: 0.0753\n",
      "Epoch 39/300 - Train Loss: 0.0796, Val Loss: 0.0688\n",
      "Epoch 40/300 - Train Loss: 0.0748, Val Loss: 0.0746\n",
      "Epoch 41/300 - Train Loss: 0.0780, Val Loss: 0.0701\n",
      "Epoch 42/300 - Train Loss: 0.0773, Val Loss: 0.0766\n",
      "Epoch 43/300 - Train Loss: 0.0739, Val Loss: 0.0715\n",
      "Epoch 44/300 - Train Loss: 0.0772, Val Loss: 0.0758\n",
      "Epoch 45/300 - Train Loss: 0.0739, Val Loss: 0.0801\n",
      "Epoch 46/300 - Train Loss: 0.0762, Val Loss: 0.0782\n",
      "Epoch 47/300 - Train Loss: 0.0748, Val Loss: 0.0727\n",
      "Epoch 48/300 - Train Loss: 0.0771, Val Loss: 0.0750\n",
      "Epoch 49/300 - Train Loss: 0.0750, Val Loss: 0.0713\n",
      "Epoch 50/300 - Train Loss: 0.0765, Val Loss: 0.0758\n",
      "Epoch 51/300 - Train Loss: 0.0768, Val Loss: 0.0831\n",
      "Epoch 52/300 - Train Loss: 0.0757, Val Loss: 0.0720\n",
      "Epoch 53/300 - Train Loss: 0.0729, Val Loss: 0.0741\n",
      "Epoch 54/300 - Train Loss: 0.0738, Val Loss: 0.0732\n",
      "Epoch 55/300 - Train Loss: 0.0728, Val Loss: 0.0795\n",
      "Epoch 56/300 - Train Loss: 0.0731, Val Loss: 0.0767\n",
      "Epoch 57/300 - Train Loss: 0.0737, Val Loss: 0.0758\n",
      "Epoch 58/300 - Train Loss: 0.0717, Val Loss: 0.0776\n",
      "Epoch 59/300 - Train Loss: 0.0743, Val Loss: 0.0720\n",
      "Epoch 60/300 - Train Loss: 0.0731, Val Loss: 0.0701\n",
      "Epoch 61/300 - Train Loss: 0.0738, Val Loss: 0.0795\n",
      "Epoch 62/300 - Train Loss: 0.0719, Val Loss: 0.0792\n",
      "Epoch 63/300 - Train Loss: 0.0728, Val Loss: 0.0726\n",
      "Epoch 64/300 - Train Loss: 0.0698, Val Loss: 0.0723\n",
      "Epoch 65/300 - Train Loss: 0.0702, Val Loss: 0.0728\n",
      "Epoch 66/300 - Train Loss: 0.0711, Val Loss: 0.0779\n",
      "Epoch 67/300 - Train Loss: 0.0714, Val Loss: 0.0693\n",
      "Epoch 68/300 - Train Loss: 0.0702, Val Loss: 0.0743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:53:49,094] Trial 60 finished with value: 0.9687766538774562 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.3296282788664616, 'learning_rate': 0.0004244449249013814, 'batch_size': 32, 'weight_decay': 0.00018135199384738122}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.0708, Val Loss: 0.0737\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9688, Macro Precision: 0.9628, Macro Recall: 0.9752\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 62\n",
      "Training with F1=8, F2=32, D=4, dropout=0.32231920449550605, LR=0.0007018391052797357, BS=32, WD=0.0002961863431059396\n",
      "Epoch 1/300 - Train Loss: 0.1684, Val Loss: 0.0976\n",
      "Epoch 2/300 - Train Loss: 0.1112, Val Loss: 0.0825\n",
      "Epoch 3/300 - Train Loss: 0.1050, Val Loss: 0.0758\n",
      "Epoch 4/300 - Train Loss: 0.1000, Val Loss: 0.0824\n",
      "Epoch 5/300 - Train Loss: 0.0983, Val Loss: 0.0777\n",
      "Epoch 6/300 - Train Loss: 0.0973, Val Loss: 0.0707\n",
      "Epoch 7/300 - Train Loss: 0.0955, Val Loss: 0.0789\n",
      "Epoch 8/300 - Train Loss: 0.0977, Val Loss: 0.0764\n",
      "Epoch 9/300 - Train Loss: 0.0951, Val Loss: 0.0726\n",
      "Epoch 10/300 - Train Loss: 0.0914, Val Loss: 0.0783\n",
      "Epoch 11/300 - Train Loss: 0.0895, Val Loss: 0.0785\n",
      "Epoch 12/300 - Train Loss: 0.0908, Val Loss: 0.0742\n",
      "Epoch 13/300 - Train Loss: 0.0893, Val Loss: 0.0712\n",
      "Epoch 14/300 - Train Loss: 0.0866, Val Loss: 0.0717\n",
      "Epoch 15/300 - Train Loss: 0.0880, Val Loss: 0.0798\n",
      "Epoch 16/300 - Train Loss: 0.0887, Val Loss: 0.0764\n",
      "Epoch 17/300 - Train Loss: 0.0830, Val Loss: 0.0755\n",
      "Epoch 18/300 - Train Loss: 0.0860, Val Loss: 0.0769\n",
      "Epoch 19/300 - Train Loss: 0.0864, Val Loss: 0.0726\n",
      "Epoch 20/300 - Train Loss: 0.0841, Val Loss: 0.0731\n",
      "Epoch 21/300 - Train Loss: 0.0845, Val Loss: 0.0745\n",
      "Epoch 22/300 - Train Loss: 0.0854, Val Loss: 0.0792\n",
      "Epoch 23/300 - Train Loss: 0.0824, Val Loss: 0.0818\n",
      "Epoch 24/300 - Train Loss: 0.0842, Val Loss: 0.0763\n",
      "Epoch 25/300 - Train Loss: 0.0836, Val Loss: 0.0818\n",
      "Epoch 26/300 - Train Loss: 0.0825, Val Loss: 0.0779\n",
      "Epoch 27/300 - Train Loss: 0.0850, Val Loss: 0.0715\n",
      "Epoch 28/300 - Train Loss: 0.0830, Val Loss: 0.0763\n",
      "Epoch 29/300 - Train Loss: 0.0830, Val Loss: 0.0868\n",
      "Epoch 30/300 - Train Loss: 0.0813, Val Loss: 0.0742\n",
      "Epoch 31/300 - Train Loss: 0.0829, Val Loss: 0.0754\n",
      "Epoch 32/300 - Train Loss: 0.0832, Val Loss: 0.0711\n",
      "Epoch 33/300 - Train Loss: 0.0831, Val Loss: 0.0764\n",
      "Epoch 34/300 - Train Loss: 0.0831, Val Loss: 0.0749\n",
      "Epoch 35/300 - Train Loss: 0.0813, Val Loss: 0.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:55:02,518] Trial 61 finished with value: 0.9599164850263483 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.32231920449550605, 'learning_rate': 0.0007018391052797357, 'batch_size': 32, 'weight_decay': 0.0002961863431059396}. Best is trial 47 with value: 0.9720323096544233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300 - Train Loss: 0.0818, Val Loss: 0.0740\n",
      "Early stopping at epoch 36\n",
      "Macro F1 Score: 0.9599, Macro Precision: 0.9609, Macro Recall: 0.9590\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.92      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 63\n",
      "Training with F1=8, F2=32, D=4, dropout=0.3392163528786703, LR=0.00042719480772711446, BS=32, WD=0.00019330535697594084\n",
      "Epoch 1/300 - Train Loss: 0.1843, Val Loss: 0.0909\n",
      "Epoch 2/300 - Train Loss: 0.1114, Val Loss: 0.0858\n",
      "Epoch 3/300 - Train Loss: 0.1095, Val Loss: 0.0766\n",
      "Epoch 4/300 - Train Loss: 0.0995, Val Loss: 0.0787\n",
      "Epoch 5/300 - Train Loss: 0.0983, Val Loss: 0.0751\n",
      "Epoch 6/300 - Train Loss: 0.0950, Val Loss: 0.0827\n",
      "Epoch 7/300 - Train Loss: 0.0972, Val Loss: 0.0837\n",
      "Epoch 8/300 - Train Loss: 0.0929, Val Loss: 0.0765\n",
      "Epoch 9/300 - Train Loss: 0.0926, Val Loss: 0.0819\n",
      "Epoch 10/300 - Train Loss: 0.0891, Val Loss: 0.0755\n",
      "Epoch 11/300 - Train Loss: 0.0883, Val Loss: 0.0708\n",
      "Epoch 12/300 - Train Loss: 0.0887, Val Loss: 0.0860\n",
      "Epoch 13/300 - Train Loss: 0.0896, Val Loss: 0.0856\n",
      "Epoch 14/300 - Train Loss: 0.0877, Val Loss: 0.0782\n",
      "Epoch 15/300 - Train Loss: 0.0859, Val Loss: 0.0714\n",
      "Epoch 16/300 - Train Loss: 0.0867, Val Loss: 0.0736\n",
      "Epoch 17/300 - Train Loss: 0.0834, Val Loss: 0.0721\n",
      "Epoch 18/300 - Train Loss: 0.0840, Val Loss: 0.0708\n",
      "Epoch 19/300 - Train Loss: 0.0848, Val Loss: 0.0750\n",
      "Epoch 20/300 - Train Loss: 0.0850, Val Loss: 0.0729\n",
      "Epoch 21/300 - Train Loss: 0.0828, Val Loss: 0.0717\n",
      "Epoch 22/300 - Train Loss: 0.0825, Val Loss: 0.0787\n",
      "Epoch 23/300 - Train Loss: 0.0831, Val Loss: 0.0766\n",
      "Epoch 24/300 - Train Loss: 0.0841, Val Loss: 0.0803\n",
      "Epoch 25/300 - Train Loss: 0.0827, Val Loss: 0.0767\n",
      "Epoch 26/300 - Train Loss: 0.0801, Val Loss: 0.0713\n",
      "Epoch 27/300 - Train Loss: 0.0784, Val Loss: 0.0826\n",
      "Epoch 28/300 - Train Loss: 0.0810, Val Loss: 0.0745\n",
      "Epoch 29/300 - Train Loss: 0.0822, Val Loss: 0.0793\n",
      "Epoch 30/300 - Train Loss: 0.0799, Val Loss: 0.0770\n",
      "Epoch 31/300 - Train Loss: 0.0806, Val Loss: 0.0691\n",
      "Epoch 32/300 - Train Loss: 0.0785, Val Loss: 0.0718\n",
      "Epoch 33/300 - Train Loss: 0.0817, Val Loss: 0.0772\n",
      "Epoch 34/300 - Train Loss: 0.0785, Val Loss: 0.0919\n",
      "Epoch 35/300 - Train Loss: 0.0787, Val Loss: 0.0869\n",
      "Epoch 36/300 - Train Loss: 0.0784, Val Loss: 0.0751\n",
      "Epoch 37/300 - Train Loss: 0.0767, Val Loss: 0.0777\n",
      "Epoch 38/300 - Train Loss: 0.0768, Val Loss: 0.0792\n",
      "Epoch 39/300 - Train Loss: 0.0750, Val Loss: 0.0739\n",
      "Epoch 40/300 - Train Loss: 0.0762, Val Loss: 0.0760\n",
      "Epoch 41/300 - Train Loss: 0.0772, Val Loss: 0.0779\n",
      "Epoch 42/300 - Train Loss: 0.0766, Val Loss: 0.0678\n",
      "Epoch 43/300 - Train Loss: 0.0756, Val Loss: 0.0694\n",
      "Epoch 44/300 - Train Loss: 0.0757, Val Loss: 0.0787\n",
      "Epoch 45/300 - Train Loss: 0.0759, Val Loss: 0.0728\n",
      "Epoch 46/300 - Train Loss: 0.0746, Val Loss: 0.0692\n",
      "Epoch 47/300 - Train Loss: 0.0772, Val Loss: 0.0751\n",
      "Epoch 48/300 - Train Loss: 0.0740, Val Loss: 0.0762\n",
      "Epoch 49/300 - Train Loss: 0.0744, Val Loss: 0.0756\n",
      "Epoch 50/300 - Train Loss: 0.0745, Val Loss: 0.0728\n",
      "Epoch 51/300 - Train Loss: 0.0727, Val Loss: 0.0783\n",
      "Epoch 52/300 - Train Loss: 0.0757, Val Loss: 0.0772\n",
      "Epoch 53/300 - Train Loss: 0.0760, Val Loss: 0.0815\n",
      "Epoch 54/300 - Train Loss: 0.0748, Val Loss: 0.0780\n",
      "Epoch 55/300 - Train Loss: 0.0719, Val Loss: 0.0773\n",
      "Epoch 56/300 - Train Loss: 0.0724, Val Loss: 0.0726\n",
      "Epoch 57/300 - Train Loss: 0.0754, Val Loss: 0.0727\n",
      "Epoch 58/300 - Train Loss: 0.0722, Val Loss: 0.0817\n",
      "Epoch 59/300 - Train Loss: 0.0702, Val Loss: 0.0772\n",
      "Epoch 60/300 - Train Loss: 0.0749, Val Loss: 0.0733\n",
      "Epoch 61/300 - Train Loss: 0.0729, Val Loss: 0.0751\n",
      "Epoch 62/300 - Train Loss: 0.0716, Val Loss: 0.0788\n",
      "Epoch 63/300 - Train Loss: 0.0742, Val Loss: 0.0825\n",
      "Epoch 64/300 - Train Loss: 0.0715, Val Loss: 0.0736\n",
      "Epoch 65/300 - Train Loss: 0.0716, Val Loss: 0.0892\n",
      "Epoch 66/300 - Train Loss: 0.0739, Val Loss: 0.0710\n",
      "Epoch 67/300 - Train Loss: 0.0725, Val Loss: 0.0798\n",
      "Epoch 68/300 - Train Loss: 0.0723, Val Loss: 0.0776\n",
      "Epoch 69/300 - Train Loss: 0.0753, Val Loss: 0.0803\n",
      "Epoch 70/300 - Train Loss: 0.0718, Val Loss: 0.0705\n",
      "Epoch 71/300 - Train Loss: 0.0720, Val Loss: 0.0780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:57:28,957] Trial 62 finished with value: 0.9742503388691485 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.3392163528786703, 'learning_rate': 0.00042719480772711446, 'batch_size': 32, 'weight_decay': 0.00019330535697594084}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300 - Train Loss: 0.0736, Val Loss: 0.0762\n",
      "Early stopping at epoch 72\n",
      "Macro F1 Score: 0.9743, Macro Precision: 0.9775, Macro Recall: 0.9711\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 64\n",
      "Training with F1=8, F2=32, D=4, dropout=0.3728459719783078, LR=0.00022167472468430195, BS=32, WD=0.000766668270876542\n",
      "Epoch 1/300 - Train Loss: 0.2544, Val Loss: 0.1064\n",
      "Epoch 2/300 - Train Loss: 0.1291, Val Loss: 0.0868\n",
      "Epoch 3/300 - Train Loss: 0.1140, Val Loss: 0.0817\n",
      "Epoch 4/300 - Train Loss: 0.1060, Val Loss: 0.0890\n",
      "Epoch 5/300 - Train Loss: 0.1003, Val Loss: 0.0777\n",
      "Epoch 6/300 - Train Loss: 0.1010, Val Loss: 0.0857\n",
      "Epoch 7/300 - Train Loss: 0.0961, Val Loss: 0.0810\n",
      "Epoch 8/300 - Train Loss: 0.0959, Val Loss: 0.0779\n",
      "Epoch 9/300 - Train Loss: 0.0997, Val Loss: 0.0724\n",
      "Epoch 10/300 - Train Loss: 0.0939, Val Loss: 0.0748\n",
      "Epoch 11/300 - Train Loss: 0.0918, Val Loss: 0.0749\n",
      "Epoch 12/300 - Train Loss: 0.0925, Val Loss: 0.0701\n",
      "Epoch 13/300 - Train Loss: 0.0927, Val Loss: 0.0766\n",
      "Epoch 14/300 - Train Loss: 0.0920, Val Loss: 0.0696\n",
      "Epoch 15/300 - Train Loss: 0.0911, Val Loss: 0.0728\n",
      "Epoch 16/300 - Train Loss: 0.0924, Val Loss: 0.0700\n",
      "Epoch 17/300 - Train Loss: 0.0890, Val Loss: 0.0762\n",
      "Epoch 18/300 - Train Loss: 0.0880, Val Loss: 0.0716\n",
      "Epoch 19/300 - Train Loss: 0.0906, Val Loss: 0.0728\n",
      "Epoch 20/300 - Train Loss: 0.0891, Val Loss: 0.0708\n",
      "Epoch 21/300 - Train Loss: 0.0914, Val Loss: 0.0734\n",
      "Epoch 22/300 - Train Loss: 0.0890, Val Loss: 0.0700\n",
      "Epoch 23/300 - Train Loss: 0.0859, Val Loss: 0.0729\n",
      "Epoch 24/300 - Train Loss: 0.0873, Val Loss: 0.0721\n",
      "Epoch 25/300 - Train Loss: 0.0899, Val Loss: 0.0668\n",
      "Epoch 26/300 - Train Loss: 0.0877, Val Loss: 0.0723\n",
      "Epoch 27/300 - Train Loss: 0.0861, Val Loss: 0.0782\n",
      "Epoch 28/300 - Train Loss: 0.0891, Val Loss: 0.0705\n",
      "Epoch 29/300 - Train Loss: 0.0864, Val Loss: 0.0700\n",
      "Epoch 30/300 - Train Loss: 0.0845, Val Loss: 0.0728\n",
      "Epoch 31/300 - Train Loss: 0.0854, Val Loss: 0.0774\n",
      "Epoch 32/300 - Train Loss: 0.0856, Val Loss: 0.0692\n",
      "Epoch 33/300 - Train Loss: 0.0848, Val Loss: 0.0702\n",
      "Epoch 34/300 - Train Loss: 0.0849, Val Loss: 0.0737\n",
      "Epoch 35/300 - Train Loss: 0.0866, Val Loss: 0.0703\n",
      "Epoch 36/300 - Train Loss: 0.0896, Val Loss: 0.0734\n",
      "Epoch 37/300 - Train Loss: 0.0865, Val Loss: 0.0701\n",
      "Epoch 38/300 - Train Loss: 0.0839, Val Loss: 0.0718\n",
      "Epoch 39/300 - Train Loss: 0.0855, Val Loss: 0.0721\n",
      "Epoch 40/300 - Train Loss: 0.0871, Val Loss: 0.0708\n",
      "Epoch 41/300 - Train Loss: 0.0870, Val Loss: 0.0756\n",
      "Epoch 42/300 - Train Loss: 0.0857, Val Loss: 0.0683\n",
      "Epoch 43/300 - Train Loss: 0.0861, Val Loss: 0.0851\n",
      "Epoch 44/300 - Train Loss: 0.0877, Val Loss: 0.0786\n",
      "Epoch 45/300 - Train Loss: 0.0836, Val Loss: 0.0730\n",
      "Epoch 46/300 - Train Loss: 0.0850, Val Loss: 0.0685\n",
      "Epoch 47/300 - Train Loss: 0.0873, Val Loss: 0.0713\n",
      "Epoch 48/300 - Train Loss: 0.0865, Val Loss: 0.0711\n",
      "Epoch 49/300 - Train Loss: 0.0841, Val Loss: 0.0696\n",
      "Epoch 50/300 - Train Loss: 0.0846, Val Loss: 0.0696\n",
      "Epoch 51/300 - Train Loss: 0.0849, Val Loss: 0.0701\n",
      "Epoch 52/300 - Train Loss: 0.0844, Val Loss: 0.0694\n",
      "Epoch 53/300 - Train Loss: 0.0818, Val Loss: 0.0725\n",
      "Epoch 54/300 - Train Loss: 0.0839, Val Loss: 0.0773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 02:59:20,344] Trial 63 finished with value: 0.9672103894004174 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.3728459719783078, 'learning_rate': 0.00022167472468430195, 'batch_size': 32, 'weight_decay': 0.000766668270876542}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300 - Train Loss: 0.0840, Val Loss: 0.0704\n",
      "Early stopping at epoch 55\n",
      "Macro F1 Score: 0.9672, Macro Precision: 0.9636, Macro Recall: 0.9711\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 65\n",
      "Training with F1=8, F2=32, D=4, dropout=0.39826079325054314, LR=0.0005515015054684768, BS=32, WD=2.414036627706466e-05\n",
      "Epoch 1/300 - Train Loss: 0.1911, Val Loss: 0.0956\n",
      "Epoch 2/300 - Train Loss: 0.1125, Val Loss: 0.0943\n",
      "Epoch 3/300 - Train Loss: 0.1087, Val Loss: 0.0729\n",
      "Epoch 4/300 - Train Loss: 0.1053, Val Loss: 0.0766\n",
      "Epoch 5/300 - Train Loss: 0.1019, Val Loss: 0.0750\n",
      "Epoch 6/300 - Train Loss: 0.1020, Val Loss: 0.0811\n",
      "Epoch 7/300 - Train Loss: 0.0983, Val Loss: 0.0829\n",
      "Epoch 8/300 - Train Loss: 0.0982, Val Loss: 0.0812\n",
      "Epoch 9/300 - Train Loss: 0.0948, Val Loss: 0.0778\n",
      "Epoch 10/300 - Train Loss: 0.0921, Val Loss: 0.0807\n",
      "Epoch 11/300 - Train Loss: 0.0917, Val Loss: 0.0768\n",
      "Epoch 12/300 - Train Loss: 0.0910, Val Loss: 0.0796\n",
      "Epoch 13/300 - Train Loss: 0.0906, Val Loss: 0.0712\n",
      "Epoch 14/300 - Train Loss: 0.0898, Val Loss: 0.0740\n",
      "Epoch 15/300 - Train Loss: 0.0902, Val Loss: 0.0790\n",
      "Epoch 16/300 - Train Loss: 0.0890, Val Loss: 0.0741\n",
      "Epoch 17/300 - Train Loss: 0.0874, Val Loss: 0.0812\n",
      "Epoch 18/300 - Train Loss: 0.0858, Val Loss: 0.0793\n",
      "Epoch 19/300 - Train Loss: 0.0868, Val Loss: 0.0774\n",
      "Epoch 20/300 - Train Loss: 0.0877, Val Loss: 0.0724\n",
      "Epoch 21/300 - Train Loss: 0.0857, Val Loss: 0.0761\n",
      "Epoch 22/300 - Train Loss: 0.0835, Val Loss: 0.0711\n",
      "Epoch 23/300 - Train Loss: 0.0864, Val Loss: 0.0789\n",
      "Epoch 24/300 - Train Loss: 0.0833, Val Loss: 0.0817\n",
      "Epoch 25/300 - Train Loss: 0.0817, Val Loss: 0.0780\n",
      "Epoch 26/300 - Train Loss: 0.0812, Val Loss: 0.0789\n",
      "Epoch 27/300 - Train Loss: 0.0817, Val Loss: 0.0719\n",
      "Epoch 28/300 - Train Loss: 0.0841, Val Loss: 0.0792\n",
      "Epoch 29/300 - Train Loss: 0.0824, Val Loss: 0.0730\n",
      "Epoch 30/300 - Train Loss: 0.0820, Val Loss: 0.0756\n",
      "Epoch 31/300 - Train Loss: 0.0813, Val Loss: 0.0776\n",
      "Epoch 32/300 - Train Loss: 0.0823, Val Loss: 0.0748\n",
      "Epoch 33/300 - Train Loss: 0.0814, Val Loss: 0.0752\n",
      "Epoch 34/300 - Train Loss: 0.0781, Val Loss: 0.0770\n",
      "Epoch 35/300 - Train Loss: 0.0800, Val Loss: 0.0808\n",
      "Epoch 36/300 - Train Loss: 0.0795, Val Loss: 0.0779\n",
      "Epoch 37/300 - Train Loss: 0.0814, Val Loss: 0.0755\n",
      "Epoch 38/300 - Train Loss: 0.0784, Val Loss: 0.0778\n",
      "Epoch 39/300 - Train Loss: 0.0813, Val Loss: 0.0818\n",
      "Epoch 40/300 - Train Loss: 0.0792, Val Loss: 0.0733\n",
      "Epoch 41/300 - Train Loss: 0.0818, Val Loss: 0.0766\n",
      "Epoch 42/300 - Train Loss: 0.0798, Val Loss: 0.0733\n",
      "Epoch 43/300 - Train Loss: 0.0783, Val Loss: 0.0759\n",
      "Epoch 44/300 - Train Loss: 0.0790, Val Loss: 0.0776\n",
      "Epoch 45/300 - Train Loss: 0.0770, Val Loss: 0.0759\n",
      "Epoch 46/300 - Train Loss: 0.0771, Val Loss: 0.0818\n",
      "Epoch 47/300 - Train Loss: 0.0747, Val Loss: 0.0810\n",
      "Epoch 48/300 - Train Loss: 0.0762, Val Loss: 0.0782\n",
      "Epoch 49/300 - Train Loss: 0.0750, Val Loss: 0.0827\n",
      "Epoch 50/300 - Train Loss: 0.0777, Val Loss: 0.0990\n",
      "Epoch 51/300 - Train Loss: 0.0781, Val Loss: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:01:07,111] Trial 64 finished with value: 0.9630917426017064 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.39826079325054314, 'learning_rate': 0.0005515015054684768, 'batch_size': 32, 'weight_decay': 2.414036627706466e-05}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300 - Train Loss: 0.0774, Val Loss: 0.0869\n",
      "Early stopping at epoch 52\n",
      "Macro F1 Score: 0.9631, Macro Precision: 0.9512, Macro Recall: 0.9764\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 66\n",
      "Training with F1=16, F2=32, D=4, dropout=0.34606855582323914, LR=0.00026408270042327207, BS=32, WD=0.0005870483344433863\n",
      "Epoch 1/300 - Train Loss: 0.2027, Val Loss: 0.1018\n",
      "Epoch 2/300 - Train Loss: 0.1116, Val Loss: 0.0840\n",
      "Epoch 3/300 - Train Loss: 0.1009, Val Loss: 0.0787\n",
      "Epoch 4/300 - Train Loss: 0.1016, Val Loss: 0.0925\n",
      "Epoch 5/300 - Train Loss: 0.0961, Val Loss: 0.0833\n",
      "Epoch 6/300 - Train Loss: 0.0923, Val Loss: 0.0702\n",
      "Epoch 7/300 - Train Loss: 0.0920, Val Loss: 0.0728\n",
      "Epoch 8/300 - Train Loss: 0.0911, Val Loss: 0.0743\n",
      "Epoch 9/300 - Train Loss: 0.0876, Val Loss: 0.0717\n",
      "Epoch 10/300 - Train Loss: 0.0877, Val Loss: 0.0749\n",
      "Epoch 11/300 - Train Loss: 0.0857, Val Loss: 0.0753\n",
      "Epoch 12/300 - Train Loss: 0.0868, Val Loss: 0.0865\n",
      "Epoch 13/300 - Train Loss: 0.0868, Val Loss: 0.0707\n",
      "Epoch 14/300 - Train Loss: 0.0871, Val Loss: 0.0743\n",
      "Epoch 15/300 - Train Loss: 0.0850, Val Loss: 0.0759\n",
      "Epoch 16/300 - Train Loss: 0.0860, Val Loss: 0.0749\n",
      "Epoch 17/300 - Train Loss: 0.0838, Val Loss: 0.0754\n",
      "Epoch 18/300 - Train Loss: 0.0865, Val Loss: 0.0694\n",
      "Epoch 19/300 - Train Loss: 0.0831, Val Loss: 0.0725\n",
      "Epoch 20/300 - Train Loss: 0.0821, Val Loss: 0.0736\n",
      "Epoch 21/300 - Train Loss: 0.0871, Val Loss: 0.0694\n",
      "Epoch 22/300 - Train Loss: 0.0847, Val Loss: 0.0681\n",
      "Epoch 23/300 - Train Loss: 0.0830, Val Loss: 0.0753\n",
      "Epoch 24/300 - Train Loss: 0.0850, Val Loss: 0.0891\n",
      "Epoch 25/300 - Train Loss: 0.0860, Val Loss: 0.0704\n",
      "Epoch 26/300 - Train Loss: 0.0847, Val Loss: 0.0748\n",
      "Epoch 27/300 - Train Loss: 0.0836, Val Loss: 0.0704\n",
      "Epoch 28/300 - Train Loss: 0.0815, Val Loss: 0.0695\n",
      "Epoch 29/300 - Train Loss: 0.0827, Val Loss: 0.0769\n",
      "Epoch 30/300 - Train Loss: 0.0815, Val Loss: 0.0749\n",
      "Epoch 31/300 - Train Loss: 0.0834, Val Loss: 0.0698\n",
      "Epoch 32/300 - Train Loss: 0.0842, Val Loss: 0.0717\n",
      "Epoch 33/300 - Train Loss: 0.0829, Val Loss: 0.0735\n",
      "Epoch 34/300 - Train Loss: 0.0828, Val Loss: 0.0683\n",
      "Epoch 35/300 - Train Loss: 0.0813, Val Loss: 0.0779\n",
      "Epoch 36/300 - Train Loss: 0.0837, Val Loss: 0.0700\n",
      "Epoch 37/300 - Train Loss: 0.0829, Val Loss: 0.0706\n",
      "Epoch 38/300 - Train Loss: 0.0815, Val Loss: 0.0745\n",
      "Epoch 39/300 - Train Loss: 0.0846, Val Loss: 0.0757\n",
      "Epoch 40/300 - Train Loss: 0.0824, Val Loss: 0.0774\n",
      "Epoch 41/300 - Train Loss: 0.0854, Val Loss: 0.0702\n",
      "Epoch 42/300 - Train Loss: 0.0836, Val Loss: 0.0743\n",
      "Epoch 43/300 - Train Loss: 0.0831, Val Loss: 0.0701\n",
      "Epoch 44/300 - Train Loss: 0.0809, Val Loss: 0.0679\n",
      "Epoch 45/300 - Train Loss: 0.0809, Val Loss: 0.0691\n",
      "Epoch 46/300 - Train Loss: 0.0807, Val Loss: 0.0720\n",
      "Epoch 47/300 - Train Loss: 0.0815, Val Loss: 0.0689\n",
      "Epoch 48/300 - Train Loss: 0.0800, Val Loss: 0.0704\n",
      "Epoch 49/300 - Train Loss: 0.0819, Val Loss: 0.0707\n",
      "Epoch 50/300 - Train Loss: 0.0816, Val Loss: 0.0679\n",
      "Epoch 51/300 - Train Loss: 0.0811, Val Loss: 0.0776\n",
      "Epoch 52/300 - Train Loss: 0.0820, Val Loss: 0.0753\n",
      "Epoch 53/300 - Train Loss: 0.0801, Val Loss: 0.0747\n",
      "Epoch 54/300 - Train Loss: 0.0795, Val Loss: 0.0720\n",
      "Epoch 55/300 - Train Loss: 0.0823, Val Loss: 0.0698\n",
      "Epoch 56/300 - Train Loss: 0.0809, Val Loss: 0.0722\n",
      "Epoch 57/300 - Train Loss: 0.0830, Val Loss: 0.0729\n",
      "Epoch 58/300 - Train Loss: 0.0793, Val Loss: 0.0728\n",
      "Epoch 59/300 - Train Loss: 0.0823, Val Loss: 0.0759\n",
      "Epoch 60/300 - Train Loss: 0.0816, Val Loss: 0.0698\n",
      "Epoch 61/300 - Train Loss: 0.0826, Val Loss: 0.0773\n",
      "Epoch 62/300 - Train Loss: 0.0815, Val Loss: 0.0709\n",
      "Epoch 63/300 - Train Loss: 0.0832, Val Loss: 0.0758\n",
      "Epoch 64/300 - Train Loss: 0.0838, Val Loss: 0.0671\n",
      "Epoch 65/300 - Train Loss: 0.0810, Val Loss: 0.0744\n",
      "Epoch 66/300 - Train Loss: 0.0794, Val Loss: 0.0656\n",
      "Epoch 67/300 - Train Loss: 0.0820, Val Loss: 0.0711\n",
      "Epoch 68/300 - Train Loss: 0.0821, Val Loss: 0.0769\n",
      "Epoch 69/300 - Train Loss: 0.0814, Val Loss: 0.0804\n",
      "Epoch 70/300 - Train Loss: 0.0824, Val Loss: 0.0696\n",
      "Epoch 71/300 - Train Loss: 0.0820, Val Loss: 0.0781\n",
      "Epoch 72/300 - Train Loss: 0.0803, Val Loss: 0.0693\n",
      "Epoch 73/300 - Train Loss: 0.0804, Val Loss: 0.0792\n",
      "Epoch 74/300 - Train Loss: 0.0774, Val Loss: 0.0760\n",
      "Epoch 75/300 - Train Loss: 0.0808, Val Loss: 0.0723\n",
      "Epoch 76/300 - Train Loss: 0.0791, Val Loss: 0.0736\n",
      "Epoch 77/300 - Train Loss: 0.0801, Val Loss: 0.0726\n",
      "Epoch 78/300 - Train Loss: 0.0787, Val Loss: 0.0687\n",
      "Epoch 79/300 - Train Loss: 0.0810, Val Loss: 0.0759\n",
      "Epoch 80/300 - Train Loss: 0.0803, Val Loss: 0.0696\n",
      "Epoch 81/300 - Train Loss: 0.0787, Val Loss: 0.0700\n",
      "Epoch 82/300 - Train Loss: 0.0802, Val Loss: 0.0739\n",
      "Epoch 83/300 - Train Loss: 0.0806, Val Loss: 0.0714\n",
      "Epoch 84/300 - Train Loss: 0.0799, Val Loss: 0.0744\n",
      "Epoch 85/300 - Train Loss: 0.0793, Val Loss: 0.0767\n",
      "Epoch 86/300 - Train Loss: 0.0809, Val Loss: 0.0714\n",
      "Epoch 87/300 - Train Loss: 0.0816, Val Loss: 0.0750\n",
      "Epoch 88/300 - Train Loss: 0.0803, Val Loss: 0.0719\n",
      "Epoch 89/300 - Train Loss: 0.0817, Val Loss: 0.0691\n",
      "Epoch 90/300 - Train Loss: 0.0798, Val Loss: 0.0703\n",
      "Epoch 91/300 - Train Loss: 0.0804, Val Loss: 0.0746\n",
      "Epoch 92/300 - Train Loss: 0.0801, Val Loss: 0.0719\n",
      "Epoch 93/300 - Train Loss: 0.0794, Val Loss: 0.0710\n",
      "Epoch 94/300 - Train Loss: 0.0808, Val Loss: 0.0723\n",
      "Epoch 95/300 - Train Loss: 0.0787, Val Loss: 0.0712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:05:08,019] Trial 65 finished with value: 0.9677748949762393 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.34606855582323914, 'learning_rate': 0.00026408270042327207, 'batch_size': 32, 'weight_decay': 0.0005870483344433863}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/300 - Train Loss: 0.0817, Val Loss: 0.0777\n",
      "Early stopping at epoch 96\n",
      "Macro F1 Score: 0.9678, Macro Precision: 0.9643, Macro Recall: 0.9715\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 67\n",
      "Training with F1=8, F2=32, D=2, dropout=0.4549273688418617, LR=0.0003724934488524772, BS=32, WD=1.2752091566766674e-05\n",
      "Epoch 1/300 - Train Loss: 0.2418, Val Loss: 0.1064\n",
      "Epoch 2/300 - Train Loss: 0.1377, Val Loss: 0.0937\n",
      "Epoch 3/300 - Train Loss: 0.1238, Val Loss: 0.0830\n",
      "Epoch 4/300 - Train Loss: 0.1125, Val Loss: 0.0847\n",
      "Epoch 5/300 - Train Loss: 0.1072, Val Loss: 0.0767\n",
      "Epoch 6/300 - Train Loss: 0.1073, Val Loss: 0.0806\n",
      "Epoch 7/300 - Train Loss: 0.1082, Val Loss: 0.0830\n",
      "Epoch 8/300 - Train Loss: 0.1016, Val Loss: 0.0755\n",
      "Epoch 9/300 - Train Loss: 0.1041, Val Loss: 0.0754\n",
      "Epoch 10/300 - Train Loss: 0.1003, Val Loss: 0.0785\n",
      "Epoch 11/300 - Train Loss: 0.0990, Val Loss: 0.0789\n",
      "Epoch 12/300 - Train Loss: 0.0978, Val Loss: 0.0744\n",
      "Epoch 13/300 - Train Loss: 0.0984, Val Loss: 0.0831\n",
      "Epoch 14/300 - Train Loss: 0.0989, Val Loss: 0.0738\n",
      "Epoch 15/300 - Train Loss: 0.0978, Val Loss: 0.0749\n",
      "Epoch 16/300 - Train Loss: 0.0991, Val Loss: 0.0761\n",
      "Epoch 17/300 - Train Loss: 0.0945, Val Loss: 0.0833\n",
      "Epoch 18/300 - Train Loss: 0.0976, Val Loss: 0.0763\n",
      "Epoch 19/300 - Train Loss: 0.0955, Val Loss: 0.0831\n",
      "Epoch 20/300 - Train Loss: 0.0968, Val Loss: 0.0871\n",
      "Epoch 21/300 - Train Loss: 0.0919, Val Loss: 0.0789\n",
      "Epoch 22/300 - Train Loss: 0.1023, Val Loss: 0.0739\n",
      "Epoch 23/300 - Train Loss: 0.0903, Val Loss: 0.0760\n",
      "Epoch 24/300 - Train Loss: 0.0921, Val Loss: 0.0776\n",
      "Epoch 25/300 - Train Loss: 0.0929, Val Loss: 0.0766\n",
      "Epoch 26/300 - Train Loss: 0.0902, Val Loss: 0.0687\n",
      "Epoch 27/300 - Train Loss: 0.0916, Val Loss: 0.0743\n",
      "Epoch 28/300 - Train Loss: 0.0884, Val Loss: 0.0909\n",
      "Epoch 29/300 - Train Loss: 0.0895, Val Loss: 0.0757\n",
      "Epoch 30/300 - Train Loss: 0.0889, Val Loss: 0.0792\n",
      "Epoch 31/300 - Train Loss: 0.0916, Val Loss: 0.0744\n",
      "Epoch 32/300 - Train Loss: 0.0889, Val Loss: 0.0829\n",
      "Epoch 33/300 - Train Loss: 0.0925, Val Loss: 0.0768\n",
      "Epoch 34/300 - Train Loss: 0.0911, Val Loss: 0.0805\n",
      "Epoch 35/300 - Train Loss: 0.0877, Val Loss: 0.0744\n",
      "Epoch 36/300 - Train Loss: 0.0903, Val Loss: 0.0780\n",
      "Epoch 37/300 - Train Loss: 0.0874, Val Loss: 0.0719\n",
      "Epoch 38/300 - Train Loss: 0.0868, Val Loss: 0.0741\n",
      "Epoch 39/300 - Train Loss: 0.0897, Val Loss: 0.0773\n",
      "Epoch 40/300 - Train Loss: 0.0869, Val Loss: 0.0793\n",
      "Epoch 41/300 - Train Loss: 0.0907, Val Loss: 0.0717\n",
      "Epoch 42/300 - Train Loss: 0.0861, Val Loss: 0.0730\n",
      "Epoch 43/300 - Train Loss: 0.0846, Val Loss: 0.0817\n",
      "Epoch 44/300 - Train Loss: 0.0883, Val Loss: 0.0778\n",
      "Epoch 45/300 - Train Loss: 0.0890, Val Loss: 0.0745\n",
      "Epoch 46/300 - Train Loss: 0.0879, Val Loss: 0.0783\n",
      "Epoch 47/300 - Train Loss: 0.0883, Val Loss: 0.0792\n",
      "Epoch 48/300 - Train Loss: 0.0850, Val Loss: 0.0841\n",
      "Epoch 49/300 - Train Loss: 0.0851, Val Loss: 0.0758\n",
      "Epoch 50/300 - Train Loss: 0.0893, Val Loss: 0.0790\n",
      "Epoch 51/300 - Train Loss: 0.0863, Val Loss: 0.0842\n",
      "Epoch 52/300 - Train Loss: 0.0838, Val Loss: 0.0788\n",
      "Epoch 53/300 - Train Loss: 0.0857, Val Loss: 0.0774\n",
      "Epoch 54/300 - Train Loss: 0.0830, Val Loss: 0.0844\n",
      "Epoch 55/300 - Train Loss: 0.0868, Val Loss: 0.0927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:06:56,448] Trial 66 finished with value: 0.9683579520373602 and parameters: {'F1': 8, 'F2': 32, 'D': 2, 'dropout': 0.4549273688418617, 'learning_rate': 0.0003724934488524772, 'batch_size': 32, 'weight_decay': 1.2752091566766674e-05}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300 - Train Loss: 0.0866, Val Loss: 0.0740\n",
      "Early stopping at epoch 56\n",
      "Macro F1 Score: 0.9684, Macro Precision: 0.9568, Macro Recall: 0.9815\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 68\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2697678394026331, LR=0.00015820844483443457, BS=32, WD=0.0004175829012620158\n",
      "Epoch 1/300 - Train Loss: 0.2083, Val Loss: 0.0926\n",
      "Epoch 2/300 - Train Loss: 0.1054, Val Loss: 0.0874\n",
      "Epoch 3/300 - Train Loss: 0.0969, Val Loss: 0.0760\n",
      "Epoch 4/300 - Train Loss: 0.0940, Val Loss: 0.0729\n",
      "Epoch 5/300 - Train Loss: 0.0902, Val Loss: 0.0717\n",
      "Epoch 6/300 - Train Loss: 0.0872, Val Loss: 0.0689\n",
      "Epoch 7/300 - Train Loss: 0.0840, Val Loss: 0.0704\n",
      "Epoch 8/300 - Train Loss: 0.0850, Val Loss: 0.0752\n",
      "Epoch 9/300 - Train Loss: 0.0841, Val Loss: 0.0787\n",
      "Epoch 10/300 - Train Loss: 0.0876, Val Loss: 0.0779\n",
      "Epoch 11/300 - Train Loss: 0.0808, Val Loss: 0.0728\n",
      "Epoch 12/300 - Train Loss: 0.0811, Val Loss: 0.0718\n",
      "Epoch 13/300 - Train Loss: 0.0803, Val Loss: 0.0680\n",
      "Epoch 14/300 - Train Loss: 0.0850, Val Loss: 0.0678\n",
      "Epoch 15/300 - Train Loss: 0.0782, Val Loss: 0.0744\n",
      "Epoch 16/300 - Train Loss: 0.0779, Val Loss: 0.0696\n",
      "Epoch 17/300 - Train Loss: 0.0787, Val Loss: 0.0659\n",
      "Epoch 18/300 - Train Loss: 0.0784, Val Loss: 0.0687\n",
      "Epoch 19/300 - Train Loss: 0.0774, Val Loss: 0.0705\n",
      "Epoch 20/300 - Train Loss: 0.0787, Val Loss: 0.0739\n",
      "Epoch 21/300 - Train Loss: 0.0769, Val Loss: 0.0667\n",
      "Epoch 22/300 - Train Loss: 0.0759, Val Loss: 0.0690\n",
      "Epoch 23/300 - Train Loss: 0.0753, Val Loss: 0.0692\n",
      "Epoch 24/300 - Train Loss: 0.0738, Val Loss: 0.0688\n",
      "Epoch 25/300 - Train Loss: 0.0739, Val Loss: 0.0750\n",
      "Epoch 26/300 - Train Loss: 0.0773, Val Loss: 0.0757\n",
      "Epoch 27/300 - Train Loss: 0.0772, Val Loss: 0.0694\n",
      "Epoch 28/300 - Train Loss: 0.0710, Val Loss: 0.0710\n",
      "Epoch 29/300 - Train Loss: 0.0741, Val Loss: 0.0680\n",
      "Epoch 30/300 - Train Loss: 0.0749, Val Loss: 0.0698\n",
      "Epoch 31/300 - Train Loss: 0.0728, Val Loss: 0.0689\n",
      "Epoch 32/300 - Train Loss: 0.0717, Val Loss: 0.0672\n",
      "Epoch 33/300 - Train Loss: 0.0737, Val Loss: 0.0721\n",
      "Epoch 34/300 - Train Loss: 0.0720, Val Loss: 0.0670\n",
      "Epoch 35/300 - Train Loss: 0.0721, Val Loss: 0.0715\n",
      "Epoch 36/300 - Train Loss: 0.0739, Val Loss: 0.0679\n",
      "Epoch 37/300 - Train Loss: 0.0729, Val Loss: 0.0712\n",
      "Epoch 38/300 - Train Loss: 0.0711, Val Loss: 0.0673\n",
      "Epoch 39/300 - Train Loss: 0.0702, Val Loss: 0.0734\n",
      "Epoch 40/300 - Train Loss: 0.0714, Val Loss: 0.0710\n",
      "Epoch 41/300 - Train Loss: 0.0697, Val Loss: 0.0691\n",
      "Epoch 42/300 - Train Loss: 0.0722, Val Loss: 0.0696\n",
      "Epoch 43/300 - Train Loss: 0.0713, Val Loss: 0.0693\n",
      "Epoch 44/300 - Train Loss: 0.0713, Val Loss: 0.0733\n",
      "Epoch 45/300 - Train Loss: 0.0717, Val Loss: 0.0693\n",
      "Epoch 46/300 - Train Loss: 0.0692, Val Loss: 0.0704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:09:57,340] Trial 67 finished with value: 0.9689334708762875 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2697678394026331, 'learning_rate': 0.00015820844483443457, 'batch_size': 32, 'weight_decay': 0.0004175829012620158}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300 - Train Loss: 0.0699, Val Loss: 0.0712\n",
      "Early stopping at epoch 47\n",
      "Macro F1 Score: 0.9689, Macro Precision: 0.9570, Macro Recall: 0.9823\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 69\n",
      "Training with F1=16, F2=8, D=8, dropout=0.49194439132324136, LR=0.00011416255368507473, BS=32, WD=0.0005143140974065124\n",
      "Epoch 1/300 - Train Loss: 0.3818, Val Loss: 0.1459\n",
      "Epoch 2/300 - Train Loss: 0.1479, Val Loss: 0.0977\n",
      "Epoch 3/300 - Train Loss: 0.1210, Val Loss: 0.0826\n",
      "Epoch 4/300 - Train Loss: 0.1089, Val Loss: 0.0865\n",
      "Epoch 5/300 - Train Loss: 0.1089, Val Loss: 0.0724\n",
      "Epoch 6/300 - Train Loss: 0.1057, Val Loss: 0.0711\n",
      "Epoch 7/300 - Train Loss: 0.1017, Val Loss: 0.0796\n",
      "Epoch 8/300 - Train Loss: 0.0998, Val Loss: 0.0734\n",
      "Epoch 9/300 - Train Loss: 0.0996, Val Loss: 0.0712\n",
      "Epoch 10/300 - Train Loss: 0.0992, Val Loss: 0.0699\n",
      "Epoch 11/300 - Train Loss: 0.0971, Val Loss: 0.0680\n",
      "Epoch 12/300 - Train Loss: 0.0960, Val Loss: 0.0738\n",
      "Epoch 13/300 - Train Loss: 0.0956, Val Loss: 0.0706\n",
      "Epoch 14/300 - Train Loss: 0.0968, Val Loss: 0.0690\n",
      "Epoch 15/300 - Train Loss: 0.0939, Val Loss: 0.0813\n",
      "Epoch 16/300 - Train Loss: 0.0946, Val Loss: 0.0708\n",
      "Epoch 17/300 - Train Loss: 0.0945, Val Loss: 0.0727\n",
      "Epoch 18/300 - Train Loss: 0.0947, Val Loss: 0.0702\n",
      "Epoch 19/300 - Train Loss: 0.0950, Val Loss: 0.0711\n",
      "Epoch 20/300 - Train Loss: 0.0931, Val Loss: 0.0695\n",
      "Epoch 21/300 - Train Loss: 0.0929, Val Loss: 0.0667\n",
      "Epoch 22/300 - Train Loss: 0.0952, Val Loss: 0.0671\n",
      "Epoch 23/300 - Train Loss: 0.0921, Val Loss: 0.0672\n",
      "Epoch 24/300 - Train Loss: 0.0907, Val Loss: 0.0688\n",
      "Epoch 25/300 - Train Loss: 0.0938, Val Loss: 0.0682\n",
      "Epoch 26/300 - Train Loss: 0.0942, Val Loss: 0.0672\n",
      "Epoch 27/300 - Train Loss: 0.0940, Val Loss: 0.0695\n",
      "Epoch 28/300 - Train Loss: 0.0907, Val Loss: 0.0708\n",
      "Epoch 29/300 - Train Loss: 0.0914, Val Loss: 0.0741\n",
      "Epoch 30/300 - Train Loss: 0.0942, Val Loss: 0.0676\n",
      "Epoch 31/300 - Train Loss: 0.0899, Val Loss: 0.0676\n",
      "Epoch 32/300 - Train Loss: 0.0923, Val Loss: 0.0710\n",
      "Epoch 33/300 - Train Loss: 0.0920, Val Loss: 0.0676\n",
      "Epoch 34/300 - Train Loss: 0.0920, Val Loss: 0.0705\n",
      "Epoch 35/300 - Train Loss: 0.0912, Val Loss: 0.0663\n",
      "Epoch 36/300 - Train Loss: 0.0893, Val Loss: 0.0683\n",
      "Epoch 37/300 - Train Loss: 0.0904, Val Loss: 0.0659\n",
      "Epoch 38/300 - Train Loss: 0.0900, Val Loss: 0.0662\n",
      "Epoch 39/300 - Train Loss: 0.0921, Val Loss: 0.0664\n",
      "Epoch 40/300 - Train Loss: 0.0915, Val Loss: 0.0682\n",
      "Epoch 41/300 - Train Loss: 0.0904, Val Loss: 0.0679\n",
      "Epoch 42/300 - Train Loss: 0.0871, Val Loss: 0.0690\n",
      "Epoch 43/300 - Train Loss: 0.0908, Val Loss: 0.0674\n",
      "Epoch 44/300 - Train Loss: 0.0879, Val Loss: 0.0698\n",
      "Epoch 45/300 - Train Loss: 0.0893, Val Loss: 0.0668\n",
      "Epoch 46/300 - Train Loss: 0.0895, Val Loss: 0.0722\n",
      "Epoch 47/300 - Train Loss: 0.0902, Val Loss: 0.0740\n",
      "Epoch 48/300 - Train Loss: 0.0913, Val Loss: 0.0693\n",
      "Epoch 49/300 - Train Loss: 0.0883, Val Loss: 0.0667\n",
      "Epoch 50/300 - Train Loss: 0.0884, Val Loss: 0.0654\n",
      "Epoch 51/300 - Train Loss: 0.0850, Val Loss: 0.0658\n",
      "Epoch 52/300 - Train Loss: 0.0889, Val Loss: 0.0650\n",
      "Epoch 53/300 - Train Loss: 0.0905, Val Loss: 0.0714\n",
      "Epoch 54/300 - Train Loss: 0.0887, Val Loss: 0.0699\n",
      "Epoch 55/300 - Train Loss: 0.0877, Val Loss: 0.0647\n",
      "Epoch 56/300 - Train Loss: 0.0879, Val Loss: 0.0693\n",
      "Epoch 57/300 - Train Loss: 0.0891, Val Loss: 0.0666\n",
      "Epoch 58/300 - Train Loss: 0.0905, Val Loss: 0.0669\n",
      "Epoch 59/300 - Train Loss: 0.0892, Val Loss: 0.0660\n",
      "Epoch 60/300 - Train Loss: 0.0890, Val Loss: 0.0686\n",
      "Epoch 61/300 - Train Loss: 0.0891, Val Loss: 0.0660\n",
      "Epoch 62/300 - Train Loss: 0.0892, Val Loss: 0.0658\n",
      "Epoch 63/300 - Train Loss: 0.0876, Val Loss: 0.0674\n",
      "Epoch 64/300 - Train Loss: 0.0861, Val Loss: 0.0670\n",
      "Epoch 65/300 - Train Loss: 0.0860, Val Loss: 0.0659\n",
      "Epoch 66/300 - Train Loss: 0.0871, Val Loss: 0.0704\n",
      "Epoch 67/300 - Train Loss: 0.0918, Val Loss: 0.0678\n",
      "Epoch 68/300 - Train Loss: 0.0888, Val Loss: 0.0680\n",
      "Epoch 69/300 - Train Loss: 0.0880, Val Loss: 0.0644\n",
      "Epoch 70/300 - Train Loss: 0.0877, Val Loss: 0.0647\n",
      "Epoch 71/300 - Train Loss: 0.0882, Val Loss: 0.0649\n",
      "Epoch 72/300 - Train Loss: 0.0866, Val Loss: 0.0676\n",
      "Epoch 73/300 - Train Loss: 0.0882, Val Loss: 0.0700\n",
      "Epoch 74/300 - Train Loss: 0.0887, Val Loss: 0.0661\n",
      "Epoch 75/300 - Train Loss: 0.0881, Val Loss: 0.0670\n",
      "Epoch 76/300 - Train Loss: 0.0863, Val Loss: 0.0653\n",
      "Epoch 77/300 - Train Loss: 0.0860, Val Loss: 0.0710\n",
      "Epoch 78/300 - Train Loss: 0.0873, Val Loss: 0.0690\n",
      "Epoch 79/300 - Train Loss: 0.0884, Val Loss: 0.0664\n",
      "Epoch 80/300 - Train Loss: 0.0885, Val Loss: 0.0709\n",
      "Epoch 81/300 - Train Loss: 0.0879, Val Loss: 0.0669\n",
      "Epoch 82/300 - Train Loss: 0.0903, Val Loss: 0.0730\n",
      "Epoch 83/300 - Train Loss: 0.0881, Val Loss: 0.0712\n",
      "Epoch 84/300 - Train Loss: 0.0872, Val Loss: 0.0653\n",
      "Epoch 85/300 - Train Loss: 0.0861, Val Loss: 0.0663\n",
      "Epoch 86/300 - Train Loss: 0.0873, Val Loss: 0.0799\n",
      "Epoch 87/300 - Train Loss: 0.0882, Val Loss: 0.0681\n",
      "Epoch 88/300 - Train Loss: 0.0895, Val Loss: 0.0674\n",
      "Epoch 89/300 - Train Loss: 0.0916, Val Loss: 0.0725\n",
      "Epoch 90/300 - Train Loss: 0.0913, Val Loss: 0.0654\n",
      "Epoch 91/300 - Train Loss: 0.0885, Val Loss: 0.0684\n",
      "Epoch 92/300 - Train Loss: 0.0872, Val Loss: 0.0646\n",
      "Epoch 93/300 - Train Loss: 0.0885, Val Loss: 0.0680\n",
      "Epoch 94/300 - Train Loss: 0.0876, Val Loss: 0.0704\n",
      "Epoch 95/300 - Train Loss: 0.0878, Val Loss: 0.0700\n",
      "Epoch 96/300 - Train Loss: 0.0868, Val Loss: 0.0652\n",
      "Epoch 97/300 - Train Loss: 0.0867, Val Loss: 0.0670\n",
      "Epoch 98/300 - Train Loss: 0.0904, Val Loss: 0.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:15:35,419] Trial 68 finished with value: 0.9644985011430262 and parameters: {'F1': 16, 'F2': 8, 'D': 8, 'dropout': 0.49194439132324136, 'learning_rate': 0.00011416255368507473, 'batch_size': 32, 'weight_decay': 0.0005143140974065124}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/300 - Train Loss: 0.0900, Val Loss: 0.0650\n",
      "Early stopping at epoch 99\n",
      "Macro F1 Score: 0.9645, Macro Precision: 0.9589, Macro Recall: 0.9705\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 70\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2659674581437484, LR=0.0001849152936349048, BS=32, WD=0.0012540730075501416\n",
      "Epoch 1/300 - Train Loss: 0.2022, Val Loss: 0.1423\n",
      "Epoch 2/300 - Train Loss: 0.1041, Val Loss: 0.0852\n",
      "Epoch 3/300 - Train Loss: 0.0972, Val Loss: 0.0859\n",
      "Epoch 4/300 - Train Loss: 0.0918, Val Loss: 0.0736\n",
      "Epoch 5/300 - Train Loss: 0.0905, Val Loss: 0.0824\n",
      "Epoch 6/300 - Train Loss: 0.0890, Val Loss: 0.0714\n",
      "Epoch 7/300 - Train Loss: 0.0869, Val Loss: 0.0709\n",
      "Epoch 8/300 - Train Loss: 0.0863, Val Loss: 0.0717\n",
      "Epoch 9/300 - Train Loss: 0.0853, Val Loss: 0.0715\n",
      "Epoch 10/300 - Train Loss: 0.0840, Val Loss: 0.0750\n",
      "Epoch 11/300 - Train Loss: 0.0847, Val Loss: 0.0786\n",
      "Epoch 12/300 - Train Loss: 0.0840, Val Loss: 0.0685\n",
      "Epoch 13/300 - Train Loss: 0.0843, Val Loss: 0.0693\n",
      "Epoch 14/300 - Train Loss: 0.0826, Val Loss: 0.0758\n",
      "Epoch 15/300 - Train Loss: 0.0828, Val Loss: 0.0716\n",
      "Epoch 16/300 - Train Loss: 0.0833, Val Loss: 0.0661\n",
      "Epoch 17/300 - Train Loss: 0.0843, Val Loss: 0.0716\n",
      "Epoch 18/300 - Train Loss: 0.0839, Val Loss: 0.0781\n",
      "Epoch 19/300 - Train Loss: 0.0824, Val Loss: 0.0725\n",
      "Epoch 20/300 - Train Loss: 0.0836, Val Loss: 0.0810\n",
      "Epoch 21/300 - Train Loss: 0.0841, Val Loss: 0.0766\n",
      "Epoch 22/300 - Train Loss: 0.0836, Val Loss: 0.0766\n",
      "Epoch 23/300 - Train Loss: 0.0828, Val Loss: 0.0773\n",
      "Epoch 24/300 - Train Loss: 0.0834, Val Loss: 0.0697\n",
      "Epoch 25/300 - Train Loss: 0.0854, Val Loss: 0.0684\n",
      "Epoch 26/300 - Train Loss: 0.0833, Val Loss: 0.0707\n",
      "Epoch 27/300 - Train Loss: 0.0837, Val Loss: 0.0780\n",
      "Epoch 28/300 - Train Loss: 0.0838, Val Loss: 0.0730\n",
      "Epoch 29/300 - Train Loss: 0.0817, Val Loss: 0.0724\n",
      "Epoch 30/300 - Train Loss: 0.0813, Val Loss: 0.0719\n",
      "Epoch 31/300 - Train Loss: 0.0808, Val Loss: 0.0710\n",
      "Epoch 32/300 - Train Loss: 0.0833, Val Loss: 0.0775\n",
      "Epoch 33/300 - Train Loss: 0.0839, Val Loss: 0.0761\n",
      "Epoch 34/300 - Train Loss: 0.0833, Val Loss: 0.0776\n",
      "Epoch 35/300 - Train Loss: 0.0790, Val Loss: 0.0742\n",
      "Epoch 36/300 - Train Loss: 0.0835, Val Loss: 0.0707\n",
      "Epoch 37/300 - Train Loss: 0.0816, Val Loss: 0.0745\n",
      "Epoch 38/300 - Train Loss: 0.0838, Val Loss: 0.0744\n",
      "Epoch 39/300 - Train Loss: 0.0825, Val Loss: 0.0812\n",
      "Epoch 40/300 - Train Loss: 0.0828, Val Loss: 0.0752\n",
      "Epoch 41/300 - Train Loss: 0.0813, Val Loss: 0.0743\n",
      "Epoch 42/300 - Train Loss: 0.0838, Val Loss: 0.0733\n",
      "Epoch 43/300 - Train Loss: 0.0826, Val Loss: 0.0774\n",
      "Epoch 44/300 - Train Loss: 0.0827, Val Loss: 0.0714\n",
      "Epoch 45/300 - Train Loss: 0.0829, Val Loss: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:18:32,484] Trial 69 finished with value: 0.9689711117426952 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2659674581437484, 'learning_rate': 0.0001849152936349048, 'batch_size': 32, 'weight_decay': 0.0012540730075501416}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300 - Train Loss: 0.0818, Val Loss: 0.0745\n",
      "Early stopping at epoch 46\n",
      "Macro F1 Score: 0.9690, Macro Precision: 0.9676, Macro Recall: 0.9705\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 71\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2706114439973995, LR=0.00016421705650016566, BS=32, WD=0.0011995132205481916\n",
      "Epoch 1/300 - Train Loss: 0.2060, Val Loss: 0.0897\n",
      "Epoch 2/300 - Train Loss: 0.1067, Val Loss: 0.0869\n",
      "Epoch 3/300 - Train Loss: 0.0977, Val Loss: 0.0824\n",
      "Epoch 4/300 - Train Loss: 0.0951, Val Loss: 0.0815\n",
      "Epoch 5/300 - Train Loss: 0.0914, Val Loss: 0.0682\n",
      "Epoch 6/300 - Train Loss: 0.0870, Val Loss: 0.0863\n",
      "Epoch 7/300 - Train Loss: 0.0901, Val Loss: 0.0769\n",
      "Epoch 8/300 - Train Loss: 0.0864, Val Loss: 0.0726\n",
      "Epoch 9/300 - Train Loss: 0.0885, Val Loss: 0.0769\n",
      "Epoch 10/300 - Train Loss: 0.0851, Val Loss: 0.0676\n",
      "Epoch 11/300 - Train Loss: 0.0843, Val Loss: 0.0755\n",
      "Epoch 12/300 - Train Loss: 0.0840, Val Loss: 0.0718\n",
      "Epoch 13/300 - Train Loss: 0.0832, Val Loss: 0.0708\n",
      "Epoch 14/300 - Train Loss: 0.0838, Val Loss: 0.0687\n",
      "Epoch 15/300 - Train Loss: 0.0851, Val Loss: 0.0730\n",
      "Epoch 16/300 - Train Loss: 0.0825, Val Loss: 0.0658\n",
      "Epoch 17/300 - Train Loss: 0.0840, Val Loss: 0.0675\n",
      "Epoch 18/300 - Train Loss: 0.0842, Val Loss: 0.0712\n",
      "Epoch 19/300 - Train Loss: 0.0834, Val Loss: 0.0681\n",
      "Epoch 20/300 - Train Loss: 0.0843, Val Loss: 0.0678\n",
      "Epoch 21/300 - Train Loss: 0.0867, Val Loss: 0.0718\n",
      "Epoch 22/300 - Train Loss: 0.0865, Val Loss: 0.0697\n",
      "Epoch 23/300 - Train Loss: 0.0832, Val Loss: 0.0731\n",
      "Epoch 24/300 - Train Loss: 0.0816, Val Loss: 0.0698\n",
      "Epoch 25/300 - Train Loss: 0.0829, Val Loss: 0.0716\n",
      "Epoch 26/300 - Train Loss: 0.0813, Val Loss: 0.0678\n",
      "Epoch 27/300 - Train Loss: 0.0813, Val Loss: 0.0695\n",
      "Epoch 28/300 - Train Loss: 0.0818, Val Loss: 0.0739\n",
      "Epoch 29/300 - Train Loss: 0.0821, Val Loss: 0.0714\n",
      "Epoch 30/300 - Train Loss: 0.0828, Val Loss: 0.0687\n",
      "Epoch 31/300 - Train Loss: 0.0811, Val Loss: 0.0705\n",
      "Epoch 32/300 - Train Loss: 0.0832, Val Loss: 0.0723\n",
      "Epoch 33/300 - Train Loss: 0.0811, Val Loss: 0.0712\n",
      "Epoch 34/300 - Train Loss: 0.0801, Val Loss: 0.0764\n",
      "Epoch 35/300 - Train Loss: 0.0821, Val Loss: 0.0723\n",
      "Epoch 36/300 - Train Loss: 0.0805, Val Loss: 0.0676\n",
      "Epoch 37/300 - Train Loss: 0.0806, Val Loss: 0.0732\n",
      "Epoch 38/300 - Train Loss: 0.0794, Val Loss: 0.0698\n",
      "Epoch 39/300 - Train Loss: 0.0817, Val Loss: 0.0712\n",
      "Epoch 40/300 - Train Loss: 0.0809, Val Loss: 0.0725\n",
      "Epoch 41/300 - Train Loss: 0.0823, Val Loss: 0.0720\n",
      "Epoch 42/300 - Train Loss: 0.0808, Val Loss: 0.0734\n",
      "Epoch 43/300 - Train Loss: 0.0815, Val Loss: 0.0711\n",
      "Epoch 44/300 - Train Loss: 0.0821, Val Loss: 0.0715\n",
      "Epoch 45/300 - Train Loss: 0.0814, Val Loss: 0.0725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:21:29,617] Trial 70 finished with value: 0.9714217443866251 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2706114439973995, 'learning_rate': 0.00016421705650016566, 'batch_size': 32, 'weight_decay': 0.0011995132205481916}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300 - Train Loss: 0.0811, Val Loss: 0.0665\n",
      "Early stopping at epoch 46\n",
      "Macro F1 Score: 0.9714, Macro Precision: 0.9721, Macro Recall: 0.9708\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 72\n",
      "Training with F1=16, F2=32, D=8, dropout=0.259397979151647, LR=0.0001588026169855008, BS=32, WD=0.0014319074405950724\n",
      "Epoch 1/300 - Train Loss: 0.2276, Val Loss: 0.0964\n",
      "Epoch 2/300 - Train Loss: 0.1136, Val Loss: 0.0872\n",
      "Epoch 3/300 - Train Loss: 0.1047, Val Loss: 0.0790\n",
      "Epoch 4/300 - Train Loss: 0.0959, Val Loss: 0.0801\n",
      "Epoch 5/300 - Train Loss: 0.0935, Val Loss: 0.0747\n",
      "Epoch 6/300 - Train Loss: 0.0902, Val Loss: 0.0740\n",
      "Epoch 7/300 - Train Loss: 0.0874, Val Loss: 0.0723\n",
      "Epoch 8/300 - Train Loss: 0.0862, Val Loss: 0.0747\n",
      "Epoch 9/300 - Train Loss: 0.0884, Val Loss: 0.0719\n",
      "Epoch 10/300 - Train Loss: 0.0843, Val Loss: 0.0688\n",
      "Epoch 11/300 - Train Loss: 0.0866, Val Loss: 0.0735\n",
      "Epoch 12/300 - Train Loss: 0.0822, Val Loss: 0.0677\n",
      "Epoch 13/300 - Train Loss: 0.0844, Val Loss: 0.0702\n",
      "Epoch 14/300 - Train Loss: 0.0836, Val Loss: 0.0757\n",
      "Epoch 15/300 - Train Loss: 0.0834, Val Loss: 0.0691\n",
      "Epoch 16/300 - Train Loss: 0.0837, Val Loss: 0.0725\n",
      "Epoch 17/300 - Train Loss: 0.0853, Val Loss: 0.0762\n",
      "Epoch 18/300 - Train Loss: 0.0820, Val Loss: 0.0736\n",
      "Epoch 19/300 - Train Loss: 0.0828, Val Loss: 0.0734\n",
      "Epoch 20/300 - Train Loss: 0.0835, Val Loss: 0.0720\n",
      "Epoch 21/300 - Train Loss: 0.0835, Val Loss: 0.0720\n",
      "Epoch 22/300 - Train Loss: 0.0842, Val Loss: 0.0750\n",
      "Epoch 23/300 - Train Loss: 0.0853, Val Loss: 0.0685\n",
      "Epoch 24/300 - Train Loss: 0.0811, Val Loss: 0.0717\n",
      "Epoch 25/300 - Train Loss: 0.0824, Val Loss: 0.0785\n",
      "Epoch 26/300 - Train Loss: 0.0853, Val Loss: 0.0693\n",
      "Epoch 27/300 - Train Loss: 0.0825, Val Loss: 0.0687\n",
      "Epoch 28/300 - Train Loss: 0.0836, Val Loss: 0.0696\n",
      "Epoch 29/300 - Train Loss: 0.0828, Val Loss: 0.0673\n",
      "Epoch 30/300 - Train Loss: 0.0850, Val Loss: 0.0682\n",
      "Epoch 31/300 - Train Loss: 0.0835, Val Loss: 0.0720\n",
      "Epoch 32/300 - Train Loss: 0.0839, Val Loss: 0.0715\n",
      "Epoch 33/300 - Train Loss: 0.0821, Val Loss: 0.0755\n",
      "Epoch 34/300 - Train Loss: 0.0821, Val Loss: 0.0692\n",
      "Epoch 35/300 - Train Loss: 0.0825, Val Loss: 0.0773\n",
      "Epoch 36/300 - Train Loss: 0.0827, Val Loss: 0.0719\n",
      "Epoch 37/300 - Train Loss: 0.0860, Val Loss: 0.0693\n",
      "Epoch 38/300 - Train Loss: 0.0862, Val Loss: 0.0777\n",
      "Epoch 39/300 - Train Loss: 0.0821, Val Loss: 0.0708\n",
      "Epoch 40/300 - Train Loss: 0.0834, Val Loss: 0.0746\n",
      "Epoch 41/300 - Train Loss: 0.0812, Val Loss: 0.0684\n",
      "Epoch 42/300 - Train Loss: 0.0813, Val Loss: 0.0699\n",
      "Epoch 43/300 - Train Loss: 0.0855, Val Loss: 0.0663\n",
      "Epoch 44/300 - Train Loss: 0.0839, Val Loss: 0.0670\n",
      "Epoch 45/300 - Train Loss: 0.0833, Val Loss: 0.0720\n",
      "Epoch 46/300 - Train Loss: 0.0840, Val Loss: 0.0651\n",
      "Epoch 47/300 - Train Loss: 0.0842, Val Loss: 0.0758\n",
      "Epoch 48/300 - Train Loss: 0.0848, Val Loss: 0.0685\n",
      "Epoch 49/300 - Train Loss: 0.0825, Val Loss: 0.0688\n",
      "Epoch 50/300 - Train Loss: 0.0824, Val Loss: 0.0818\n",
      "Epoch 51/300 - Train Loss: 0.0846, Val Loss: 0.0686\n",
      "Epoch 52/300 - Train Loss: 0.0834, Val Loss: 0.0704\n",
      "Epoch 53/300 - Train Loss: 0.0861, Val Loss: 0.0739\n",
      "Epoch 54/300 - Train Loss: 0.0859, Val Loss: 0.0690\n",
      "Epoch 55/300 - Train Loss: 0.0844, Val Loss: 0.0663\n",
      "Epoch 56/300 - Train Loss: 0.0838, Val Loss: 0.0782\n",
      "Epoch 57/300 - Train Loss: 0.0831, Val Loss: 0.0740\n",
      "Epoch 58/300 - Train Loss: 0.0840, Val Loss: 0.0805\n",
      "Epoch 59/300 - Train Loss: 0.0834, Val Loss: 0.0671\n",
      "Epoch 60/300 - Train Loss: 0.0830, Val Loss: 0.0653\n",
      "Epoch 61/300 - Train Loss: 0.0835, Val Loss: 0.0686\n",
      "Epoch 62/300 - Train Loss: 0.0840, Val Loss: 0.0832\n",
      "Epoch 63/300 - Train Loss: 0.0863, Val Loss: 0.0800\n",
      "Epoch 64/300 - Train Loss: 0.0839, Val Loss: 0.0654\n",
      "Epoch 65/300 - Train Loss: 0.0849, Val Loss: 0.0697\n",
      "Epoch 66/300 - Train Loss: 0.0851, Val Loss: 0.0749\n",
      "Epoch 67/300 - Train Loss: 0.0860, Val Loss: 0.0671\n",
      "Epoch 68/300 - Train Loss: 0.0837, Val Loss: 0.0664\n",
      "Epoch 69/300 - Train Loss: 0.0854, Val Loss: 0.0672\n",
      "Epoch 70/300 - Train Loss: 0.0831, Val Loss: 0.0677\n",
      "Epoch 71/300 - Train Loss: 0.0828, Val Loss: 0.0694\n",
      "Epoch 72/300 - Train Loss: 0.0837, Val Loss: 0.0739\n",
      "Epoch 73/300 - Train Loss: 0.0844, Val Loss: 0.0678\n",
      "Epoch 74/300 - Train Loss: 0.0872, Val Loss: 0.0707\n",
      "Epoch 75/300 - Train Loss: 0.0829, Val Loss: 0.0756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:26:21,762] Trial 71 finished with value: 0.9685424859463718 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.259397979151647, 'learning_rate': 0.0001588026169855008, 'batch_size': 32, 'weight_decay': 0.0014319074405950724}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300 - Train Loss: 0.0840, Val Loss: 0.0665\n",
      "Early stopping at epoch 76\n",
      "Macro F1 Score: 0.9685, Macro Precision: 0.9675, Macro Recall: 0.9698\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 73\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2784970033182508, LR=0.00014171117747229604, BS=32, WD=0.001174851365107639\n",
      "Epoch 1/300 - Train Loss: 0.2034, Val Loss: 0.1178\n",
      "Epoch 2/300 - Train Loss: 0.1079, Val Loss: 0.0777\n",
      "Epoch 3/300 - Train Loss: 0.0983, Val Loss: 0.0830\n",
      "Epoch 4/300 - Train Loss: 0.0962, Val Loss: 0.0730\n",
      "Epoch 5/300 - Train Loss: 0.0935, Val Loss: 0.0699\n",
      "Epoch 6/300 - Train Loss: 0.0905, Val Loss: 0.0741\n",
      "Epoch 7/300 - Train Loss: 0.0889, Val Loss: 0.0757\n",
      "Epoch 8/300 - Train Loss: 0.0878, Val Loss: 0.0745\n",
      "Epoch 9/300 - Train Loss: 0.0856, Val Loss: 0.0685\n",
      "Epoch 10/300 - Train Loss: 0.0861, Val Loss: 0.0694\n",
      "Epoch 11/300 - Train Loss: 0.0888, Val Loss: 0.0674\n",
      "Epoch 12/300 - Train Loss: 0.0862, Val Loss: 0.0693\n",
      "Epoch 13/300 - Train Loss: 0.0860, Val Loss: 0.0741\n",
      "Epoch 14/300 - Train Loss: 0.0853, Val Loss: 0.0712\n",
      "Epoch 15/300 - Train Loss: 0.0834, Val Loss: 0.0662\n",
      "Epoch 16/300 - Train Loss: 0.0837, Val Loss: 0.0702\n",
      "Epoch 17/300 - Train Loss: 0.0832, Val Loss: 0.0671\n",
      "Epoch 18/300 - Train Loss: 0.0835, Val Loss: 0.0693\n",
      "Epoch 19/300 - Train Loss: 0.0830, Val Loss: 0.0710\n",
      "Epoch 20/300 - Train Loss: 0.0869, Val Loss: 0.0714\n",
      "Epoch 21/300 - Train Loss: 0.0845, Val Loss: 0.0679\n",
      "Epoch 22/300 - Train Loss: 0.0820, Val Loss: 0.0661\n",
      "Epoch 23/300 - Train Loss: 0.0836, Val Loss: 0.0722\n",
      "Epoch 24/300 - Train Loss: 0.0815, Val Loss: 0.0670\n",
      "Epoch 25/300 - Train Loss: 0.0833, Val Loss: 0.0693\n",
      "Epoch 26/300 - Train Loss: 0.0806, Val Loss: 0.0711\n",
      "Epoch 27/300 - Train Loss: 0.0819, Val Loss: 0.0703\n",
      "Epoch 28/300 - Train Loss: 0.0829, Val Loss: 0.0671\n",
      "Epoch 29/300 - Train Loss: 0.0806, Val Loss: 0.0738\n",
      "Epoch 30/300 - Train Loss: 0.0812, Val Loss: 0.0652\n",
      "Epoch 31/300 - Train Loss: 0.0804, Val Loss: 0.0683\n",
      "Epoch 32/300 - Train Loss: 0.0839, Val Loss: 0.0689\n",
      "Epoch 33/300 - Train Loss: 0.0818, Val Loss: 0.0692\n",
      "Epoch 34/300 - Train Loss: 0.0824, Val Loss: 0.0687\n",
      "Epoch 35/300 - Train Loss: 0.0820, Val Loss: 0.0680\n",
      "Epoch 36/300 - Train Loss: 0.0813, Val Loss: 0.0686\n",
      "Epoch 37/300 - Train Loss: 0.0800, Val Loss: 0.0729\n",
      "Epoch 38/300 - Train Loss: 0.0793, Val Loss: 0.0693\n",
      "Epoch 39/300 - Train Loss: 0.0802, Val Loss: 0.0691\n",
      "Epoch 40/300 - Train Loss: 0.0811, Val Loss: 0.0768\n",
      "Epoch 41/300 - Train Loss: 0.0800, Val Loss: 0.0673\n",
      "Epoch 42/300 - Train Loss: 0.0799, Val Loss: 0.0729\n",
      "Epoch 43/300 - Train Loss: 0.0824, Val Loss: 0.0807\n",
      "Epoch 44/300 - Train Loss: 0.0805, Val Loss: 0.0680\n",
      "Epoch 45/300 - Train Loss: 0.0834, Val Loss: 0.0688\n",
      "Epoch 46/300 - Train Loss: 0.0827, Val Loss: 0.0709\n",
      "Epoch 47/300 - Train Loss: 0.0828, Val Loss: 0.0699\n",
      "Epoch 48/300 - Train Loss: 0.0814, Val Loss: 0.0690\n",
      "Epoch 49/300 - Train Loss: 0.0805, Val Loss: 0.0653\n",
      "Epoch 50/300 - Train Loss: 0.0808, Val Loss: 0.0693\n",
      "Epoch 51/300 - Train Loss: 0.0805, Val Loss: 0.0660\n",
      "Epoch 52/300 - Train Loss: 0.0811, Val Loss: 0.0739\n",
      "Epoch 53/300 - Train Loss: 0.0808, Val Loss: 0.0715\n",
      "Epoch 54/300 - Train Loss: 0.0802, Val Loss: 0.0845\n",
      "Epoch 55/300 - Train Loss: 0.0790, Val Loss: 0.0700\n",
      "Epoch 56/300 - Train Loss: 0.0817, Val Loss: 0.0716\n",
      "Epoch 57/300 - Train Loss: 0.0815, Val Loss: 0.0716\n",
      "Epoch 58/300 - Train Loss: 0.0804, Val Loss: 0.0679\n",
      "Epoch 59/300 - Train Loss: 0.0793, Val Loss: 0.0699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:30:12,255] Trial 72 finished with value: 0.9628222456440403 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2784970033182508, 'learning_rate': 0.00014171117747229604, 'batch_size': 32, 'weight_decay': 0.001174851365107639}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300 - Train Loss: 0.0798, Val Loss: 0.0762\n",
      "Early stopping at epoch 60\n",
      "Macro F1 Score: 0.9628, Macro Precision: 0.9551, Macro Recall: 0.9712\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 74\n",
      "Training with F1=16, F2=32, D=8, dropout=0.20001883191133177, LR=0.0001692449040259305, BS=32, WD=0.002721150637805172\n",
      "Epoch 1/300 - Train Loss: 0.2039, Val Loss: 0.0840\n",
      "Epoch 2/300 - Train Loss: 0.1054, Val Loss: 0.0926\n",
      "Epoch 3/300 - Train Loss: 0.0980, Val Loss: 0.0708\n",
      "Epoch 4/300 - Train Loss: 0.0927, Val Loss: 0.0758\n",
      "Epoch 5/300 - Train Loss: 0.0929, Val Loss: 0.0716\n",
      "Epoch 6/300 - Train Loss: 0.0907, Val Loss: 0.0714\n",
      "Epoch 7/300 - Train Loss: 0.0925, Val Loss: 0.0764\n",
      "Epoch 8/300 - Train Loss: 0.0925, Val Loss: 0.0690\n",
      "Epoch 9/300 - Train Loss: 0.0897, Val Loss: 0.0762\n",
      "Epoch 10/300 - Train Loss: 0.0931, Val Loss: 0.0639\n",
      "Epoch 11/300 - Train Loss: 0.0918, Val Loss: 0.0771\n",
      "Epoch 12/300 - Train Loss: 0.0887, Val Loss: 0.0703\n",
      "Epoch 13/300 - Train Loss: 0.0908, Val Loss: 0.0714\n",
      "Epoch 14/300 - Train Loss: 0.0890, Val Loss: 0.0738\n",
      "Epoch 15/300 - Train Loss: 0.0901, Val Loss: 0.0680\n",
      "Epoch 16/300 - Train Loss: 0.0902, Val Loss: 0.0716\n",
      "Epoch 17/300 - Train Loss: 0.0891, Val Loss: 0.0693\n",
      "Epoch 18/300 - Train Loss: 0.0936, Val Loss: 0.0700\n",
      "Epoch 19/300 - Train Loss: 0.0897, Val Loss: 0.0762\n",
      "Epoch 20/300 - Train Loss: 0.0896, Val Loss: 0.0777\n",
      "Epoch 21/300 - Train Loss: 0.0924, Val Loss: 0.0724\n",
      "Epoch 22/300 - Train Loss: 0.0881, Val Loss: 0.0705\n",
      "Epoch 23/300 - Train Loss: 0.0911, Val Loss: 0.0750\n",
      "Epoch 24/300 - Train Loss: 0.0898, Val Loss: 0.0710\n",
      "Epoch 25/300 - Train Loss: 0.0921, Val Loss: 0.0686\n",
      "Epoch 26/300 - Train Loss: 0.0885, Val Loss: 0.0748\n",
      "Epoch 27/300 - Train Loss: 0.0926, Val Loss: 0.0741\n",
      "Epoch 28/300 - Train Loss: 0.0901, Val Loss: 0.0782\n",
      "Epoch 29/300 - Train Loss: 0.0894, Val Loss: 0.0833\n",
      "Epoch 30/300 - Train Loss: 0.0910, Val Loss: 0.0719\n",
      "Epoch 31/300 - Train Loss: 0.0921, Val Loss: 0.0691\n",
      "Epoch 32/300 - Train Loss: 0.0893, Val Loss: 0.0734\n",
      "Epoch 33/300 - Train Loss: 0.0881, Val Loss: 0.0733\n",
      "Epoch 34/300 - Train Loss: 0.0901, Val Loss: 0.0723\n",
      "Epoch 35/300 - Train Loss: 0.0891, Val Loss: 0.0716\n",
      "Epoch 36/300 - Train Loss: 0.0919, Val Loss: 0.0698\n",
      "Epoch 37/300 - Train Loss: 0.0920, Val Loss: 0.0736\n",
      "Epoch 38/300 - Train Loss: 0.0926, Val Loss: 0.0741\n",
      "Epoch 39/300 - Train Loss: 0.0895, Val Loss: 0.0832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:32:45,804] Trial 73 finished with value: 0.9733158063386526 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.20001883191133177, 'learning_rate': 0.0001692449040259305, 'batch_size': 32, 'weight_decay': 0.002721150637805172}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300 - Train Loss: 0.0927, Val Loss: 0.0760\n",
      "Early stopping at epoch 40\n",
      "Macro F1 Score: 0.9733, Macro Precision: 0.9742, Macro Recall: 0.9725\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 75\n",
      "Training with F1=16, F2=32, D=8, dropout=0.19360223092515666, LR=9.373104390151068e-05, BS=32, WD=0.0042911072202606\n",
      "Epoch 1/300 - Train Loss: 0.2425, Val Loss: 0.1212\n",
      "Epoch 2/300 - Train Loss: 0.1121, Val Loss: 0.0879\n",
      "Epoch 3/300 - Train Loss: 0.1053, Val Loss: 0.0911\n",
      "Epoch 4/300 - Train Loss: 0.1041, Val Loss: 0.0856\n",
      "Epoch 5/300 - Train Loss: 0.0996, Val Loss: 0.0883\n",
      "Epoch 6/300 - Train Loss: 0.0972, Val Loss: 0.0840\n",
      "Epoch 7/300 - Train Loss: 0.0964, Val Loss: 0.0793\n",
      "Epoch 8/300 - Train Loss: 0.0939, Val Loss: 0.0733\n",
      "Epoch 9/300 - Train Loss: 0.0922, Val Loss: 0.0746\n",
      "Epoch 10/300 - Train Loss: 0.0900, Val Loss: 0.0700\n",
      "Epoch 11/300 - Train Loss: 0.0911, Val Loss: 0.0756\n",
      "Epoch 12/300 - Train Loss: 0.0888, Val Loss: 0.0739\n",
      "Epoch 13/300 - Train Loss: 0.0897, Val Loss: 0.0712\n",
      "Epoch 14/300 - Train Loss: 0.0929, Val Loss: 0.0739\n",
      "Epoch 15/300 - Train Loss: 0.0903, Val Loss: 0.0746\n",
      "Epoch 16/300 - Train Loss: 0.0886, Val Loss: 0.0761\n",
      "Epoch 17/300 - Train Loss: 0.0902, Val Loss: 0.0745\n",
      "Epoch 18/300 - Train Loss: 0.0901, Val Loss: 0.0714\n",
      "Epoch 19/300 - Train Loss: 0.0911, Val Loss: 0.0717\n",
      "Epoch 20/300 - Train Loss: 0.0915, Val Loss: 0.0759\n",
      "Epoch 21/300 - Train Loss: 0.0934, Val Loss: 0.0752\n",
      "Epoch 22/300 - Train Loss: 0.0900, Val Loss: 0.0725\n",
      "Epoch 23/300 - Train Loss: 0.0926, Val Loss: 0.0779\n",
      "Epoch 24/300 - Train Loss: 0.0925, Val Loss: 0.0725\n",
      "Epoch 25/300 - Train Loss: 0.0917, Val Loss: 0.0731\n",
      "Epoch 26/300 - Train Loss: 0.0933, Val Loss: 0.0705\n",
      "Epoch 27/300 - Train Loss: 0.0905, Val Loss: 0.0781\n",
      "Epoch 28/300 - Train Loss: 0.0932, Val Loss: 0.0719\n",
      "Epoch 29/300 - Train Loss: 0.0922, Val Loss: 0.0716\n",
      "Epoch 30/300 - Train Loss: 0.0898, Val Loss: 0.0778\n",
      "Epoch 31/300 - Train Loss: 0.0913, Val Loss: 0.0772\n",
      "Epoch 32/300 - Train Loss: 0.0912, Val Loss: 0.0687\n",
      "Epoch 33/300 - Train Loss: 0.0921, Val Loss: 0.0709\n",
      "Epoch 34/300 - Train Loss: 0.0933, Val Loss: 0.0753\n",
      "Epoch 35/300 - Train Loss: 0.0929, Val Loss: 0.0707\n",
      "Epoch 36/300 - Train Loss: 0.0929, Val Loss: 0.0757\n",
      "Epoch 37/300 - Train Loss: 0.0912, Val Loss: 0.0724\n",
      "Epoch 38/300 - Train Loss: 0.0933, Val Loss: 0.0719\n",
      "Epoch 39/300 - Train Loss: 0.0936, Val Loss: 0.0726\n",
      "Epoch 40/300 - Train Loss: 0.0931, Val Loss: 0.0776\n",
      "Epoch 41/300 - Train Loss: 0.0919, Val Loss: 0.0749\n",
      "Epoch 42/300 - Train Loss: 0.0927, Val Loss: 0.0719\n",
      "Epoch 43/300 - Train Loss: 0.0927, Val Loss: 0.0707\n",
      "Epoch 44/300 - Train Loss: 0.0932, Val Loss: 0.0743\n",
      "Epoch 45/300 - Train Loss: 0.0960, Val Loss: 0.0779\n",
      "Epoch 46/300 - Train Loss: 0.0944, Val Loss: 0.0749\n",
      "Epoch 47/300 - Train Loss: 0.0955, Val Loss: 0.0732\n",
      "Epoch 48/300 - Train Loss: 0.0932, Val Loss: 0.0730\n",
      "Epoch 49/300 - Train Loss: 0.0922, Val Loss: 0.0739\n",
      "Epoch 50/300 - Train Loss: 0.0976, Val Loss: 0.0765\n",
      "Epoch 51/300 - Train Loss: 0.0941, Val Loss: 0.0729\n",
      "Epoch 52/300 - Train Loss: 0.0922, Val Loss: 0.0751\n",
      "Epoch 53/300 - Train Loss: 0.0971, Val Loss: 0.0745\n",
      "Epoch 54/300 - Train Loss: 0.0949, Val Loss: 0.0697\n",
      "Epoch 55/300 - Train Loss: 0.0953, Val Loss: 0.0856\n",
      "Epoch 56/300 - Train Loss: 0.0954, Val Loss: 0.0756\n",
      "Epoch 57/300 - Train Loss: 0.0943, Val Loss: 0.0746\n",
      "Epoch 58/300 - Train Loss: 0.0938, Val Loss: 0.0791\n",
      "Epoch 59/300 - Train Loss: 0.0947, Val Loss: 0.0723\n",
      "Epoch 60/300 - Train Loss: 0.0938, Val Loss: 0.0754\n",
      "Epoch 61/300 - Train Loss: 0.0959, Val Loss: 0.0696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:36:43,820] Trial 74 finished with value: 0.9643881227100843 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.19360223092515666, 'learning_rate': 9.373104390151068e-05, 'batch_size': 32, 'weight_decay': 0.0042911072202606}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/300 - Train Loss: 0.0953, Val Loss: 0.0805\n",
      "Early stopping at epoch 62\n",
      "Macro F1 Score: 0.9644, Macro Precision: 0.9656, Macro Recall: 0.9634\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 76\n",
      "Training with F1=16, F2=32, D=8, dropout=0.20699566937880448, LR=0.00017553380838274932, BS=32, WD=0.002423743156914283\n",
      "Epoch 1/300 - Train Loss: 0.1941, Val Loss: 0.0837\n",
      "Epoch 2/300 - Train Loss: 0.1046, Val Loss: 0.1105\n",
      "Epoch 3/300 - Train Loss: 0.1015, Val Loss: 0.0747\n",
      "Epoch 4/300 - Train Loss: 0.0925, Val Loss: 0.0750\n",
      "Epoch 5/300 - Train Loss: 0.0936, Val Loss: 0.0734\n",
      "Epoch 6/300 - Train Loss: 0.0921, Val Loss: 0.0793\n",
      "Epoch 7/300 - Train Loss: 0.0893, Val Loss: 0.0706\n",
      "Epoch 8/300 - Train Loss: 0.0899, Val Loss: 0.0700\n",
      "Epoch 9/300 - Train Loss: 0.0876, Val Loss: 0.0666\n",
      "Epoch 10/300 - Train Loss: 0.0876, Val Loss: 0.0745\n",
      "Epoch 11/300 - Train Loss: 0.0889, Val Loss: 0.0735\n",
      "Epoch 12/300 - Train Loss: 0.0887, Val Loss: 0.0687\n",
      "Epoch 13/300 - Train Loss: 0.0875, Val Loss: 0.0690\n",
      "Epoch 14/300 - Train Loss: 0.0872, Val Loss: 0.0696\n",
      "Epoch 15/300 - Train Loss: 0.0877, Val Loss: 0.0720\n",
      "Epoch 16/300 - Train Loss: 0.0897, Val Loss: 0.0684\n",
      "Epoch 17/300 - Train Loss: 0.0892, Val Loss: 0.0685\n",
      "Epoch 18/300 - Train Loss: 0.0876, Val Loss: 0.0683\n",
      "Epoch 19/300 - Train Loss: 0.0893, Val Loss: 0.0795\n",
      "Epoch 20/300 - Train Loss: 0.0893, Val Loss: 0.0701\n",
      "Epoch 21/300 - Train Loss: 0.0873, Val Loss: 0.0660\n",
      "Epoch 22/300 - Train Loss: 0.0888, Val Loss: 0.0742\n",
      "Epoch 23/300 - Train Loss: 0.0915, Val Loss: 0.0750\n",
      "Epoch 24/300 - Train Loss: 0.0893, Val Loss: 0.0681\n",
      "Epoch 25/300 - Train Loss: 0.0884, Val Loss: 0.0678\n",
      "Epoch 26/300 - Train Loss: 0.0906, Val Loss: 0.0753\n",
      "Epoch 27/300 - Train Loss: 0.0908, Val Loss: 0.0821\n",
      "Epoch 28/300 - Train Loss: 0.0904, Val Loss: 0.0663\n",
      "Epoch 29/300 - Train Loss: 0.0902, Val Loss: 0.0679\n",
      "Epoch 30/300 - Train Loss: 0.0909, Val Loss: 0.0692\n",
      "Epoch 31/300 - Train Loss: 0.0880, Val Loss: 0.0683\n",
      "Epoch 32/300 - Train Loss: 0.0877, Val Loss: 0.0721\n",
      "Epoch 33/300 - Train Loss: 0.0897, Val Loss: 0.0703\n",
      "Epoch 34/300 - Train Loss: 0.0900, Val Loss: 0.0701\n",
      "Epoch 35/300 - Train Loss: 0.0921, Val Loss: 0.1138\n",
      "Epoch 36/300 - Train Loss: 0.0909, Val Loss: 0.0742\n",
      "Epoch 37/300 - Train Loss: 0.0899, Val Loss: 0.0724\n",
      "Epoch 38/300 - Train Loss: 0.0892, Val Loss: 0.0689\n",
      "Epoch 39/300 - Train Loss: 0.0898, Val Loss: 0.0931\n",
      "Epoch 40/300 - Train Loss: 0.0918, Val Loss: 0.0698\n",
      "Epoch 41/300 - Train Loss: 0.0897, Val Loss: 0.0666\n",
      "Epoch 42/300 - Train Loss: 0.0890, Val Loss: 0.0670\n",
      "Epoch 43/300 - Train Loss: 0.0899, Val Loss: 0.0684\n",
      "Epoch 44/300 - Train Loss: 0.0887, Val Loss: 0.0698\n",
      "Epoch 45/300 - Train Loss: 0.0900, Val Loss: 0.0733\n",
      "Epoch 46/300 - Train Loss: 0.0924, Val Loss: 0.0887\n",
      "Epoch 47/300 - Train Loss: 0.0896, Val Loss: 0.0725\n",
      "Epoch 48/300 - Train Loss: 0.0911, Val Loss: 0.0848\n",
      "Epoch 49/300 - Train Loss: 0.0883, Val Loss: 0.0698\n",
      "Epoch 50/300 - Train Loss: 0.0891, Val Loss: 0.0675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:39:59,641] Trial 75 finished with value: 0.964002155515811 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.20699566937880448, 'learning_rate': 0.00017553380838274932, 'batch_size': 32, 'weight_decay': 0.002423743156914283}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300 - Train Loss: 0.0909, Val Loss: 0.0734\n",
      "Early stopping at epoch 51\n",
      "Macro F1 Score: 0.9640, Macro Precision: 0.9585, Macro Recall: 0.9700\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 77\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2488095390630334, LR=0.00010661139930406617, BS=32, WD=0.008796719362130395\n",
      "Epoch 1/300 - Train Loss: 0.2651, Val Loss: 0.1679\n",
      "Epoch 2/300 - Train Loss: 0.1214, Val Loss: 0.0858\n",
      "Epoch 3/300 - Train Loss: 0.1085, Val Loss: 0.0810\n",
      "Epoch 4/300 - Train Loss: 0.1031, Val Loss: 0.0818\n",
      "Epoch 5/300 - Train Loss: 0.1009, Val Loss: 0.0736\n",
      "Epoch 6/300 - Train Loss: 0.1013, Val Loss: 0.0727\n",
      "Epoch 7/300 - Train Loss: 0.0980, Val Loss: 0.0741\n",
      "Epoch 8/300 - Train Loss: 0.0977, Val Loss: 0.0816\n",
      "Epoch 9/300 - Train Loss: 0.0998, Val Loss: 0.0743\n",
      "Epoch 10/300 - Train Loss: 0.0997, Val Loss: 0.0775\n",
      "Epoch 11/300 - Train Loss: 0.0986, Val Loss: 0.0755\n",
      "Epoch 12/300 - Train Loss: 0.0995, Val Loss: 0.0775\n",
      "Epoch 13/300 - Train Loss: 0.1033, Val Loss: 0.0734\n",
      "Epoch 14/300 - Train Loss: 0.1032, Val Loss: 0.0759\n",
      "Epoch 15/300 - Train Loss: 0.1031, Val Loss: 0.0731\n",
      "Epoch 16/300 - Train Loss: 0.1008, Val Loss: 0.0875\n",
      "Epoch 17/300 - Train Loss: 0.1018, Val Loss: 0.0753\n",
      "Epoch 18/300 - Train Loss: 0.1044, Val Loss: 0.0777\n",
      "Epoch 19/300 - Train Loss: 0.1022, Val Loss: 0.0774\n",
      "Epoch 20/300 - Train Loss: 0.1033, Val Loss: 0.0747\n",
      "Epoch 21/300 - Train Loss: 0.1061, Val Loss: 0.0772\n",
      "Epoch 22/300 - Train Loss: 0.1042, Val Loss: 0.0858\n",
      "Epoch 23/300 - Train Loss: 0.1040, Val Loss: 0.0719\n",
      "Epoch 24/300 - Train Loss: 0.1061, Val Loss: 0.0781\n",
      "Epoch 25/300 - Train Loss: 0.1053, Val Loss: 0.0807\n",
      "Epoch 26/300 - Train Loss: 0.1079, Val Loss: 0.0780\n",
      "Epoch 27/300 - Train Loss: 0.1085, Val Loss: 0.0737\n",
      "Epoch 28/300 - Train Loss: 0.1088, Val Loss: 0.0730\n",
      "Epoch 29/300 - Train Loss: 0.1066, Val Loss: 0.0758\n",
      "Epoch 30/300 - Train Loss: 0.1071, Val Loss: 0.0736\n",
      "Epoch 31/300 - Train Loss: 0.1078, Val Loss: 0.1065\n",
      "Epoch 32/300 - Train Loss: 0.1079, Val Loss: 0.0732\n",
      "Epoch 33/300 - Train Loss: 0.1086, Val Loss: 0.0817\n",
      "Epoch 34/300 - Train Loss: 0.1069, Val Loss: 0.0854\n",
      "Epoch 35/300 - Train Loss: 0.1053, Val Loss: 0.0790\n",
      "Epoch 36/300 - Train Loss: 0.1086, Val Loss: 0.0729\n",
      "Epoch 37/300 - Train Loss: 0.1078, Val Loss: 0.0867\n",
      "Epoch 38/300 - Train Loss: 0.1095, Val Loss: 0.0885\n",
      "Epoch 39/300 - Train Loss: 0.1077, Val Loss: 0.0746\n",
      "Epoch 40/300 - Train Loss: 0.1079, Val Loss: 0.0780\n",
      "Epoch 41/300 - Train Loss: 0.1064, Val Loss: 0.0747\n",
      "Epoch 42/300 - Train Loss: 0.1072, Val Loss: 0.0875\n",
      "Epoch 43/300 - Train Loss: 0.1098, Val Loss: 0.0783\n",
      "Epoch 44/300 - Train Loss: 0.1062, Val Loss: 0.0789\n",
      "Epoch 45/300 - Train Loss: 0.1108, Val Loss: 0.0757\n",
      "Epoch 46/300 - Train Loss: 0.1079, Val Loss: 0.0953\n",
      "Epoch 47/300 - Train Loss: 0.1075, Val Loss: 0.0746\n",
      "Epoch 48/300 - Train Loss: 0.1080, Val Loss: 0.0897\n",
      "Epoch 49/300 - Train Loss: 0.1102, Val Loss: 0.0758\n",
      "Epoch 50/300 - Train Loss: 0.1058, Val Loss: 0.0816\n",
      "Epoch 51/300 - Train Loss: 0.1060, Val Loss: 0.0758\n",
      "Epoch 52/300 - Train Loss: 0.1094, Val Loss: 0.0850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:43:23,107] Trial 76 finished with value: 0.9712418909770012 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2488095390630334, 'learning_rate': 0.00010661139930406617, 'batch_size': 32, 'weight_decay': 0.008796719362130395}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300 - Train Loss: 0.1104, Val Loss: 0.0939\n",
      "Early stopping at epoch 53\n",
      "Macro F1 Score: 0.9712, Macro Precision: 0.9730, Macro Recall: 0.9698\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       1.00      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 78\n",
      "Training with F1=16, F2=32, D=8, dropout=0.24670966909881226, LR=7.281938699395265e-05, BS=32, WD=0.00873200841375529\n",
      "Epoch 1/300 - Train Loss: 0.2696, Val Loss: 0.1102\n",
      "Epoch 2/300 - Train Loss: 0.1183, Val Loss: 0.1132\n",
      "Epoch 3/300 - Train Loss: 0.1045, Val Loss: 0.0774\n",
      "Epoch 4/300 - Train Loss: 0.1013, Val Loss: 0.0820\n",
      "Epoch 5/300 - Train Loss: 0.0977, Val Loss: 0.0756\n",
      "Epoch 6/300 - Train Loss: 0.0977, Val Loss: 0.0759\n",
      "Epoch 7/300 - Train Loss: 0.0999, Val Loss: 0.1064\n",
      "Epoch 8/300 - Train Loss: 0.0957, Val Loss: 0.0912\n",
      "Epoch 9/300 - Train Loss: 0.0936, Val Loss: 0.0793\n",
      "Epoch 10/300 - Train Loss: 0.0963, Val Loss: 0.0721\n",
      "Epoch 11/300 - Train Loss: 0.0973, Val Loss: 0.0725\n",
      "Epoch 12/300 - Train Loss: 0.0955, Val Loss: 0.0739\n",
      "Epoch 13/300 - Train Loss: 0.0967, Val Loss: 0.0711\n",
      "Epoch 14/300 - Train Loss: 0.0979, Val Loss: 0.0746\n",
      "Epoch 15/300 - Train Loss: 0.0969, Val Loss: 0.0813\n",
      "Epoch 16/300 - Train Loss: 0.0968, Val Loss: 0.0803\n",
      "Epoch 17/300 - Train Loss: 0.0987, Val Loss: 0.0748\n",
      "Epoch 18/300 - Train Loss: 0.0991, Val Loss: 0.0769\n",
      "Epoch 19/300 - Train Loss: 0.0983, Val Loss: 0.0740\n",
      "Epoch 20/300 - Train Loss: 0.0968, Val Loss: 0.0788\n",
      "Epoch 21/300 - Train Loss: 0.1032, Val Loss: 0.0769\n",
      "Epoch 22/300 - Train Loss: 0.0986, Val Loss: 0.0723\n",
      "Epoch 23/300 - Train Loss: 0.0970, Val Loss: 0.0774\n",
      "Epoch 24/300 - Train Loss: 0.0980, Val Loss: 0.0732\n",
      "Epoch 25/300 - Train Loss: 0.0996, Val Loss: 0.0769\n",
      "Epoch 26/300 - Train Loss: 0.0984, Val Loss: 0.0807\n",
      "Epoch 27/300 - Train Loss: 0.1000, Val Loss: 0.0730\n",
      "Epoch 28/300 - Train Loss: 0.1011, Val Loss: 0.0805\n",
      "Epoch 29/300 - Train Loss: 0.1027, Val Loss: 0.0775\n",
      "Epoch 30/300 - Train Loss: 0.1015, Val Loss: 0.0769\n",
      "Epoch 31/300 - Train Loss: 0.1012, Val Loss: 0.0733\n",
      "Epoch 32/300 - Train Loss: 0.1007, Val Loss: 0.0728\n",
      "Epoch 33/300 - Train Loss: 0.1001, Val Loss: 0.0764\n",
      "Epoch 34/300 - Train Loss: 0.1008, Val Loss: 0.0743\n",
      "Epoch 35/300 - Train Loss: 0.1057, Val Loss: 0.0711\n",
      "Epoch 36/300 - Train Loss: 0.1013, Val Loss: 0.0730\n",
      "Epoch 37/300 - Train Loss: 0.1009, Val Loss: 0.0732\n",
      "Epoch 38/300 - Train Loss: 0.1044, Val Loss: 0.0757\n",
      "Epoch 39/300 - Train Loss: 0.1020, Val Loss: 0.0770\n",
      "Epoch 40/300 - Train Loss: 0.1035, Val Loss: 0.0784\n",
      "Epoch 41/300 - Train Loss: 0.1026, Val Loss: 0.0737\n",
      "Epoch 42/300 - Train Loss: 0.1031, Val Loss: 0.0751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:46:08,226] Trial 77 finished with value: 0.9709922482280325 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.24670966909881226, 'learning_rate': 7.281938699395265e-05, 'batch_size': 32, 'weight_decay': 0.00873200841375529}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/300 - Train Loss: 0.1064, Val Loss: 0.0753\n",
      "Early stopping at epoch 43\n",
      "Macro F1 Score: 0.9710, Macro Precision: 0.9771, Macro Recall: 0.9652\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 79\n",
      "Training with F1=16, F2=32, D=8, dropout=0.17110825542979247, LR=6.896687517429105e-05, BS=32, WD=0.00840919212412602\n",
      "Epoch 1/300 - Train Loss: 0.2765, Val Loss: 0.1193\n",
      "Epoch 2/300 - Train Loss: 0.1195, Val Loss: 0.0886\n",
      "Epoch 3/300 - Train Loss: 0.1063, Val Loss: 0.0829\n",
      "Epoch 4/300 - Train Loss: 0.0993, Val Loss: 0.0906\n",
      "Epoch 5/300 - Train Loss: 0.0955, Val Loss: 0.0745\n",
      "Epoch 6/300 - Train Loss: 0.0974, Val Loss: 0.0785\n",
      "Epoch 7/300 - Train Loss: 0.0977, Val Loss: 0.0910\n",
      "Epoch 8/300 - Train Loss: 0.0956, Val Loss: 0.0775\n",
      "Epoch 9/300 - Train Loss: 0.0943, Val Loss: 0.0768\n",
      "Epoch 10/300 - Train Loss: 0.0946, Val Loss: 0.0751\n",
      "Epoch 11/300 - Train Loss: 0.0941, Val Loss: 0.0792\n",
      "Epoch 12/300 - Train Loss: 0.0931, Val Loss: 0.0718\n",
      "Epoch 13/300 - Train Loss: 0.0934, Val Loss: 0.0744\n",
      "Epoch 14/300 - Train Loss: 0.0933, Val Loss: 0.0756\n",
      "Epoch 15/300 - Train Loss: 0.0950, Val Loss: 0.0757\n",
      "Epoch 16/300 - Train Loss: 0.0960, Val Loss: 0.0734\n",
      "Epoch 17/300 - Train Loss: 0.0950, Val Loss: 0.0736\n",
      "Epoch 18/300 - Train Loss: 0.0964, Val Loss: 0.0725\n",
      "Epoch 19/300 - Train Loss: 0.0946, Val Loss: 0.0783\n",
      "Epoch 20/300 - Train Loss: 0.0969, Val Loss: 0.0736\n",
      "Epoch 21/300 - Train Loss: 0.0945, Val Loss: 0.0760\n",
      "Epoch 22/300 - Train Loss: 0.0988, Val Loss: 0.0749\n",
      "Epoch 23/300 - Train Loss: 0.0976, Val Loss: 0.0712\n",
      "Epoch 24/300 - Train Loss: 0.0942, Val Loss: 0.0735\n",
      "Epoch 25/300 - Train Loss: 0.0980, Val Loss: 0.0862\n",
      "Epoch 26/300 - Train Loss: 0.0963, Val Loss: 0.0766\n",
      "Epoch 27/300 - Train Loss: 0.0972, Val Loss: 0.0774\n",
      "Epoch 28/300 - Train Loss: 0.0978, Val Loss: 0.0750\n",
      "Epoch 29/300 - Train Loss: 0.0985, Val Loss: 0.0808\n",
      "Epoch 30/300 - Train Loss: 0.0990, Val Loss: 0.0706\n",
      "Epoch 31/300 - Train Loss: 0.1022, Val Loss: 0.0875\n",
      "Epoch 32/300 - Train Loss: 0.0990, Val Loss: 0.0709\n",
      "Epoch 33/300 - Train Loss: 0.0961, Val Loss: 0.0764\n",
      "Epoch 34/300 - Train Loss: 0.0990, Val Loss: 0.0756\n",
      "Epoch 35/300 - Train Loss: 0.1005, Val Loss: 0.1038\n",
      "Epoch 36/300 - Train Loss: 0.0980, Val Loss: 0.0769\n",
      "Epoch 37/300 - Train Loss: 0.0983, Val Loss: 0.0764\n",
      "Epoch 38/300 - Train Loss: 0.1006, Val Loss: 0.0749\n",
      "Epoch 39/300 - Train Loss: 0.1013, Val Loss: 0.0732\n",
      "Epoch 40/300 - Train Loss: 0.0997, Val Loss: 0.0719\n",
      "Epoch 41/300 - Train Loss: 0.0998, Val Loss: 0.0734\n",
      "Epoch 42/300 - Train Loss: 0.0993, Val Loss: 0.0723\n",
      "Epoch 43/300 - Train Loss: 0.0996, Val Loss: 0.0740\n",
      "Epoch 44/300 - Train Loss: 0.1004, Val Loss: 0.0840\n",
      "Epoch 45/300 - Train Loss: 0.1005, Val Loss: 0.0735\n",
      "Epoch 46/300 - Train Loss: 0.1028, Val Loss: 0.0846\n",
      "Epoch 47/300 - Train Loss: 0.0995, Val Loss: 0.0747\n",
      "Epoch 48/300 - Train Loss: 0.0995, Val Loss: 0.0747\n",
      "Epoch 49/300 - Train Loss: 0.1013, Val Loss: 0.0741\n",
      "Epoch 50/300 - Train Loss: 0.1006, Val Loss: 0.0785\n",
      "Epoch 51/300 - Train Loss: 0.1007, Val Loss: 0.0790\n",
      "Epoch 52/300 - Train Loss: 0.1009, Val Loss: 0.0749\n",
      "Epoch 53/300 - Train Loss: 0.1033, Val Loss: 0.0743\n",
      "Epoch 54/300 - Train Loss: 0.1002, Val Loss: 0.0754\n",
      "Epoch 55/300 - Train Loss: 0.1003, Val Loss: 0.0728\n",
      "Epoch 56/300 - Train Loss: 0.1036, Val Loss: 0.0829\n",
      "Epoch 57/300 - Train Loss: 0.1032, Val Loss: 0.0772\n",
      "Epoch 58/300 - Train Loss: 0.1020, Val Loss: 0.0719\n",
      "Epoch 59/300 - Train Loss: 0.1019, Val Loss: 0.0705\n",
      "Epoch 60/300 - Train Loss: 0.0995, Val Loss: 0.0719\n",
      "Epoch 61/300 - Train Loss: 0.1006, Val Loss: 0.0731\n",
      "Epoch 62/300 - Train Loss: 0.1024, Val Loss: 0.0747\n",
      "Epoch 63/300 - Train Loss: 0.1023, Val Loss: 0.0796\n",
      "Epoch 64/300 - Train Loss: 0.1022, Val Loss: 0.0746\n",
      "Epoch 65/300 - Train Loss: 0.1000, Val Loss: 0.0794\n",
      "Epoch 66/300 - Train Loss: 0.1034, Val Loss: 0.0760\n",
      "Epoch 67/300 - Train Loss: 0.0999, Val Loss: 0.0730\n",
      "Epoch 68/300 - Train Loss: 0.1033, Val Loss: 0.0744\n",
      "Epoch 69/300 - Train Loss: 0.1022, Val Loss: 0.0807\n",
      "Epoch 70/300 - Train Loss: 0.1019, Val Loss: 0.0780\n",
      "Epoch 71/300 - Train Loss: 0.1017, Val Loss: 0.0771\n",
      "Epoch 72/300 - Train Loss: 0.0972, Val Loss: 0.0708\n",
      "Epoch 73/300 - Train Loss: 0.1012, Val Loss: 0.0780\n",
      "Epoch 74/300 - Train Loss: 0.1015, Val Loss: 0.0735\n",
      "Epoch 75/300 - Train Loss: 0.1019, Val Loss: 0.0847\n",
      "Epoch 76/300 - Train Loss: 0.1021, Val Loss: 0.0794\n",
      "Epoch 77/300 - Train Loss: 0.1011, Val Loss: 0.0917\n",
      "Epoch 78/300 - Train Loss: 0.1039, Val Loss: 0.0894\n",
      "Epoch 79/300 - Train Loss: 0.1024, Val Loss: 0.0747\n",
      "Epoch 80/300 - Train Loss: 0.1024, Val Loss: 0.0734\n",
      "Epoch 81/300 - Train Loss: 0.1054, Val Loss: 0.1006\n",
      "Epoch 82/300 - Train Loss: 0.1021, Val Loss: 0.0787\n",
      "Epoch 83/300 - Train Loss: 0.1037, Val Loss: 0.0836\n",
      "Epoch 84/300 - Train Loss: 0.1015, Val Loss: 0.0732\n",
      "Epoch 85/300 - Train Loss: 0.1029, Val Loss: 0.0833\n",
      "Epoch 86/300 - Train Loss: 0.1005, Val Loss: 0.0906\n",
      "Epoch 87/300 - Train Loss: 0.1004, Val Loss: 0.0812\n",
      "Epoch 88/300 - Train Loss: 0.1034, Val Loss: 0.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:51:50,097] Trial 78 finished with value: 0.9676662128072785 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.17110825542979247, 'learning_rate': 6.896687517429105e-05, 'batch_size': 32, 'weight_decay': 0.00840919212412602}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/300 - Train Loss: 0.1034, Val Loss: 0.0768\n",
      "Early stopping at epoch 89\n",
      "Macro F1 Score: 0.9677, Macro Precision: 0.9713, Macro Recall: 0.9642\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 80\n",
      "Training with F1=16, F2=32, D=8, dropout=0.24930477510994953, LR=5.659736656823643e-05, BS=32, WD=0.006009221420235837\n",
      "Epoch 1/300 - Train Loss: 0.3290, Val Loss: 0.1533\n",
      "Epoch 2/300 - Train Loss: 0.1439, Val Loss: 0.0958\n",
      "Epoch 3/300 - Train Loss: 0.1115, Val Loss: 0.0883\n",
      "Epoch 4/300 - Train Loss: 0.1061, Val Loss: 0.0793\n",
      "Epoch 5/300 - Train Loss: 0.1044, Val Loss: 0.0840\n",
      "Epoch 6/300 - Train Loss: 0.1007, Val Loss: 0.0751\n",
      "Epoch 7/300 - Train Loss: 0.0991, Val Loss: 0.5389\n",
      "Epoch 8/300 - Train Loss: 0.0994, Val Loss: 0.0861\n",
      "Epoch 9/300 - Train Loss: 0.0972, Val Loss: 0.0783\n",
      "Epoch 10/300 - Train Loss: 0.1002, Val Loss: 0.0770\n",
      "Epoch 11/300 - Train Loss: 0.0952, Val Loss: 0.0736\n",
      "Epoch 12/300 - Train Loss: 0.0950, Val Loss: 0.0711\n",
      "Epoch 13/300 - Train Loss: 0.0942, Val Loss: 0.0730\n",
      "Epoch 14/300 - Train Loss: 0.0956, Val Loss: 0.0730\n",
      "Epoch 15/300 - Train Loss: 0.0972, Val Loss: 0.0710\n",
      "Epoch 16/300 - Train Loss: 0.0963, Val Loss: 0.0775\n",
      "Epoch 17/300 - Train Loss: 0.0961, Val Loss: 0.0728\n",
      "Epoch 18/300 - Train Loss: 0.0938, Val Loss: 0.0757\n",
      "Epoch 19/300 - Train Loss: 0.0951, Val Loss: 0.0736\n",
      "Epoch 20/300 - Train Loss: 0.0923, Val Loss: 0.0724\n",
      "Epoch 21/300 - Train Loss: 0.0959, Val Loss: 0.0720\n",
      "Epoch 22/300 - Train Loss: 0.0954, Val Loss: 0.0711\n",
      "Epoch 23/300 - Train Loss: 0.0963, Val Loss: 0.0705\n",
      "Epoch 24/300 - Train Loss: 0.0943, Val Loss: 0.0741\n",
      "Epoch 25/300 - Train Loss: 0.0945, Val Loss: 0.0710\n",
      "Epoch 26/300 - Train Loss: 0.0932, Val Loss: 0.0746\n",
      "Epoch 27/300 - Train Loss: 0.0942, Val Loss: 0.0727\n",
      "Epoch 28/300 - Train Loss: 0.0954, Val Loss: 0.0712\n",
      "Epoch 29/300 - Train Loss: 0.0968, Val Loss: 0.0817\n",
      "Epoch 30/300 - Train Loss: 0.0951, Val Loss: 0.0721\n",
      "Epoch 31/300 - Train Loss: 0.0945, Val Loss: 0.0719\n",
      "Epoch 32/300 - Train Loss: 0.0949, Val Loss: 0.0732\n",
      "Epoch 33/300 - Train Loss: 0.0944, Val Loss: 0.0735\n",
      "Epoch 34/300 - Train Loss: 0.0945, Val Loss: 0.0769\n",
      "Epoch 35/300 - Train Loss: 0.0969, Val Loss: 0.0701\n",
      "Epoch 36/300 - Train Loss: 0.0966, Val Loss: 0.0708\n",
      "Epoch 37/300 - Train Loss: 0.0955, Val Loss: 0.0741\n",
      "Epoch 38/300 - Train Loss: 0.0958, Val Loss: 0.0753\n",
      "Epoch 39/300 - Train Loss: 0.0969, Val Loss: 0.0726\n",
      "Epoch 40/300 - Train Loss: 0.0971, Val Loss: 0.0712\n",
      "Epoch 41/300 - Train Loss: 0.0948, Val Loss: 0.0708\n",
      "Epoch 42/300 - Train Loss: 0.0976, Val Loss: 0.0694\n",
      "Epoch 43/300 - Train Loss: 0.0981, Val Loss: 0.0717\n",
      "Epoch 44/300 - Train Loss: 0.0979, Val Loss: 0.1072\n",
      "Epoch 45/300 - Train Loss: 0.0989, Val Loss: 0.0749\n",
      "Epoch 46/300 - Train Loss: 0.0986, Val Loss: 0.0782\n",
      "Epoch 47/300 - Train Loss: 0.0984, Val Loss: 0.0735\n",
      "Epoch 48/300 - Train Loss: 0.0993, Val Loss: 0.0726\n",
      "Epoch 49/300 - Train Loss: 0.0975, Val Loss: 0.0703\n",
      "Epoch 50/300 - Train Loss: 0.0981, Val Loss: 0.0712\n",
      "Epoch 51/300 - Train Loss: 0.0990, Val Loss: 0.0801\n",
      "Epoch 52/300 - Train Loss: 0.0984, Val Loss: 0.0790\n",
      "Epoch 53/300 - Train Loss: 0.0977, Val Loss: 0.0780\n",
      "Epoch 54/300 - Train Loss: 0.0969, Val Loss: 0.0746\n",
      "Epoch 55/300 - Train Loss: 0.0976, Val Loss: 0.0744\n",
      "Epoch 56/300 - Train Loss: 0.0981, Val Loss: 0.0721\n",
      "Epoch 57/300 - Train Loss: 0.0981, Val Loss: 0.0704\n",
      "Epoch 58/300 - Train Loss: 0.1002, Val Loss: 0.0731\n",
      "Epoch 59/300 - Train Loss: 0.0988, Val Loss: 0.0697\n",
      "Epoch 60/300 - Train Loss: 0.0987, Val Loss: 0.0766\n",
      "Epoch 61/300 - Train Loss: 0.1007, Val Loss: 0.0722\n",
      "Epoch 62/300 - Train Loss: 0.0989, Val Loss: 0.0751\n",
      "Epoch 63/300 - Train Loss: 0.0975, Val Loss: 0.0742\n",
      "Epoch 64/300 - Train Loss: 0.0968, Val Loss: 0.0704\n",
      "Epoch 65/300 - Train Loss: 0.0997, Val Loss: 0.0745\n",
      "Epoch 66/300 - Train Loss: 0.1012, Val Loss: 0.0754\n",
      "Epoch 67/300 - Train Loss: 0.1004, Val Loss: 0.0750\n",
      "Epoch 68/300 - Train Loss: 0.0989, Val Loss: 0.0784\n",
      "Epoch 69/300 - Train Loss: 0.0985, Val Loss: 0.0740\n",
      "Epoch 70/300 - Train Loss: 0.0992, Val Loss: 0.0738\n",
      "Epoch 71/300 - Train Loss: 0.0974, Val Loss: 0.0726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 03:56:27,087] Trial 79 finished with value: 0.9620197050264152 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.24930477510994953, 'learning_rate': 5.659736656823643e-05, 'batch_size': 32, 'weight_decay': 0.006009221420235837}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300 - Train Loss: 0.1030, Val Loss: 0.0738\n",
      "Early stopping at epoch 72\n",
      "Macro F1 Score: 0.9620, Macro Precision: 0.9567, Macro Recall: 0.9679\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 81\n",
      "Training with F1=32, F2=32, D=8, dropout=0.2338018295457339, LR=0.00011395404219832441, BS=128, WD=0.00907975308365307\n",
      "Epoch 1/300 - Train Loss: 0.3224, Val Loss: 0.1744\n",
      "Epoch 2/300 - Train Loss: 0.1383, Val Loss: 0.1262\n",
      "Epoch 3/300 - Train Loss: 0.1071, Val Loss: 0.0945\n",
      "Epoch 4/300 - Train Loss: 0.0957, Val Loss: 0.1081\n",
      "Epoch 5/300 - Train Loss: 0.0925, Val Loss: 0.1023\n",
      "Epoch 6/300 - Train Loss: 0.0875, Val Loss: 0.0839\n",
      "Epoch 7/300 - Train Loss: 0.0865, Val Loss: 0.0792\n",
      "Epoch 8/300 - Train Loss: 0.0843, Val Loss: 0.0896\n",
      "Epoch 9/300 - Train Loss: 0.0829, Val Loss: 0.0844\n",
      "Epoch 10/300 - Train Loss: 0.0834, Val Loss: 0.0731\n",
      "Epoch 11/300 - Train Loss: 0.0826, Val Loss: 0.0788\n",
      "Epoch 12/300 - Train Loss: 0.0810, Val Loss: 0.0778\n",
      "Epoch 13/300 - Train Loss: 0.0808, Val Loss: 0.0771\n",
      "Epoch 14/300 - Train Loss: 0.0809, Val Loss: 0.0733\n",
      "Epoch 15/300 - Train Loss: 0.0813, Val Loss: 0.0746\n",
      "Epoch 16/300 - Train Loss: 0.0804, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.0788, Val Loss: 0.0766\n",
      "Epoch 18/300 - Train Loss: 0.0797, Val Loss: 0.1363\n",
      "Epoch 19/300 - Train Loss: 0.0808, Val Loss: 0.0730\n",
      "Epoch 20/300 - Train Loss: 0.0811, Val Loss: 0.0881\n",
      "Epoch 21/300 - Train Loss: 0.0815, Val Loss: 0.0748\n",
      "Epoch 22/300 - Train Loss: 0.0815, Val Loss: 0.0760\n",
      "Epoch 23/300 - Train Loss: 0.0803, Val Loss: 0.0708\n",
      "Epoch 24/300 - Train Loss: 0.0815, Val Loss: 0.0749\n",
      "Epoch 25/300 - Train Loss: 0.0814, Val Loss: 0.0709\n",
      "Epoch 26/300 - Train Loss: 0.0823, Val Loss: 0.0696\n",
      "Epoch 27/300 - Train Loss: 0.0812, Val Loss: 0.0762\n",
      "Epoch 28/300 - Train Loss: 0.0807, Val Loss: 0.0701\n",
      "Epoch 29/300 - Train Loss: 0.0814, Val Loss: 0.0712\n",
      "Epoch 30/300 - Train Loss: 0.0822, Val Loss: 0.0728\n",
      "Epoch 31/300 - Train Loss: 0.0827, Val Loss: 0.0810\n",
      "Epoch 32/300 - Train Loss: 0.0850, Val Loss: 0.0734\n",
      "Epoch 33/300 - Train Loss: 0.0841, Val Loss: 0.0721\n",
      "Epoch 34/300 - Train Loss: 0.0862, Val Loss: 0.0748\n",
      "Epoch 35/300 - Train Loss: 0.0851, Val Loss: 0.0746\n",
      "Epoch 36/300 - Train Loss: 0.0859, Val Loss: 0.0738\n",
      "Epoch 37/300 - Train Loss: 0.0844, Val Loss: 0.0717\n",
      "Epoch 38/300 - Train Loss: 0.0854, Val Loss: 0.0732\n",
      "Epoch 39/300 - Train Loss: 0.0850, Val Loss: 0.0731\n",
      "Epoch 40/300 - Train Loss: 0.0867, Val Loss: 0.0730\n",
      "Epoch 41/300 - Train Loss: 0.0875, Val Loss: 0.0732\n",
      "Epoch 42/300 - Train Loss: 0.0866, Val Loss: 0.0786\n",
      "Epoch 43/300 - Train Loss: 0.0877, Val Loss: 0.0736\n",
      "Epoch 44/300 - Train Loss: 0.0883, Val Loss: 0.0740\n",
      "Epoch 45/300 - Train Loss: 0.0881, Val Loss: 0.0715\n",
      "Epoch 46/300 - Train Loss: 0.0891, Val Loss: 0.0720\n",
      "Epoch 47/300 - Train Loss: 0.0902, Val Loss: 0.0757\n",
      "Epoch 48/300 - Train Loss: 0.0896, Val Loss: 0.0751\n",
      "Epoch 49/300 - Train Loss: 0.0890, Val Loss: 0.0802\n",
      "Epoch 50/300 - Train Loss: 0.0908, Val Loss: 0.0851\n",
      "Epoch 51/300 - Train Loss: 0.0906, Val Loss: 0.0759\n",
      "Epoch 52/300 - Train Loss: 0.0912, Val Loss: 0.0784\n",
      "Epoch 53/300 - Train Loss: 0.0906, Val Loss: 0.0781\n",
      "Epoch 54/300 - Train Loss: 0.0892, Val Loss: 0.0857\n",
      "Epoch 55/300 - Train Loss: 0.0908, Val Loss: 0.0976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:01:39,062] Trial 80 finished with value: 0.9684641547298704 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.2338018295457339, 'learning_rate': 0.00011395404219832441, 'batch_size': 128, 'weight_decay': 0.00907975308365307}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300 - Train Loss: 0.0929, Val Loss: 0.0762\n",
      "Early stopping at epoch 56\n",
      "Macro F1 Score: 0.9685, Macro Precision: 0.9675, Macro Recall: 0.9697\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 82\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2873743310898396, LR=8.284646362280421e-05, BS=32, WD=0.0030080766043060303\n",
      "Epoch 1/300 - Train Loss: 0.2825, Val Loss: 0.1257\n",
      "Epoch 2/300 - Train Loss: 0.1270, Val Loss: 0.1047\n",
      "Epoch 3/300 - Train Loss: 0.1124, Val Loss: 0.0883\n",
      "Epoch 4/300 - Train Loss: 0.1056, Val Loss: 0.0806\n",
      "Epoch 5/300 - Train Loss: 0.1004, Val Loss: 0.0875\n",
      "Epoch 6/300 - Train Loss: 0.0966, Val Loss: 0.0739\n",
      "Epoch 7/300 - Train Loss: 0.0974, Val Loss: 0.0898\n",
      "Epoch 8/300 - Train Loss: 0.0940, Val Loss: 0.0710\n",
      "Epoch 9/300 - Train Loss: 0.0940, Val Loss: 0.0778\n",
      "Epoch 10/300 - Train Loss: 0.0925, Val Loss: 0.0701\n",
      "Epoch 11/300 - Train Loss: 0.0902, Val Loss: 0.0709\n",
      "Epoch 12/300 - Train Loss: 0.0898, Val Loss: 0.0721\n",
      "Epoch 13/300 - Train Loss: 0.0893, Val Loss: 0.0769\n",
      "Epoch 14/300 - Train Loss: 0.0906, Val Loss: 0.0732\n",
      "Epoch 15/300 - Train Loss: 0.0874, Val Loss: 0.0680\n",
      "Epoch 16/300 - Train Loss: 0.0881, Val Loss: 0.0706\n",
      "Epoch 17/300 - Train Loss: 0.0885, Val Loss: 0.0706\n",
      "Epoch 18/300 - Train Loss: 0.0905, Val Loss: 0.0694\n",
      "Epoch 19/300 - Train Loss: 0.0886, Val Loss: 0.0739\n",
      "Epoch 20/300 - Train Loss: 0.0880, Val Loss: 0.0707\n",
      "Epoch 21/300 - Train Loss: 0.0867, Val Loss: 0.0695\n",
      "Epoch 22/300 - Train Loss: 0.0890, Val Loss: 0.0726\n",
      "Epoch 23/300 - Train Loss: 0.0899, Val Loss: 0.0698\n",
      "Epoch 24/300 - Train Loss: 0.0877, Val Loss: 0.0762\n",
      "Epoch 25/300 - Train Loss: 0.0861, Val Loss: 0.0691\n",
      "Epoch 26/300 - Train Loss: 0.0890, Val Loss: 0.0715\n",
      "Epoch 27/300 - Train Loss: 0.0904, Val Loss: 0.0682\n",
      "Epoch 28/300 - Train Loss: 0.0873, Val Loss: 0.0692\n",
      "Epoch 29/300 - Train Loss: 0.0896, Val Loss: 0.0701\n",
      "Epoch 30/300 - Train Loss: 0.0890, Val Loss: 0.0733\n",
      "Epoch 31/300 - Train Loss: 0.0904, Val Loss: 0.0765\n",
      "Epoch 32/300 - Train Loss: 0.0898, Val Loss: 0.0743\n",
      "Epoch 33/300 - Train Loss: 0.0886, Val Loss: 0.0770\n",
      "Epoch 34/300 - Train Loss: 0.0885, Val Loss: 0.0720\n",
      "Epoch 35/300 - Train Loss: 0.0872, Val Loss: 0.0722\n",
      "Epoch 36/300 - Train Loss: 0.0889, Val Loss: 0.0665\n",
      "Epoch 37/300 - Train Loss: 0.0906, Val Loss: 0.0691\n",
      "Epoch 38/300 - Train Loss: 0.0905, Val Loss: 0.0683\n",
      "Epoch 39/300 - Train Loss: 0.0891, Val Loss: 0.0720\n",
      "Epoch 40/300 - Train Loss: 0.0883, Val Loss: 0.0719\n",
      "Epoch 41/300 - Train Loss: 0.0911, Val Loss: 0.0698\n",
      "Epoch 42/300 - Train Loss: 0.0886, Val Loss: 0.0720\n",
      "Epoch 43/300 - Train Loss: 0.0891, Val Loss: 0.0722\n",
      "Epoch 44/300 - Train Loss: 0.0909, Val Loss: 0.0724\n",
      "Epoch 45/300 - Train Loss: 0.0880, Val Loss: 0.0703\n",
      "Epoch 46/300 - Train Loss: 0.0883, Val Loss: 0.0682\n",
      "Epoch 47/300 - Train Loss: 0.0896, Val Loss: 0.0698\n",
      "Epoch 48/300 - Train Loss: 0.0916, Val Loss: 0.0724\n",
      "Epoch 49/300 - Train Loss: 0.0911, Val Loss: 0.0746\n",
      "Epoch 50/300 - Train Loss: 0.0916, Val Loss: 0.0912\n",
      "Epoch 51/300 - Train Loss: 0.0897, Val Loss: 0.0702\n",
      "Epoch 52/300 - Train Loss: 0.0914, Val Loss: 0.0708\n",
      "Epoch 53/300 - Train Loss: 0.0920, Val Loss: 0.0712\n",
      "Epoch 54/300 - Train Loss: 0.0888, Val Loss: 0.0712\n",
      "Epoch 55/300 - Train Loss: 0.0915, Val Loss: 0.0740\n",
      "Epoch 56/300 - Train Loss: 0.0918, Val Loss: 0.0707\n",
      "Epoch 57/300 - Train Loss: 0.0918, Val Loss: 0.0820\n",
      "Epoch 58/300 - Train Loss: 0.0902, Val Loss: 0.0750\n",
      "Epoch 59/300 - Train Loss: 0.0915, Val Loss: 0.0706\n",
      "Epoch 60/300 - Train Loss: 0.0931, Val Loss: 0.0737\n",
      "Epoch 61/300 - Train Loss: 0.0916, Val Loss: 0.0686\n",
      "Epoch 62/300 - Train Loss: 0.0911, Val Loss: 0.0701\n",
      "Epoch 63/300 - Train Loss: 0.0912, Val Loss: 0.0693\n",
      "Epoch 64/300 - Train Loss: 0.0906, Val Loss: 0.0686\n",
      "Epoch 65/300 - Train Loss: 0.0914, Val Loss: 0.0725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:05:52,542] Trial 81 finished with value: 0.9718282267249952 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2873743310898396, 'learning_rate': 8.284646362280421e-05, 'batch_size': 32, 'weight_decay': 0.0030080766043060303}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/300 - Train Loss: 0.0891, Val Loss: 0.0714\n",
      "Early stopping at epoch 66\n",
      "Macro F1 Score: 0.9718, Macro Precision: 0.9730, Macro Recall: 0.9708\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 83\n",
      "Training with F1=16, F2=32, D=8, dropout=0.29880401697065045, LR=7.0007147387756e-05, BS=32, WD=0.0028820885648283947\n",
      "Epoch 1/300 - Train Loss: 0.3196, Val Loss: 0.1522\n",
      "Epoch 2/300 - Train Loss: 0.1414, Val Loss: 0.1061\n",
      "Epoch 3/300 - Train Loss: 0.1116, Val Loss: 0.0939\n",
      "Epoch 4/300 - Train Loss: 0.1052, Val Loss: 0.0819\n",
      "Epoch 5/300 - Train Loss: 0.0992, Val Loss: 0.0891\n",
      "Epoch 6/300 - Train Loss: 0.0968, Val Loss: 0.0871\n",
      "Epoch 7/300 - Train Loss: 0.0975, Val Loss: 0.0745\n",
      "Epoch 8/300 - Train Loss: 0.0919, Val Loss: 0.0780\n",
      "Epoch 9/300 - Train Loss: 0.0926, Val Loss: 0.0854\n",
      "Epoch 10/300 - Train Loss: 0.0935, Val Loss: 0.0756\n",
      "Epoch 11/300 - Train Loss: 0.0909, Val Loss: 0.0685\n",
      "Epoch 12/300 - Train Loss: 0.0890, Val Loss: 0.0698\n",
      "Epoch 13/300 - Train Loss: 0.0887, Val Loss: 0.0798\n",
      "Epoch 14/300 - Train Loss: 0.0891, Val Loss: 0.0690\n",
      "Epoch 15/300 - Train Loss: 0.0877, Val Loss: 0.0728\n",
      "Epoch 16/300 - Train Loss: 0.0886, Val Loss: 0.0771\n",
      "Epoch 17/300 - Train Loss: 0.0876, Val Loss: 0.0755\n",
      "Epoch 18/300 - Train Loss: 0.0880, Val Loss: 0.0716\n",
      "Epoch 19/300 - Train Loss: 0.0896, Val Loss: 0.0717\n",
      "Epoch 20/300 - Train Loss: 0.0875, Val Loss: 0.0740\n",
      "Epoch 21/300 - Train Loss: 0.0887, Val Loss: 0.0710\n",
      "Epoch 22/300 - Train Loss: 0.0882, Val Loss: 0.0709\n",
      "Epoch 23/300 - Train Loss: 0.0880, Val Loss: 0.0752\n",
      "Epoch 24/300 - Train Loss: 0.0875, Val Loss: 0.0707\n",
      "Epoch 25/300 - Train Loss: 0.0870, Val Loss: 0.0715\n",
      "Epoch 26/300 - Train Loss: 0.0910, Val Loss: 0.0753\n",
      "Epoch 27/300 - Train Loss: 0.0878, Val Loss: 0.0717\n",
      "Epoch 28/300 - Train Loss: 0.0898, Val Loss: 0.0728\n",
      "Epoch 29/300 - Train Loss: 0.0887, Val Loss: 0.0711\n",
      "Epoch 30/300 - Train Loss: 0.0860, Val Loss: 0.0743\n",
      "Epoch 31/300 - Train Loss: 0.0875, Val Loss: 0.0692\n",
      "Epoch 32/300 - Train Loss: 0.0877, Val Loss: 0.0745\n",
      "Epoch 33/300 - Train Loss: 0.0883, Val Loss: 0.0736\n",
      "Epoch 34/300 - Train Loss: 0.0891, Val Loss: 0.0708\n",
      "Epoch 35/300 - Train Loss: 0.0879, Val Loss: 0.0747\n",
      "Epoch 36/300 - Train Loss: 0.0895, Val Loss: 0.0738\n",
      "Epoch 37/300 - Train Loss: 0.0883, Val Loss: 0.0728\n",
      "Epoch 38/300 - Train Loss: 0.0901, Val Loss: 0.0714\n",
      "Epoch 39/300 - Train Loss: 0.0908, Val Loss: 0.0720\n",
      "Epoch 40/300 - Train Loss: 0.0894, Val Loss: 0.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:08:30,070] Trial 82 finished with value: 0.9645025514302571 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.29880401697065045, 'learning_rate': 7.0007147387756e-05, 'batch_size': 32, 'weight_decay': 0.0028820885648283947}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300 - Train Loss: 0.0912, Val Loss: 0.0750\n",
      "Early stopping at epoch 41\n",
      "Macro F1 Score: 0.9645, Macro Precision: 0.9588, Macro Recall: 0.9707\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 84\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2862649905693901, LR=0.00010381868830682552, BS=32, WD=0.005459025599501411\n",
      "Epoch 1/300 - Train Loss: 0.2566, Val Loss: 0.1044\n",
      "Epoch 2/300 - Train Loss: 0.1141, Val Loss: 0.0817\n",
      "Epoch 3/300 - Train Loss: 0.1029, Val Loss: 0.0870\n",
      "Epoch 4/300 - Train Loss: 0.1026, Val Loss: 0.0769\n",
      "Epoch 5/300 - Train Loss: 0.0983, Val Loss: 0.0846\n",
      "Epoch 6/300 - Train Loss: 0.0957, Val Loss: 0.0694\n",
      "Epoch 7/300 - Train Loss: 0.0972, Val Loss: 0.0716\n",
      "Epoch 8/300 - Train Loss: 0.0956, Val Loss: 0.0747\n",
      "Epoch 9/300 - Train Loss: 0.0971, Val Loss: 0.0716\n",
      "Epoch 10/300 - Train Loss: 0.0971, Val Loss: 0.0773\n",
      "Epoch 11/300 - Train Loss: 0.0949, Val Loss: 0.0665\n",
      "Epoch 12/300 - Train Loss: 0.0970, Val Loss: 0.0702\n",
      "Epoch 13/300 - Train Loss: 0.0953, Val Loss: 0.0728\n",
      "Epoch 14/300 - Train Loss: 0.0937, Val Loss: 0.0738\n",
      "Epoch 15/300 - Train Loss: 0.0966, Val Loss: 0.0679\n",
      "Epoch 16/300 - Train Loss: 0.0973, Val Loss: 0.0702\n",
      "Epoch 17/300 - Train Loss: 0.0974, Val Loss: 0.0713\n",
      "Epoch 18/300 - Train Loss: 0.0977, Val Loss: 0.0720\n",
      "Epoch 19/300 - Train Loss: 0.0951, Val Loss: 0.0754\n",
      "Epoch 20/300 - Train Loss: 0.0971, Val Loss: 0.0711\n",
      "Epoch 21/300 - Train Loss: 0.0983, Val Loss: 0.0700\n",
      "Epoch 22/300 - Train Loss: 0.0980, Val Loss: 0.0689\n",
      "Epoch 23/300 - Train Loss: 0.0979, Val Loss: 0.0733\n",
      "Epoch 24/300 - Train Loss: 0.0967, Val Loss: 0.0684\n",
      "Epoch 25/300 - Train Loss: 0.0992, Val Loss: 0.0714\n",
      "Epoch 26/300 - Train Loss: 0.1000, Val Loss: 0.0718\n",
      "Epoch 27/300 - Train Loss: 0.0996, Val Loss: 0.0701\n",
      "Epoch 28/300 - Train Loss: 0.0998, Val Loss: 0.0718\n",
      "Epoch 29/300 - Train Loss: 0.1012, Val Loss: 0.0695\n",
      "Epoch 30/300 - Train Loss: 0.0980, Val Loss: 0.0698\n",
      "Epoch 31/300 - Train Loss: 0.0992, Val Loss: 0.0769\n",
      "Epoch 32/300 - Train Loss: 0.1001, Val Loss: 0.0704\n",
      "Epoch 33/300 - Train Loss: 0.1022, Val Loss: 0.0899\n",
      "Epoch 34/300 - Train Loss: 0.1003, Val Loss: 0.0719\n",
      "Epoch 35/300 - Train Loss: 0.0998, Val Loss: 0.0721\n",
      "Epoch 36/300 - Train Loss: 0.1021, Val Loss: 0.0717\n",
      "Epoch 37/300 - Train Loss: 0.1014, Val Loss: 0.0705\n",
      "Epoch 38/300 - Train Loss: 0.1009, Val Loss: 0.0731\n",
      "Epoch 39/300 - Train Loss: 0.1018, Val Loss: 0.0741\n",
      "Epoch 40/300 - Train Loss: 0.1015, Val Loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:11:07,917] Trial 83 finished with value: 0.9667142874391952 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2862649905693901, 'learning_rate': 0.00010381868830682552, 'batch_size': 32, 'weight_decay': 0.005459025599501411}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300 - Train Loss: 0.1009, Val Loss: 0.0783\n",
      "Early stopping at epoch 41\n",
      "Macro F1 Score: 0.9667, Macro Precision: 0.9632, Macro Recall: 0.9705\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 85\n",
      "Training with F1=16, F2=32, D=8, dropout=0.220949457254649, LR=0.000216780177113754, BS=32, WD=0.007355314087120881\n",
      "Epoch 1/300 - Train Loss: 0.1854, Val Loss: 0.1097\n",
      "Epoch 2/300 - Train Loss: 0.1071, Val Loss: 0.1221\n",
      "Epoch 3/300 - Train Loss: 0.1046, Val Loss: 0.1247\n",
      "Epoch 4/300 - Train Loss: 0.1001, Val Loss: 0.0813\n",
      "Epoch 5/300 - Train Loss: 0.1003, Val Loss: 0.0932\n",
      "Epoch 6/300 - Train Loss: 0.1017, Val Loss: 0.0734\n",
      "Epoch 7/300 - Train Loss: 0.1022, Val Loss: 0.0757\n",
      "Epoch 8/300 - Train Loss: 0.1022, Val Loss: 0.0778\n",
      "Epoch 9/300 - Train Loss: 0.1030, Val Loss: 0.0803\n",
      "Epoch 10/300 - Train Loss: 0.1048, Val Loss: 0.0744\n",
      "Epoch 11/300 - Train Loss: 0.1030, Val Loss: 0.0856\n",
      "Epoch 12/300 - Train Loss: 0.1042, Val Loss: 0.0909\n",
      "Epoch 13/300 - Train Loss: 0.1046, Val Loss: 0.0888\n",
      "Epoch 14/300 - Train Loss: 0.1056, Val Loss: 0.0774\n",
      "Epoch 15/300 - Train Loss: 0.1075, Val Loss: 0.0871\n",
      "Epoch 16/300 - Train Loss: 0.1085, Val Loss: 0.0741\n",
      "Epoch 17/300 - Train Loss: 0.1076, Val Loss: 0.0780\n",
      "Epoch 18/300 - Train Loss: 0.1047, Val Loss: 0.0757\n",
      "Epoch 19/300 - Train Loss: 0.1059, Val Loss: 0.0886\n",
      "Epoch 20/300 - Train Loss: 0.1032, Val Loss: 0.0741\n",
      "Epoch 21/300 - Train Loss: 0.1071, Val Loss: 0.0845\n",
      "Epoch 22/300 - Train Loss: 0.1050, Val Loss: 0.0780\n",
      "Epoch 23/300 - Train Loss: 0.1082, Val Loss: 0.0772\n",
      "Epoch 24/300 - Train Loss: 0.1081, Val Loss: 0.0854\n",
      "Epoch 25/300 - Train Loss: 0.1059, Val Loss: 0.0812\n",
      "Epoch 26/300 - Train Loss: 0.1093, Val Loss: 0.0851\n",
      "Epoch 27/300 - Train Loss: 0.1059, Val Loss: 0.0852\n",
      "Epoch 28/300 - Train Loss: 0.1053, Val Loss: 0.0759\n",
      "Epoch 29/300 - Train Loss: 0.1091, Val Loss: 0.0841\n",
      "Epoch 30/300 - Train Loss: 0.1073, Val Loss: 0.0933\n",
      "Epoch 31/300 - Train Loss: 0.1061, Val Loss: 0.0807\n",
      "Epoch 32/300 - Train Loss: 0.1067, Val Loss: 0.0753\n",
      "Epoch 33/300 - Train Loss: 0.1063, Val Loss: 0.0851\n",
      "Epoch 34/300 - Train Loss: 0.1075, Val Loss: 0.0801\n",
      "Epoch 35/300 - Train Loss: 0.1057, Val Loss: 0.1062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:13:26,376] Trial 84 finished with value: 0.9597646761749995 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.220949457254649, 'learning_rate': 0.000216780177113754, 'batch_size': 32, 'weight_decay': 0.007355314087120881}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300 - Train Loss: 0.1060, Val Loss: 0.0946\n",
      "Early stopping at epoch 36\n",
      "Macro F1 Score: 0.9598, Macro Precision: 0.9527, Macro Recall: 0.9677\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 86\n",
      "Training with F1=16, F2=32, D=8, dropout=0.24951767904036687, LR=0.00013744859994135615, BS=256, WD=0.0040907835898518485\n",
      "Epoch 1/300 - Train Loss: 0.4353, Val Loss: 0.2359\n",
      "Epoch 2/300 - Train Loss: 0.1947, Val Loss: 0.1586\n",
      "Epoch 3/300 - Train Loss: 0.1378, Val Loss: 0.1191\n",
      "Epoch 4/300 - Train Loss: 0.1111, Val Loss: 0.1031\n",
      "Epoch 5/300 - Train Loss: 0.1007, Val Loss: 0.0901\n",
      "Epoch 6/300 - Train Loss: 0.0956, Val Loss: 0.0872\n",
      "Epoch 7/300 - Train Loss: 0.0903, Val Loss: 0.0824\n",
      "Epoch 8/300 - Train Loss: 0.0878, Val Loss: 0.0893\n",
      "Epoch 9/300 - Train Loss: 0.0866, Val Loss: 0.0816\n",
      "Epoch 10/300 - Train Loss: 0.0857, Val Loss: 0.0814\n",
      "Epoch 11/300 - Train Loss: 0.0843, Val Loss: 0.0768\n",
      "Epoch 12/300 - Train Loss: 0.0826, Val Loss: 0.0932\n",
      "Epoch 13/300 - Train Loss: 0.0818, Val Loss: 0.0778\n",
      "Epoch 14/300 - Train Loss: 0.0818, Val Loss: 0.0699\n",
      "Epoch 15/300 - Train Loss: 0.0797, Val Loss: 0.0706\n",
      "Epoch 16/300 - Train Loss: 0.0802, Val Loss: 0.0710\n",
      "Epoch 17/300 - Train Loss: 0.0782, Val Loss: 0.0684\n",
      "Epoch 18/300 - Train Loss: 0.0815, Val Loss: 0.0771\n",
      "Epoch 19/300 - Train Loss: 0.0780, Val Loss: 0.0774\n",
      "Epoch 20/300 - Train Loss: 0.0781, Val Loss: 0.0773\n",
      "Epoch 21/300 - Train Loss: 0.0782, Val Loss: 0.0699\n",
      "Epoch 22/300 - Train Loss: 0.0765, Val Loss: 0.0688\n",
      "Epoch 23/300 - Train Loss: 0.0772, Val Loss: 0.0676\n",
      "Epoch 24/300 - Train Loss: 0.0762, Val Loss: 0.0668\n",
      "Epoch 25/300 - Train Loss: 0.0765, Val Loss: 0.0695\n",
      "Epoch 26/300 - Train Loss: 0.0773, Val Loss: 0.0689\n",
      "Epoch 27/300 - Train Loss: 0.0763, Val Loss: 0.0894\n",
      "Epoch 28/300 - Train Loss: 0.0751, Val Loss: 0.0705\n",
      "Epoch 29/300 - Train Loss: 0.0750, Val Loss: 0.0663\n",
      "Epoch 30/300 - Train Loss: 0.0745, Val Loss: 0.0754\n",
      "Epoch 31/300 - Train Loss: 0.0776, Val Loss: 0.0705\n",
      "Epoch 32/300 - Train Loss: 0.0759, Val Loss: 0.0681\n",
      "Epoch 33/300 - Train Loss: 0.0785, Val Loss: 0.0764\n",
      "Epoch 34/300 - Train Loss: 0.0767, Val Loss: 0.0709\n",
      "Epoch 35/300 - Train Loss: 0.0747, Val Loss: 0.0746\n",
      "Epoch 36/300 - Train Loss: 0.0753, Val Loss: 0.0666\n",
      "Epoch 37/300 - Train Loss: 0.0755, Val Loss: 0.0726\n",
      "Epoch 38/300 - Train Loss: 0.0754, Val Loss: 0.0681\n",
      "Epoch 39/300 - Train Loss: 0.0749, Val Loss: 0.0697\n",
      "Epoch 40/300 - Train Loss: 0.0752, Val Loss: 0.0682\n",
      "Epoch 41/300 - Train Loss: 0.0757, Val Loss: 0.0675\n",
      "Epoch 42/300 - Train Loss: 0.0763, Val Loss: 0.0671\n",
      "Epoch 43/300 - Train Loss: 0.0751, Val Loss: 0.0680\n",
      "Epoch 44/300 - Train Loss: 0.0742, Val Loss: 0.0680\n",
      "Epoch 45/300 - Train Loss: 0.0762, Val Loss: 0.0647\n",
      "Epoch 46/300 - Train Loss: 0.0765, Val Loss: 0.0688\n",
      "Epoch 47/300 - Train Loss: 0.0786, Val Loss: 0.0677\n",
      "Epoch 48/300 - Train Loss: 0.0769, Val Loss: 0.0695\n",
      "Epoch 49/300 - Train Loss: 0.0758, Val Loss: 0.0667\n",
      "Epoch 50/300 - Train Loss: 0.0767, Val Loss: 0.0682\n",
      "Epoch 51/300 - Train Loss: 0.0770, Val Loss: 0.0688\n",
      "Epoch 52/300 - Train Loss: 0.0769, Val Loss: 0.0690\n",
      "Epoch 53/300 - Train Loss: 0.0764, Val Loss: 0.0688\n",
      "Epoch 54/300 - Train Loss: 0.0759, Val Loss: 0.0681\n",
      "Epoch 55/300 - Train Loss: 0.0753, Val Loss: 0.0682\n",
      "Epoch 56/300 - Train Loss: 0.0761, Val Loss: 0.0681\n",
      "Epoch 57/300 - Train Loss: 0.0769, Val Loss: 0.0677\n",
      "Epoch 58/300 - Train Loss: 0.0771, Val Loss: 0.0680\n",
      "Epoch 59/300 - Train Loss: 0.0764, Val Loss: 0.0682\n",
      "Epoch 60/300 - Train Loss: 0.0764, Val Loss: 0.0684\n",
      "Epoch 61/300 - Train Loss: 0.0785, Val Loss: 0.0665\n",
      "Epoch 62/300 - Train Loss: 0.0771, Val Loss: 0.0675\n",
      "Epoch 63/300 - Train Loss: 0.0776, Val Loss: 0.0677\n",
      "Epoch 64/300 - Train Loss: 0.0781, Val Loss: 0.0666\n",
      "Epoch 65/300 - Train Loss: 0.0769, Val Loss: 0.0687\n",
      "Epoch 66/300 - Train Loss: 0.0768, Val Loss: 0.0669\n",
      "Epoch 67/300 - Train Loss: 0.0774, Val Loss: 0.0654\n",
      "Epoch 68/300 - Train Loss: 0.0778, Val Loss: 0.0689\n",
      "Epoch 69/300 - Train Loss: 0.0777, Val Loss: 0.0663\n",
      "Epoch 70/300 - Train Loss: 0.0783, Val Loss: 0.0696\n",
      "Epoch 71/300 - Train Loss: 0.0782, Val Loss: 0.0672\n",
      "Epoch 72/300 - Train Loss: 0.0785, Val Loss: 0.0682\n",
      "Epoch 73/300 - Train Loss: 0.0767, Val Loss: 0.0673\n",
      "Epoch 74/300 - Train Loss: 0.0787, Val Loss: 0.0696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:17:17,694] Trial 85 finished with value: 0.9715167470982385 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.24951767904036687, 'learning_rate': 0.00013744859994135615, 'batch_size': 256, 'weight_decay': 0.0040907835898518485}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300 - Train Loss: 0.0793, Val Loss: 0.0690\n",
      "Early stopping at epoch 75\n",
      "Macro F1 Score: 0.9715, Macro Precision: 0.9681, Macro Recall: 0.9753\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 87\n",
      "Training with F1=16, F2=32, D=8, dropout=0.24985704462513456, LR=8.698596690645769e-05, BS=256, WD=0.0042393755163692365\n",
      "Epoch 1/300 - Train Loss: 0.5201, Val Loss: 0.2731\n",
      "Epoch 2/300 - Train Loss: 0.2129, Val Loss: 0.1767\n",
      "Epoch 3/300 - Train Loss: 0.1540, Val Loss: 0.1431\n",
      "Epoch 4/300 - Train Loss: 0.1253, Val Loss: 0.1189\n",
      "Epoch 5/300 - Train Loss: 0.1143, Val Loss: 0.1040\n",
      "Epoch 6/300 - Train Loss: 0.1050, Val Loss: 0.0995\n",
      "Epoch 7/300 - Train Loss: 0.0990, Val Loss: 0.0929\n",
      "Epoch 8/300 - Train Loss: 0.0959, Val Loss: 0.0847\n",
      "Epoch 9/300 - Train Loss: 0.0914, Val Loss: 0.0900\n",
      "Epoch 10/300 - Train Loss: 0.0904, Val Loss: 0.0844\n",
      "Epoch 11/300 - Train Loss: 0.0891, Val Loss: 0.0832\n",
      "Epoch 12/300 - Train Loss: 0.0867, Val Loss: 0.0820\n",
      "Epoch 13/300 - Train Loss: 0.0871, Val Loss: 0.0778\n",
      "Epoch 14/300 - Train Loss: 0.0849, Val Loss: 0.0768\n",
      "Epoch 15/300 - Train Loss: 0.0850, Val Loss: 0.0755\n",
      "Epoch 16/300 - Train Loss: 0.0827, Val Loss: 0.0764\n",
      "Epoch 17/300 - Train Loss: 0.0822, Val Loss: 0.0760\n",
      "Epoch 18/300 - Train Loss: 0.0816, Val Loss: 0.0816\n",
      "Epoch 19/300 - Train Loss: 0.0805, Val Loss: 0.0738\n",
      "Epoch 20/300 - Train Loss: 0.0812, Val Loss: 0.0914\n",
      "Epoch 21/300 - Train Loss: 0.0801, Val Loss: 0.0817\n",
      "Epoch 22/300 - Train Loss: 0.0802, Val Loss: 0.0805\n",
      "Epoch 23/300 - Train Loss: 0.0783, Val Loss: 0.0729\n",
      "Epoch 24/300 - Train Loss: 0.0785, Val Loss: 0.0857\n",
      "Epoch 25/300 - Train Loss: 0.0785, Val Loss: 0.0747\n",
      "Epoch 26/300 - Train Loss: 0.0779, Val Loss: 0.0755\n",
      "Epoch 27/300 - Train Loss: 0.0773, Val Loss: 0.0822\n",
      "Epoch 28/300 - Train Loss: 0.0769, Val Loss: 0.0746\n",
      "Epoch 29/300 - Train Loss: 0.0777, Val Loss: 0.0804\n",
      "Epoch 30/300 - Train Loss: 0.0756, Val Loss: 0.0764\n",
      "Epoch 31/300 - Train Loss: 0.0766, Val Loss: 0.0730\n",
      "Epoch 32/300 - Train Loss: 0.0765, Val Loss: 0.0775\n",
      "Epoch 33/300 - Train Loss: 0.0768, Val Loss: 0.0750\n",
      "Epoch 34/300 - Train Loss: 0.0755, Val Loss: 0.0863\n",
      "Epoch 35/300 - Train Loss: 0.0769, Val Loss: 0.0688\n",
      "Epoch 36/300 - Train Loss: 0.0769, Val Loss: 0.0774\n",
      "Epoch 37/300 - Train Loss: 0.0764, Val Loss: 0.0747\n",
      "Epoch 38/300 - Train Loss: 0.0750, Val Loss: 0.0710\n",
      "Epoch 39/300 - Train Loss: 0.0747, Val Loss: 0.0695\n",
      "Epoch 40/300 - Train Loss: 0.0773, Val Loss: 0.0725\n",
      "Epoch 41/300 - Train Loss: 0.0746, Val Loss: 0.0726\n",
      "Epoch 42/300 - Train Loss: 0.0744, Val Loss: 0.0731\n",
      "Epoch 43/300 - Train Loss: 0.0767, Val Loss: 0.0692\n",
      "Epoch 44/300 - Train Loss: 0.0751, Val Loss: 0.0738\n",
      "Epoch 45/300 - Train Loss: 0.0747, Val Loss: 0.0698\n",
      "Epoch 46/300 - Train Loss: 0.0750, Val Loss: 0.0801\n",
      "Epoch 47/300 - Train Loss: 0.0745, Val Loss: 0.0687\n",
      "Epoch 48/300 - Train Loss: 0.0744, Val Loss: 0.0751\n",
      "Epoch 49/300 - Train Loss: 0.0745, Val Loss: 0.0701\n",
      "Epoch 50/300 - Train Loss: 0.0750, Val Loss: 0.0700\n",
      "Epoch 51/300 - Train Loss: 0.0740, Val Loss: 0.0770\n",
      "Epoch 52/300 - Train Loss: 0.0735, Val Loss: 0.0738\n",
      "Epoch 53/300 - Train Loss: 0.0765, Val Loss: 0.0717\n",
      "Epoch 54/300 - Train Loss: 0.0755, Val Loss: 0.0730\n",
      "Epoch 55/300 - Train Loss: 0.0759, Val Loss: 0.0675\n",
      "Epoch 56/300 - Train Loss: 0.0772, Val Loss: 0.0681\n",
      "Epoch 57/300 - Train Loss: 0.0760, Val Loss: 0.0687\n",
      "Epoch 58/300 - Train Loss: 0.0759, Val Loss: 0.0705\n",
      "Epoch 59/300 - Train Loss: 0.0758, Val Loss: 0.0726\n",
      "Epoch 60/300 - Train Loss: 0.0752, Val Loss: 0.0711\n",
      "Epoch 61/300 - Train Loss: 0.0750, Val Loss: 0.0715\n",
      "Epoch 62/300 - Train Loss: 0.0740, Val Loss: 0.0713\n",
      "Epoch 63/300 - Train Loss: 0.0736, Val Loss: 0.0700\n",
      "Epoch 64/300 - Train Loss: 0.0765, Val Loss: 0.0697\n",
      "Epoch 65/300 - Train Loss: 0.0755, Val Loss: 0.0687\n",
      "Epoch 66/300 - Train Loss: 0.0751, Val Loss: 0.0689\n",
      "Epoch 67/300 - Train Loss: 0.0751, Val Loss: 0.0703\n",
      "Epoch 68/300 - Train Loss: 0.0756, Val Loss: 0.0706\n",
      "Epoch 69/300 - Train Loss: 0.0743, Val Loss: 0.0689\n",
      "Epoch 70/300 - Train Loss: 0.0759, Val Loss: 0.0737\n",
      "Epoch 71/300 - Train Loss: 0.0759, Val Loss: 0.0700\n",
      "Epoch 72/300 - Train Loss: 0.0754, Val Loss: 0.0694\n",
      "Epoch 73/300 - Train Loss: 0.0754, Val Loss: 0.0710\n",
      "Epoch 74/300 - Train Loss: 0.0753, Val Loss: 0.0687\n",
      "Epoch 75/300 - Train Loss: 0.0753, Val Loss: 0.0704\n",
      "Epoch 76/300 - Train Loss: 0.0752, Val Loss: 0.0689\n",
      "Epoch 77/300 - Train Loss: 0.0753, Val Loss: 0.0706\n",
      "Epoch 78/300 - Train Loss: 0.0754, Val Loss: 0.0686\n",
      "Epoch 79/300 - Train Loss: 0.0754, Val Loss: 0.0691\n",
      "Epoch 80/300 - Train Loss: 0.0770, Val Loss: 0.0685\n",
      "Epoch 81/300 - Train Loss: 0.0769, Val Loss: 0.0685\n",
      "Epoch 82/300 - Train Loss: 0.0748, Val Loss: 0.0687\n",
      "Epoch 83/300 - Train Loss: 0.0762, Val Loss: 0.0684\n",
      "Epoch 84/300 - Train Loss: 0.0786, Val Loss: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:21:39,626] Trial 86 finished with value: 0.9639027399336731 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.24985704462513456, 'learning_rate': 8.698596690645769e-05, 'batch_size': 256, 'weight_decay': 0.0042393755163692365}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300 - Train Loss: 0.0755, Val Loss: 0.0692\n",
      "Early stopping at epoch 85\n",
      "Macro F1 Score: 0.9639, Macro Precision: 0.9648, Macro Recall: 0.9631\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 88\n",
      "Training with F1=16, F2=32, D=8, dropout=0.18860045331383207, LR=0.00013407550788849536, BS=256, WD=0.0032817906573662323\n",
      "Epoch 1/300 - Train Loss: 0.4310, Val Loss: 0.2392\n",
      "Epoch 2/300 - Train Loss: 0.1863, Val Loss: 0.1574\n",
      "Epoch 3/300 - Train Loss: 0.1387, Val Loss: 0.1115\n",
      "Epoch 4/300 - Train Loss: 0.1119, Val Loss: 0.0949\n",
      "Epoch 5/300 - Train Loss: 0.1020, Val Loss: 0.0964\n",
      "Epoch 6/300 - Train Loss: 0.0963, Val Loss: 0.0972\n",
      "Epoch 7/300 - Train Loss: 0.0946, Val Loss: 0.0888\n",
      "Epoch 8/300 - Train Loss: 0.0893, Val Loss: 0.0792\n",
      "Epoch 9/300 - Train Loss: 0.0877, Val Loss: 0.1073\n",
      "Epoch 10/300 - Train Loss: 0.0868, Val Loss: 0.0782\n",
      "Epoch 11/300 - Train Loss: 0.0850, Val Loss: 0.0766\n",
      "Epoch 12/300 - Train Loss: 0.0839, Val Loss: 0.0929\n",
      "Epoch 13/300 - Train Loss: 0.0816, Val Loss: 0.0770\n",
      "Epoch 14/300 - Train Loss: 0.0808, Val Loss: 0.0756\n",
      "Epoch 15/300 - Train Loss: 0.0818, Val Loss: 0.0823\n",
      "Epoch 16/300 - Train Loss: 0.0803, Val Loss: 0.0888\n",
      "Epoch 17/300 - Train Loss: 0.0784, Val Loss: 0.0703\n",
      "Epoch 18/300 - Train Loss: 0.0787, Val Loss: 0.0728\n",
      "Epoch 19/300 - Train Loss: 0.0781, Val Loss: 0.0778\n",
      "Epoch 20/300 - Train Loss: 0.0775, Val Loss: 0.0783\n",
      "Epoch 21/300 - Train Loss: 0.0770, Val Loss: 0.0801\n",
      "Epoch 22/300 - Train Loss: 0.0781, Val Loss: 0.0706\n",
      "Epoch 23/300 - Train Loss: 0.0769, Val Loss: 0.0727\n",
      "Epoch 24/300 - Train Loss: 0.0761, Val Loss: 0.0706\n",
      "Epoch 25/300 - Train Loss: 0.0760, Val Loss: 0.0702\n",
      "Epoch 26/300 - Train Loss: 0.0744, Val Loss: 0.0737\n",
      "Epoch 27/300 - Train Loss: 0.0756, Val Loss: 0.0752\n",
      "Epoch 28/300 - Train Loss: 0.0746, Val Loss: 0.0762\n",
      "Epoch 29/300 - Train Loss: 0.0756, Val Loss: 0.0704\n",
      "Epoch 30/300 - Train Loss: 0.0750, Val Loss: 0.0848\n",
      "Epoch 31/300 - Train Loss: 0.0739, Val Loss: 0.0706\n",
      "Epoch 32/300 - Train Loss: 0.0743, Val Loss: 0.0687\n",
      "Epoch 33/300 - Train Loss: 0.0726, Val Loss: 0.0681\n",
      "Epoch 34/300 - Train Loss: 0.0740, Val Loss: 0.0777\n",
      "Epoch 35/300 - Train Loss: 0.0737, Val Loss: 0.0674\n",
      "Epoch 36/300 - Train Loss: 0.0728, Val Loss: 0.0750\n",
      "Epoch 37/300 - Train Loss: 0.0739, Val Loss: 0.0748\n",
      "Epoch 38/300 - Train Loss: 0.0729, Val Loss: 0.0752\n",
      "Epoch 39/300 - Train Loss: 0.0734, Val Loss: 0.0717\n",
      "Epoch 40/300 - Train Loss: 0.0734, Val Loss: 0.0722\n",
      "Epoch 41/300 - Train Loss: 0.0737, Val Loss: 0.0695\n",
      "Epoch 42/300 - Train Loss: 0.0735, Val Loss: 0.0718\n",
      "Epoch 43/300 - Train Loss: 0.0734, Val Loss: 0.0690\n",
      "Epoch 44/300 - Train Loss: 0.0735, Val Loss: 0.0714\n",
      "Epoch 45/300 - Train Loss: 0.0731, Val Loss: 0.0750\n",
      "Epoch 46/300 - Train Loss: 0.0724, Val Loss: 0.0689\n",
      "Epoch 47/300 - Train Loss: 0.0740, Val Loss: 0.0668\n",
      "Epoch 48/300 - Train Loss: 0.0732, Val Loss: 0.0699\n",
      "Epoch 49/300 - Train Loss: 0.0748, Val Loss: 0.0691\n",
      "Epoch 50/300 - Train Loss: 0.0731, Val Loss: 0.0692\n",
      "Epoch 51/300 - Train Loss: 0.0750, Val Loss: 0.0665\n",
      "Epoch 52/300 - Train Loss: 0.0752, Val Loss: 0.0689\n",
      "Epoch 53/300 - Train Loss: 0.0725, Val Loss: 0.0712\n",
      "Epoch 54/300 - Train Loss: 0.0730, Val Loss: 0.0708\n",
      "Epoch 55/300 - Train Loss: 0.0732, Val Loss: 0.0673\n",
      "Epoch 56/300 - Train Loss: 0.0732, Val Loss: 0.0694\n",
      "Epoch 57/300 - Train Loss: 0.0737, Val Loss: 0.0693\n",
      "Epoch 58/300 - Train Loss: 0.0720, Val Loss: 0.0671\n",
      "Epoch 59/300 - Train Loss: 0.0740, Val Loss: 0.0679\n",
      "Epoch 60/300 - Train Loss: 0.0729, Val Loss: 0.0684\n",
      "Epoch 61/300 - Train Loss: 0.0748, Val Loss: 0.0665\n",
      "Epoch 62/300 - Train Loss: 0.0739, Val Loss: 0.0698\n",
      "Epoch 63/300 - Train Loss: 0.0749, Val Loss: 0.0690\n",
      "Epoch 64/300 - Train Loss: 0.0745, Val Loss: 0.0684\n",
      "Epoch 65/300 - Train Loss: 0.0746, Val Loss: 0.0676\n",
      "Epoch 66/300 - Train Loss: 0.0732, Val Loss: 0.0679\n",
      "Epoch 67/300 - Train Loss: 0.0729, Val Loss: 0.0667\n",
      "Epoch 68/300 - Train Loss: 0.0743, Val Loss: 0.0651\n",
      "Epoch 69/300 - Train Loss: 0.0741, Val Loss: 0.0684\n",
      "Epoch 70/300 - Train Loss: 0.0727, Val Loss: 0.0662\n",
      "Epoch 71/300 - Train Loss: 0.0747, Val Loss: 0.0688\n",
      "Epoch 72/300 - Train Loss: 0.0761, Val Loss: 0.0665\n",
      "Epoch 73/300 - Train Loss: 0.0744, Val Loss: 0.0674\n",
      "Epoch 74/300 - Train Loss: 0.0735, Val Loss: 0.0670\n",
      "Epoch 75/300 - Train Loss: 0.0742, Val Loss: 0.0694\n",
      "Epoch 76/300 - Train Loss: 0.0732, Val Loss: 0.0702\n",
      "Epoch 77/300 - Train Loss: 0.0737, Val Loss: 0.0665\n",
      "Epoch 78/300 - Train Loss: 0.0758, Val Loss: 0.0655\n",
      "Epoch 79/300 - Train Loss: 0.0735, Val Loss: 0.0685\n",
      "Epoch 80/300 - Train Loss: 0.0746, Val Loss: 0.0677\n",
      "Epoch 81/300 - Train Loss: 0.0766, Val Loss: 0.0684\n",
      "Epoch 82/300 - Train Loss: 0.0747, Val Loss: 0.0659\n",
      "Epoch 83/300 - Train Loss: 0.0731, Val Loss: 0.0708\n",
      "Epoch 84/300 - Train Loss: 0.0750, Val Loss: 0.0667\n",
      "Epoch 85/300 - Train Loss: 0.0760, Val Loss: 0.0706\n",
      "Epoch 86/300 - Train Loss: 0.0748, Val Loss: 0.0683\n",
      "Epoch 87/300 - Train Loss: 0.0736, Val Loss: 0.0697\n",
      "Epoch 88/300 - Train Loss: 0.0744, Val Loss: 0.0701\n",
      "Epoch 89/300 - Train Loss: 0.0774, Val Loss: 0.0672\n",
      "Epoch 90/300 - Train Loss: 0.0735, Val Loss: 0.0651\n",
      "Epoch 91/300 - Train Loss: 0.0756, Val Loss: 0.0732\n",
      "Epoch 92/300 - Train Loss: 0.0765, Val Loss: 0.0675\n",
      "Epoch 93/300 - Train Loss: 0.0739, Val Loss: 0.0671\n",
      "Epoch 94/300 - Train Loss: 0.0750, Val Loss: 0.0696\n",
      "Epoch 95/300 - Train Loss: 0.0757, Val Loss: 0.0668\n",
      "Epoch 96/300 - Train Loss: 0.0754, Val Loss: 0.0697\n",
      "Epoch 97/300 - Train Loss: 0.0758, Val Loss: 0.0718\n",
      "Epoch 98/300 - Train Loss: 0.0741, Val Loss: 0.0705\n",
      "Epoch 99/300 - Train Loss: 0.0760, Val Loss: 0.0669\n",
      "Epoch 100/300 - Train Loss: 0.0750, Val Loss: 0.0818\n",
      "Epoch 101/300 - Train Loss: 0.0738, Val Loss: 0.0690\n",
      "Epoch 102/300 - Train Loss: 0.0749, Val Loss: 0.0673\n",
      "Epoch 103/300 - Train Loss: 0.0759, Val Loss: 0.0877\n",
      "Epoch 104/300 - Train Loss: 0.0750, Val Loss: 0.0681\n",
      "Epoch 105/300 - Train Loss: 0.0745, Val Loss: 0.0689\n",
      "Epoch 106/300 - Train Loss: 0.0755, Val Loss: 0.0669\n",
      "Epoch 107/300 - Train Loss: 0.0765, Val Loss: 0.0645\n",
      "Epoch 108/300 - Train Loss: 0.0770, Val Loss: 0.0722\n",
      "Epoch 109/300 - Train Loss: 0.0769, Val Loss: 0.0684\n",
      "Epoch 110/300 - Train Loss: 0.0761, Val Loss: 0.0688\n",
      "Epoch 111/300 - Train Loss: 0.0745, Val Loss: 0.0675\n",
      "Epoch 112/300 - Train Loss: 0.0760, Val Loss: 0.0714\n",
      "Epoch 113/300 - Train Loss: 0.0756, Val Loss: 0.0773\n",
      "Epoch 114/300 - Train Loss: 0.0762, Val Loss: 0.0775\n",
      "Epoch 115/300 - Train Loss: 0.0755, Val Loss: 0.0656\n",
      "Epoch 116/300 - Train Loss: 0.0760, Val Loss: 0.0700\n",
      "Epoch 117/300 - Train Loss: 0.0763, Val Loss: 0.0676\n",
      "Epoch 118/300 - Train Loss: 0.0761, Val Loss: 0.1042\n",
      "Epoch 119/300 - Train Loss: 0.0753, Val Loss: 0.0733\n",
      "Epoch 120/300 - Train Loss: 0.0763, Val Loss: 0.0722\n",
      "Epoch 121/300 - Train Loss: 0.0762, Val Loss: 0.0675\n",
      "Epoch 122/300 - Train Loss: 0.0748, Val Loss: 0.0739\n",
      "Epoch 123/300 - Train Loss: 0.0765, Val Loss: 0.0826\n",
      "Epoch 124/300 - Train Loss: 0.0766, Val Loss: 0.0699\n",
      "Epoch 125/300 - Train Loss: 0.0776, Val Loss: 0.0711\n",
      "Epoch 126/300 - Train Loss: 0.0762, Val Loss: 0.0731\n",
      "Epoch 127/300 - Train Loss: 0.0768, Val Loss: 0.0760\n",
      "Epoch 128/300 - Train Loss: 0.0778, Val Loss: 0.0643\n",
      "Epoch 129/300 - Train Loss: 0.0762, Val Loss: 0.0676\n",
      "Epoch 130/300 - Train Loss: 0.0775, Val Loss: 0.1209\n",
      "Epoch 131/300 - Train Loss: 0.0778, Val Loss: 0.0691\n",
      "Epoch 132/300 - Train Loss: 0.0770, Val Loss: 0.0967\n",
      "Epoch 133/300 - Train Loss: 0.0765, Val Loss: 0.0935\n",
      "Epoch 134/300 - Train Loss: 0.0769, Val Loss: 0.0868\n",
      "Epoch 135/300 - Train Loss: 0.0768, Val Loss: 0.0717\n",
      "Epoch 136/300 - Train Loss: 0.0763, Val Loss: 0.0800\n",
      "Epoch 137/300 - Train Loss: 0.0765, Val Loss: 0.0696\n",
      "Epoch 138/300 - Train Loss: 0.0779, Val Loss: 0.0736\n",
      "Epoch 139/300 - Train Loss: 0.0751, Val Loss: 0.0754\n",
      "Epoch 140/300 - Train Loss: 0.0776, Val Loss: 0.0708\n",
      "Epoch 141/300 - Train Loss: 0.0776, Val Loss: 0.0653\n",
      "Epoch 142/300 - Train Loss: 0.0775, Val Loss: 0.0778\n",
      "Epoch 143/300 - Train Loss: 0.0775, Val Loss: 0.0747\n",
      "Epoch 144/300 - Train Loss: 0.0756, Val Loss: 0.0765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0768, Val Loss: 0.0686\n",
      "Epoch 146/300 - Train Loss: 0.0768, Val Loss: 0.0661\n",
      "Epoch 147/300 - Train Loss: 0.0754, Val Loss: 0.0737\n",
      "Epoch 148/300 - Train Loss: 0.0756, Val Loss: 0.0675\n",
      "Epoch 149/300 - Train Loss: 0.0783, Val Loss: 0.0925\n",
      "Epoch 150/300 - Train Loss: 0.0770, Val Loss: 0.0681\n",
      "Epoch 151/300 - Train Loss: 0.0775, Val Loss: 0.0698\n",
      "Epoch 152/300 - Train Loss: 0.0758, Val Loss: 0.0676\n",
      "Epoch 153/300 - Train Loss: 0.0765, Val Loss: 0.0799\n",
      "Epoch 154/300 - Train Loss: 0.0786, Val Loss: 0.0845\n",
      "Epoch 155/300 - Train Loss: 0.0771, Val Loss: 0.0822\n",
      "Epoch 156/300 - Train Loss: 0.0778, Val Loss: 0.0776\n",
      "Epoch 157/300 - Train Loss: 0.0772, Val Loss: 0.0664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:29:46,500] Trial 87 finished with value: 0.9662227149482154 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.18860045331383207, 'learning_rate': 0.00013407550788849536, 'batch_size': 256, 'weight_decay': 0.0032817906573662323}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 158/300 - Train Loss: 0.0780, Val Loss: 0.0776\n",
      "Early stopping at epoch 158\n",
      "Macro F1 Score: 0.9662, Macro Precision: 0.9626, Macro Recall: 0.9701\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 89\n",
      "Training with F1=16, F2=32, D=8, dropout=0.3120813556275509, LR=7.246533092529531e-05, BS=256, WD=0.009955243463071023\n",
      "Epoch 1/300 - Train Loss: 0.6598, Val Loss: 0.3581\n",
      "Epoch 2/300 - Train Loss: 0.2809, Val Loss: 0.2427\n",
      "Epoch 3/300 - Train Loss: 0.2159, Val Loss: 0.1998\n",
      "Epoch 4/300 - Train Loss: 0.1866, Val Loss: 0.1820\n",
      "Epoch 5/300 - Train Loss: 0.1668, Val Loss: 0.1479\n",
      "Epoch 6/300 - Train Loss: 0.1472, Val Loss: 0.1204\n",
      "Epoch 7/300 - Train Loss: 0.1294, Val Loss: 0.1210\n",
      "Epoch 8/300 - Train Loss: 0.1169, Val Loss: 0.1039\n",
      "Epoch 9/300 - Train Loss: 0.1090, Val Loss: 0.0926\n",
      "Epoch 10/300 - Train Loss: 0.1023, Val Loss: 0.0895\n",
      "Epoch 11/300 - Train Loss: 0.0970, Val Loss: 0.0902\n",
      "Epoch 12/300 - Train Loss: 0.0938, Val Loss: 0.0849\n",
      "Epoch 13/300 - Train Loss: 0.0924, Val Loss: 0.0861\n",
      "Epoch 14/300 - Train Loss: 0.0900, Val Loss: 0.0832\n",
      "Epoch 15/300 - Train Loss: 0.0877, Val Loss: 0.0818\n",
      "Epoch 16/300 - Train Loss: 0.0861, Val Loss: 0.0770\n",
      "Epoch 17/300 - Train Loss: 0.0858, Val Loss: 0.0777\n",
      "Epoch 18/300 - Train Loss: 0.0849, Val Loss: 0.0780\n",
      "Epoch 19/300 - Train Loss: 0.0836, Val Loss: 0.0749\n",
      "Epoch 20/300 - Train Loss: 0.0835, Val Loss: 0.0736\n",
      "Epoch 21/300 - Train Loss: 0.0831, Val Loss: 0.0819\n",
      "Epoch 22/300 - Train Loss: 0.0825, Val Loss: 0.0820\n",
      "Epoch 23/300 - Train Loss: 0.0831, Val Loss: 0.0731\n",
      "Epoch 24/300 - Train Loss: 0.0830, Val Loss: 0.0804\n",
      "Epoch 25/300 - Train Loss: 0.0819, Val Loss: 0.0739\n",
      "Epoch 26/300 - Train Loss: 0.0820, Val Loss: 0.0720\n",
      "Epoch 27/300 - Train Loss: 0.0821, Val Loss: 0.0717\n",
      "Epoch 28/300 - Train Loss: 0.0808, Val Loss: 0.0784\n",
      "Epoch 29/300 - Train Loss: 0.0793, Val Loss: 0.0729\n",
      "Epoch 30/300 - Train Loss: 0.0810, Val Loss: 0.0741\n",
      "Epoch 31/300 - Train Loss: 0.0811, Val Loss: 0.0755\n",
      "Epoch 32/300 - Train Loss: 0.0804, Val Loss: 0.0695\n",
      "Epoch 33/300 - Train Loss: 0.0809, Val Loss: 0.0746\n",
      "Epoch 34/300 - Train Loss: 0.0799, Val Loss: 0.0709\n",
      "Epoch 35/300 - Train Loss: 0.0814, Val Loss: 0.0734\n",
      "Epoch 36/300 - Train Loss: 0.0804, Val Loss: 0.0740\n",
      "Epoch 37/300 - Train Loss: 0.0806, Val Loss: 0.0687\n",
      "Epoch 38/300 - Train Loss: 0.0807, Val Loss: 0.1069\n",
      "Epoch 39/300 - Train Loss: 0.0801, Val Loss: 0.0712\n",
      "Epoch 40/300 - Train Loss: 0.0803, Val Loss: 0.0745\n",
      "Epoch 41/300 - Train Loss: 0.0809, Val Loss: 0.0804\n",
      "Epoch 42/300 - Train Loss: 0.0802, Val Loss: 0.0734\n",
      "Epoch 43/300 - Train Loss: 0.0795, Val Loss: 0.0772\n",
      "Epoch 44/300 - Train Loss: 0.0803, Val Loss: 0.0731\n",
      "Epoch 45/300 - Train Loss: 0.0804, Val Loss: 0.0720\n",
      "Epoch 46/300 - Train Loss: 0.0787, Val Loss: 0.0697\n",
      "Epoch 47/300 - Train Loss: 0.0797, Val Loss: 0.0756\n",
      "Epoch 48/300 - Train Loss: 0.0792, Val Loss: 0.0735\n",
      "Epoch 49/300 - Train Loss: 0.0799, Val Loss: 0.0737\n",
      "Epoch 50/300 - Train Loss: 0.0798, Val Loss: 0.0753\n",
      "Epoch 51/300 - Train Loss: 0.0818, Val Loss: 0.0686\n",
      "Epoch 52/300 - Train Loss: 0.0809, Val Loss: 0.0725\n",
      "Epoch 53/300 - Train Loss: 0.0797, Val Loss: 0.0796\n",
      "Epoch 54/300 - Train Loss: 0.0811, Val Loss: 0.0735\n",
      "Epoch 55/300 - Train Loss: 0.0811, Val Loss: 0.0698\n",
      "Epoch 56/300 - Train Loss: 0.0823, Val Loss: 0.0785\n",
      "Epoch 57/300 - Train Loss: 0.0813, Val Loss: 0.0749\n",
      "Epoch 58/300 - Train Loss: 0.0786, Val Loss: 0.0759\n",
      "Epoch 59/300 - Train Loss: 0.0805, Val Loss: 0.0793\n",
      "Epoch 60/300 - Train Loss: 0.0805, Val Loss: 0.0676\n",
      "Epoch 61/300 - Train Loss: 0.0811, Val Loss: 0.0703\n",
      "Epoch 62/300 - Train Loss: 0.0809, Val Loss: 0.0736\n",
      "Epoch 63/300 - Train Loss: 0.0791, Val Loss: 0.0712\n",
      "Epoch 64/300 - Train Loss: 0.0813, Val Loss: 0.0705\n",
      "Epoch 65/300 - Train Loss: 0.0812, Val Loss: 0.0732\n",
      "Epoch 66/300 - Train Loss: 0.0822, Val Loss: 0.0700\n",
      "Epoch 67/300 - Train Loss: 0.0820, Val Loss: 0.0730\n",
      "Epoch 68/300 - Train Loss: 0.0806, Val Loss: 0.0721\n",
      "Epoch 69/300 - Train Loss: 0.0818, Val Loss: 0.0770\n",
      "Epoch 70/300 - Train Loss: 0.0796, Val Loss: 0.0707\n",
      "Epoch 71/300 - Train Loss: 0.0818, Val Loss: 0.0703\n",
      "Epoch 72/300 - Train Loss: 0.0818, Val Loss: 0.0709\n",
      "Epoch 73/300 - Train Loss: 0.0817, Val Loss: 0.0673\n",
      "Epoch 74/300 - Train Loss: 0.0832, Val Loss: 0.0716\n",
      "Epoch 75/300 - Train Loss: 0.0811, Val Loss: 0.0757\n",
      "Epoch 76/300 - Train Loss: 0.0796, Val Loss: 0.0743\n",
      "Epoch 77/300 - Train Loss: 0.0821, Val Loss: 0.0720\n",
      "Epoch 78/300 - Train Loss: 0.0822, Val Loss: 0.0692\n",
      "Epoch 79/300 - Train Loss: 0.0822, Val Loss: 0.0703\n",
      "Epoch 80/300 - Train Loss: 0.0823, Val Loss: 0.0734\n",
      "Epoch 81/300 - Train Loss: 0.0834, Val Loss: 0.0698\n",
      "Epoch 82/300 - Train Loss: 0.0817, Val Loss: 0.0709\n",
      "Epoch 83/300 - Train Loss: 0.0828, Val Loss: 0.0694\n",
      "Epoch 84/300 - Train Loss: 0.0823, Val Loss: 0.0695\n",
      "Epoch 85/300 - Train Loss: 0.0825, Val Loss: 0.0707\n",
      "Epoch 86/300 - Train Loss: 0.0839, Val Loss: 0.0708\n",
      "Epoch 87/300 - Train Loss: 0.0831, Val Loss: 0.0724\n",
      "Epoch 88/300 - Train Loss: 0.0825, Val Loss: 0.0720\n",
      "Epoch 89/300 - Train Loss: 0.0823, Val Loss: 0.0711\n",
      "Epoch 90/300 - Train Loss: 0.0824, Val Loss: 0.0696\n",
      "Epoch 91/300 - Train Loss: 0.0826, Val Loss: 0.0709\n",
      "Epoch 92/300 - Train Loss: 0.0849, Val Loss: 0.0706\n",
      "Epoch 93/300 - Train Loss: 0.0834, Val Loss: 0.0710\n",
      "Epoch 94/300 - Train Loss: 0.0846, Val Loss: 0.0713\n",
      "Epoch 95/300 - Train Loss: 0.0841, Val Loss: 0.0720\n",
      "Epoch 96/300 - Train Loss: 0.0850, Val Loss: 0.0697\n",
      "Epoch 97/300 - Train Loss: 0.0850, Val Loss: 0.0689\n",
      "Epoch 98/300 - Train Loss: 0.0849, Val Loss: 0.0703\n",
      "Epoch 99/300 - Train Loss: 0.0850, Val Loss: 0.0707\n",
      "Epoch 100/300 - Train Loss: 0.0850, Val Loss: 0.0701\n",
      "Epoch 101/300 - Train Loss: 0.0842, Val Loss: 0.0724\n",
      "Epoch 102/300 - Train Loss: 0.0852, Val Loss: 0.0711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:35:04,022] Trial 88 finished with value: 0.9726731789286028 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.3120813556275509, 'learning_rate': 7.246533092529531e-05, 'batch_size': 256, 'weight_decay': 0.009955243463071023}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/300 - Train Loss: 0.0867, Val Loss: 0.0716\n",
      "Early stopping at epoch 103\n",
      "Macro F1 Score: 0.9727, Macro Precision: 0.9767, Macro Recall: 0.9690\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 90\n",
      "Training with F1=16, F2=32, D=8, dropout=0.31329744433756235, LR=0.00010239133247413816, BS=256, WD=0.0018535860429203786\n",
      "Epoch 1/300 - Train Loss: 0.5283, Val Loss: 0.2504\n",
      "Epoch 2/300 - Train Loss: 0.2238, Val Loss: 0.1948\n",
      "Epoch 3/300 - Train Loss: 0.1671, Val Loss: 0.1456\n",
      "Epoch 4/300 - Train Loss: 0.1301, Val Loss: 0.1122\n",
      "Epoch 5/300 - Train Loss: 0.1114, Val Loss: 0.1029\n",
      "Epoch 6/300 - Train Loss: 0.1020, Val Loss: 0.0905\n",
      "Epoch 7/300 - Train Loss: 0.0973, Val Loss: 0.0860\n",
      "Epoch 8/300 - Train Loss: 0.0939, Val Loss: 0.0864\n",
      "Epoch 9/300 - Train Loss: 0.0924, Val Loss: 0.0752\n",
      "Epoch 10/300 - Train Loss: 0.0882, Val Loss: 0.0812\n",
      "Epoch 11/300 - Train Loss: 0.0872, Val Loss: 0.0844\n",
      "Epoch 12/300 - Train Loss: 0.0873, Val Loss: 0.0761\n",
      "Epoch 13/300 - Train Loss: 0.0853, Val Loss: 0.0782\n",
      "Epoch 14/300 - Train Loss: 0.0840, Val Loss: 0.0794\n",
      "Epoch 15/300 - Train Loss: 0.0833, Val Loss: 0.0805\n",
      "Epoch 16/300 - Train Loss: 0.0833, Val Loss: 0.0858\n",
      "Epoch 17/300 - Train Loss: 0.0811, Val Loss: 0.0855\n",
      "Epoch 18/300 - Train Loss: 0.0815, Val Loss: 0.0729\n",
      "Epoch 19/300 - Train Loss: 0.0809, Val Loss: 0.0772\n",
      "Epoch 20/300 - Train Loss: 0.0797, Val Loss: 0.0754\n",
      "Epoch 21/300 - Train Loss: 0.0802, Val Loss: 0.0756\n",
      "Epoch 22/300 - Train Loss: 0.0781, Val Loss: 0.0769\n",
      "Epoch 23/300 - Train Loss: 0.0772, Val Loss: 0.0702\n",
      "Epoch 24/300 - Train Loss: 0.0776, Val Loss: 0.0694\n",
      "Epoch 25/300 - Train Loss: 0.0780, Val Loss: 0.0723\n",
      "Epoch 26/300 - Train Loss: 0.0786, Val Loss: 0.0709\n",
      "Epoch 27/300 - Train Loss: 0.0772, Val Loss: 0.0735\n",
      "Epoch 28/300 - Train Loss: 0.0788, Val Loss: 0.0710\n",
      "Epoch 29/300 - Train Loss: 0.0762, Val Loss: 0.0763\n",
      "Epoch 30/300 - Train Loss: 0.0759, Val Loss: 0.0700\n",
      "Epoch 31/300 - Train Loss: 0.0751, Val Loss: 0.0702\n",
      "Epoch 32/300 - Train Loss: 0.0752, Val Loss: 0.0717\n",
      "Epoch 33/300 - Train Loss: 0.0755, Val Loss: 0.0707\n",
      "Epoch 34/300 - Train Loss: 0.0729, Val Loss: 0.0700\n",
      "Epoch 35/300 - Train Loss: 0.0752, Val Loss: 0.0668\n",
      "Epoch 36/300 - Train Loss: 0.0748, Val Loss: 0.0680\n",
      "Epoch 37/300 - Train Loss: 0.0742, Val Loss: 0.0707\n",
      "Epoch 38/300 - Train Loss: 0.0738, Val Loss: 0.0716\n",
      "Epoch 39/300 - Train Loss: 0.0743, Val Loss: 0.0725\n",
      "Epoch 40/300 - Train Loss: 0.0723, Val Loss: 0.0678\n",
      "Epoch 41/300 - Train Loss: 0.0752, Val Loss: 0.0706\n",
      "Epoch 42/300 - Train Loss: 0.0723, Val Loss: 0.0713\n",
      "Epoch 43/300 - Train Loss: 0.0725, Val Loss: 0.0723\n",
      "Epoch 44/300 - Train Loss: 0.0741, Val Loss: 0.0683\n",
      "Epoch 45/300 - Train Loss: 0.0722, Val Loss: 0.0677\n",
      "Epoch 46/300 - Train Loss: 0.0716, Val Loss: 0.0672\n",
      "Epoch 47/300 - Train Loss: 0.0728, Val Loss: 0.0669\n",
      "Epoch 48/300 - Train Loss: 0.0726, Val Loss: 0.0676\n",
      "Epoch 49/300 - Train Loss: 0.0721, Val Loss: 0.0694\n",
      "Epoch 50/300 - Train Loss: 0.0733, Val Loss: 0.0678\n",
      "Epoch 51/300 - Train Loss: 0.0749, Val Loss: 0.0727\n",
      "Epoch 52/300 - Train Loss: 0.0717, Val Loss: 0.0693\n",
      "Epoch 53/300 - Train Loss: 0.0735, Val Loss: 0.0637\n",
      "Epoch 54/300 - Train Loss: 0.0716, Val Loss: 0.0655\n",
      "Epoch 55/300 - Train Loss: 0.0734, Val Loss: 0.0653\n",
      "Epoch 56/300 - Train Loss: 0.0725, Val Loss: 0.0694\n",
      "Epoch 57/300 - Train Loss: 0.0711, Val Loss: 0.0694\n",
      "Epoch 58/300 - Train Loss: 0.0733, Val Loss: 0.0729\n",
      "Epoch 59/300 - Train Loss: 0.0730, Val Loss: 0.0667\n",
      "Epoch 60/300 - Train Loss: 0.0712, Val Loss: 0.0693\n",
      "Epoch 61/300 - Train Loss: 0.0701, Val Loss: 0.0674\n",
      "Epoch 62/300 - Train Loss: 0.0709, Val Loss: 0.0675\n",
      "Epoch 63/300 - Train Loss: 0.0716, Val Loss: 0.0679\n",
      "Epoch 64/300 - Train Loss: 0.0712, Val Loss: 0.0670\n",
      "Epoch 65/300 - Train Loss: 0.0716, Val Loss: 0.0661\n",
      "Epoch 66/300 - Train Loss: 0.0750, Val Loss: 0.0682\n",
      "Epoch 67/300 - Train Loss: 0.0716, Val Loss: 0.0684\n",
      "Epoch 68/300 - Train Loss: 0.0711, Val Loss: 0.0686\n",
      "Epoch 69/300 - Train Loss: 0.0708, Val Loss: 0.0686\n",
      "Epoch 70/300 - Train Loss: 0.0706, Val Loss: 0.0694\n",
      "Epoch 71/300 - Train Loss: 0.0700, Val Loss: 0.0699\n",
      "Epoch 72/300 - Train Loss: 0.0705, Val Loss: 0.0676\n",
      "Epoch 73/300 - Train Loss: 0.0700, Val Loss: 0.0674\n",
      "Epoch 74/300 - Train Loss: 0.0695, Val Loss: 0.0681\n",
      "Epoch 75/300 - Train Loss: 0.0721, Val Loss: 0.0681\n",
      "Epoch 76/300 - Train Loss: 0.0703, Val Loss: 0.0683\n",
      "Epoch 77/300 - Train Loss: 0.0707, Val Loss: 0.0687\n",
      "Epoch 78/300 - Train Loss: 0.0704, Val Loss: 0.0659\n",
      "Epoch 79/300 - Train Loss: 0.0705, Val Loss: 0.0683\n",
      "Epoch 80/300 - Train Loss: 0.0708, Val Loss: 0.0680\n",
      "Epoch 81/300 - Train Loss: 0.0699, Val Loss: 0.0684\n",
      "Epoch 82/300 - Train Loss: 0.0700, Val Loss: 0.0668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:39:20,083] Trial 89 finished with value: 0.967201691759967 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.31329744433756235, 'learning_rate': 0.00010239133247413816, 'batch_size': 256, 'weight_decay': 0.0018535860429203786}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/300 - Train Loss: 0.0696, Val Loss: 0.0658\n",
      "Early stopping at epoch 83\n",
      "Macro F1 Score: 0.9672, Macro Precision: 0.9639, Macro Recall: 0.9708\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 91\n",
      "Training with F1=4, F2=32, D=8, dropout=0.28327942057909794, LR=5.522409426416756e-05, BS=256, WD=0.006934822179330014\n",
      "Epoch 1/300 - Train Loss: 0.7885, Val Loss: 0.5294\n",
      "Epoch 2/300 - Train Loss: 0.3905, Val Loss: 0.3070\n",
      "Epoch 3/300 - Train Loss: 0.2776, Val Loss: 0.2405\n",
      "Epoch 4/300 - Train Loss: 0.2247, Val Loss: 0.1968\n",
      "Epoch 5/300 - Train Loss: 0.1941, Val Loss: 0.1679\n",
      "Epoch 6/300 - Train Loss: 0.1746, Val Loss: 0.1498\n",
      "Epoch 7/300 - Train Loss: 0.1632, Val Loss: 0.1383\n",
      "Epoch 8/300 - Train Loss: 0.1510, Val Loss: 0.1294\n",
      "Epoch 9/300 - Train Loss: 0.1418, Val Loss: 0.1201\n",
      "Epoch 10/300 - Train Loss: 0.1355, Val Loss: 0.1146\n",
      "Epoch 11/300 - Train Loss: 0.1294, Val Loss: 0.1097\n",
      "Epoch 12/300 - Train Loss: 0.1222, Val Loss: 0.1068\n",
      "Epoch 13/300 - Train Loss: 0.1151, Val Loss: 0.1052\n",
      "Epoch 14/300 - Train Loss: 0.1124, Val Loss: 0.1013\n",
      "Epoch 15/300 - Train Loss: 0.1099, Val Loss: 0.0996\n",
      "Epoch 16/300 - Train Loss: 0.1074, Val Loss: 0.0981\n",
      "Epoch 17/300 - Train Loss: 0.1059, Val Loss: 0.0957\n",
      "Epoch 18/300 - Train Loss: 0.1030, Val Loss: 0.0954\n",
      "Epoch 19/300 - Train Loss: 0.1021, Val Loss: 0.0906\n",
      "Epoch 20/300 - Train Loss: 0.1012, Val Loss: 0.0901\n",
      "Epoch 21/300 - Train Loss: 0.0999, Val Loss: 0.0874\n",
      "Epoch 22/300 - Train Loss: 0.0997, Val Loss: 0.0845\n",
      "Epoch 23/300 - Train Loss: 0.0990, Val Loss: 0.0871\n",
      "Epoch 24/300 - Train Loss: 0.0964, Val Loss: 0.0886\n",
      "Epoch 25/300 - Train Loss: 0.0973, Val Loss: 0.0870\n",
      "Epoch 26/300 - Train Loss: 0.0969, Val Loss: 0.0849\n",
      "Epoch 27/300 - Train Loss: 0.0970, Val Loss: 0.0866\n",
      "Epoch 28/300 - Train Loss: 0.0943, Val Loss: 0.0837\n",
      "Epoch 29/300 - Train Loss: 0.0939, Val Loss: 0.0827\n",
      "Epoch 30/300 - Train Loss: 0.0953, Val Loss: 0.0824\n",
      "Epoch 31/300 - Train Loss: 0.0948, Val Loss: 0.0822\n",
      "Epoch 32/300 - Train Loss: 0.0924, Val Loss: 0.0806\n",
      "Epoch 33/300 - Train Loss: 0.0963, Val Loss: 0.0820\n",
      "Epoch 34/300 - Train Loss: 0.0944, Val Loss: 0.0801\n",
      "Epoch 35/300 - Train Loss: 0.0942, Val Loss: 0.0815\n",
      "Epoch 36/300 - Train Loss: 0.0930, Val Loss: 0.0811\n",
      "Epoch 37/300 - Train Loss: 0.0947, Val Loss: 0.0813\n",
      "Epoch 38/300 - Train Loss: 0.0945, Val Loss: 0.0804\n",
      "Epoch 39/300 - Train Loss: 0.0924, Val Loss: 0.0821\n",
      "Epoch 40/300 - Train Loss: 0.0906, Val Loss: 0.0796\n",
      "Epoch 41/300 - Train Loss: 0.0911, Val Loss: 0.0803\n",
      "Epoch 42/300 - Train Loss: 0.0906, Val Loss: 0.0786\n",
      "Epoch 43/300 - Train Loss: 0.0924, Val Loss: 0.0775\n",
      "Epoch 44/300 - Train Loss: 0.0899, Val Loss: 0.0773\n",
      "Epoch 45/300 - Train Loss: 0.0902, Val Loss: 0.0807\n",
      "Epoch 46/300 - Train Loss: 0.0904, Val Loss: 0.0803\n",
      "Epoch 47/300 - Train Loss: 0.0920, Val Loss: 0.0776\n",
      "Epoch 48/300 - Train Loss: 0.0893, Val Loss: 0.0793\n",
      "Epoch 49/300 - Train Loss: 0.0903, Val Loss: 0.0799\n",
      "Epoch 50/300 - Train Loss: 0.0915, Val Loss: 0.0785\n",
      "Epoch 51/300 - Train Loss: 0.0898, Val Loss: 0.0778\n",
      "Epoch 52/300 - Train Loss: 0.0897, Val Loss: 0.0769\n",
      "Epoch 53/300 - Train Loss: 0.0914, Val Loss: 0.0777\n",
      "Epoch 54/300 - Train Loss: 0.0904, Val Loss: 0.0783\n",
      "Epoch 55/300 - Train Loss: 0.0903, Val Loss: 0.0787\n",
      "Epoch 56/300 - Train Loss: 0.0883, Val Loss: 0.0780\n",
      "Epoch 57/300 - Train Loss: 0.0895, Val Loss: 0.0782\n",
      "Epoch 58/300 - Train Loss: 0.0904, Val Loss: 0.0791\n",
      "Epoch 59/300 - Train Loss: 0.0888, Val Loss: 0.0772\n",
      "Epoch 60/300 - Train Loss: 0.0891, Val Loss: 0.0789\n",
      "Epoch 61/300 - Train Loss: 0.0875, Val Loss: 0.0762\n",
      "Epoch 62/300 - Train Loss: 0.0893, Val Loss: 0.0772\n",
      "Epoch 63/300 - Train Loss: 0.0910, Val Loss: 0.0758\n",
      "Epoch 64/300 - Train Loss: 0.0904, Val Loss: 0.0769\n",
      "Epoch 65/300 - Train Loss: 0.0892, Val Loss: 0.0776\n",
      "Epoch 66/300 - Train Loss: 0.0898, Val Loss: 0.0784\n",
      "Epoch 67/300 - Train Loss: 0.0885, Val Loss: 0.0765\n",
      "Epoch 68/300 - Train Loss: 0.0894, Val Loss: 0.0778\n",
      "Epoch 69/300 - Train Loss: 0.0896, Val Loss: 0.0762\n",
      "Epoch 70/300 - Train Loss: 0.0902, Val Loss: 0.0757\n",
      "Epoch 71/300 - Train Loss: 0.0903, Val Loss: 0.0765\n",
      "Epoch 72/300 - Train Loss: 0.0902, Val Loss: 0.0778\n",
      "Epoch 73/300 - Train Loss: 0.0886, Val Loss: 0.0771\n",
      "Epoch 74/300 - Train Loss: 0.0905, Val Loss: 0.0759\n",
      "Epoch 75/300 - Train Loss: 0.0899, Val Loss: 0.0769\n",
      "Epoch 76/300 - Train Loss: 0.0905, Val Loss: 0.0776\n",
      "Epoch 77/300 - Train Loss: 0.0890, Val Loss: 0.0771\n",
      "Epoch 78/300 - Train Loss: 0.0895, Val Loss: 0.0759\n",
      "Epoch 79/300 - Train Loss: 0.0899, Val Loss: 0.0776\n",
      "Epoch 80/300 - Train Loss: 0.0887, Val Loss: 0.0765\n",
      "Epoch 81/300 - Train Loss: 0.0886, Val Loss: 0.0776\n",
      "Epoch 82/300 - Train Loss: 0.0896, Val Loss: 0.0784\n",
      "Epoch 83/300 - Train Loss: 0.0900, Val Loss: 0.0785\n",
      "Epoch 84/300 - Train Loss: 0.0906, Val Loss: 0.0768\n",
      "Epoch 85/300 - Train Loss: 0.0888, Val Loss: 0.0759\n",
      "Epoch 86/300 - Train Loss: 0.0905, Val Loss: 0.0770\n",
      "Epoch 87/300 - Train Loss: 0.0909, Val Loss: 0.0762\n",
      "Epoch 88/300 - Train Loss: 0.0906, Val Loss: 0.0763\n",
      "Epoch 89/300 - Train Loss: 0.0886, Val Loss: 0.0776\n",
      "Epoch 90/300 - Train Loss: 0.0898, Val Loss: 0.0771\n",
      "Epoch 91/300 - Train Loss: 0.0919, Val Loss: 0.0760\n",
      "Epoch 92/300 - Train Loss: 0.0941, Val Loss: 0.0766\n",
      "Epoch 93/300 - Train Loss: 0.0917, Val Loss: 0.0767\n",
      "Epoch 94/300 - Train Loss: 0.0899, Val Loss: 0.0771\n",
      "Epoch 95/300 - Train Loss: 0.0924, Val Loss: 0.0765\n",
      "Epoch 96/300 - Train Loss: 0.0896, Val Loss: 0.0770\n",
      "Epoch 97/300 - Train Loss: 0.0929, Val Loss: 0.0769\n",
      "Epoch 98/300 - Train Loss: 0.0908, Val Loss: 0.0769\n",
      "Epoch 99/300 - Train Loss: 0.0904, Val Loss: 0.0772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:41:20,612] Trial 90 finished with value: 0.9635009674173826 and parameters: {'F1': 4, 'F2': 32, 'D': 8, 'dropout': 0.28327942057909794, 'learning_rate': 5.522409426416756e-05, 'batch_size': 256, 'weight_decay': 0.006934822179330014}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100/300 - Train Loss: 0.0897, Val Loss: 0.0770\n",
      "Early stopping at epoch 100\n",
      "Macro F1 Score: 0.9635, Macro Precision: 0.9583, Macro Recall: 0.9693\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 92\n",
      "Training with F1=16, F2=32, D=8, dropout=0.20966786980412722, LR=7.183672149648235e-05, BS=256, WD=0.005657278763762858\n",
      "Epoch 1/300 - Train Loss: 0.6134, Val Loss: 0.3267\n",
      "Epoch 2/300 - Train Loss: 0.2705, Val Loss: 0.2273\n",
      "Epoch 3/300 - Train Loss: 0.2036, Val Loss: 0.1774\n",
      "Epoch 4/300 - Train Loss: 0.1718, Val Loss: 0.1465\n",
      "Epoch 5/300 - Train Loss: 0.1461, Val Loss: 0.1344\n",
      "Epoch 6/300 - Train Loss: 0.1280, Val Loss: 0.1157\n",
      "Epoch 7/300 - Train Loss: 0.1178, Val Loss: 0.1045\n",
      "Epoch 8/300 - Train Loss: 0.1077, Val Loss: 0.0993\n",
      "Epoch 9/300 - Train Loss: 0.1036, Val Loss: 0.1000\n",
      "Epoch 10/300 - Train Loss: 0.1008, Val Loss: 0.0903\n",
      "Epoch 11/300 - Train Loss: 0.0977, Val Loss: 0.0906\n",
      "Epoch 12/300 - Train Loss: 0.0953, Val Loss: 0.0976\n",
      "Epoch 13/300 - Train Loss: 0.0929, Val Loss: 0.0817\n",
      "Epoch 14/300 - Train Loss: 0.0928, Val Loss: 0.0866\n",
      "Epoch 15/300 - Train Loss: 0.0905, Val Loss: 0.0808\n",
      "Epoch 16/300 - Train Loss: 0.0882, Val Loss: 0.0822\n",
      "Epoch 17/300 - Train Loss: 0.0887, Val Loss: 0.0865\n",
      "Epoch 18/300 - Train Loss: 0.0869, Val Loss: 0.0808\n",
      "Epoch 19/300 - Train Loss: 0.0862, Val Loss: 0.0844\n",
      "Epoch 20/300 - Train Loss: 0.0855, Val Loss: 0.0772\n",
      "Epoch 21/300 - Train Loss: 0.0850, Val Loss: 0.0749\n",
      "Epoch 22/300 - Train Loss: 0.0836, Val Loss: 0.0825\n",
      "Epoch 23/300 - Train Loss: 0.0834, Val Loss: 0.0759\n",
      "Epoch 24/300 - Train Loss: 0.0827, Val Loss: 0.0742\n",
      "Epoch 25/300 - Train Loss: 0.0817, Val Loss: 0.0846\n",
      "Epoch 26/300 - Train Loss: 0.0827, Val Loss: 0.0712\n",
      "Epoch 27/300 - Train Loss: 0.0818, Val Loss: 0.0809\n",
      "Epoch 28/300 - Train Loss: 0.0793, Val Loss: 0.0809\n",
      "Epoch 29/300 - Train Loss: 0.0795, Val Loss: 0.0811\n",
      "Epoch 30/300 - Train Loss: 0.0796, Val Loss: 0.0740\n",
      "Epoch 31/300 - Train Loss: 0.0779, Val Loss: 0.0715\n",
      "Epoch 32/300 - Train Loss: 0.0787, Val Loss: 0.0748\n",
      "Epoch 33/300 - Train Loss: 0.0804, Val Loss: 0.0707\n",
      "Epoch 34/300 - Train Loss: 0.0806, Val Loss: 0.0758\n",
      "Epoch 35/300 - Train Loss: 0.0782, Val Loss: 0.0715\n",
      "Epoch 36/300 - Train Loss: 0.0776, Val Loss: 0.0748\n",
      "Epoch 37/300 - Train Loss: 0.0775, Val Loss: 0.0750\n",
      "Epoch 38/300 - Train Loss: 0.0787, Val Loss: 0.0707\n",
      "Epoch 39/300 - Train Loss: 0.0778, Val Loss: 0.0755\n",
      "Epoch 40/300 - Train Loss: 0.0778, Val Loss: 0.0779\n",
      "Epoch 41/300 - Train Loss: 0.0764, Val Loss: 0.0709\n",
      "Epoch 42/300 - Train Loss: 0.0779, Val Loss: 0.0765\n",
      "Epoch 43/300 - Train Loss: 0.0753, Val Loss: 0.0704\n",
      "Epoch 44/300 - Train Loss: 0.0752, Val Loss: 0.0721\n",
      "Epoch 45/300 - Train Loss: 0.0750, Val Loss: 0.0716\n",
      "Epoch 46/300 - Train Loss: 0.0769, Val Loss: 0.0707\n",
      "Epoch 47/300 - Train Loss: 0.0761, Val Loss: 0.0741\n",
      "Epoch 48/300 - Train Loss: 0.0757, Val Loss: 0.0698\n",
      "Epoch 49/300 - Train Loss: 0.0757, Val Loss: 0.0719\n",
      "Epoch 50/300 - Train Loss: 0.0763, Val Loss: 0.0717\n",
      "Epoch 51/300 - Train Loss: 0.0760, Val Loss: 0.0727\n",
      "Epoch 52/300 - Train Loss: 0.0744, Val Loss: 0.0719\n",
      "Epoch 53/300 - Train Loss: 0.0748, Val Loss: 0.0720\n",
      "Epoch 54/300 - Train Loss: 0.0750, Val Loss: 0.0757\n",
      "Epoch 55/300 - Train Loss: 0.0735, Val Loss: 0.0662\n",
      "Epoch 56/300 - Train Loss: 0.0757, Val Loss: 0.0681\n",
      "Epoch 57/300 - Train Loss: 0.0755, Val Loss: 0.0741\n",
      "Epoch 58/300 - Train Loss: 0.0752, Val Loss: 0.0843\n",
      "Epoch 59/300 - Train Loss: 0.0747, Val Loss: 0.0656\n",
      "Epoch 60/300 - Train Loss: 0.0743, Val Loss: 0.0673\n",
      "Epoch 61/300 - Train Loss: 0.0763, Val Loss: 0.0679\n",
      "Epoch 62/300 - Train Loss: 0.0766, Val Loss: 0.0729\n",
      "Epoch 63/300 - Train Loss: 0.0737, Val Loss: 0.0691\n",
      "Epoch 64/300 - Train Loss: 0.0742, Val Loss: 0.0718\n",
      "Epoch 65/300 - Train Loss: 0.0734, Val Loss: 0.0705\n",
      "Epoch 66/300 - Train Loss: 0.0747, Val Loss: 0.0676\n",
      "Epoch 67/300 - Train Loss: 0.0745, Val Loss: 0.0710\n",
      "Epoch 68/300 - Train Loss: 0.0743, Val Loss: 0.0662\n",
      "Epoch 69/300 - Train Loss: 0.0730, Val Loss: 0.0672\n",
      "Epoch 70/300 - Train Loss: 0.0739, Val Loss: 0.0702\n",
      "Epoch 71/300 - Train Loss: 0.0768, Val Loss: 0.0717\n",
      "Epoch 72/300 - Train Loss: 0.0734, Val Loss: 0.0696\n",
      "Epoch 73/300 - Train Loss: 0.0736, Val Loss: 0.0675\n",
      "Epoch 74/300 - Train Loss: 0.0757, Val Loss: 0.0701\n",
      "Epoch 75/300 - Train Loss: 0.0748, Val Loss: 0.0682\n",
      "Epoch 76/300 - Train Loss: 0.0748, Val Loss: 0.0674\n",
      "Epoch 77/300 - Train Loss: 0.0751, Val Loss: 0.0664\n",
      "Epoch 78/300 - Train Loss: 0.0754, Val Loss: 0.0731\n",
      "Epoch 79/300 - Train Loss: 0.0749, Val Loss: 0.0714\n",
      "Epoch 80/300 - Train Loss: 0.0750, Val Loss: 0.0694\n",
      "Epoch 81/300 - Train Loss: 0.0749, Val Loss: 0.0692\n",
      "Epoch 82/300 - Train Loss: 0.0737, Val Loss: 0.0684\n",
      "Epoch 83/300 - Train Loss: 0.0756, Val Loss: 0.0681\n",
      "Epoch 84/300 - Train Loss: 0.0764, Val Loss: 0.0686\n",
      "Epoch 85/300 - Train Loss: 0.0753, Val Loss: 0.0669\n",
      "Epoch 86/300 - Train Loss: 0.0735, Val Loss: 0.0686\n",
      "Epoch 87/300 - Train Loss: 0.0767, Val Loss: 0.0675\n",
      "Epoch 88/300 - Train Loss: 0.0752, Val Loss: 0.0690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:45:55,038] Trial 91 finished with value: 0.9695349040099591 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.20966786980412722, 'learning_rate': 7.183672149648235e-05, 'batch_size': 256, 'weight_decay': 0.005657278763762858}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/300 - Train Loss: 0.0772, Val Loss: 0.0670\n",
      "Early stopping at epoch 89\n",
      "Macro F1 Score: 0.9695, Macro Precision: 0.9683, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 93\n",
      "Training with F1=16, F2=32, D=8, dropout=0.30069723687157707, LR=9.351459472101092e-05, BS=256, WD=0.004624422932465172\n",
      "Epoch 1/300 - Train Loss: 0.5099, Val Loss: 0.2825\n",
      "Epoch 2/300 - Train Loss: 0.2280, Val Loss: 0.1977\n",
      "Epoch 3/300 - Train Loss: 0.1790, Val Loss: 0.1620\n",
      "Epoch 4/300 - Train Loss: 0.1466, Val Loss: 0.1215\n",
      "Epoch 5/300 - Train Loss: 0.1186, Val Loss: 0.1095\n",
      "Epoch 6/300 - Train Loss: 0.1036, Val Loss: 0.0918\n",
      "Epoch 7/300 - Train Loss: 0.0985, Val Loss: 0.0984\n",
      "Epoch 8/300 - Train Loss: 0.0954, Val Loss: 0.0867\n",
      "Epoch 9/300 - Train Loss: 0.0927, Val Loss: 0.0936\n",
      "Epoch 10/300 - Train Loss: 0.0910, Val Loss: 0.0937\n",
      "Epoch 11/300 - Train Loss: 0.0882, Val Loss: 0.0918\n",
      "Epoch 12/300 - Train Loss: 0.0861, Val Loss: 0.0787\n",
      "Epoch 13/300 - Train Loss: 0.0866, Val Loss: 0.0860\n",
      "Epoch 14/300 - Train Loss: 0.0853, Val Loss: 0.0778\n",
      "Epoch 15/300 - Train Loss: 0.0845, Val Loss: 0.0796\n",
      "Epoch 16/300 - Train Loss: 0.0831, Val Loss: 0.0745\n",
      "Epoch 17/300 - Train Loss: 0.0826, Val Loss: 0.0728\n",
      "Epoch 18/300 - Train Loss: 0.0832, Val Loss: 0.0772\n",
      "Epoch 19/300 - Train Loss: 0.0852, Val Loss: 0.0819\n",
      "Epoch 20/300 - Train Loss: 0.0826, Val Loss: 0.0792\n",
      "Epoch 21/300 - Train Loss: 0.0804, Val Loss: 0.0778\n",
      "Epoch 22/300 - Train Loss: 0.0787, Val Loss: 0.0687\n",
      "Epoch 23/300 - Train Loss: 0.0814, Val Loss: 0.0795\n",
      "Epoch 24/300 - Train Loss: 0.0797, Val Loss: 0.0712\n",
      "Epoch 25/300 - Train Loss: 0.0785, Val Loss: 0.0692\n",
      "Epoch 26/300 - Train Loss: 0.0786, Val Loss: 0.0760\n",
      "Epoch 27/300 - Train Loss: 0.0790, Val Loss: 0.0802\n",
      "Epoch 28/300 - Train Loss: 0.0799, Val Loss: 0.0709\n",
      "Epoch 29/300 - Train Loss: 0.0789, Val Loss: 0.0741\n",
      "Epoch 30/300 - Train Loss: 0.0799, Val Loss: 0.0673\n",
      "Epoch 31/300 - Train Loss: 0.0778, Val Loss: 0.0701\n",
      "Epoch 32/300 - Train Loss: 0.0777, Val Loss: 0.0707\n",
      "Epoch 33/300 - Train Loss: 0.0774, Val Loss: 0.0677\n",
      "Epoch 34/300 - Train Loss: 0.0769, Val Loss: 0.0697\n",
      "Epoch 35/300 - Train Loss: 0.0774, Val Loss: 0.0781\n",
      "Epoch 36/300 - Train Loss: 0.0770, Val Loss: 0.0696\n",
      "Epoch 37/300 - Train Loss: 0.0771, Val Loss: 0.0782\n",
      "Epoch 38/300 - Train Loss: 0.0799, Val Loss: 0.0698\n",
      "Epoch 39/300 - Train Loss: 0.0779, Val Loss: 0.0711\n",
      "Epoch 40/300 - Train Loss: 0.0772, Val Loss: 0.0727\n",
      "Epoch 41/300 - Train Loss: 0.0770, Val Loss: 0.0728\n",
      "Epoch 42/300 - Train Loss: 0.0764, Val Loss: 0.0794\n",
      "Epoch 43/300 - Train Loss: 0.0793, Val Loss: 0.0685\n",
      "Epoch 44/300 - Train Loss: 0.0767, Val Loss: 0.0726\n",
      "Epoch 45/300 - Train Loss: 0.0772, Val Loss: 0.0757\n",
      "Epoch 46/300 - Train Loss: 0.0756, Val Loss: 0.0694\n",
      "Epoch 47/300 - Train Loss: 0.0762, Val Loss: 0.0705\n",
      "Epoch 48/300 - Train Loss: 0.0754, Val Loss: 0.0738\n",
      "Epoch 49/300 - Train Loss: 0.0757, Val Loss: 0.0697\n",
      "Epoch 50/300 - Train Loss: 0.0769, Val Loss: 0.0698\n",
      "Epoch 51/300 - Train Loss: 0.0760, Val Loss: 0.0705\n",
      "Epoch 52/300 - Train Loss: 0.0779, Val Loss: 0.0696\n",
      "Epoch 53/300 - Train Loss: 0.0777, Val Loss: 0.0688\n",
      "Epoch 54/300 - Train Loss: 0.0771, Val Loss: 0.0697\n",
      "Epoch 55/300 - Train Loss: 0.0757, Val Loss: 0.0688\n",
      "Epoch 56/300 - Train Loss: 0.0765, Val Loss: 0.0713\n",
      "Epoch 57/300 - Train Loss: 0.0758, Val Loss: 0.0700\n",
      "Epoch 58/300 - Train Loss: 0.0764, Val Loss: 0.0693\n",
      "Epoch 59/300 - Train Loss: 0.0754, Val Loss: 0.0725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:49:00,115] Trial 92 finished with value: 0.967201691759967 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.30069723687157707, 'learning_rate': 9.351459472101092e-05, 'batch_size': 256, 'weight_decay': 0.004624422932465172}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300 - Train Loss: 0.0766, Val Loss: 0.0701\n",
      "Early stopping at epoch 60\n",
      "Macro F1 Score: 0.9672, Macro Precision: 0.9639, Macro Recall: 0.9708\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 94\n",
      "Training with F1=16, F2=32, D=8, dropout=0.24295214941672513, LR=6.245227718671456e-05, BS=256, WD=0.009797754115208765\n",
      "Epoch 1/300 - Train Loss: 0.6457, Val Loss: 0.3342\n",
      "Epoch 2/300 - Train Loss: 0.2717, Val Loss: 0.2247\n",
      "Epoch 3/300 - Train Loss: 0.2056, Val Loss: 0.1863\n",
      "Epoch 4/300 - Train Loss: 0.1668, Val Loss: 0.1494\n",
      "Epoch 5/300 - Train Loss: 0.1383, Val Loss: 0.1245\n",
      "Epoch 6/300 - Train Loss: 0.1219, Val Loss: 0.1059\n",
      "Epoch 7/300 - Train Loss: 0.1113, Val Loss: 0.1044\n",
      "Epoch 8/300 - Train Loss: 0.1042, Val Loss: 0.1060\n",
      "Epoch 9/300 - Train Loss: 0.0986, Val Loss: 0.0975\n",
      "Epoch 10/300 - Train Loss: 0.0960, Val Loss: 0.0886\n",
      "Epoch 11/300 - Train Loss: 0.0942, Val Loss: 0.0863\n",
      "Epoch 12/300 - Train Loss: 0.0920, Val Loss: 0.0795\n",
      "Epoch 13/300 - Train Loss: 0.0912, Val Loss: 0.0811\n",
      "Epoch 14/300 - Train Loss: 0.0901, Val Loss: 0.0847\n",
      "Epoch 15/300 - Train Loss: 0.0887, Val Loss: 0.0894\n",
      "Epoch 16/300 - Train Loss: 0.0871, Val Loss: 0.0939\n",
      "Epoch 17/300 - Train Loss: 0.0879, Val Loss: 0.0794\n",
      "Epoch 18/300 - Train Loss: 0.0862, Val Loss: 0.0805\n",
      "Epoch 19/300 - Train Loss: 0.0862, Val Loss: 0.0759\n",
      "Epoch 20/300 - Train Loss: 0.0848, Val Loss: 0.0779\n",
      "Epoch 21/300 - Train Loss: 0.0862, Val Loss: 0.0790\n",
      "Epoch 22/300 - Train Loss: 0.0869, Val Loss: 0.0837\n",
      "Epoch 23/300 - Train Loss: 0.0849, Val Loss: 0.0776\n",
      "Epoch 24/300 - Train Loss: 0.0825, Val Loss: 0.0738\n",
      "Epoch 25/300 - Train Loss: 0.0820, Val Loss: 0.0772\n",
      "Epoch 26/300 - Train Loss: 0.0820, Val Loss: 0.0745\n",
      "Epoch 27/300 - Train Loss: 0.0833, Val Loss: 0.0810\n",
      "Epoch 28/300 - Train Loss: 0.0831, Val Loss: 0.0853\n",
      "Epoch 29/300 - Train Loss: 0.0806, Val Loss: 0.0724\n",
      "Epoch 30/300 - Train Loss: 0.0808, Val Loss: 0.0698\n",
      "Epoch 31/300 - Train Loss: 0.0806, Val Loss: 0.0725\n",
      "Epoch 32/300 - Train Loss: 0.0805, Val Loss: 0.0684\n",
      "Epoch 33/300 - Train Loss: 0.0789, Val Loss: 0.0707\n",
      "Epoch 34/300 - Train Loss: 0.0782, Val Loss: 0.0686\n",
      "Epoch 35/300 - Train Loss: 0.0808, Val Loss: 0.0711\n",
      "Epoch 36/300 - Train Loss: 0.0788, Val Loss: 0.0890\n",
      "Epoch 37/300 - Train Loss: 0.0773, Val Loss: 0.0737\n",
      "Epoch 38/300 - Train Loss: 0.0783, Val Loss: 0.0782\n",
      "Epoch 39/300 - Train Loss: 0.0773, Val Loss: 0.0681\n",
      "Epoch 40/300 - Train Loss: 0.0771, Val Loss: 0.0710\n",
      "Epoch 41/300 - Train Loss: 0.0766, Val Loss: 0.0674\n",
      "Epoch 42/300 - Train Loss: 0.0782, Val Loss: 0.0692\n",
      "Epoch 43/300 - Train Loss: 0.0758, Val Loss: 0.0709\n",
      "Epoch 44/300 - Train Loss: 0.0779, Val Loss: 0.0707\n",
      "Epoch 45/300 - Train Loss: 0.0767, Val Loss: 0.0681\n",
      "Epoch 46/300 - Train Loss: 0.0761, Val Loss: 0.0716\n",
      "Epoch 47/300 - Train Loss: 0.0758, Val Loss: 0.0664\n",
      "Epoch 48/300 - Train Loss: 0.0772, Val Loss: 0.0655\n",
      "Epoch 49/300 - Train Loss: 0.0762, Val Loss: 0.0690\n",
      "Epoch 50/300 - Train Loss: 0.0752, Val Loss: 0.0673\n",
      "Epoch 51/300 - Train Loss: 0.0763, Val Loss: 0.0673\n",
      "Epoch 52/300 - Train Loss: 0.0779, Val Loss: 0.0710\n",
      "Epoch 53/300 - Train Loss: 0.0791, Val Loss: 0.0739\n",
      "Epoch 54/300 - Train Loss: 0.0769, Val Loss: 0.0738\n",
      "Epoch 55/300 - Train Loss: 0.0785, Val Loss: 0.0714\n",
      "Epoch 56/300 - Train Loss: 0.0773, Val Loss: 0.0673\n",
      "Epoch 57/300 - Train Loss: 0.0767, Val Loss: 0.0863\n",
      "Epoch 58/300 - Train Loss: 0.0765, Val Loss: 0.0748\n",
      "Epoch 59/300 - Train Loss: 0.0765, Val Loss: 0.0722\n",
      "Epoch 60/300 - Train Loss: 0.0767, Val Loss: 0.0735\n",
      "Epoch 61/300 - Train Loss: 0.0773, Val Loss: 0.0759\n",
      "Epoch 62/300 - Train Loss: 0.0771, Val Loss: 0.0692\n",
      "Epoch 63/300 - Train Loss: 0.0765, Val Loss: 0.0725\n",
      "Epoch 64/300 - Train Loss: 0.0744, Val Loss: 0.0696\n",
      "Epoch 65/300 - Train Loss: 0.0758, Val Loss: 0.0712\n",
      "Epoch 66/300 - Train Loss: 0.0763, Val Loss: 0.0786\n",
      "Epoch 67/300 - Train Loss: 0.0779, Val Loss: 0.0706\n",
      "Epoch 68/300 - Train Loss: 0.0766, Val Loss: 0.0693\n",
      "Epoch 69/300 - Train Loss: 0.0763, Val Loss: 0.0678\n",
      "Epoch 70/300 - Train Loss: 0.0774, Val Loss: 0.0690\n",
      "Epoch 71/300 - Train Loss: 0.0775, Val Loss: 0.0655\n",
      "Epoch 72/300 - Train Loss: 0.0780, Val Loss: 0.0731\n",
      "Epoch 73/300 - Train Loss: 0.0784, Val Loss: 0.0664\n",
      "Epoch 74/300 - Train Loss: 0.0759, Val Loss: 0.0686\n",
      "Epoch 75/300 - Train Loss: 0.0777, Val Loss: 0.0719\n",
      "Epoch 76/300 - Train Loss: 0.0771, Val Loss: 0.0692\n",
      "Epoch 77/300 - Train Loss: 0.0778, Val Loss: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:53:00,680] Trial 93 finished with value: 0.9697547760608843 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.24295214941672513, 'learning_rate': 6.245227718671456e-05, 'batch_size': 256, 'weight_decay': 0.009797754115208765}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300 - Train Loss: 0.0768, Val Loss: 0.0685\n",
      "Early stopping at epoch 78\n",
      "Macro F1 Score: 0.9698, Macro Precision: 0.9642, Macro Recall: 0.9759\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 95\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2249706630485494, LR=4.303679934837843e-05, BS=32, WD=0.007823883143519474\n",
      "Epoch 1/300 - Train Loss: 0.3340, Val Loss: 0.1632\n",
      "Epoch 2/300 - Train Loss: 0.1564, Val Loss: 0.1217\n",
      "Epoch 3/300 - Train Loss: 0.1209, Val Loss: 0.1031\n",
      "Epoch 4/300 - Train Loss: 0.1073, Val Loss: 0.0889\n",
      "Epoch 5/300 - Train Loss: 0.1040, Val Loss: 0.0793\n",
      "Epoch 6/300 - Train Loss: 0.0994, Val Loss: 0.0801\n",
      "Epoch 7/300 - Train Loss: 0.0971, Val Loss: 0.0799\n",
      "Epoch 8/300 - Train Loss: 0.0943, Val Loss: 0.0776\n",
      "Epoch 9/300 - Train Loss: 0.0959, Val Loss: 0.0740\n",
      "Epoch 10/300 - Train Loss: 0.0938, Val Loss: 0.0733\n",
      "Epoch 11/300 - Train Loss: 0.0936, Val Loss: 0.0722\n",
      "Epoch 12/300 - Train Loss: 0.0940, Val Loss: 0.0742\n",
      "Epoch 13/300 - Train Loss: 0.0914, Val Loss: 0.0788\n",
      "Epoch 14/300 - Train Loss: 0.0918, Val Loss: 0.0691\n",
      "Epoch 15/300 - Train Loss: 0.0942, Val Loss: 0.0766\n",
      "Epoch 16/300 - Train Loss: 0.0935, Val Loss: 0.0733\n",
      "Epoch 17/300 - Train Loss: 0.0918, Val Loss: 0.0775\n",
      "Epoch 18/300 - Train Loss: 0.0931, Val Loss: 0.0744\n",
      "Epoch 19/300 - Train Loss: 0.0943, Val Loss: 0.0745\n",
      "Epoch 20/300 - Train Loss: 0.0940, Val Loss: 0.0787\n",
      "Epoch 21/300 - Train Loss: 0.0933, Val Loss: 0.0735\n",
      "Epoch 22/300 - Train Loss: 0.0914, Val Loss: 0.0711\n",
      "Epoch 23/300 - Train Loss: 0.0923, Val Loss: 0.0721\n",
      "Epoch 24/300 - Train Loss: 0.0933, Val Loss: 0.0740\n",
      "Epoch 25/300 - Train Loss: 0.0930, Val Loss: 0.0717\n",
      "Epoch 26/300 - Train Loss: 0.0952, Val Loss: 0.0737\n",
      "Epoch 27/300 - Train Loss: 0.0923, Val Loss: 0.0736\n",
      "Epoch 28/300 - Train Loss: 0.0931, Val Loss: 0.0705\n",
      "Epoch 29/300 - Train Loss: 0.0920, Val Loss: 0.0733\n",
      "Epoch 30/300 - Train Loss: 0.0963, Val Loss: 0.0715\n",
      "Epoch 31/300 - Train Loss: 0.0953, Val Loss: 0.0734\n",
      "Epoch 32/300 - Train Loss: 0.0966, Val Loss: 0.0735\n",
      "Epoch 33/300 - Train Loss: 0.0959, Val Loss: 0.0713\n",
      "Epoch 34/300 - Train Loss: 0.0967, Val Loss: 0.0698\n",
      "Epoch 35/300 - Train Loss: 0.0963, Val Loss: 0.0772\n",
      "Epoch 36/300 - Train Loss: 0.0951, Val Loss: 0.0738\n",
      "Epoch 37/300 - Train Loss: 0.0949, Val Loss: 0.0714\n",
      "Epoch 38/300 - Train Loss: 0.0953, Val Loss: 0.0752\n",
      "Epoch 39/300 - Train Loss: 0.0941, Val Loss: 0.0727\n",
      "Epoch 40/300 - Train Loss: 0.0957, Val Loss: 0.0727\n",
      "Epoch 41/300 - Train Loss: 0.0963, Val Loss: 0.0730\n",
      "Epoch 42/300 - Train Loss: 0.0967, Val Loss: 0.0758\n",
      "Epoch 43/300 - Train Loss: 0.0982, Val Loss: 0.0706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 04:55:49,769] Trial 94 finished with value: 0.971331894914181 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2249706630485494, 'learning_rate': 4.303679934837843e-05, 'batch_size': 32, 'weight_decay': 0.007823883143519474}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300 - Train Loss: 0.0979, Val Loss: 0.0728\n",
      "Early stopping at epoch 44\n",
      "Macro F1 Score: 0.9713, Macro Precision: 0.9726, Macro Recall: 0.9703\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 96\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2039504981682997, LR=4.17455931913573e-05, BS=32, WD=0.0037221315477012585\n",
      "Epoch 1/300 - Train Loss: 0.3508, Val Loss: 0.1756\n",
      "Epoch 2/300 - Train Loss: 0.1557, Val Loss: 0.1114\n",
      "Epoch 3/300 - Train Loss: 0.1182, Val Loss: 0.0947\n",
      "Epoch 4/300 - Train Loss: 0.1108, Val Loss: 0.0906\n",
      "Epoch 5/300 - Train Loss: 0.1021, Val Loss: 0.0921\n",
      "Epoch 6/300 - Train Loss: 0.1022, Val Loss: 0.0790\n",
      "Epoch 7/300 - Train Loss: 0.0973, Val Loss: 0.0775\n",
      "Epoch 8/300 - Train Loss: 0.0959, Val Loss: 0.0759\n",
      "Epoch 9/300 - Train Loss: 0.0931, Val Loss: 0.0742\n",
      "Epoch 10/300 - Train Loss: 0.0906, Val Loss: 0.0789\n",
      "Epoch 11/300 - Train Loss: 0.0948, Val Loss: 0.0919\n",
      "Epoch 12/300 - Train Loss: 0.0908, Val Loss: 0.0777\n",
      "Epoch 13/300 - Train Loss: 0.0907, Val Loss: 0.0812\n",
      "Epoch 14/300 - Train Loss: 0.0901, Val Loss: 0.0748\n",
      "Epoch 15/300 - Train Loss: 0.0919, Val Loss: 0.0726\n",
      "Epoch 16/300 - Train Loss: 0.0877, Val Loss: 0.0733\n",
      "Epoch 17/300 - Train Loss: 0.0873, Val Loss: 0.0740\n",
      "Epoch 18/300 - Train Loss: 0.0856, Val Loss: 0.0710\n",
      "Epoch 19/300 - Train Loss: 0.0855, Val Loss: 0.0689\n",
      "Epoch 20/300 - Train Loss: 0.0885, Val Loss: 0.0737\n",
      "Epoch 21/300 - Train Loss: 0.0875, Val Loss: 0.0689\n",
      "Epoch 22/300 - Train Loss: 0.0842, Val Loss: 0.0711\n",
      "Epoch 23/300 - Train Loss: 0.0859, Val Loss: 0.0714\n",
      "Epoch 24/300 - Train Loss: 0.0868, Val Loss: 0.0730\n",
      "Epoch 25/300 - Train Loss: 0.0865, Val Loss: 0.0720\n",
      "Epoch 26/300 - Train Loss: 0.0851, Val Loss: 0.0703\n",
      "Epoch 27/300 - Train Loss: 0.0912, Val Loss: 0.0721\n",
      "Epoch 28/300 - Train Loss: 0.0855, Val Loss: 0.0728\n",
      "Epoch 29/300 - Train Loss: 0.0866, Val Loss: 0.0675\n",
      "Epoch 30/300 - Train Loss: 0.0858, Val Loss: 0.0679\n",
      "Epoch 31/300 - Train Loss: 0.0849, Val Loss: 0.0738\n",
      "Epoch 32/300 - Train Loss: 0.0889, Val Loss: 0.0688\n",
      "Epoch 33/300 - Train Loss: 0.0854, Val Loss: 0.0688\n",
      "Epoch 34/300 - Train Loss: 0.0871, Val Loss: 0.0762\n",
      "Epoch 35/300 - Train Loss: 0.0853, Val Loss: 0.0708\n",
      "Epoch 36/300 - Train Loss: 0.0855, Val Loss: 0.0680\n",
      "Epoch 37/300 - Train Loss: 0.0850, Val Loss: 0.0701\n",
      "Epoch 38/300 - Train Loss: 0.0852, Val Loss: 0.0688\n",
      "Epoch 39/300 - Train Loss: 0.0840, Val Loss: 0.0700\n",
      "Epoch 40/300 - Train Loss: 0.0863, Val Loss: 0.0751\n",
      "Epoch 41/300 - Train Loss: 0.0845, Val Loss: 0.0747\n",
      "Epoch 42/300 - Train Loss: 0.0867, Val Loss: 0.0686\n",
      "Epoch 43/300 - Train Loss: 0.0847, Val Loss: 0.0695\n",
      "Epoch 44/300 - Train Loss: 0.0900, Val Loss: 0.0702\n",
      "Epoch 45/300 - Train Loss: 0.0872, Val Loss: 0.0716\n",
      "Epoch 46/300 - Train Loss: 0.0867, Val Loss: 0.0736\n",
      "Epoch 47/300 - Train Loss: 0.0872, Val Loss: 0.0682\n",
      "Epoch 48/300 - Train Loss: 0.0848, Val Loss: 0.0694\n",
      "Epoch 49/300 - Train Loss: 0.0896, Val Loss: 0.0710\n",
      "Epoch 50/300 - Train Loss: 0.0897, Val Loss: 0.0702\n",
      "Epoch 51/300 - Train Loss: 0.0877, Val Loss: 0.0736\n",
      "Epoch 52/300 - Train Loss: 0.0868, Val Loss: 0.0673\n",
      "Epoch 53/300 - Train Loss: 0.0880, Val Loss: 0.0739\n",
      "Epoch 54/300 - Train Loss: 0.0871, Val Loss: 0.0718\n",
      "Epoch 55/300 - Train Loss: 0.0859, Val Loss: 0.0701\n",
      "Epoch 56/300 - Train Loss: 0.0860, Val Loss: 0.0697\n",
      "Epoch 57/300 - Train Loss: 0.0890, Val Loss: 0.0729\n",
      "Epoch 58/300 - Train Loss: 0.0867, Val Loss: 0.0733\n",
      "Epoch 59/300 - Train Loss: 0.0861, Val Loss: 0.0696\n",
      "Epoch 60/300 - Train Loss: 0.0861, Val Loss: 0.0700\n",
      "Epoch 61/300 - Train Loss: 0.0869, Val Loss: 0.0692\n",
      "Epoch 62/300 - Train Loss: 0.0900, Val Loss: 0.0677\n",
      "Epoch 63/300 - Train Loss: 0.0905, Val Loss: 0.0728\n",
      "Epoch 64/300 - Train Loss: 0.0870, Val Loss: 0.0730\n",
      "Epoch 65/300 - Train Loss: 0.0859, Val Loss: 0.0796\n",
      "Epoch 66/300 - Train Loss: 0.0888, Val Loss: 0.0689\n",
      "Epoch 67/300 - Train Loss: 0.0861, Val Loss: 0.0677\n",
      "Epoch 68/300 - Train Loss: 0.0882, Val Loss: 0.0705\n",
      "Epoch 69/300 - Train Loss: 0.0875, Val Loss: 0.0673\n",
      "Epoch 70/300 - Train Loss: 0.0885, Val Loss: 0.0717\n",
      "Epoch 71/300 - Train Loss: 0.0870, Val Loss: 0.0701\n",
      "Epoch 72/300 - Train Loss: 0.0900, Val Loss: 0.0734\n",
      "Epoch 73/300 - Train Loss: 0.0877, Val Loss: 0.0695\n",
      "Epoch 74/300 - Train Loss: 0.0901, Val Loss: 0.0679\n",
      "Epoch 75/300 - Train Loss: 0.0904, Val Loss: 0.0673\n",
      "Epoch 76/300 - Train Loss: 0.0868, Val Loss: 0.0704\n",
      "Epoch 77/300 - Train Loss: 0.0876, Val Loss: 0.0747\n",
      "Epoch 78/300 - Train Loss: 0.0872, Val Loss: 0.0705\n",
      "Epoch 79/300 - Train Loss: 0.0876, Val Loss: 0.0710\n",
      "Epoch 80/300 - Train Loss: 0.0886, Val Loss: 0.0698\n",
      "Epoch 81/300 - Train Loss: 0.0888, Val Loss: 0.0672\n",
      "Epoch 82/300 - Train Loss: 0.0890, Val Loss: 0.0692\n",
      "Epoch 83/300 - Train Loss: 0.0896, Val Loss: 0.0687\n",
      "Epoch 84/300 - Train Loss: 0.0901, Val Loss: 0.0717\n",
      "Epoch 85/300 - Train Loss: 0.0902, Val Loss: 0.0716\n",
      "Epoch 86/300 - Train Loss: 0.0873, Val Loss: 0.0680\n",
      "Epoch 87/300 - Train Loss: 0.0882, Val Loss: 0.0700\n",
      "Epoch 88/300 - Train Loss: 0.0911, Val Loss: 0.0734\n",
      "Epoch 89/300 - Train Loss: 0.0887, Val Loss: 0.0726\n",
      "Epoch 90/300 - Train Loss: 0.0889, Val Loss: 0.0684\n",
      "Epoch 91/300 - Train Loss: 0.0912, Val Loss: 0.0750\n",
      "Epoch 92/300 - Train Loss: 0.0895, Val Loss: 0.0683\n",
      "Epoch 93/300 - Train Loss: 0.0907, Val Loss: 0.0684\n",
      "Epoch 94/300 - Train Loss: 0.0875, Val Loss: 0.0708\n",
      "Epoch 95/300 - Train Loss: 0.0903, Val Loss: 0.0706\n",
      "Epoch 96/300 - Train Loss: 0.0879, Val Loss: 0.0733\n",
      "Epoch 97/300 - Train Loss: 0.0900, Val Loss: 0.0665\n",
      "Epoch 98/300 - Train Loss: 0.0899, Val Loss: 0.0719\n",
      "Epoch 99/300 - Train Loss: 0.0911, Val Loss: 0.0713\n",
      "Epoch 100/300 - Train Loss: 0.0879, Val Loss: 0.0674\n",
      "Epoch 101/300 - Train Loss: 0.0873, Val Loss: 0.0682\n",
      "Epoch 102/300 - Train Loss: 0.0883, Val Loss: 0.0706\n",
      "Epoch 103/300 - Train Loss: 0.0886, Val Loss: 0.0795\n",
      "Epoch 104/300 - Train Loss: 0.0921, Val Loss: 0.0697\n",
      "Epoch 105/300 - Train Loss: 0.0900, Val Loss: 0.0693\n",
      "Epoch 106/300 - Train Loss: 0.0886, Val Loss: 0.0726\n",
      "Epoch 107/300 - Train Loss: 0.0899, Val Loss: 0.0683\n",
      "Epoch 108/300 - Train Loss: 0.0912, Val Loss: 0.0739\n",
      "Epoch 109/300 - Train Loss: 0.0876, Val Loss: 0.0659\n",
      "Epoch 110/300 - Train Loss: 0.0896, Val Loss: 0.0739\n",
      "Epoch 111/300 - Train Loss: 0.0907, Val Loss: 0.0759\n",
      "Epoch 112/300 - Train Loss: 0.0898, Val Loss: 0.0703\n",
      "Epoch 113/300 - Train Loss: 0.0933, Val Loss: 0.0701\n",
      "Epoch 114/300 - Train Loss: 0.0890, Val Loss: 0.0754\n",
      "Epoch 115/300 - Train Loss: 0.0885, Val Loss: 0.0700\n",
      "Epoch 116/300 - Train Loss: 0.0880, Val Loss: 0.0709\n",
      "Epoch 117/300 - Train Loss: 0.0901, Val Loss: 0.0743\n",
      "Epoch 118/300 - Train Loss: 0.0916, Val Loss: 0.0775\n",
      "Epoch 119/300 - Train Loss: 0.0897, Val Loss: 0.0687\n",
      "Epoch 120/300 - Train Loss: 0.0901, Val Loss: 0.0674\n",
      "Epoch 121/300 - Train Loss: 0.0884, Val Loss: 0.0710\n",
      "Epoch 122/300 - Train Loss: 0.0887, Val Loss: 0.0697\n",
      "Epoch 123/300 - Train Loss: 0.0924, Val Loss: 0.0698\n",
      "Epoch 124/300 - Train Loss: 0.0898, Val Loss: 0.0673\n",
      "Epoch 125/300 - Train Loss: 0.0916, Val Loss: 0.0722\n",
      "Epoch 126/300 - Train Loss: 0.0891, Val Loss: 0.0715\n",
      "Epoch 127/300 - Train Loss: 0.0934, Val Loss: 0.0687\n",
      "Epoch 128/300 - Train Loss: 0.0899, Val Loss: 0.0730\n",
      "Epoch 129/300 - Train Loss: 0.0907, Val Loss: 0.0676\n",
      "Epoch 130/300 - Train Loss: 0.0916, Val Loss: 0.0672\n",
      "Epoch 131/300 - Train Loss: 0.0930, Val Loss: 0.0685\n",
      "Epoch 132/300 - Train Loss: 0.0911, Val Loss: 0.0693\n",
      "Epoch 133/300 - Train Loss: 0.0919, Val Loss: 0.0690\n",
      "Epoch 134/300 - Train Loss: 0.0918, Val Loss: 0.0716\n",
      "Epoch 135/300 - Train Loss: 0.0913, Val Loss: 0.0688\n",
      "Epoch 136/300 - Train Loss: 0.0889, Val Loss: 0.0707\n",
      "Epoch 137/300 - Train Loss: 0.0882, Val Loss: 0.0686\n",
      "Epoch 138/300 - Train Loss: 0.0902, Val Loss: 0.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:04:43,849] Trial 95 finished with value: 0.9608416210405532 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2039504981682997, 'learning_rate': 4.17455931913573e-05, 'batch_size': 32, 'weight_decay': 0.0037221315477012585}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139/300 - Train Loss: 0.0900, Val Loss: 0.0725\n",
      "Early stopping at epoch 139\n",
      "Macro F1 Score: 0.9608, Macro Precision: 0.9533, Macro Recall: 0.9691\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 97\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2237260511687793, LR=0.00013334474557196384, BS=256, WD=0.0026078104369183946\n",
      "Epoch 1/300 - Train Loss: 0.4310, Val Loss: 0.2378\n",
      "Epoch 2/300 - Train Loss: 0.1955, Val Loss: 0.1835\n",
      "Epoch 3/300 - Train Loss: 0.1487, Val Loss: 0.1117\n",
      "Epoch 4/300 - Train Loss: 0.1149, Val Loss: 0.1109\n",
      "Epoch 5/300 - Train Loss: 0.1024, Val Loss: 0.0889\n",
      "Epoch 6/300 - Train Loss: 0.0940, Val Loss: 0.0857\n",
      "Epoch 7/300 - Train Loss: 0.0907, Val Loss: 0.0829\n",
      "Epoch 8/300 - Train Loss: 0.0881, Val Loss: 0.0793\n",
      "Epoch 9/300 - Train Loss: 0.0868, Val Loss: 0.0776\n",
      "Epoch 10/300 - Train Loss: 0.0836, Val Loss: 0.0727\n",
      "Epoch 11/300 - Train Loss: 0.0827, Val Loss: 0.0724\n",
      "Epoch 12/300 - Train Loss: 0.0803, Val Loss: 0.0817\n",
      "Epoch 13/300 - Train Loss: 0.0847, Val Loss: 0.0754\n",
      "Epoch 14/300 - Train Loss: 0.0819, Val Loss: 0.0847\n",
      "Epoch 15/300 - Train Loss: 0.0795, Val Loss: 0.0768\n",
      "Epoch 16/300 - Train Loss: 0.0800, Val Loss: 0.0804\n",
      "Epoch 17/300 - Train Loss: 0.0786, Val Loss: 0.0735\n",
      "Epoch 18/300 - Train Loss: 0.0781, Val Loss: 0.0771\n",
      "Epoch 19/300 - Train Loss: 0.0755, Val Loss: 0.0683\n",
      "Epoch 20/300 - Train Loss: 0.0769, Val Loss: 0.0769\n",
      "Epoch 21/300 - Train Loss: 0.0776, Val Loss: 0.0766\n",
      "Epoch 22/300 - Train Loss: 0.0772, Val Loss: 0.0683\n",
      "Epoch 23/300 - Train Loss: 0.0752, Val Loss: 0.0768\n",
      "Epoch 24/300 - Train Loss: 0.0753, Val Loss: 0.0735\n",
      "Epoch 25/300 - Train Loss: 0.0753, Val Loss: 0.0711\n",
      "Epoch 26/300 - Train Loss: 0.0737, Val Loss: 0.0710\n",
      "Epoch 27/300 - Train Loss: 0.0750, Val Loss: 0.0724\n",
      "Epoch 28/300 - Train Loss: 0.0741, Val Loss: 0.0718\n",
      "Epoch 29/300 - Train Loss: 0.0746, Val Loss: 0.0697\n",
      "Epoch 30/300 - Train Loss: 0.0750, Val Loss: 0.0697\n",
      "Epoch 31/300 - Train Loss: 0.0722, Val Loss: 0.0684\n",
      "Epoch 32/300 - Train Loss: 0.0728, Val Loss: 0.0794\n",
      "Epoch 33/300 - Train Loss: 0.0736, Val Loss: 0.0673\n",
      "Epoch 34/300 - Train Loss: 0.0734, Val Loss: 0.0700\n",
      "Epoch 35/300 - Train Loss: 0.0725, Val Loss: 0.0695\n",
      "Epoch 36/300 - Train Loss: 0.0723, Val Loss: 0.0696\n",
      "Epoch 37/300 - Train Loss: 0.0722, Val Loss: 0.0732\n",
      "Epoch 38/300 - Train Loss: 0.0730, Val Loss: 0.0699\n",
      "Epoch 39/300 - Train Loss: 0.0728, Val Loss: 0.0656\n",
      "Epoch 40/300 - Train Loss: 0.0738, Val Loss: 0.0746\n",
      "Epoch 41/300 - Train Loss: 0.0732, Val Loss: 0.0680\n",
      "Epoch 42/300 - Train Loss: 0.0729, Val Loss: 0.0686\n",
      "Epoch 43/300 - Train Loss: 0.0737, Val Loss: 0.0712\n",
      "Epoch 44/300 - Train Loss: 0.0722, Val Loss: 0.0722\n",
      "Epoch 45/300 - Train Loss: 0.0727, Val Loss: 0.0743\n",
      "Epoch 46/300 - Train Loss: 0.0725, Val Loss: 0.0685\n",
      "Epoch 47/300 - Train Loss: 0.0714, Val Loss: 0.0683\n",
      "Epoch 48/300 - Train Loss: 0.0733, Val Loss: 0.0710\n",
      "Epoch 49/300 - Train Loss: 0.0732, Val Loss: 0.0680\n",
      "Epoch 50/300 - Train Loss: 0.0735, Val Loss: 0.0687\n",
      "Epoch 51/300 - Train Loss: 0.0725, Val Loss: 0.0663\n",
      "Epoch 52/300 - Train Loss: 0.0728, Val Loss: 0.0685\n",
      "Epoch 53/300 - Train Loss: 0.0732, Val Loss: 0.0664\n",
      "Epoch 54/300 - Train Loss: 0.0736, Val Loss: 0.0687\n",
      "Epoch 55/300 - Train Loss: 0.0726, Val Loss: 0.0664\n",
      "Epoch 56/300 - Train Loss: 0.0730, Val Loss: 0.0695\n",
      "Epoch 57/300 - Train Loss: 0.0717, Val Loss: 0.0714\n",
      "Epoch 58/300 - Train Loss: 0.0714, Val Loss: 0.0677\n",
      "Epoch 59/300 - Train Loss: 0.0716, Val Loss: 0.0685\n",
      "Epoch 60/300 - Train Loss: 0.0719, Val Loss: 0.0678\n",
      "Epoch 61/300 - Train Loss: 0.0726, Val Loss: 0.0682\n",
      "Epoch 62/300 - Train Loss: 0.0717, Val Loss: 0.0686\n",
      "Epoch 63/300 - Train Loss: 0.0711, Val Loss: 0.0705\n",
      "Epoch 64/300 - Train Loss: 0.0722, Val Loss: 0.0688\n",
      "Epoch 65/300 - Train Loss: 0.0716, Val Loss: 0.0691\n",
      "Epoch 66/300 - Train Loss: 0.0732, Val Loss: 0.0680\n",
      "Epoch 67/300 - Train Loss: 0.0721, Val Loss: 0.0707\n",
      "Epoch 68/300 - Train Loss: 0.0710, Val Loss: 0.0690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:08:16,665] Trial 96 finished with value: 0.9708454467749822 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2237260511687793, 'learning_rate': 0.00013334474557196384, 'batch_size': 256, 'weight_decay': 0.0026078104369183946}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.0711, Val Loss: 0.0703\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9708, Macro Precision: 0.9718, Macro Recall: 0.9700\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 98\n",
      "Training with F1=16, F2=32, D=8, dropout=0.16448003210903828, LR=8.695491509420192e-05, BS=32, WD=0.007279776174243545\n",
      "Epoch 1/300 - Train Loss: 0.2664, Val Loss: 0.1238\n",
      "Epoch 2/300 - Train Loss: 0.1180, Val Loss: 0.1204\n",
      "Epoch 3/300 - Train Loss: 0.1045, Val Loss: 0.0895\n",
      "Epoch 4/300 - Train Loss: 0.1009, Val Loss: 0.0886\n",
      "Epoch 5/300 - Train Loss: 0.0976, Val Loss: 0.0908\n",
      "Epoch 6/300 - Train Loss: 0.0947, Val Loss: 0.0768\n",
      "Epoch 7/300 - Train Loss: 0.0942, Val Loss: 0.0774\n",
      "Epoch 8/300 - Train Loss: 0.0901, Val Loss: 0.0755\n",
      "Epoch 9/300 - Train Loss: 0.0928, Val Loss: 0.0802\n",
      "Epoch 10/300 - Train Loss: 0.0950, Val Loss: 0.0761\n",
      "Epoch 11/300 - Train Loss: 0.0927, Val Loss: 0.0718\n",
      "Epoch 12/300 - Train Loss: 0.0919, Val Loss: 0.0756\n",
      "Epoch 13/300 - Train Loss: 0.0925, Val Loss: 0.0745\n",
      "Epoch 14/300 - Train Loss: 0.0939, Val Loss: 0.0734\n",
      "Epoch 15/300 - Train Loss: 0.0926, Val Loss: 0.0770\n",
      "Epoch 16/300 - Train Loss: 0.0940, Val Loss: 0.0724\n",
      "Epoch 17/300 - Train Loss: 0.0954, Val Loss: 0.0772\n",
      "Epoch 18/300 - Train Loss: 0.0954, Val Loss: 0.0729\n",
      "Epoch 19/300 - Train Loss: 0.0939, Val Loss: 0.0776\n",
      "Epoch 20/300 - Train Loss: 0.0968, Val Loss: 0.0767\n",
      "Epoch 21/300 - Train Loss: 0.0979, Val Loss: 0.0837\n",
      "Epoch 22/300 - Train Loss: 0.0955, Val Loss: 0.0755\n",
      "Epoch 23/300 - Train Loss: 0.0959, Val Loss: 0.0760\n",
      "Epoch 24/300 - Train Loss: 0.0957, Val Loss: 0.0739\n",
      "Epoch 25/300 - Train Loss: 0.0968, Val Loss: 0.0751\n",
      "Epoch 26/300 - Train Loss: 0.0957, Val Loss: 0.0727\n",
      "Epoch 27/300 - Train Loss: 0.0969, Val Loss: 0.0716\n",
      "Epoch 28/300 - Train Loss: 0.0994, Val Loss: 0.0777\n",
      "Epoch 29/300 - Train Loss: 0.0984, Val Loss: 0.0760\n",
      "Epoch 30/300 - Train Loss: 0.0982, Val Loss: 0.0758\n",
      "Epoch 31/300 - Train Loss: 0.0980, Val Loss: 0.0751\n",
      "Epoch 32/300 - Train Loss: 0.0988, Val Loss: 0.0747\n",
      "Epoch 33/300 - Train Loss: 0.0974, Val Loss: 0.0736\n",
      "Epoch 34/300 - Train Loss: 0.0968, Val Loss: 0.0731\n",
      "Epoch 35/300 - Train Loss: 0.1010, Val Loss: 0.0768\n",
      "Epoch 36/300 - Train Loss: 0.0962, Val Loss: 0.0744\n",
      "Epoch 37/300 - Train Loss: 0.0997, Val Loss: 0.0772\n",
      "Epoch 38/300 - Train Loss: 0.0996, Val Loss: 0.0739\n",
      "Epoch 39/300 - Train Loss: 0.0984, Val Loss: 0.0839\n",
      "Epoch 40/300 - Train Loss: 0.0998, Val Loss: 0.0740\n",
      "Epoch 41/300 - Train Loss: 0.1012, Val Loss: 0.0740\n",
      "Epoch 42/300 - Train Loss: 0.1011, Val Loss: 0.0740\n",
      "Epoch 43/300 - Train Loss: 0.0991, Val Loss: 0.0837\n",
      "Epoch 44/300 - Train Loss: 0.0992, Val Loss: 0.0755\n",
      "Epoch 45/300 - Train Loss: 0.0971, Val Loss: 0.1127\n",
      "Epoch 46/300 - Train Loss: 0.1009, Val Loss: 0.0960\n",
      "Epoch 47/300 - Train Loss: 0.1029, Val Loss: 0.0787\n",
      "Epoch 48/300 - Train Loss: 0.0997, Val Loss: 0.0846\n",
      "Epoch 49/300 - Train Loss: 0.1027, Val Loss: 0.0730\n",
      "Epoch 50/300 - Train Loss: 0.0990, Val Loss: 0.0806\n",
      "Epoch 51/300 - Train Loss: 0.1007, Val Loss: 0.0733\n",
      "Epoch 52/300 - Train Loss: 0.0999, Val Loss: 0.0733\n",
      "Epoch 53/300 - Train Loss: 0.0980, Val Loss: 0.0762\n",
      "Epoch 54/300 - Train Loss: 0.1028, Val Loss: 0.0853\n",
      "Epoch 55/300 - Train Loss: 0.1013, Val Loss: 0.0704\n",
      "Epoch 56/300 - Train Loss: 0.1021, Val Loss: 0.1124\n",
      "Epoch 57/300 - Train Loss: 0.1018, Val Loss: 0.0801\n",
      "Epoch 58/300 - Train Loss: 0.1014, Val Loss: 0.0746\n",
      "Epoch 59/300 - Train Loss: 0.1010, Val Loss: 0.0783\n",
      "Epoch 60/300 - Train Loss: 0.1010, Val Loss: 0.0738\n",
      "Epoch 61/300 - Train Loss: 0.0992, Val Loss: 0.0787\n",
      "Epoch 62/300 - Train Loss: 0.1042, Val Loss: 0.0711\n",
      "Epoch 63/300 - Train Loss: 0.1010, Val Loss: 0.0778\n",
      "Epoch 64/300 - Train Loss: 0.1021, Val Loss: 0.0768\n",
      "Epoch 65/300 - Train Loss: 0.0997, Val Loss: 0.0731\n",
      "Epoch 66/300 - Train Loss: 0.1014, Val Loss: 0.0869\n",
      "Epoch 67/300 - Train Loss: 0.1026, Val Loss: 0.0724\n",
      "Epoch 68/300 - Train Loss: 0.1038, Val Loss: 0.0819\n",
      "Epoch 69/300 - Train Loss: 0.1019, Val Loss: 0.0750\n",
      "Epoch 70/300 - Train Loss: 0.1009, Val Loss: 0.0745\n",
      "Epoch 71/300 - Train Loss: 0.1002, Val Loss: 0.0704\n",
      "Epoch 72/300 - Train Loss: 0.1012, Val Loss: 0.0750\n",
      "Epoch 73/300 - Train Loss: 0.1024, Val Loss: 0.0739\n",
      "Epoch 74/300 - Train Loss: 0.0983, Val Loss: 0.0781\n",
      "Epoch 75/300 - Train Loss: 0.1047, Val Loss: 0.0890\n",
      "Epoch 76/300 - Train Loss: 0.1016, Val Loss: 0.0770\n",
      "Epoch 77/300 - Train Loss: 0.1029, Val Loss: 0.0811\n",
      "Epoch 78/300 - Train Loss: 0.1016, Val Loss: 0.0759\n",
      "Epoch 79/300 - Train Loss: 0.1003, Val Loss: 0.0787\n",
      "Epoch 80/300 - Train Loss: 0.1020, Val Loss: 0.0843\n",
      "Epoch 81/300 - Train Loss: 0.1010, Val Loss: 0.0736\n",
      "Epoch 82/300 - Train Loss: 0.1010, Val Loss: 0.0841\n",
      "Epoch 83/300 - Train Loss: 0.1037, Val Loss: 0.0791\n",
      "Epoch 84/300 - Train Loss: 0.0988, Val Loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:13:43,384] Trial 97 finished with value: 0.966779382197998 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.16448003210903828, 'learning_rate': 8.695491509420192e-05, 'batch_size': 32, 'weight_decay': 0.007279776174243545}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300 - Train Loss: 0.1005, Val Loss: 0.0772\n",
      "Early stopping at epoch 85\n",
      "Macro F1 Score: 0.9668, Macro Precision: 0.9756, Macro Recall: 0.9586\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.92      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 99\n",
      "Training with F1=16, F2=32, D=8, dropout=0.33859973511489444, LR=0.00011058081967994739, BS=32, WD=0.000916071232976949\n",
      "Epoch 1/300 - Train Loss: 0.2590, Val Loss: 0.1093\n",
      "Epoch 2/300 - Train Loss: 0.1147, Val Loss: 0.0889\n",
      "Epoch 3/300 - Train Loss: 0.1015, Val Loss: 0.0784\n",
      "Epoch 4/300 - Train Loss: 0.1000, Val Loss: 0.0733\n",
      "Epoch 5/300 - Train Loss: 0.0967, Val Loss: 0.0726\n",
      "Epoch 6/300 - Train Loss: 0.0924, Val Loss: 0.0763\n",
      "Epoch 7/300 - Train Loss: 0.0903, Val Loss: 0.0717\n",
      "Epoch 8/300 - Train Loss: 0.0904, Val Loss: 0.0821\n",
      "Epoch 9/300 - Train Loss: 0.0899, Val Loss: 0.0921\n",
      "Epoch 10/300 - Train Loss: 0.0879, Val Loss: 0.0736\n",
      "Epoch 11/300 - Train Loss: 0.0880, Val Loss: 0.0735\n",
      "Epoch 12/300 - Train Loss: 0.0874, Val Loss: 0.0705\n",
      "Epoch 13/300 - Train Loss: 0.0855, Val Loss: 0.0774\n",
      "Epoch 14/300 - Train Loss: 0.0866, Val Loss: 0.0733\n",
      "Epoch 15/300 - Train Loss: 0.0859, Val Loss: 0.0742\n",
      "Epoch 16/300 - Train Loss: 0.0839, Val Loss: 0.0735\n",
      "Epoch 17/300 - Train Loss: 0.0834, Val Loss: 0.0751\n",
      "Epoch 18/300 - Train Loss: 0.0828, Val Loss: 0.0713\n",
      "Epoch 19/300 - Train Loss: 0.0830, Val Loss: 0.0729\n",
      "Epoch 20/300 - Train Loss: 0.0801, Val Loss: 0.0721\n",
      "Epoch 21/300 - Train Loss: 0.0834, Val Loss: 0.0716\n",
      "Epoch 22/300 - Train Loss: 0.0798, Val Loss: 0.0743\n",
      "Epoch 23/300 - Train Loss: 0.0825, Val Loss: 0.0680\n",
      "Epoch 24/300 - Train Loss: 0.0812, Val Loss: 0.0711\n",
      "Epoch 25/300 - Train Loss: 0.0808, Val Loss: 0.0719\n",
      "Epoch 26/300 - Train Loss: 0.0818, Val Loss: 0.0727\n",
      "Epoch 27/300 - Train Loss: 0.0823, Val Loss: 0.0730\n",
      "Epoch 28/300 - Train Loss: 0.0833, Val Loss: 0.0703\n",
      "Epoch 29/300 - Train Loss: 0.0806, Val Loss: 0.0702\n",
      "Epoch 30/300 - Train Loss: 0.0807, Val Loss: 0.0667\n",
      "Epoch 31/300 - Train Loss: 0.0798, Val Loss: 0.0744\n",
      "Epoch 32/300 - Train Loss: 0.0801, Val Loss: 0.0691\n",
      "Epoch 33/300 - Train Loss: 0.0808, Val Loss: 0.0674\n",
      "Epoch 34/300 - Train Loss: 0.0782, Val Loss: 0.0715\n",
      "Epoch 35/300 - Train Loss: 0.0823, Val Loss: 0.0791\n",
      "Epoch 36/300 - Train Loss: 0.0805, Val Loss: 0.0703\n",
      "Epoch 37/300 - Train Loss: 0.0794, Val Loss: 0.0696\n",
      "Epoch 38/300 - Train Loss: 0.0779, Val Loss: 0.0703\n",
      "Epoch 39/300 - Train Loss: 0.0804, Val Loss: 0.0681\n",
      "Epoch 40/300 - Train Loss: 0.0796, Val Loss: 0.0682\n",
      "Epoch 41/300 - Train Loss: 0.0809, Val Loss: 0.0689\n",
      "Epoch 42/300 - Train Loss: 0.0781, Val Loss: 0.0778\n",
      "Epoch 43/300 - Train Loss: 0.0794, Val Loss: 0.0719\n",
      "Epoch 44/300 - Train Loss: 0.0786, Val Loss: 0.0678\n",
      "Epoch 45/300 - Train Loss: 0.0787, Val Loss: 0.0699\n",
      "Epoch 46/300 - Train Loss: 0.0798, Val Loss: 0.0678\n",
      "Epoch 47/300 - Train Loss: 0.0786, Val Loss: 0.0720\n",
      "Epoch 48/300 - Train Loss: 0.0821, Val Loss: 0.0713\n",
      "Epoch 49/300 - Train Loss: 0.0767, Val Loss: 0.0724\n",
      "Epoch 50/300 - Train Loss: 0.0793, Val Loss: 0.0688\n",
      "Epoch 51/300 - Train Loss: 0.0805, Val Loss: 0.0744\n",
      "Epoch 52/300 - Train Loss: 0.0796, Val Loss: 0.0664\n",
      "Epoch 53/300 - Train Loss: 0.0779, Val Loss: 0.0692\n",
      "Epoch 54/300 - Train Loss: 0.0792, Val Loss: 0.0681\n",
      "Epoch 55/300 - Train Loss: 0.0788, Val Loss: 0.0680\n",
      "Epoch 56/300 - Train Loss: 0.0791, Val Loss: 0.0692\n",
      "Epoch 57/300 - Train Loss: 0.0798, Val Loss: 0.0735\n",
      "Epoch 58/300 - Train Loss: 0.0784, Val Loss: 0.0768\n",
      "Epoch 59/300 - Train Loss: 0.0795, Val Loss: 0.0708\n",
      "Epoch 60/300 - Train Loss: 0.0786, Val Loss: 0.0707\n",
      "Epoch 61/300 - Train Loss: 0.0793, Val Loss: 0.0708\n",
      "Epoch 62/300 - Train Loss: 0.0783, Val Loss: 0.0682\n",
      "Epoch 63/300 - Train Loss: 0.0794, Val Loss: 0.0646\n",
      "Epoch 64/300 - Train Loss: 0.0790, Val Loss: 0.0671\n",
      "Epoch 65/300 - Train Loss: 0.0794, Val Loss: 0.0676\n",
      "Epoch 66/300 - Train Loss: 0.0762, Val Loss: 0.0719\n",
      "Epoch 67/300 - Train Loss: 0.0777, Val Loss: 0.0672\n",
      "Epoch 68/300 - Train Loss: 0.0798, Val Loss: 0.0748\n",
      "Epoch 69/300 - Train Loss: 0.0790, Val Loss: 0.0664\n",
      "Epoch 70/300 - Train Loss: 0.0784, Val Loss: 0.0698\n",
      "Epoch 71/300 - Train Loss: 0.0794, Val Loss: 0.0685\n",
      "Epoch 72/300 - Train Loss: 0.0770, Val Loss: 0.0693\n",
      "Epoch 73/300 - Train Loss: 0.0764, Val Loss: 0.0696\n",
      "Epoch 74/300 - Train Loss: 0.0784, Val Loss: 0.0665\n",
      "Epoch 75/300 - Train Loss: 0.0802, Val Loss: 0.0754\n",
      "Epoch 76/300 - Train Loss: 0.0764, Val Loss: 0.0689\n",
      "Epoch 77/300 - Train Loss: 0.0763, Val Loss: 0.0714\n",
      "Epoch 78/300 - Train Loss: 0.0770, Val Loss: 0.0672\n",
      "Epoch 79/300 - Train Loss: 0.0770, Val Loss: 0.0704\n",
      "Epoch 80/300 - Train Loss: 0.0792, Val Loss: 0.0693\n",
      "Epoch 81/300 - Train Loss: 0.0796, Val Loss: 0.0773\n",
      "Epoch 82/300 - Train Loss: 0.0749, Val Loss: 0.0663\n",
      "Epoch 83/300 - Train Loss: 0.0760, Val Loss: 0.0683\n",
      "Epoch 84/300 - Train Loss: 0.0808, Val Loss: 0.0675\n",
      "Epoch 85/300 - Train Loss: 0.0766, Val Loss: 0.0651\n",
      "Epoch 86/300 - Train Loss: 0.0784, Val Loss: 0.0682\n",
      "Epoch 87/300 - Train Loss: 0.0788, Val Loss: 0.0687\n",
      "Epoch 88/300 - Train Loss: 0.0768, Val Loss: 0.0663\n",
      "Epoch 89/300 - Train Loss: 0.0785, Val Loss: 0.0710\n",
      "Epoch 90/300 - Train Loss: 0.0775, Val Loss: 0.0754\n",
      "Epoch 91/300 - Train Loss: 0.0753, Val Loss: 0.0685\n",
      "Epoch 92/300 - Train Loss: 0.0773, Val Loss: 0.0733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:19:41,119] Trial 98 finished with value: 0.9690330833375214 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.33859973511489444, 'learning_rate': 0.00011058081967994739, 'batch_size': 32, 'weight_decay': 0.000916071232976949}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/300 - Train Loss: 0.0775, Val Loss: 0.0671\n",
      "Early stopping at epoch 93\n",
      "Macro F1 Score: 0.9690, Macro Precision: 0.9609, Macro Recall: 0.9778\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 100\n",
      "Training with F1=32, F2=32, D=8, dropout=0.5497619949288474, LR=0.0001222122385550009, BS=128, WD=0.0020333512952230583\n",
      "Epoch 1/300 - Train Loss: 0.3785, Val Loss: 0.2204\n",
      "Epoch 2/300 - Train Loss: 0.1709, Val Loss: 0.1338\n",
      "Epoch 3/300 - Train Loss: 0.1222, Val Loss: 0.1032\n",
      "Epoch 4/300 - Train Loss: 0.1047, Val Loss: 0.0955\n",
      "Epoch 5/300 - Train Loss: 0.0992, Val Loss: 0.0884\n",
      "Epoch 6/300 - Train Loss: 0.0971, Val Loss: 0.0883\n",
      "Epoch 7/300 - Train Loss: 0.0929, Val Loss: 0.0901\n",
      "Epoch 8/300 - Train Loss: 0.0926, Val Loss: 0.0959\n",
      "Epoch 9/300 - Train Loss: 0.0916, Val Loss: 0.0804\n",
      "Epoch 10/300 - Train Loss: 0.0888, Val Loss: 0.0829\n",
      "Epoch 11/300 - Train Loss: 0.0885, Val Loss: 0.0839\n",
      "Epoch 12/300 - Train Loss: 0.0865, Val Loss: 0.0775\n",
      "Epoch 13/300 - Train Loss: 0.0848, Val Loss: 0.0800\n",
      "Epoch 14/300 - Train Loss: 0.0852, Val Loss: 0.0752\n",
      "Epoch 15/300 - Train Loss: 0.0855, Val Loss: 0.0810\n",
      "Epoch 16/300 - Train Loss: 0.0842, Val Loss: 0.0748\n",
      "Epoch 17/300 - Train Loss: 0.0828, Val Loss: 0.0735\n",
      "Epoch 18/300 - Train Loss: 0.0826, Val Loss: 0.0779\n",
      "Epoch 19/300 - Train Loss: 0.0818, Val Loss: 0.0751\n",
      "Epoch 20/300 - Train Loss: 0.0842, Val Loss: 0.0767\n",
      "Epoch 21/300 - Train Loss: 0.0824, Val Loss: 0.0733\n",
      "Epoch 22/300 - Train Loss: 0.0818, Val Loss: 0.0735\n",
      "Epoch 23/300 - Train Loss: 0.0822, Val Loss: 0.0726\n",
      "Epoch 24/300 - Train Loss: 0.0803, Val Loss: 0.0730\n",
      "Epoch 25/300 - Train Loss: 0.0823, Val Loss: 0.0719\n",
      "Epoch 26/300 - Train Loss: 0.0815, Val Loss: 0.0734\n",
      "Epoch 27/300 - Train Loss: 0.0810, Val Loss: 0.0717\n",
      "Epoch 28/300 - Train Loss: 0.0822, Val Loss: 0.0718\n",
      "Epoch 29/300 - Train Loss: 0.0824, Val Loss: 0.0715\n",
      "Epoch 30/300 - Train Loss: 0.0815, Val Loss: 0.0731\n",
      "Epoch 31/300 - Train Loss: 0.0814, Val Loss: 0.0736\n",
      "Epoch 32/300 - Train Loss: 0.0817, Val Loss: 0.0718\n",
      "Epoch 33/300 - Train Loss: 0.0809, Val Loss: 0.0715\n",
      "Epoch 34/300 - Train Loss: 0.0811, Val Loss: 0.0727\n",
      "Epoch 35/300 - Train Loss: 0.0818, Val Loss: 0.0725\n",
      "Epoch 36/300 - Train Loss: 0.0821, Val Loss: 0.0719\n",
      "Epoch 37/300 - Train Loss: 0.0823, Val Loss: 0.0721\n",
      "Epoch 38/300 - Train Loss: 0.0826, Val Loss: 0.0721\n",
      "Epoch 39/300 - Train Loss: 0.0818, Val Loss: 0.0719\n",
      "Epoch 40/300 - Train Loss: 0.0826, Val Loss: 0.0733\n",
      "Epoch 41/300 - Train Loss: 0.0820, Val Loss: 0.0753\n",
      "Epoch 42/300 - Train Loss: 0.0825, Val Loss: 0.0717\n",
      "Epoch 43/300 - Train Loss: 0.0845, Val Loss: 0.0727\n",
      "Epoch 44/300 - Train Loss: 0.0844, Val Loss: 0.0736\n",
      "Epoch 45/300 - Train Loss: 0.0825, Val Loss: 0.0730\n",
      "Epoch 46/300 - Train Loss: 0.0833, Val Loss: 0.0766\n",
      "Epoch 47/300 - Train Loss: 0.0833, Val Loss: 0.0726\n",
      "Epoch 48/300 - Train Loss: 0.0843, Val Loss: 0.0739\n",
      "Epoch 49/300 - Train Loss: 0.0827, Val Loss: 0.0710\n",
      "Epoch 50/300 - Train Loss: 0.0852, Val Loss: 0.0712\n",
      "Epoch 51/300 - Train Loss: 0.0843, Val Loss: 0.0736\n",
      "Epoch 52/300 - Train Loss: 0.0827, Val Loss: 0.0747\n",
      "Epoch 53/300 - Train Loss: 0.0835, Val Loss: 0.0714\n",
      "Epoch 54/300 - Train Loss: 0.0839, Val Loss: 0.0717\n",
      "Epoch 55/300 - Train Loss: 0.0845, Val Loss: 0.0726\n",
      "Epoch 56/300 - Train Loss: 0.0841, Val Loss: 0.0720\n",
      "Epoch 57/300 - Train Loss: 0.0860, Val Loss: 0.0712\n",
      "Epoch 58/300 - Train Loss: 0.0840, Val Loss: 0.0711\n",
      "Epoch 59/300 - Train Loss: 0.0861, Val Loss: 0.0705\n",
      "Epoch 60/300 - Train Loss: 0.0864, Val Loss: 0.0699\n",
      "Epoch 61/300 - Train Loss: 0.0849, Val Loss: 0.0728\n",
      "Epoch 62/300 - Train Loss: 0.0845, Val Loss: 0.0714\n",
      "Epoch 63/300 - Train Loss: 0.0836, Val Loss: 0.0730\n",
      "Epoch 64/300 - Train Loss: 0.0841, Val Loss: 0.0749\n",
      "Epoch 65/300 - Train Loss: 0.0854, Val Loss: 0.0721\n",
      "Epoch 66/300 - Train Loss: 0.0837, Val Loss: 0.0730\n",
      "Epoch 67/300 - Train Loss: 0.0851, Val Loss: 0.0715\n",
      "Epoch 68/300 - Train Loss: 0.0849, Val Loss: 0.0736\n",
      "Epoch 69/300 - Train Loss: 0.0869, Val Loss: 0.0710\n",
      "Epoch 70/300 - Train Loss: 0.0875, Val Loss: 0.0708\n",
      "Epoch 71/300 - Train Loss: 0.0849, Val Loss: 0.0743\n",
      "Epoch 72/300 - Train Loss: 0.0863, Val Loss: 0.0705\n",
      "Epoch 73/300 - Train Loss: 0.0871, Val Loss: 0.0769\n",
      "Epoch 74/300 - Train Loss: 0.0856, Val Loss: 0.0721\n",
      "Epoch 75/300 - Train Loss: 0.0871, Val Loss: 0.0705\n",
      "Epoch 76/300 - Train Loss: 0.0870, Val Loss: 0.0725\n",
      "Epoch 77/300 - Train Loss: 0.0867, Val Loss: 0.0718\n",
      "Epoch 78/300 - Train Loss: 0.0862, Val Loss: 0.0766\n",
      "Epoch 79/300 - Train Loss: 0.0867, Val Loss: 0.0753\n",
      "Epoch 80/300 - Train Loss: 0.0870, Val Loss: 0.0773\n",
      "Epoch 81/300 - Train Loss: 0.0875, Val Loss: 0.0702\n",
      "Epoch 82/300 - Train Loss: 0.0867, Val Loss: 0.0736\n",
      "Epoch 83/300 - Train Loss: 0.0863, Val Loss: 0.0720\n",
      "Epoch 84/300 - Train Loss: 0.0877, Val Loss: 0.0709\n",
      "Epoch 85/300 - Train Loss: 0.0866, Val Loss: 0.0744\n",
      "Epoch 86/300 - Train Loss: 0.0864, Val Loss: 0.0709\n",
      "Epoch 87/300 - Train Loss: 0.0882, Val Loss: 0.0790\n",
      "Epoch 88/300 - Train Loss: 0.0874, Val Loss: 0.0715\n",
      "Epoch 89/300 - Train Loss: 0.0866, Val Loss: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:28:02,820] Trial 99 finished with value: 0.9689558348325357 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.5497619949288474, 'learning_rate': 0.0001222122385550009, 'batch_size': 128, 'weight_decay': 0.0020333512952230583}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/300 - Train Loss: 0.0890, Val Loss: 0.0736\n",
      "Early stopping at epoch 90\n",
      "Macro F1 Score: 0.9690, Macro Precision: 0.9681, Macro Recall: 0.9701\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 101\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2267497936884703, LR=0.00027832170364323243, BS=32, WD=0.005146583759422341\n",
      "Epoch 1/300 - Train Loss: 0.1755, Val Loss: 0.0846\n",
      "Epoch 2/300 - Train Loss: 0.1044, Val Loss: 0.0847\n",
      "Epoch 3/300 - Train Loss: 0.1026, Val Loss: 0.0782\n",
      "Epoch 4/300 - Train Loss: 0.0993, Val Loss: 0.0727\n",
      "Epoch 5/300 - Train Loss: 0.0997, Val Loss: 0.0725\n",
      "Epoch 6/300 - Train Loss: 0.0988, Val Loss: 0.0752\n",
      "Epoch 7/300 - Train Loss: 0.0998, Val Loss: 0.0801\n",
      "Epoch 8/300 - Train Loss: 0.1029, Val Loss: 0.0724\n",
      "Epoch 9/300 - Train Loss: 0.1024, Val Loss: 0.0781\n",
      "Epoch 10/300 - Train Loss: 0.1005, Val Loss: 0.0772\n",
      "Epoch 11/300 - Train Loss: 0.1017, Val Loss: 0.0729\n",
      "Epoch 12/300 - Train Loss: 0.1050, Val Loss: 0.0919\n",
      "Epoch 13/300 - Train Loss: 0.0998, Val Loss: 0.0851\n",
      "Epoch 14/300 - Train Loss: 0.1033, Val Loss: 0.0909\n",
      "Epoch 15/300 - Train Loss: 0.1003, Val Loss: 0.0818\n",
      "Epoch 16/300 - Train Loss: 0.1040, Val Loss: 0.0859\n",
      "Epoch 17/300 - Train Loss: 0.1007, Val Loss: 0.0711\n",
      "Epoch 18/300 - Train Loss: 0.1035, Val Loss: 0.0720\n",
      "Epoch 19/300 - Train Loss: 0.1048, Val Loss: 0.0786\n",
      "Epoch 20/300 - Train Loss: 0.1042, Val Loss: 0.0765\n",
      "Epoch 21/300 - Train Loss: 0.1028, Val Loss: 0.0942\n",
      "Epoch 22/300 - Train Loss: 0.1048, Val Loss: 0.0869\n",
      "Epoch 23/300 - Train Loss: 0.1013, Val Loss: 0.0836\n",
      "Epoch 24/300 - Train Loss: 0.1035, Val Loss: 0.0758\n",
      "Epoch 25/300 - Train Loss: 0.1036, Val Loss: 0.1065\n",
      "Epoch 26/300 - Train Loss: 0.1029, Val Loss: 0.0776\n",
      "Epoch 27/300 - Train Loss: 0.1015, Val Loss: 0.0797\n",
      "Epoch 28/300 - Train Loss: 0.1042, Val Loss: 0.0763\n",
      "Epoch 29/300 - Train Loss: 0.1022, Val Loss: 0.1017\n",
      "Epoch 30/300 - Train Loss: 0.1066, Val Loss: 0.0806\n",
      "Epoch 31/300 - Train Loss: 0.1037, Val Loss: 0.0851\n",
      "Epoch 32/300 - Train Loss: 0.1036, Val Loss: 0.0793\n",
      "Epoch 33/300 - Train Loss: 0.1041, Val Loss: 0.0721\n",
      "Epoch 34/300 - Train Loss: 0.1046, Val Loss: 0.0782\n",
      "Epoch 35/300 - Train Loss: 0.1038, Val Loss: 0.0724\n",
      "Epoch 36/300 - Train Loss: 0.1043, Val Loss: 0.0785\n",
      "Epoch 37/300 - Train Loss: 0.1043, Val Loss: 0.0774\n",
      "Epoch 38/300 - Train Loss: 0.1010, Val Loss: 0.0875\n",
      "Epoch 39/300 - Train Loss: 0.1036, Val Loss: 0.0765\n",
      "Epoch 40/300 - Train Loss: 0.1029, Val Loss: 0.0734\n",
      "Epoch 41/300 - Train Loss: 0.1028, Val Loss: 0.0744\n",
      "Epoch 42/300 - Train Loss: 0.1021, Val Loss: 0.0765\n",
      "Epoch 43/300 - Train Loss: 0.1078, Val Loss: 0.0754\n",
      "Epoch 44/300 - Train Loss: 0.1028, Val Loss: 0.0765\n",
      "Epoch 45/300 - Train Loss: 0.1027, Val Loss: 0.1091\n",
      "Epoch 46/300 - Train Loss: 0.1015, Val Loss: 0.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:31:03,392] Trial 100 finished with value: 0.9670898191760783 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2267497936884703, 'learning_rate': 0.00027832170364323243, 'batch_size': 32, 'weight_decay': 0.005146583759422341}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300 - Train Loss: 0.1020, Val Loss: 0.0723\n",
      "Early stopping at epoch 47\n",
      "Macro F1 Score: 0.9671, Macro Precision: 0.9709, Macro Recall: 0.9635\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 102\n",
      "Training with F1=16, F2=32, D=8, dropout=0.31348666334377573, LR=7.551937164254858e-05, BS=32, WD=0.00799221739442268\n",
      "Epoch 1/300 - Train Loss: 0.2881, Val Loss: 0.1370\n",
      "Epoch 2/300 - Train Loss: 0.1216, Val Loss: 0.0898\n",
      "Epoch 3/300 - Train Loss: 0.1073, Val Loss: 0.0830\n",
      "Epoch 4/300 - Train Loss: 0.1061, Val Loss: 0.0849\n",
      "Epoch 5/300 - Train Loss: 0.0999, Val Loss: 0.0787\n",
      "Epoch 6/300 - Train Loss: 0.0995, Val Loss: 0.0785\n",
      "Epoch 7/300 - Train Loss: 0.0979, Val Loss: 0.0878\n",
      "Epoch 8/300 - Train Loss: 0.0989, Val Loss: 0.0781\n",
      "Epoch 9/300 - Train Loss: 0.0972, Val Loss: 0.0769\n",
      "Epoch 10/300 - Train Loss: 0.0978, Val Loss: 0.0772\n",
      "Epoch 11/300 - Train Loss: 0.0991, Val Loss: 0.0735\n",
      "Epoch 12/300 - Train Loss: 0.0987, Val Loss: 0.0802\n",
      "Epoch 13/300 - Train Loss: 0.0977, Val Loss: 0.0772\n",
      "Epoch 14/300 - Train Loss: 0.0984, Val Loss: 0.0803\n",
      "Epoch 15/300 - Train Loss: 0.0986, Val Loss: 0.0757\n",
      "Epoch 16/300 - Train Loss: 0.0994, Val Loss: 0.0760\n",
      "Epoch 17/300 - Train Loss: 0.0985, Val Loss: 0.0723\n",
      "Epoch 18/300 - Train Loss: 0.1006, Val Loss: 0.0797\n",
      "Epoch 19/300 - Train Loss: 0.1009, Val Loss: 0.0821\n",
      "Epoch 20/300 - Train Loss: 0.1020, Val Loss: 0.0740\n",
      "Epoch 21/300 - Train Loss: 0.0997, Val Loss: 0.0851\n",
      "Epoch 22/300 - Train Loss: 0.1017, Val Loss: 0.0750\n",
      "Epoch 23/300 - Train Loss: 0.1052, Val Loss: 0.0731\n",
      "Epoch 24/300 - Train Loss: 0.1034, Val Loss: 0.0735\n",
      "Epoch 25/300 - Train Loss: 0.1009, Val Loss: 0.0752\n",
      "Epoch 26/300 - Train Loss: 0.1026, Val Loss: 0.0758\n",
      "Epoch 27/300 - Train Loss: 0.1033, Val Loss: 0.0739\n",
      "Epoch 28/300 - Train Loss: 0.1043, Val Loss: 0.0837\n",
      "Epoch 29/300 - Train Loss: 0.1041, Val Loss: 0.0777\n",
      "Epoch 30/300 - Train Loss: 0.1047, Val Loss: 0.0749\n",
      "Epoch 31/300 - Train Loss: 0.1073, Val Loss: 0.0769\n",
      "Epoch 32/300 - Train Loss: 0.1064, Val Loss: 0.0889\n",
      "Epoch 33/300 - Train Loss: 0.1063, Val Loss: 0.0785\n",
      "Epoch 34/300 - Train Loss: 0.1057, Val Loss: 0.0767\n",
      "Epoch 35/300 - Train Loss: 0.1044, Val Loss: 0.0850\n",
      "Epoch 36/300 - Train Loss: 0.1044, Val Loss: 0.0743\n",
      "Epoch 37/300 - Train Loss: 0.1050, Val Loss: 0.0836\n",
      "Epoch 38/300 - Train Loss: 0.1066, Val Loss: 0.0725\n",
      "Epoch 39/300 - Train Loss: 0.1051, Val Loss: 0.0784\n",
      "Epoch 40/300 - Train Loss: 0.1076, Val Loss: 0.0789\n",
      "Epoch 41/300 - Train Loss: 0.1086, Val Loss: 0.0777\n",
      "Epoch 42/300 - Train Loss: 0.1043, Val Loss: 0.0743\n",
      "Epoch 43/300 - Train Loss: 0.1060, Val Loss: 0.0751\n",
      "Epoch 44/300 - Train Loss: 0.1070, Val Loss: 0.0756\n",
      "Epoch 45/300 - Train Loss: 0.1076, Val Loss: 0.0775\n",
      "Epoch 46/300 - Train Loss: 0.1083, Val Loss: 0.0751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:34:04,166] Trial 101 finished with value: 0.9704838479130933 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.31348666334377573, 'learning_rate': 7.551937164254858e-05, 'batch_size': 32, 'weight_decay': 0.00799221739442268}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300 - Train Loss: 0.1063, Val Loss: 0.0755\n",
      "Early stopping at epoch 47\n",
      "Macro F1 Score: 0.9705, Macro Precision: 0.9771, Macro Recall: 0.9644\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 103\n",
      "Training with F1=16, F2=32, D=8, dropout=0.25661231627088693, LR=4.4144527289811745e-05, BS=32, WD=0.006715944449843334\n",
      "Epoch 1/300 - Train Loss: 0.3755, Val Loss: 0.1856\n",
      "Epoch 2/300 - Train Loss: 0.1799, Val Loss: 0.1369\n",
      "Epoch 3/300 - Train Loss: 0.1357, Val Loss: 0.1026\n",
      "Epoch 4/300 - Train Loss: 0.1170, Val Loss: 0.0883\n",
      "Epoch 5/300 - Train Loss: 0.1088, Val Loss: 0.0817\n",
      "Epoch 6/300 - Train Loss: 0.1043, Val Loss: 0.0806\n",
      "Epoch 7/300 - Train Loss: 0.1047, Val Loss: 0.0879\n",
      "Epoch 8/300 - Train Loss: 0.0985, Val Loss: 0.0767\n",
      "Epoch 9/300 - Train Loss: 0.0990, Val Loss: 0.0812\n",
      "Epoch 10/300 - Train Loss: 0.0959, Val Loss: 0.0767\n",
      "Epoch 11/300 - Train Loss: 0.0964, Val Loss: 0.0798\n",
      "Epoch 12/300 - Train Loss: 0.0924, Val Loss: 0.0744\n",
      "Epoch 13/300 - Train Loss: 0.0965, Val Loss: 0.0780\n",
      "Epoch 14/300 - Train Loss: 0.0959, Val Loss: 0.0729\n",
      "Epoch 15/300 - Train Loss: 0.0947, Val Loss: 0.0778\n",
      "Epoch 16/300 - Train Loss: 0.0945, Val Loss: 0.0727\n",
      "Epoch 17/300 - Train Loss: 0.0927, Val Loss: 0.0711\n",
      "Epoch 18/300 - Train Loss: 0.0920, Val Loss: 0.0798\n",
      "Epoch 19/300 - Train Loss: 0.0951, Val Loss: 0.0736\n",
      "Epoch 20/300 - Train Loss: 0.0919, Val Loss: 0.0713\n",
      "Epoch 21/300 - Train Loss: 0.0925, Val Loss: 0.0730\n",
      "Epoch 22/300 - Train Loss: 0.0935, Val Loss: 0.0767\n",
      "Epoch 23/300 - Train Loss: 0.0938, Val Loss: 0.0714\n",
      "Epoch 24/300 - Train Loss: 0.0939, Val Loss: 0.0696\n",
      "Epoch 25/300 - Train Loss: 0.0954, Val Loss: 0.0738\n",
      "Epoch 26/300 - Train Loss: 0.0938, Val Loss: 0.0719\n",
      "Epoch 27/300 - Train Loss: 0.0938, Val Loss: 0.0715\n",
      "Epoch 28/300 - Train Loss: 0.0932, Val Loss: 0.0691\n",
      "Epoch 29/300 - Train Loss: 0.0931, Val Loss: 0.0708\n",
      "Epoch 30/300 - Train Loss: 0.0955, Val Loss: 0.0685\n",
      "Epoch 31/300 - Train Loss: 0.0938, Val Loss: 0.0717\n",
      "Epoch 32/300 - Train Loss: 0.0945, Val Loss: 0.0719\n",
      "Epoch 33/300 - Train Loss: 0.0946, Val Loss: 0.0743\n",
      "Epoch 34/300 - Train Loss: 0.0947, Val Loss: 0.0719\n",
      "Epoch 35/300 - Train Loss: 0.0963, Val Loss: 0.0708\n",
      "Epoch 36/300 - Train Loss: 0.0950, Val Loss: 0.0695\n",
      "Epoch 37/300 - Train Loss: 0.0963, Val Loss: 0.0713\n",
      "Epoch 38/300 - Train Loss: 0.0960, Val Loss: 0.0703\n",
      "Epoch 39/300 - Train Loss: 0.0942, Val Loss: 0.0724\n",
      "Epoch 40/300 - Train Loss: 0.0976, Val Loss: 0.0796\n",
      "Epoch 41/300 - Train Loss: 0.0941, Val Loss: 0.0686\n",
      "Epoch 42/300 - Train Loss: 0.0958, Val Loss: 0.0697\n",
      "Epoch 43/300 - Train Loss: 0.0963, Val Loss: 0.0708\n",
      "Epoch 44/300 - Train Loss: 0.0959, Val Loss: 0.0692\n",
      "Epoch 45/300 - Train Loss: 0.0958, Val Loss: 0.0692\n",
      "Epoch 46/300 - Train Loss: 0.0966, Val Loss: 0.0783\n",
      "Epoch 47/300 - Train Loss: 0.0946, Val Loss: 0.0698\n",
      "Epoch 48/300 - Train Loss: 0.0950, Val Loss: 0.0705\n",
      "Epoch 49/300 - Train Loss: 0.0979, Val Loss: 0.0771\n",
      "Epoch 50/300 - Train Loss: 0.0977, Val Loss: 0.0704\n",
      "Epoch 51/300 - Train Loss: 0.0981, Val Loss: 0.0739\n",
      "Epoch 52/300 - Train Loss: 0.0980, Val Loss: 0.0723\n",
      "Epoch 53/300 - Train Loss: 0.0980, Val Loss: 0.0712\n",
      "Epoch 54/300 - Train Loss: 0.0977, Val Loss: 0.0787\n",
      "Epoch 55/300 - Train Loss: 0.0968, Val Loss: 0.0726\n",
      "Epoch 56/300 - Train Loss: 0.0971, Val Loss: 0.0690\n",
      "Epoch 57/300 - Train Loss: 0.0989, Val Loss: 0.0721\n",
      "Epoch 58/300 - Train Loss: 0.0975, Val Loss: 0.0735\n",
      "Epoch 59/300 - Train Loss: 0.0982, Val Loss: 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:37:54,903] Trial 102 finished with value: 0.9667004117021416 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.25661231627088693, 'learning_rate': 4.4144527289811745e-05, 'batch_size': 32, 'weight_decay': 0.006715944449843334}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300 - Train Loss: 0.0977, Val Loss: 0.0729\n",
      "Early stopping at epoch 60\n",
      "Macro F1 Score: 0.9667, Macro Precision: 0.9637, Macro Recall: 0.9701\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       1.00      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 104\n",
      "Training with F1=16, F2=32, D=8, dropout=0.18082269965344794, LR=5.2615790496500225e-05, BS=32, WD=0.006187987817092372\n",
      "Epoch 1/300 - Train Loss: 0.3249, Val Loss: 0.1637\n",
      "Epoch 2/300 - Train Loss: 0.1457, Val Loss: 0.0935\n",
      "Epoch 3/300 - Train Loss: 0.1083, Val Loss: 0.0901\n",
      "Epoch 4/300 - Train Loss: 0.1016, Val Loss: 0.0852\n",
      "Epoch 5/300 - Train Loss: 0.0995, Val Loss: 0.0846\n",
      "Epoch 6/300 - Train Loss: 0.0955, Val Loss: 0.0807\n",
      "Epoch 7/300 - Train Loss: 0.0931, Val Loss: 0.0757\n",
      "Epoch 8/300 - Train Loss: 0.0922, Val Loss: 0.0738\n",
      "Epoch 9/300 - Train Loss: 0.0928, Val Loss: 0.0696\n",
      "Epoch 10/300 - Train Loss: 0.0903, Val Loss: 0.0727\n",
      "Epoch 11/300 - Train Loss: 0.0910, Val Loss: 0.0735\n",
      "Epoch 12/300 - Train Loss: 0.0936, Val Loss: 0.0869\n",
      "Epoch 13/300 - Train Loss: 0.0898, Val Loss: 0.0706\n",
      "Epoch 14/300 - Train Loss: 0.0916, Val Loss: 0.0702\n",
      "Epoch 15/300 - Train Loss: 0.0901, Val Loss: 0.0791\n",
      "Epoch 16/300 - Train Loss: 0.0903, Val Loss: 0.0768\n",
      "Epoch 17/300 - Train Loss: 0.0884, Val Loss: 0.0678\n",
      "Epoch 18/300 - Train Loss: 0.0885, Val Loss: 0.0706\n",
      "Epoch 19/300 - Train Loss: 0.0878, Val Loss: 0.0709\n",
      "Epoch 20/300 - Train Loss: 0.0877, Val Loss: 0.0707\n",
      "Epoch 21/300 - Train Loss: 0.0900, Val Loss: 0.0752\n",
      "Epoch 22/300 - Train Loss: 0.0934, Val Loss: 0.0771\n",
      "Epoch 23/300 - Train Loss: 0.0913, Val Loss: 0.0702\n",
      "Epoch 24/300 - Train Loss: 0.0904, Val Loss: 0.0805\n",
      "Epoch 25/300 - Train Loss: 0.0903, Val Loss: 0.0717\n",
      "Epoch 26/300 - Train Loss: 0.0909, Val Loss: 0.0763\n",
      "Epoch 27/300 - Train Loss: 0.0915, Val Loss: 0.0698\n",
      "Epoch 28/300 - Train Loss: 0.0916, Val Loss: 0.0719\n",
      "Epoch 29/300 - Train Loss: 0.0899, Val Loss: 0.0734\n",
      "Epoch 30/300 - Train Loss: 0.0907, Val Loss: 0.0720\n",
      "Epoch 31/300 - Train Loss: 0.0914, Val Loss: 0.0695\n",
      "Epoch 32/300 - Train Loss: 0.0929, Val Loss: 0.0724\n",
      "Epoch 33/300 - Train Loss: 0.0934, Val Loss: 0.0717\n",
      "Epoch 34/300 - Train Loss: 0.0922, Val Loss: 0.0731\n",
      "Epoch 35/300 - Train Loss: 0.0913, Val Loss: 0.0725\n",
      "Epoch 36/300 - Train Loss: 0.0909, Val Loss: 0.0902\n",
      "Epoch 37/300 - Train Loss: 0.0916, Val Loss: 0.0733\n",
      "Epoch 38/300 - Train Loss: 0.0941, Val Loss: 0.0757\n",
      "Epoch 39/300 - Train Loss: 0.0899, Val Loss: 0.0733\n",
      "Epoch 40/300 - Train Loss: 0.0926, Val Loss: 0.0739\n",
      "Epoch 41/300 - Train Loss: 0.0934, Val Loss: 0.0761\n",
      "Epoch 42/300 - Train Loss: 0.0936, Val Loss: 0.0737\n",
      "Epoch 43/300 - Train Loss: 0.0918, Val Loss: 0.0728\n",
      "Epoch 44/300 - Train Loss: 0.0940, Val Loss: 0.0709\n",
      "Epoch 45/300 - Train Loss: 0.0935, Val Loss: 0.0738\n",
      "Epoch 46/300 - Train Loss: 0.0948, Val Loss: 0.0740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:40:55,687] Trial 103 finished with value: 0.9667232646160624 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.18082269965344794, 'learning_rate': 5.2615790496500225e-05, 'batch_size': 32, 'weight_decay': 0.006187987817092372}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300 - Train Loss: 0.0939, Val Loss: 0.0745\n",
      "Early stopping at epoch 47\n",
      "Macro F1 Score: 0.9667, Macro Precision: 0.9628, Macro Recall: 0.9708\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 105\n",
      "Training with F1=16, F2=32, D=8, dropout=0.281823850445446, LR=6.435625270429218e-05, BS=32, WD=0.00968330422022004\n",
      "Epoch 1/300 - Train Loss: 0.3123, Val Loss: 0.2084\n",
      "Epoch 2/300 - Train Loss: 0.1685, Val Loss: 0.1607\n",
      "Epoch 3/300 - Train Loss: 0.1294, Val Loss: 0.0997\n",
      "Epoch 4/300 - Train Loss: 0.1135, Val Loss: 0.0909\n",
      "Epoch 5/300 - Train Loss: 0.1085, Val Loss: 0.0925\n",
      "Epoch 6/300 - Train Loss: 0.1048, Val Loss: 0.0831\n",
      "Epoch 7/300 - Train Loss: 0.1052, Val Loss: 0.0953\n",
      "Epoch 8/300 - Train Loss: 0.1056, Val Loss: 0.0868\n",
      "Epoch 9/300 - Train Loss: 0.1024, Val Loss: 0.0798\n",
      "Epoch 10/300 - Train Loss: 0.1040, Val Loss: 0.0884\n",
      "Epoch 11/300 - Train Loss: 0.1030, Val Loss: 0.0838\n",
      "Epoch 12/300 - Train Loss: 0.1053, Val Loss: 0.0862\n",
      "Epoch 13/300 - Train Loss: 0.1016, Val Loss: 0.0823\n",
      "Epoch 14/300 - Train Loss: 0.1017, Val Loss: 0.0803\n",
      "Epoch 15/300 - Train Loss: 0.1021, Val Loss: 0.0791\n",
      "Epoch 16/300 - Train Loss: 0.1005, Val Loss: 0.0797\n",
      "Epoch 17/300 - Train Loss: 0.1036, Val Loss: 0.0794\n",
      "Epoch 18/300 - Train Loss: 0.1040, Val Loss: 0.0785\n",
      "Epoch 19/300 - Train Loss: 0.1045, Val Loss: 0.0760\n",
      "Epoch 20/300 - Train Loss: 0.1062, Val Loss: 0.0817\n",
      "Epoch 21/300 - Train Loss: 0.1026, Val Loss: 0.0773\n",
      "Epoch 22/300 - Train Loss: 0.1048, Val Loss: 0.0759\n",
      "Epoch 23/300 - Train Loss: 0.1075, Val Loss: 0.0795\n",
      "Epoch 24/300 - Train Loss: 0.1055, Val Loss: 0.0768\n",
      "Epoch 25/300 - Train Loss: 0.1054, Val Loss: 0.0792\n",
      "Epoch 26/300 - Train Loss: 0.1062, Val Loss: 0.0751\n",
      "Epoch 27/300 - Train Loss: 0.1044, Val Loss: 0.0752\n",
      "Epoch 28/300 - Train Loss: 0.1034, Val Loss: 0.0769\n",
      "Epoch 29/300 - Train Loss: 0.1045, Val Loss: 0.0807\n",
      "Epoch 30/300 - Train Loss: 0.1039, Val Loss: 0.0768\n",
      "Epoch 31/300 - Train Loss: 0.1067, Val Loss: 0.0876\n",
      "Epoch 32/300 - Train Loss: 0.1056, Val Loss: 0.0738\n",
      "Epoch 33/300 - Train Loss: 0.1037, Val Loss: 0.0752\n",
      "Epoch 34/300 - Train Loss: 0.1093, Val Loss: 0.0808\n",
      "Epoch 35/300 - Train Loss: 0.1085, Val Loss: 0.0758\n",
      "Epoch 36/300 - Train Loss: 0.1081, Val Loss: 0.0921\n",
      "Epoch 37/300 - Train Loss: 0.1084, Val Loss: 0.0728\n",
      "Epoch 38/300 - Train Loss: 0.1083, Val Loss: 0.0912\n",
      "Epoch 39/300 - Train Loss: 0.1066, Val Loss: 0.0918\n",
      "Epoch 40/300 - Train Loss: 0.1084, Val Loss: 0.0732\n",
      "Epoch 41/300 - Train Loss: 0.1080, Val Loss: 0.0753\n",
      "Epoch 42/300 - Train Loss: 0.1090, Val Loss: 0.0760\n",
      "Epoch 43/300 - Train Loss: 0.1092, Val Loss: 0.0769\n",
      "Epoch 44/300 - Train Loss: 0.1083, Val Loss: 0.0863\n",
      "Epoch 45/300 - Train Loss: 0.1110, Val Loss: 0.0858\n",
      "Epoch 46/300 - Train Loss: 0.1065, Val Loss: 0.0992\n",
      "Epoch 47/300 - Train Loss: 0.1082, Val Loss: 0.0760\n",
      "Epoch 48/300 - Train Loss: 0.1070, Val Loss: 0.0781\n",
      "Epoch 49/300 - Train Loss: 0.1071, Val Loss: 0.0779\n",
      "Epoch 50/300 - Train Loss: 0.1071, Val Loss: 0.0763\n",
      "Epoch 51/300 - Train Loss: 0.1104, Val Loss: 0.0750\n",
      "Epoch 52/300 - Train Loss: 0.1111, Val Loss: 0.0988\n",
      "Epoch 53/300 - Train Loss: 0.1091, Val Loss: 0.0776\n",
      "Epoch 54/300 - Train Loss: 0.1102, Val Loss: 0.0837\n",
      "Epoch 55/300 - Train Loss: 0.1081, Val Loss: 0.0824\n",
      "Epoch 56/300 - Train Loss: 0.1065, Val Loss: 0.0763\n",
      "Epoch 57/300 - Train Loss: 0.1087, Val Loss: 0.0800\n",
      "Epoch 58/300 - Train Loss: 0.1091, Val Loss: 0.0746\n",
      "Epoch 59/300 - Train Loss: 0.1080, Val Loss: 0.0740\n",
      "Epoch 60/300 - Train Loss: 0.1073, Val Loss: 0.0768\n",
      "Epoch 61/300 - Train Loss: 0.1102, Val Loss: 0.0792\n",
      "Epoch 62/300 - Train Loss: 0.1104, Val Loss: 0.0785\n",
      "Epoch 63/300 - Train Loss: 0.1106, Val Loss: 0.0768\n",
      "Epoch 64/300 - Train Loss: 0.1088, Val Loss: 0.0771\n",
      "Epoch 65/300 - Train Loss: 0.1076, Val Loss: 0.0918\n",
      "Epoch 66/300 - Train Loss: 0.1131, Val Loss: 0.0790\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:45:13,439] Trial 104 finished with value: 0.9689558348325357 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.281823850445446, 'learning_rate': 6.435625270429218e-05, 'batch_size': 32, 'weight_decay': 0.00968330422022004}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/300 - Train Loss: 0.1115, Val Loss: 0.0947\n",
      "Early stopping at epoch 67\n",
      "Macro F1 Score: 0.9690, Macro Precision: 0.9681, Macro Recall: 0.9701\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 106\n",
      "Training with F1=16, F2=32, D=8, dropout=0.10104050015452917, LR=0.00014491899760216218, BS=256, WD=0.0016677942331937005\n",
      "Epoch 1/300 - Train Loss: 0.4179, Val Loss: 0.2090\n",
      "Epoch 2/300 - Train Loss: 0.1607, Val Loss: 0.1190\n",
      "Epoch 3/300 - Train Loss: 0.1132, Val Loss: 0.0888\n",
      "Epoch 4/300 - Train Loss: 0.0976, Val Loss: 0.1215\n",
      "Epoch 5/300 - Train Loss: 0.0929, Val Loss: 0.0809\n",
      "Epoch 6/300 - Train Loss: 0.0866, Val Loss: 0.0859\n",
      "Epoch 7/300 - Train Loss: 0.0853, Val Loss: 0.0789\n",
      "Epoch 8/300 - Train Loss: 0.0832, Val Loss: 0.0769\n",
      "Epoch 9/300 - Train Loss: 0.0801, Val Loss: 0.0784\n",
      "Epoch 10/300 - Train Loss: 0.0797, Val Loss: 0.0768\n",
      "Epoch 11/300 - Train Loss: 0.0775, Val Loss: 0.0798\n",
      "Epoch 12/300 - Train Loss: 0.0772, Val Loss: 0.1022\n",
      "Epoch 13/300 - Train Loss: 0.0757, Val Loss: 0.0707\n",
      "Epoch 14/300 - Train Loss: 0.0763, Val Loss: 0.0757\n",
      "Epoch 15/300 - Train Loss: 0.0752, Val Loss: 0.0797\n",
      "Epoch 16/300 - Train Loss: 0.0741, Val Loss: 0.0946\n",
      "Epoch 17/300 - Train Loss: 0.0746, Val Loss: 0.0837\n",
      "Epoch 18/300 - Train Loss: 0.0733, Val Loss: 0.0719\n",
      "Epoch 19/300 - Train Loss: 0.0718, Val Loss: 0.0665\n",
      "Epoch 20/300 - Train Loss: 0.0714, Val Loss: 0.0803\n",
      "Epoch 21/300 - Train Loss: 0.0697, Val Loss: 0.0815\n",
      "Epoch 22/300 - Train Loss: 0.0704, Val Loss: 0.0674\n",
      "Epoch 23/300 - Train Loss: 0.0697, Val Loss: 0.0730\n",
      "Epoch 24/300 - Train Loss: 0.0694, Val Loss: 0.0714\n",
      "Epoch 25/300 - Train Loss: 0.0705, Val Loss: 0.0674\n",
      "Epoch 26/300 - Train Loss: 0.0693, Val Loss: 0.0771\n",
      "Epoch 27/300 - Train Loss: 0.0698, Val Loss: 0.0774\n",
      "Epoch 28/300 - Train Loss: 0.0687, Val Loss: 0.0751\n",
      "Epoch 29/300 - Train Loss: 0.0686, Val Loss: 0.0648\n",
      "Epoch 30/300 - Train Loss: 0.0674, Val Loss: 0.0660\n",
      "Epoch 31/300 - Train Loss: 0.0690, Val Loss: 0.0768\n",
      "Epoch 32/300 - Train Loss: 0.0696, Val Loss: 0.0735\n",
      "Epoch 33/300 - Train Loss: 0.0675, Val Loss: 0.0668\n",
      "Epoch 34/300 - Train Loss: 0.0659, Val Loss: 0.0807\n",
      "Epoch 35/300 - Train Loss: 0.0657, Val Loss: 0.0697\n",
      "Epoch 36/300 - Train Loss: 0.0682, Val Loss: 0.0780\n",
      "Epoch 37/300 - Train Loss: 0.0656, Val Loss: 0.0746\n",
      "Epoch 38/300 - Train Loss: 0.0647, Val Loss: 0.0647\n",
      "Epoch 39/300 - Train Loss: 0.0648, Val Loss: 0.0673\n",
      "Epoch 40/300 - Train Loss: 0.0641, Val Loss: 0.0672\n",
      "Epoch 41/300 - Train Loss: 0.0659, Val Loss: 0.0657\n",
      "Epoch 42/300 - Train Loss: 0.0651, Val Loss: 0.0726\n",
      "Epoch 43/300 - Train Loss: 0.0654, Val Loss: 0.0653\n",
      "Epoch 44/300 - Train Loss: 0.0637, Val Loss: 0.0726\n",
      "Epoch 45/300 - Train Loss: 0.0643, Val Loss: 0.0729\n",
      "Epoch 46/300 - Train Loss: 0.0642, Val Loss: 0.0701\n",
      "Epoch 47/300 - Train Loss: 0.0649, Val Loss: 0.0737\n",
      "Epoch 48/300 - Train Loss: 0.0638, Val Loss: 0.0687\n",
      "Epoch 49/300 - Train Loss: 0.0642, Val Loss: 0.0660\n",
      "Epoch 50/300 - Train Loss: 0.0628, Val Loss: 0.0663\n",
      "Epoch 51/300 - Train Loss: 0.0626, Val Loss: 0.0707\n",
      "Epoch 52/300 - Train Loss: 0.0628, Val Loss: 0.0671\n",
      "Epoch 53/300 - Train Loss: 0.0626, Val Loss: 0.0692\n",
      "Epoch 54/300 - Train Loss: 0.0615, Val Loss: 0.0682\n",
      "Epoch 55/300 - Train Loss: 0.0625, Val Loss: 0.0697\n",
      "Epoch 56/300 - Train Loss: 0.0623, Val Loss: 0.0657\n",
      "Epoch 57/300 - Train Loss: 0.0622, Val Loss: 0.0660\n",
      "Epoch 58/300 - Train Loss: 0.0618, Val Loss: 0.0649\n",
      "Epoch 59/300 - Train Loss: 0.0623, Val Loss: 0.0652\n",
      "Epoch 60/300 - Train Loss: 0.0626, Val Loss: 0.0651\n",
      "Epoch 61/300 - Train Loss: 0.0610, Val Loss: 0.0687\n",
      "Epoch 62/300 - Train Loss: 0.0612, Val Loss: 0.0724\n",
      "Epoch 63/300 - Train Loss: 0.0631, Val Loss: 0.0690\n",
      "Epoch 64/300 - Train Loss: 0.0624, Val Loss: 0.0693\n",
      "Epoch 65/300 - Train Loss: 0.0606, Val Loss: 0.0666\n",
      "Epoch 66/300 - Train Loss: 0.0604, Val Loss: 0.0672\n",
      "Epoch 67/300 - Train Loss: 0.0617, Val Loss: 0.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:48:42,858] Trial 105 finished with value: 0.9660655468159328 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.10104050015452917, 'learning_rate': 0.00014491899760216218, 'batch_size': 256, 'weight_decay': 0.0016677942331937005}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/300 - Train Loss: 0.0612, Val Loss: 0.0680\n",
      "Early stopping at epoch 68\n",
      "Macro F1 Score: 0.9661, Macro Precision: 0.9581, Macro Recall: 0.9747\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 107\n",
      "Training with F1=4, F2=32, D=8, dropout=0.6242908856880177, LR=8.209342172371387e-05, BS=32, WD=0.0038377639241892487\n",
      "Epoch 1/300 - Train Loss: 0.4827, Val Loss: 0.3153\n",
      "Epoch 2/300 - Train Loss: 0.2710, Val Loss: 0.2328\n",
      "Epoch 3/300 - Train Loss: 0.2302, Val Loss: 0.1655\n",
      "Epoch 4/300 - Train Loss: 0.1872, Val Loss: 0.1362\n",
      "Epoch 5/300 - Train Loss: 0.1469, Val Loss: 0.1140\n",
      "Epoch 6/300 - Train Loss: 0.1400, Val Loss: 0.1061\n",
      "Epoch 7/300 - Train Loss: 0.1355, Val Loss: 0.1060\n",
      "Epoch 8/300 - Train Loss: 0.1311, Val Loss: 0.0957\n",
      "Epoch 9/300 - Train Loss: 0.1299, Val Loss: 0.0969\n",
      "Epoch 10/300 - Train Loss: 0.1278, Val Loss: 0.0904\n",
      "Epoch 11/300 - Train Loss: 0.1295, Val Loss: 0.0921\n",
      "Epoch 12/300 - Train Loss: 0.1284, Val Loss: 0.0903\n",
      "Epoch 13/300 - Train Loss: 0.1268, Val Loss: 0.0913\n",
      "Epoch 14/300 - Train Loss: 0.1296, Val Loss: 0.0904\n",
      "Epoch 15/300 - Train Loss: 0.1271, Val Loss: 0.0852\n",
      "Epoch 16/300 - Train Loss: 0.1295, Val Loss: 0.0873\n",
      "Epoch 17/300 - Train Loss: 0.1254, Val Loss: 0.0884\n",
      "Epoch 18/300 - Train Loss: 0.1260, Val Loss: 0.0866\n",
      "Epoch 19/300 - Train Loss: 0.1256, Val Loss: 0.0815\n",
      "Epoch 20/300 - Train Loss: 0.1296, Val Loss: 0.0856\n",
      "Epoch 21/300 - Train Loss: 0.1265, Val Loss: 0.0892\n",
      "Epoch 22/300 - Train Loss: 0.1253, Val Loss: 0.0840\n",
      "Epoch 23/300 - Train Loss: 0.1267, Val Loss: 0.0828\n",
      "Epoch 24/300 - Train Loss: 0.1245, Val Loss: 0.0807\n",
      "Epoch 25/300 - Train Loss: 0.1264, Val Loss: 0.0792\n",
      "Epoch 26/300 - Train Loss: 0.1244, Val Loss: 0.0815\n",
      "Epoch 27/300 - Train Loss: 0.1231, Val Loss: 0.0802\n",
      "Epoch 28/300 - Train Loss: 0.1232, Val Loss: 0.0822\n",
      "Epoch 29/300 - Train Loss: 0.1190, Val Loss: 0.0820\n",
      "Epoch 30/300 - Train Loss: 0.1159, Val Loss: 0.0816\n",
      "Epoch 31/300 - Train Loss: 0.1174, Val Loss: 0.0795\n",
      "Epoch 32/300 - Train Loss: 0.1188, Val Loss: 0.0770\n",
      "Epoch 33/300 - Train Loss: 0.1159, Val Loss: 0.0786\n",
      "Epoch 34/300 - Train Loss: 0.1193, Val Loss: 0.0781\n",
      "Epoch 35/300 - Train Loss: 0.1176, Val Loss: 0.0764\n",
      "Epoch 36/300 - Train Loss: 0.1199, Val Loss: 0.0743\n",
      "Epoch 37/300 - Train Loss: 0.1189, Val Loss: 0.0846\n",
      "Epoch 38/300 - Train Loss: 0.1154, Val Loss: 0.0794\n",
      "Epoch 39/300 - Train Loss: 0.1187, Val Loss: 0.0774\n",
      "Epoch 40/300 - Train Loss: 0.1182, Val Loss: 0.0803\n",
      "Epoch 41/300 - Train Loss: 0.1180, Val Loss: 0.0890\n",
      "Epoch 42/300 - Train Loss: 0.1186, Val Loss: 0.0800\n",
      "Epoch 43/300 - Train Loss: 0.1174, Val Loss: 0.0824\n",
      "Epoch 44/300 - Train Loss: 0.1211, Val Loss: 0.0815\n",
      "Epoch 45/300 - Train Loss: 0.1166, Val Loss: 0.0813\n",
      "Epoch 46/300 - Train Loss: 0.1163, Val Loss: 0.0769\n",
      "Epoch 47/300 - Train Loss: 0.1191, Val Loss: 0.0767\n",
      "Epoch 48/300 - Train Loss: 0.1189, Val Loss: 0.0771\n",
      "Epoch 49/300 - Train Loss: 0.1179, Val Loss: 0.0786\n",
      "Epoch 50/300 - Train Loss: 0.1171, Val Loss: 0.0839\n",
      "Epoch 51/300 - Train Loss: 0.1195, Val Loss: 0.0773\n",
      "Epoch 52/300 - Train Loss: 0.1180, Val Loss: 0.0832\n",
      "Epoch 53/300 - Train Loss: 0.1186, Val Loss: 0.0771\n",
      "Epoch 54/300 - Train Loss: 0.1154, Val Loss: 0.0792\n",
      "Epoch 55/300 - Train Loss: 0.1189, Val Loss: 0.0814\n",
      "Epoch 56/300 - Train Loss: 0.1190, Val Loss: 0.0779\n",
      "Epoch 57/300 - Train Loss: 0.1189, Val Loss: 0.0777\n",
      "Epoch 58/300 - Train Loss: 0.1184, Val Loss: 0.0810\n",
      "Epoch 59/300 - Train Loss: 0.1174, Val Loss: 0.0833\n",
      "Epoch 60/300 - Train Loss: 0.1144, Val Loss: 0.0835\n",
      "Epoch 61/300 - Train Loss: 0.1191, Val Loss: 0.0822\n",
      "Epoch 62/300 - Train Loss: 0.1184, Val Loss: 0.0775\n",
      "Epoch 63/300 - Train Loss: 0.1149, Val Loss: 0.0779\n",
      "Epoch 64/300 - Train Loss: 0.1177, Val Loss: 0.0779\n",
      "Epoch 65/300 - Train Loss: 0.1176, Val Loss: 0.0836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:51:03,776] Trial 106 finished with value: 0.9688099798777184 and parameters: {'F1': 4, 'F2': 32, 'D': 8, 'dropout': 0.6242908856880177, 'learning_rate': 8.209342172371387e-05, 'batch_size': 32, 'weight_decay': 0.0038377639241892487}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/300 - Train Loss: 0.1192, Val Loss: 0.0824\n",
      "Early stopping at epoch 66\n",
      "Macro F1 Score: 0.9688, Macro Precision: 0.9806, Macro Recall: 0.9581\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.98      0.92      0.95        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 108\n",
      "Training with F1=16, F2=32, D=8, dropout=0.24254196066934794, LR=0.0009002180217203852, BS=32, WD=0.008245759229596611\n",
      "Epoch 1/300 - Train Loss: 0.1490, Val Loss: 0.0883\n",
      "Epoch 2/300 - Train Loss: 0.1150, Val Loss: 0.0868\n",
      "Epoch 3/300 - Train Loss: 0.1216, Val Loss: 0.1579\n",
      "Epoch 4/300 - Train Loss: 0.1237, Val Loss: 0.0937\n",
      "Epoch 5/300 - Train Loss: 0.1201, Val Loss: 0.1026\n",
      "Epoch 6/300 - Train Loss: 0.1203, Val Loss: 0.0856\n",
      "Epoch 7/300 - Train Loss: 0.1200, Val Loss: 0.0942\n",
      "Epoch 8/300 - Train Loss: 0.1194, Val Loss: 0.0805\n",
      "Epoch 9/300 - Train Loss: 0.1194, Val Loss: 0.0876\n",
      "Epoch 10/300 - Train Loss: 0.1180, Val Loss: 0.1061\n",
      "Epoch 11/300 - Train Loss: 0.1190, Val Loss: 0.0868\n",
      "Epoch 12/300 - Train Loss: 0.1196, Val Loss: 0.1219\n",
      "Epoch 13/300 - Train Loss: 0.1176, Val Loss: 0.1197\n",
      "Epoch 14/300 - Train Loss: 0.1208, Val Loss: 0.0994\n",
      "Epoch 15/300 - Train Loss: 0.1201, Val Loss: 0.0912\n",
      "Epoch 16/300 - Train Loss: 0.1172, Val Loss: 0.0897\n",
      "Epoch 17/300 - Train Loss: 0.1164, Val Loss: 0.0826\n",
      "Epoch 18/300 - Train Loss: 0.1176, Val Loss: 0.0895\n",
      "Epoch 19/300 - Train Loss: 0.1175, Val Loss: 0.1091\n",
      "Epoch 20/300 - Train Loss: 0.1196, Val Loss: 0.0853\n",
      "Epoch 21/300 - Train Loss: 0.1181, Val Loss: 0.0853\n",
      "Epoch 22/300 - Train Loss: 0.1190, Val Loss: 0.0871\n",
      "Epoch 23/300 - Train Loss: 0.1196, Val Loss: 0.0829\n",
      "Epoch 24/300 - Train Loss: 0.1188, Val Loss: 0.0886\n",
      "Epoch 25/300 - Train Loss: 0.1175, Val Loss: 0.1119\n",
      "Epoch 26/300 - Train Loss: 0.1196, Val Loss: 0.1067\n",
      "Epoch 27/300 - Train Loss: 0.1177, Val Loss: 0.1012\n",
      "Epoch 28/300 - Train Loss: 0.1183, Val Loss: 0.0800\n",
      "Epoch 29/300 - Train Loss: 0.1200, Val Loss: 0.0871\n",
      "Epoch 30/300 - Train Loss: 0.1186, Val Loss: 0.0852\n",
      "Epoch 31/300 - Train Loss: 0.1178, Val Loss: 0.0943\n",
      "Epoch 32/300 - Train Loss: 0.1206, Val Loss: 0.0879\n",
      "Epoch 33/300 - Train Loss: 0.1186, Val Loss: 0.0959\n",
      "Epoch 34/300 - Train Loss: 0.1184, Val Loss: 0.0858\n",
      "Epoch 35/300 - Train Loss: 0.1192, Val Loss: 0.0799\n",
      "Epoch 36/300 - Train Loss: 0.1181, Val Loss: 0.0835\n",
      "Epoch 37/300 - Train Loss: 0.1193, Val Loss: 0.0902\n",
      "Epoch 38/300 - Train Loss: 0.1186, Val Loss: 0.1106\n",
      "Epoch 39/300 - Train Loss: 0.1186, Val Loss: 0.1016\n",
      "Epoch 40/300 - Train Loss: 0.1170, Val Loss: 0.0885\n",
      "Epoch 41/300 - Train Loss: 0.1166, Val Loss: 0.0803\n",
      "Epoch 42/300 - Train Loss: 0.1177, Val Loss: 0.0986\n",
      "Epoch 43/300 - Train Loss: 0.1183, Val Loss: 0.0885\n",
      "Epoch 44/300 - Train Loss: 0.1174, Val Loss: 0.0980\n",
      "Epoch 45/300 - Train Loss: 0.1169, Val Loss: 0.0842\n",
      "Epoch 46/300 - Train Loss: 0.1150, Val Loss: 0.0891\n",
      "Epoch 47/300 - Train Loss: 0.1153, Val Loss: 0.0899\n",
      "Epoch 48/300 - Train Loss: 0.1157, Val Loss: 0.1015\n",
      "Epoch 49/300 - Train Loss: 0.1215, Val Loss: 0.0815\n",
      "Epoch 50/300 - Train Loss: 0.1163, Val Loss: 0.1023\n",
      "Epoch 51/300 - Train Loss: 0.1189, Val Loss: 0.0912\n",
      "Epoch 52/300 - Train Loss: 0.1172, Val Loss: 0.0936\n",
      "Epoch 53/300 - Train Loss: 0.1163, Val Loss: 0.0832\n",
      "Epoch 54/300 - Train Loss: 0.1161, Val Loss: 0.0943\n",
      "Epoch 55/300 - Train Loss: 0.1185, Val Loss: 0.0809\n",
      "Epoch 56/300 - Train Loss: 0.1166, Val Loss: 0.0819\n",
      "Epoch 57/300 - Train Loss: 0.1156, Val Loss: 0.1175\n",
      "Epoch 58/300 - Train Loss: 0.1179, Val Loss: 0.1038\n",
      "Epoch 59/300 - Train Loss: 0.1185, Val Loss: 0.1229\n",
      "Epoch 60/300 - Train Loss: 0.1182, Val Loss: 0.0831\n",
      "Epoch 61/300 - Train Loss: 0.1172, Val Loss: 0.0872\n",
      "Epoch 62/300 - Train Loss: 0.1187, Val Loss: 0.0969\n",
      "Epoch 63/300 - Train Loss: 0.1223, Val Loss: 0.0891\n",
      "Epoch 64/300 - Train Loss: 0.1199, Val Loss: 0.0900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:55:14,262] Trial 107 finished with value: 0.9670051599088764 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.24254196066934794, 'learning_rate': 0.0009002180217203852, 'batch_size': 32, 'weight_decay': 0.008245759229596611}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300 - Train Loss: 0.1172, Val Loss: 0.0837\n",
      "Early stopping at epoch 65\n",
      "Macro F1 Score: 0.9670, Macro Precision: 0.9741, Macro Recall: 0.9606\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.99      0.95      0.97       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.97      1443\n",
      "\n",
      "\n",
      "Trial 109\n",
      "Training with F1=16, F2=32, D=8, dropout=0.26617000017923576, LR=0.0006011854898614242, BS=32, WD=0.0022863416676536263\n",
      "Epoch 1/300 - Train Loss: 0.1591, Val Loss: 0.0766\n",
      "Epoch 2/300 - Train Loss: 0.1042, Val Loss: 0.0778\n",
      "Epoch 3/300 - Train Loss: 0.1020, Val Loss: 0.0772\n",
      "Epoch 4/300 - Train Loss: 0.1000, Val Loss: 0.0725\n",
      "Epoch 5/300 - Train Loss: 0.0983, Val Loss: 0.0734\n",
      "Epoch 6/300 - Train Loss: 0.0988, Val Loss: 0.0788\n",
      "Epoch 7/300 - Train Loss: 0.1003, Val Loss: 0.0809\n",
      "Epoch 8/300 - Train Loss: 0.0978, Val Loss: 0.0711\n",
      "Epoch 9/300 - Train Loss: 0.0988, Val Loss: 0.0802\n",
      "Epoch 10/300 - Train Loss: 0.0996, Val Loss: 0.1035\n",
      "Epoch 11/300 - Train Loss: 0.0988, Val Loss: 0.0811\n",
      "Epoch 12/300 - Train Loss: 0.1010, Val Loss: 0.0758\n",
      "Epoch 13/300 - Train Loss: 0.0991, Val Loss: 0.0920\n",
      "Epoch 14/300 - Train Loss: 0.0995, Val Loss: 0.0980\n",
      "Epoch 15/300 - Train Loss: 0.1007, Val Loss: 0.0858\n",
      "Epoch 16/300 - Train Loss: 0.1008, Val Loss: 0.0898\n",
      "Epoch 17/300 - Train Loss: 0.0995, Val Loss: 0.0780\n",
      "Epoch 18/300 - Train Loss: 0.1000, Val Loss: 0.0742\n",
      "Epoch 19/300 - Train Loss: 0.1038, Val Loss: 0.0999\n",
      "Epoch 20/300 - Train Loss: 0.0992, Val Loss: 0.0749\n",
      "Epoch 21/300 - Train Loss: 0.1039, Val Loss: 0.1334\n",
      "Epoch 22/300 - Train Loss: 0.0994, Val Loss: 0.0789\n",
      "Epoch 23/300 - Train Loss: 0.1012, Val Loss: 0.1092\n",
      "Epoch 24/300 - Train Loss: 0.0989, Val Loss: 0.0904\n",
      "Epoch 25/300 - Train Loss: 0.1001, Val Loss: 0.0728\n",
      "Epoch 26/300 - Train Loss: 0.0996, Val Loss: 0.0773\n",
      "Epoch 27/300 - Train Loss: 0.1015, Val Loss: 0.0837\n",
      "Epoch 28/300 - Train Loss: 0.0988, Val Loss: 0.0732\n",
      "Epoch 29/300 - Train Loss: 0.0998, Val Loss: 0.0810\n",
      "Epoch 30/300 - Train Loss: 0.0998, Val Loss: 0.0776\n",
      "Epoch 31/300 - Train Loss: 0.0986, Val Loss: 0.0739\n",
      "Epoch 32/300 - Train Loss: 0.1008, Val Loss: 0.0791\n",
      "Epoch 33/300 - Train Loss: 0.1013, Val Loss: 0.0790\n",
      "Epoch 34/300 - Train Loss: 0.1013, Val Loss: 0.0740\n",
      "Epoch 35/300 - Train Loss: 0.1015, Val Loss: 0.0745\n",
      "Epoch 36/300 - Train Loss: 0.0979, Val Loss: 0.0888\n",
      "Epoch 37/300 - Train Loss: 0.0983, Val Loss: 0.0842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 05:57:40,645] Trial 108 finished with value: 0.96254703154452 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.26617000017923576, 'learning_rate': 0.0006011854898614242, 'batch_size': 32, 'weight_decay': 0.0022863416676536263}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300 - Train Loss: 0.1024, Val Loss: 0.1043\n",
      "Early stopping at epoch 38\n",
      "Macro F1 Score: 0.9625, Macro Precision: 0.9617, Macro Recall: 0.9637\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.92      0.93      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 110\n",
      "Training with F1=16, F2=32, D=8, dropout=0.29428020220870305, LR=0.00016825823350393387, BS=32, WD=0.004843312940357343\n",
      "Epoch 1/300 - Train Loss: 0.2115, Val Loss: 0.0973\n",
      "Epoch 2/300 - Train Loss: 0.1078, Val Loss: 0.1133\n",
      "Epoch 3/300 - Train Loss: 0.1020, Val Loss: 0.0786\n",
      "Epoch 4/300 - Train Loss: 0.0983, Val Loss: 0.0751\n",
      "Epoch 5/300 - Train Loss: 0.0959, Val Loss: 0.0712\n",
      "Epoch 6/300 - Train Loss: 0.0965, Val Loss: 0.0764\n",
      "Epoch 7/300 - Train Loss: 0.0966, Val Loss: 0.0746\n",
      "Epoch 8/300 - Train Loss: 0.0973, Val Loss: 0.0715\n",
      "Epoch 9/300 - Train Loss: 0.0972, Val Loss: 0.0724\n",
      "Epoch 10/300 - Train Loss: 0.0977, Val Loss: 0.0729\n",
      "Epoch 11/300 - Train Loss: 0.0982, Val Loss: 0.0704\n",
      "Epoch 12/300 - Train Loss: 0.1006, Val Loss: 0.0752\n",
      "Epoch 13/300 - Train Loss: 0.0989, Val Loss: 0.0726\n",
      "Epoch 14/300 - Train Loss: 0.0978, Val Loss: 0.0819\n",
      "Epoch 15/300 - Train Loss: 0.0988, Val Loss: 0.0739\n",
      "Epoch 16/300 - Train Loss: 0.0988, Val Loss: 0.0801\n",
      "Epoch 17/300 - Train Loss: 0.1006, Val Loss: 0.0729\n",
      "Epoch 18/300 - Train Loss: 0.1003, Val Loss: 0.0726\n",
      "Epoch 19/300 - Train Loss: 0.0997, Val Loss: 0.0770\n",
      "Epoch 20/300 - Train Loss: 0.1006, Val Loss: 0.0835\n",
      "Epoch 21/300 - Train Loss: 0.1030, Val Loss: 0.0807\n",
      "Epoch 22/300 - Train Loss: 0.1033, Val Loss: 0.0785\n",
      "Epoch 23/300 - Train Loss: 0.1007, Val Loss: 0.0825\n",
      "Epoch 24/300 - Train Loss: 0.1020, Val Loss: 0.0833\n",
      "Epoch 25/300 - Train Loss: 0.1047, Val Loss: 0.0777\n",
      "Epoch 26/300 - Train Loss: 0.1046, Val Loss: 0.0905\n",
      "Epoch 27/300 - Train Loss: 0.1021, Val Loss: 0.0720\n",
      "Epoch 28/300 - Train Loss: 0.1030, Val Loss: 0.0766\n",
      "Epoch 29/300 - Train Loss: 0.1039, Val Loss: 0.0736\n",
      "Epoch 30/300 - Train Loss: 0.1020, Val Loss: 0.0850\n",
      "Epoch 31/300 - Train Loss: 0.1044, Val Loss: 0.0770\n",
      "Epoch 32/300 - Train Loss: 0.1040, Val Loss: 0.0965\n",
      "Epoch 33/300 - Train Loss: 0.1029, Val Loss: 0.0769\n",
      "Epoch 34/300 - Train Loss: 0.1037, Val Loss: 0.0779\n",
      "Epoch 35/300 - Train Loss: 0.1062, Val Loss: 0.0778\n",
      "Epoch 36/300 - Train Loss: 0.1052, Val Loss: 0.0769\n",
      "Epoch 37/300 - Train Loss: 0.1057, Val Loss: 0.0931\n",
      "Epoch 38/300 - Train Loss: 0.1046, Val Loss: 0.0767\n",
      "Epoch 39/300 - Train Loss: 0.1042, Val Loss: 0.0762\n",
      "Epoch 40/300 - Train Loss: 0.1023, Val Loss: 0.0808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:00:18,645] Trial 109 finished with value: 0.9635510575421532 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.29428020220870305, 'learning_rate': 0.00016825823350393387, 'batch_size': 32, 'weight_decay': 0.004843312940357343}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300 - Train Loss: 0.1036, Val Loss: 0.0954\n",
      "Early stopping at epoch 41\n",
      "Macro F1 Score: 0.9636, Macro Precision: 0.9622, Macro Recall: 0.9651\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 111\n",
      "Training with F1=8, F2=32, D=8, dropout=0.19837321088508153, LR=0.00020870880220776267, BS=256, WD=0.0031313190404923985\n",
      "Epoch 1/300 - Train Loss: 0.4086, Val Loss: 0.2057\n",
      "Epoch 2/300 - Train Loss: 0.1642, Val Loss: 0.1273\n",
      "Epoch 3/300 - Train Loss: 0.1141, Val Loss: 0.0979\n",
      "Epoch 4/300 - Train Loss: 0.1021, Val Loss: 0.0874\n",
      "Epoch 5/300 - Train Loss: 0.0977, Val Loss: 0.0823\n",
      "Epoch 6/300 - Train Loss: 0.0914, Val Loss: 0.0896\n",
      "Epoch 7/300 - Train Loss: 0.0894, Val Loss: 0.0815\n",
      "Epoch 8/300 - Train Loss: 0.0882, Val Loss: 0.0784\n",
      "Epoch 9/300 - Train Loss: 0.0859, Val Loss: 0.0812\n",
      "Epoch 10/300 - Train Loss: 0.0846, Val Loss: 0.0770\n",
      "Epoch 11/300 - Train Loss: 0.0838, Val Loss: 0.0741\n",
      "Epoch 12/300 - Train Loss: 0.0827, Val Loss: 0.0763\n",
      "Epoch 13/300 - Train Loss: 0.0812, Val Loss: 0.0867\n",
      "Epoch 14/300 - Train Loss: 0.0804, Val Loss: 0.0750\n",
      "Epoch 15/300 - Train Loss: 0.0820, Val Loss: 0.0724\n",
      "Epoch 16/300 - Train Loss: 0.0809, Val Loss: 0.0720\n",
      "Epoch 17/300 - Train Loss: 0.0803, Val Loss: 0.0755\n",
      "Epoch 18/300 - Train Loss: 0.0791, Val Loss: 0.0732\n",
      "Epoch 19/300 - Train Loss: 0.0785, Val Loss: 0.0707\n",
      "Epoch 20/300 - Train Loss: 0.0777, Val Loss: 0.0698\n",
      "Epoch 21/300 - Train Loss: 0.0780, Val Loss: 0.0783\n",
      "Epoch 22/300 - Train Loss: 0.0780, Val Loss: 0.0746\n",
      "Epoch 23/300 - Train Loss: 0.0799, Val Loss: 0.0733\n",
      "Epoch 24/300 - Train Loss: 0.0775, Val Loss: 0.0702\n",
      "Epoch 25/300 - Train Loss: 0.0763, Val Loss: 0.0715\n",
      "Epoch 26/300 - Train Loss: 0.0796, Val Loss: 0.0719\n",
      "Epoch 27/300 - Train Loss: 0.0786, Val Loss: 0.0718\n",
      "Epoch 28/300 - Train Loss: 0.0768, Val Loss: 0.0705\n",
      "Epoch 29/300 - Train Loss: 0.0777, Val Loss: 0.0703\n",
      "Epoch 30/300 - Train Loss: 0.0775, Val Loss: 0.0709\n",
      "Epoch 31/300 - Train Loss: 0.0760, Val Loss: 0.0709\n",
      "Epoch 32/300 - Train Loss: 0.0772, Val Loss: 0.0696\n",
      "Epoch 33/300 - Train Loss: 0.0768, Val Loss: 0.0678\n",
      "Epoch 34/300 - Train Loss: 0.0820, Val Loss: 0.0717\n",
      "Epoch 35/300 - Train Loss: 0.0797, Val Loss: 0.0706\n",
      "Epoch 36/300 - Train Loss: 0.0789, Val Loss: 0.0697\n",
      "Epoch 37/300 - Train Loss: 0.0770, Val Loss: 0.0720\n",
      "Epoch 38/300 - Train Loss: 0.0765, Val Loss: 0.0712\n",
      "Epoch 39/300 - Train Loss: 0.0774, Val Loss: 0.0715\n",
      "Epoch 40/300 - Train Loss: 0.0775, Val Loss: 0.0713\n",
      "Epoch 41/300 - Train Loss: 0.0776, Val Loss: 0.0705\n",
      "Epoch 42/300 - Train Loss: 0.0773, Val Loss: 0.0699\n",
      "Epoch 43/300 - Train Loss: 0.0763, Val Loss: 0.0692\n",
      "Epoch 44/300 - Train Loss: 0.0772, Val Loss: 0.0710\n",
      "Epoch 45/300 - Train Loss: 0.0786, Val Loss: 0.0692\n",
      "Epoch 46/300 - Train Loss: 0.0798, Val Loss: 0.0725\n",
      "Epoch 47/300 - Train Loss: 0.0794, Val Loss: 0.0709\n",
      "Epoch 48/300 - Train Loss: 0.0769, Val Loss: 0.0692\n",
      "Epoch 49/300 - Train Loss: 0.0793, Val Loss: 0.0689\n",
      "Epoch 50/300 - Train Loss: 0.0784, Val Loss: 0.0704\n",
      "Epoch 51/300 - Train Loss: 0.0777, Val Loss: 0.0699\n",
      "Epoch 52/300 - Train Loss: 0.0777, Val Loss: 0.0715\n",
      "Epoch 53/300 - Train Loss: 0.0790, Val Loss: 0.0744\n",
      "Epoch 54/300 - Train Loss: 0.0793, Val Loss: 0.0714\n",
      "Epoch 55/300 - Train Loss: 0.0768, Val Loss: 0.0750\n",
      "Epoch 56/300 - Train Loss: 0.0785, Val Loss: 0.0751\n",
      "Epoch 57/300 - Train Loss: 0.0769, Val Loss: 0.0704\n",
      "Epoch 58/300 - Train Loss: 0.0784, Val Loss: 0.0720\n",
      "Epoch 59/300 - Train Loss: 0.0771, Val Loss: 0.0754\n",
      "Epoch 60/300 - Train Loss: 0.0774, Val Loss: 0.0722\n",
      "Epoch 61/300 - Train Loss: 0.0779, Val Loss: 0.0715\n",
      "Epoch 62/300 - Train Loss: 0.0800, Val Loss: 0.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:02:11,478] Trial 110 finished with value: 0.9653068419991664 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.19837321088508153, 'learning_rate': 0.00020870880220776267, 'batch_size': 256, 'weight_decay': 0.0031313190404923985}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300 - Train Loss: 0.0772, Val Loss: 0.0735\n",
      "Early stopping at epoch 63\n",
      "Macro F1 Score: 0.9653, Macro Precision: 0.9618, Macro Recall: 0.9691\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 112\n",
      "Training with F1=16, F2=32, D=8, dropout=0.22287752669536418, LR=0.0001323917965334595, BS=256, WD=0.003941535604360215\n",
      "Epoch 1/300 - Train Loss: 0.4316, Val Loss: 0.2296\n",
      "Epoch 2/300 - Train Loss: 0.1999, Val Loss: 0.2222\n",
      "Epoch 3/300 - Train Loss: 0.1552, Val Loss: 0.1261\n",
      "Epoch 4/300 - Train Loss: 0.1223, Val Loss: 0.0987\n",
      "Epoch 5/300 - Train Loss: 0.1046, Val Loss: 0.0917\n",
      "Epoch 6/300 - Train Loss: 0.0962, Val Loss: 0.0841\n",
      "Epoch 7/300 - Train Loss: 0.0917, Val Loss: 0.0793\n",
      "Epoch 8/300 - Train Loss: 0.0904, Val Loss: 0.0899\n",
      "Epoch 9/300 - Train Loss: 0.0883, Val Loss: 0.0875\n",
      "Epoch 10/300 - Train Loss: 0.0869, Val Loss: 0.0777\n",
      "Epoch 11/300 - Train Loss: 0.0846, Val Loss: 0.0794\n",
      "Epoch 12/300 - Train Loss: 0.0848, Val Loss: 0.0739\n",
      "Epoch 13/300 - Train Loss: 0.0845, Val Loss: 0.0803\n",
      "Epoch 14/300 - Train Loss: 0.0810, Val Loss: 0.0797\n",
      "Epoch 15/300 - Train Loss: 0.0822, Val Loss: 0.0907\n",
      "Epoch 16/300 - Train Loss: 0.0804, Val Loss: 0.0747\n",
      "Epoch 17/300 - Train Loss: 0.0805, Val Loss: 0.0791\n",
      "Epoch 18/300 - Train Loss: 0.0792, Val Loss: 0.0867\n",
      "Epoch 19/300 - Train Loss: 0.0787, Val Loss: 0.0744\n",
      "Epoch 20/300 - Train Loss: 0.0792, Val Loss: 0.0767\n",
      "Epoch 21/300 - Train Loss: 0.0795, Val Loss: 0.0718\n",
      "Epoch 22/300 - Train Loss: 0.0782, Val Loss: 0.0713\n",
      "Epoch 23/300 - Train Loss: 0.0805, Val Loss: 0.0775\n",
      "Epoch 24/300 - Train Loss: 0.0768, Val Loss: 0.0675\n",
      "Epoch 25/300 - Train Loss: 0.0758, Val Loss: 0.0733\n",
      "Epoch 26/300 - Train Loss: 0.0758, Val Loss: 0.0745\n",
      "Epoch 27/300 - Train Loss: 0.0771, Val Loss: 0.0753\n",
      "Epoch 28/300 - Train Loss: 0.0760, Val Loss: 0.0709\n",
      "Epoch 29/300 - Train Loss: 0.0752, Val Loss: 0.0688\n",
      "Epoch 30/300 - Train Loss: 0.0751, Val Loss: 0.0820\n",
      "Epoch 31/300 - Train Loss: 0.0756, Val Loss: 0.0703\n",
      "Epoch 32/300 - Train Loss: 0.0747, Val Loss: 0.0688\n",
      "Epoch 33/300 - Train Loss: 0.0756, Val Loss: 0.0719\n",
      "Epoch 34/300 - Train Loss: 0.0751, Val Loss: 0.0742\n",
      "Epoch 35/300 - Train Loss: 0.0753, Val Loss: 0.0672\n",
      "Epoch 36/300 - Train Loss: 0.0753, Val Loss: 0.0760\n",
      "Epoch 37/300 - Train Loss: 0.0751, Val Loss: 0.0688\n",
      "Epoch 38/300 - Train Loss: 0.0752, Val Loss: 0.0737\n",
      "Epoch 39/300 - Train Loss: 0.0749, Val Loss: 0.0724\n",
      "Epoch 40/300 - Train Loss: 0.0744, Val Loss: 0.0704\n",
      "Epoch 41/300 - Train Loss: 0.0749, Val Loss: 0.0697\n",
      "Epoch 42/300 - Train Loss: 0.0783, Val Loss: 0.0678\n",
      "Epoch 43/300 - Train Loss: 0.0778, Val Loss: 0.0722\n",
      "Epoch 44/300 - Train Loss: 0.0739, Val Loss: 0.0738\n",
      "Epoch 45/300 - Train Loss: 0.0751, Val Loss: 0.0702\n",
      "Epoch 46/300 - Train Loss: 0.0753, Val Loss: 0.0677\n",
      "Epoch 47/300 - Train Loss: 0.0756, Val Loss: 0.0682\n",
      "Epoch 48/300 - Train Loss: 0.0739, Val Loss: 0.0674\n",
      "Epoch 49/300 - Train Loss: 0.0756, Val Loss: 0.0724\n",
      "Epoch 50/300 - Train Loss: 0.0770, Val Loss: 0.0669\n",
      "Epoch 51/300 - Train Loss: 0.0750, Val Loss: 0.0738\n",
      "Epoch 52/300 - Train Loss: 0.0751, Val Loss: 0.0710\n",
      "Epoch 53/300 - Train Loss: 0.0788, Val Loss: 0.0672\n",
      "Epoch 54/300 - Train Loss: 0.0743, Val Loss: 0.0660\n",
      "Epoch 55/300 - Train Loss: 0.0752, Val Loss: 0.0692\n",
      "Epoch 56/300 - Train Loss: 0.0753, Val Loss: 0.0690\n",
      "Epoch 57/300 - Train Loss: 0.0757, Val Loss: 0.0674\n",
      "Epoch 58/300 - Train Loss: 0.0758, Val Loss: 0.0698\n",
      "Epoch 59/300 - Train Loss: 0.0748, Val Loss: 0.0675\n",
      "Epoch 60/300 - Train Loss: 0.0772, Val Loss: 0.0664\n",
      "Epoch 61/300 - Train Loss: 0.0770, Val Loss: 0.0676\n",
      "Epoch 62/300 - Train Loss: 0.0769, Val Loss: 0.0702\n",
      "Epoch 63/300 - Train Loss: 0.0745, Val Loss: 0.0685\n",
      "Epoch 64/300 - Train Loss: 0.0750, Val Loss: 0.0686\n",
      "Epoch 65/300 - Train Loss: 0.0761, Val Loss: 0.0675\n",
      "Epoch 66/300 - Train Loss: 0.0764, Val Loss: 0.0685\n",
      "Epoch 67/300 - Train Loss: 0.0778, Val Loss: 0.0689\n",
      "Epoch 68/300 - Train Loss: 0.0771, Val Loss: 0.0664\n",
      "Epoch 69/300 - Train Loss: 0.0758, Val Loss: 0.0656\n",
      "Epoch 70/300 - Train Loss: 0.0757, Val Loss: 0.0669\n",
      "Epoch 71/300 - Train Loss: 0.0749, Val Loss: 0.0672\n",
      "Epoch 72/300 - Train Loss: 0.0752, Val Loss: 0.0678\n",
      "Epoch 73/300 - Train Loss: 0.0772, Val Loss: 0.0690\n",
      "Epoch 74/300 - Train Loss: 0.0772, Val Loss: 0.0691\n",
      "Epoch 75/300 - Train Loss: 0.0768, Val Loss: 0.0668\n",
      "Epoch 76/300 - Train Loss: 0.0770, Val Loss: 0.0689\n",
      "Epoch 77/300 - Train Loss: 0.0778, Val Loss: 0.0712\n",
      "Epoch 78/300 - Train Loss: 0.0763, Val Loss: 0.0671\n",
      "Epoch 79/300 - Train Loss: 0.0781, Val Loss: 0.0711\n",
      "Epoch 80/300 - Train Loss: 0.0766, Val Loss: 0.0684\n",
      "Epoch 81/300 - Train Loss: 0.0769, Val Loss: 0.0787\n",
      "Epoch 82/300 - Train Loss: 0.0785, Val Loss: 0.0717\n",
      "Epoch 83/300 - Train Loss: 0.0769, Val Loss: 0.0729\n",
      "Epoch 84/300 - Train Loss: 0.0780, Val Loss: 0.0682\n",
      "Epoch 85/300 - Train Loss: 0.0782, Val Loss: 0.0689\n",
      "Epoch 86/300 - Train Loss: 0.0781, Val Loss: 0.0663\n",
      "Epoch 87/300 - Train Loss: 0.0781, Val Loss: 0.0698\n",
      "Epoch 88/300 - Train Loss: 0.0784, Val Loss: 0.0685\n",
      "Epoch 89/300 - Train Loss: 0.0784, Val Loss: 0.0709\n",
      "Epoch 90/300 - Train Loss: 0.0771, Val Loss: 0.0685\n",
      "Epoch 91/300 - Train Loss: 0.0772, Val Loss: 0.0671\n",
      "Epoch 92/300 - Train Loss: 0.0782, Val Loss: 0.0761\n",
      "Epoch 93/300 - Train Loss: 0.0799, Val Loss: 0.0675\n",
      "Epoch 94/300 - Train Loss: 0.0797, Val Loss: 0.0684\n",
      "Epoch 95/300 - Train Loss: 0.0789, Val Loss: 0.0677\n",
      "Epoch 96/300 - Train Loss: 0.0773, Val Loss: 0.0703\n",
      "Epoch 97/300 - Train Loss: 0.0775, Val Loss: 0.0696\n",
      "Epoch 98/300 - Train Loss: 0.0772, Val Loss: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:07:16,805] Trial 111 finished with value: 0.96770218179888 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.22287752669536418, 'learning_rate': 0.0001323917965334595, 'batch_size': 256, 'weight_decay': 0.003941535604360215}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/300 - Train Loss: 0.0786, Val Loss: 0.0749\n",
      "Early stopping at epoch 99\n",
      "Macro F1 Score: 0.9677, Macro Precision: 0.9642, Macro Recall: 0.9715\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 113\n",
      "Training with F1=16, F2=32, D=8, dropout=0.21558863838616082, LR=0.00010843706938031297, BS=256, WD=0.002786347471661268\n",
      "Epoch 1/300 - Train Loss: 0.4502, Val Loss: 0.2384\n",
      "Epoch 2/300 - Train Loss: 0.1940, Val Loss: 0.1770\n",
      "Epoch 3/300 - Train Loss: 0.1424, Val Loss: 0.1346\n",
      "Epoch 4/300 - Train Loss: 0.1145, Val Loss: 0.1045\n",
      "Epoch 5/300 - Train Loss: 0.1030, Val Loss: 0.0996\n",
      "Epoch 6/300 - Train Loss: 0.0953, Val Loss: 0.0843\n",
      "Epoch 7/300 - Train Loss: 0.0920, Val Loss: 0.0852\n",
      "Epoch 8/300 - Train Loss: 0.0896, Val Loss: 0.0930\n",
      "Epoch 9/300 - Train Loss: 0.0874, Val Loss: 0.0819\n",
      "Epoch 10/300 - Train Loss: 0.0862, Val Loss: 0.0820\n",
      "Epoch 11/300 - Train Loss: 0.0851, Val Loss: 0.0855\n",
      "Epoch 12/300 - Train Loss: 0.0833, Val Loss: 0.0799\n",
      "Epoch 13/300 - Train Loss: 0.0832, Val Loss: 0.0797\n",
      "Epoch 14/300 - Train Loss: 0.0811, Val Loss: 0.0803\n",
      "Epoch 15/300 - Train Loss: 0.0808, Val Loss: 0.0748\n",
      "Epoch 16/300 - Train Loss: 0.0810, Val Loss: 0.0748\n",
      "Epoch 17/300 - Train Loss: 0.0802, Val Loss: 0.0734\n",
      "Epoch 18/300 - Train Loss: 0.0789, Val Loss: 0.0791\n",
      "Epoch 19/300 - Train Loss: 0.0776, Val Loss: 0.0756\n",
      "Epoch 20/300 - Train Loss: 0.0765, Val Loss: 0.0797\n",
      "Epoch 21/300 - Train Loss: 0.0774, Val Loss: 0.0775\n",
      "Epoch 22/300 - Train Loss: 0.0770, Val Loss: 0.0738\n",
      "Epoch 23/300 - Train Loss: 0.0745, Val Loss: 0.0771\n",
      "Epoch 24/300 - Train Loss: 0.0755, Val Loss: 0.0748\n",
      "Epoch 25/300 - Train Loss: 0.0744, Val Loss: 0.0749\n",
      "Epoch 26/300 - Train Loss: 0.0759, Val Loss: 0.0893\n",
      "Epoch 27/300 - Train Loss: 0.0753, Val Loss: 0.0776\n",
      "Epoch 28/300 - Train Loss: 0.0737, Val Loss: 0.0709\n",
      "Epoch 29/300 - Train Loss: 0.0745, Val Loss: 0.0736\n",
      "Epoch 30/300 - Train Loss: 0.0729, Val Loss: 0.0790\n",
      "Epoch 31/300 - Train Loss: 0.0724, Val Loss: 0.0723\n",
      "Epoch 32/300 - Train Loss: 0.0733, Val Loss: 0.0728\n",
      "Epoch 33/300 - Train Loss: 0.0723, Val Loss: 0.0742\n",
      "Epoch 34/300 - Train Loss: 0.0734, Val Loss: 0.0713\n",
      "Epoch 35/300 - Train Loss: 0.0725, Val Loss: 0.0712\n",
      "Epoch 36/300 - Train Loss: 0.0722, Val Loss: 0.0882\n",
      "Epoch 37/300 - Train Loss: 0.0727, Val Loss: 0.0742\n",
      "Epoch 38/300 - Train Loss: 0.0724, Val Loss: 0.0695\n",
      "Epoch 39/300 - Train Loss: 0.0747, Val Loss: 0.0696\n",
      "Epoch 40/300 - Train Loss: 0.0710, Val Loss: 0.0718\n",
      "Epoch 41/300 - Train Loss: 0.0745, Val Loss: 0.0731\n",
      "Epoch 42/300 - Train Loss: 0.0731, Val Loss: 0.0721\n",
      "Epoch 43/300 - Train Loss: 0.0713, Val Loss: 0.0709\n",
      "Epoch 44/300 - Train Loss: 0.0710, Val Loss: 0.0755\n",
      "Epoch 45/300 - Train Loss: 0.0712, Val Loss: 0.0696\n",
      "Epoch 46/300 - Train Loss: 0.0721, Val Loss: 0.0733\n",
      "Epoch 47/300 - Train Loss: 0.0725, Val Loss: 0.0684\n",
      "Epoch 48/300 - Train Loss: 0.0701, Val Loss: 0.0757\n",
      "Epoch 49/300 - Train Loss: 0.0711, Val Loss: 0.0720\n",
      "Epoch 50/300 - Train Loss: 0.0703, Val Loss: 0.0746\n",
      "Epoch 51/300 - Train Loss: 0.0710, Val Loss: 0.0676\n",
      "Epoch 52/300 - Train Loss: 0.0708, Val Loss: 0.0742\n",
      "Epoch 53/300 - Train Loss: 0.0703, Val Loss: 0.0730\n",
      "Epoch 54/300 - Train Loss: 0.0706, Val Loss: 0.0691\n",
      "Epoch 55/300 - Train Loss: 0.0699, Val Loss: 0.0726\n",
      "Epoch 56/300 - Train Loss: 0.0699, Val Loss: 0.0724\n",
      "Epoch 57/300 - Train Loss: 0.0703, Val Loss: 0.0681\n",
      "Epoch 58/300 - Train Loss: 0.0715, Val Loss: 0.0715\n",
      "Epoch 59/300 - Train Loss: 0.0712, Val Loss: 0.0704\n",
      "Epoch 60/300 - Train Loss: 0.0721, Val Loss: 0.0730\n",
      "Epoch 61/300 - Train Loss: 0.0709, Val Loss: 0.0705\n",
      "Epoch 62/300 - Train Loss: 0.0699, Val Loss: 0.0693\n",
      "Epoch 63/300 - Train Loss: 0.0692, Val Loss: 0.0729\n",
      "Epoch 64/300 - Train Loss: 0.0720, Val Loss: 0.0690\n",
      "Epoch 65/300 - Train Loss: 0.0705, Val Loss: 0.0680\n",
      "Epoch 66/300 - Train Loss: 0.0703, Val Loss: 0.0703\n",
      "Epoch 67/300 - Train Loss: 0.0687, Val Loss: 0.0683\n",
      "Epoch 68/300 - Train Loss: 0.0696, Val Loss: 0.0690\n",
      "Epoch 69/300 - Train Loss: 0.0707, Val Loss: 0.0678\n",
      "Epoch 70/300 - Train Loss: 0.0703, Val Loss: 0.0672\n",
      "Epoch 71/300 - Train Loss: 0.0711, Val Loss: 0.0700\n",
      "Epoch 72/300 - Train Loss: 0.0710, Val Loss: 0.0707\n",
      "Epoch 73/300 - Train Loss: 0.0707, Val Loss: 0.0694\n",
      "Epoch 74/300 - Train Loss: 0.0692, Val Loss: 0.0699\n",
      "Epoch 75/300 - Train Loss: 0.0694, Val Loss: 0.0689\n",
      "Epoch 76/300 - Train Loss: 0.0728, Val Loss: 0.0692\n",
      "Epoch 77/300 - Train Loss: 0.0710, Val Loss: 0.0689\n",
      "Epoch 78/300 - Train Loss: 0.0711, Val Loss: 0.0676\n",
      "Epoch 79/300 - Train Loss: 0.0704, Val Loss: 0.0703\n",
      "Epoch 80/300 - Train Loss: 0.0712, Val Loss: 0.0680\n",
      "Epoch 81/300 - Train Loss: 0.0711, Val Loss: 0.0677\n",
      "Epoch 82/300 - Train Loss: 0.0695, Val Loss: 0.0685\n",
      "Epoch 83/300 - Train Loss: 0.0705, Val Loss: 0.0681\n",
      "Epoch 84/300 - Train Loss: 0.0708, Val Loss: 0.0665\n",
      "Epoch 85/300 - Train Loss: 0.0708, Val Loss: 0.0685\n",
      "Epoch 86/300 - Train Loss: 0.0725, Val Loss: 0.0665\n",
      "Epoch 87/300 - Train Loss: 0.0722, Val Loss: 0.0687\n",
      "Epoch 88/300 - Train Loss: 0.0707, Val Loss: 0.0688\n",
      "Epoch 89/300 - Train Loss: 0.0724, Val Loss: 0.0700\n",
      "Epoch 90/300 - Train Loss: 0.0716, Val Loss: 0.0690\n",
      "Epoch 91/300 - Train Loss: 0.0706, Val Loss: 0.0702\n",
      "Epoch 92/300 - Train Loss: 0.0706, Val Loss: 0.0676\n",
      "Epoch 93/300 - Train Loss: 0.0700, Val Loss: 0.0700\n",
      "Epoch 94/300 - Train Loss: 0.0711, Val Loss: 0.0704\n",
      "Epoch 95/300 - Train Loss: 0.0695, Val Loss: 0.0669\n",
      "Epoch 96/300 - Train Loss: 0.0695, Val Loss: 0.0669\n",
      "Epoch 97/300 - Train Loss: 0.0708, Val Loss: 0.0671\n",
      "Epoch 98/300 - Train Loss: 0.0711, Val Loss: 0.0669\n",
      "Epoch 99/300 - Train Loss: 0.0717, Val Loss: 0.0671\n",
      "Epoch 100/300 - Train Loss: 0.0705, Val Loss: 0.0690\n",
      "Epoch 101/300 - Train Loss: 0.0723, Val Loss: 0.0707\n",
      "Epoch 102/300 - Train Loss: 0.0714, Val Loss: 0.0675\n",
      "Epoch 103/300 - Train Loss: 0.0709, Val Loss: 0.0701\n",
      "Epoch 104/300 - Train Loss: 0.0728, Val Loss: 0.0676\n",
      "Epoch 105/300 - Train Loss: 0.0714, Val Loss: 0.0716\n",
      "Epoch 106/300 - Train Loss: 0.0736, Val Loss: 0.0675\n",
      "Epoch 107/300 - Train Loss: 0.0709, Val Loss: 0.0659\n",
      "Epoch 108/300 - Train Loss: 0.0716, Val Loss: 0.0703\n",
      "Epoch 109/300 - Train Loss: 0.0712, Val Loss: 0.0676\n",
      "Epoch 110/300 - Train Loss: 0.0709, Val Loss: 0.0678\n",
      "Epoch 111/300 - Train Loss: 0.0707, Val Loss: 0.0705\n",
      "Epoch 112/300 - Train Loss: 0.0716, Val Loss: 0.0663\n",
      "Epoch 113/300 - Train Loss: 0.0722, Val Loss: 0.0764\n",
      "Epoch 114/300 - Train Loss: 0.0719, Val Loss: 0.0684\n",
      "Epoch 115/300 - Train Loss: 0.0712, Val Loss: 0.0678\n",
      "Epoch 116/300 - Train Loss: 0.0722, Val Loss: 0.0681\n",
      "Epoch 117/300 - Train Loss: 0.0708, Val Loss: 0.0698\n",
      "Epoch 118/300 - Train Loss: 0.0741, Val Loss: 0.0728\n",
      "Epoch 119/300 - Train Loss: 0.0728, Val Loss: 0.0725\n",
      "Epoch 120/300 - Train Loss: 0.0710, Val Loss: 0.0680\n",
      "Epoch 121/300 - Train Loss: 0.0727, Val Loss: 0.0706\n",
      "Epoch 122/300 - Train Loss: 0.0722, Val Loss: 0.0688\n",
      "Epoch 123/300 - Train Loss: 0.0736, Val Loss: 0.0712\n",
      "Epoch 124/300 - Train Loss: 0.0724, Val Loss: 0.0707\n",
      "Epoch 125/300 - Train Loss: 0.0715, Val Loss: 0.0717\n",
      "Epoch 126/300 - Train Loss: 0.0717, Val Loss: 0.0703\n",
      "Epoch 127/300 - Train Loss: 0.0729, Val Loss: 0.0768\n",
      "Epoch 128/300 - Train Loss: 0.0718, Val Loss: 0.0697\n",
      "Epoch 129/300 - Train Loss: 0.0727, Val Loss: 0.0715\n",
      "Epoch 130/300 - Train Loss: 0.0710, Val Loss: 0.0691\n",
      "Epoch 131/300 - Train Loss: 0.0724, Val Loss: 0.0707\n",
      "Epoch 132/300 - Train Loss: 0.0730, Val Loss: 0.0683\n",
      "Epoch 133/300 - Train Loss: 0.0721, Val Loss: 0.0699\n",
      "Epoch 134/300 - Train Loss: 0.0725, Val Loss: 0.0674\n",
      "Epoch 135/300 - Train Loss: 0.0727, Val Loss: 0.0695\n",
      "Epoch 136/300 - Train Loss: 0.0759, Val Loss: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:14:19,244] Trial 112 finished with value: 0.9625065751858205 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.21558863838616082, 'learning_rate': 0.00010843706938031297, 'batch_size': 256, 'weight_decay': 0.002786347471661268}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 137/300 - Train Loss: 0.0733, Val Loss: 0.0777\n",
      "Early stopping at epoch 137\n",
      "Macro F1 Score: 0.9625, Macro Precision: 0.9575, Macro Recall: 0.9681\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 114\n",
      "Training with F1=16, F2=32, D=8, dropout=0.134501553582562, LR=0.000362373846140375, BS=256, WD=0.0025823038062115734\n",
      "Epoch 1/300 - Train Loss: 0.3060, Val Loss: 0.1504\n",
      "Epoch 2/300 - Train Loss: 0.1344, Val Loss: 0.0898\n",
      "Epoch 3/300 - Train Loss: 0.0973, Val Loss: 0.1096\n",
      "Epoch 4/300 - Train Loss: 0.0893, Val Loss: 0.0879\n",
      "Epoch 5/300 - Train Loss: 0.0844, Val Loss: 0.0718\n",
      "Epoch 6/300 - Train Loss: 0.0815, Val Loss: 0.0696\n",
      "Epoch 7/300 - Train Loss: 0.0787, Val Loss: 0.0830\n",
      "Epoch 8/300 - Train Loss: 0.0775, Val Loss: 0.0683\n",
      "Epoch 9/300 - Train Loss: 0.0763, Val Loss: 0.0690\n",
      "Epoch 10/300 - Train Loss: 0.0761, Val Loss: 0.0675\n",
      "Epoch 11/300 - Train Loss: 0.0746, Val Loss: 0.0739\n",
      "Epoch 12/300 - Train Loss: 0.0740, Val Loss: 0.0683\n",
      "Epoch 13/300 - Train Loss: 0.0746, Val Loss: 0.0703\n",
      "Epoch 14/300 - Train Loss: 0.0731, Val Loss: 0.0656\n",
      "Epoch 15/300 - Train Loss: 0.0736, Val Loss: 0.0723\n",
      "Epoch 16/300 - Train Loss: 0.0740, Val Loss: 0.0677\n",
      "Epoch 17/300 - Train Loss: 0.0732, Val Loss: 0.0700\n",
      "Epoch 18/300 - Train Loss: 0.0728, Val Loss: 0.0710\n",
      "Epoch 19/300 - Train Loss: 0.0731, Val Loss: 0.0704\n",
      "Epoch 20/300 - Train Loss: 0.0725, Val Loss: 0.0739\n",
      "Epoch 21/300 - Train Loss: 0.0734, Val Loss: 0.0694\n",
      "Epoch 22/300 - Train Loss: 0.0733, Val Loss: 0.0708\n",
      "Epoch 23/300 - Train Loss: 0.0758, Val Loss: 0.0674\n",
      "Epoch 24/300 - Train Loss: 0.0740, Val Loss: 0.0703\n",
      "Epoch 25/300 - Train Loss: 0.0727, Val Loss: 0.0685\n",
      "Epoch 26/300 - Train Loss: 0.0716, Val Loss: 0.0673\n",
      "Epoch 27/300 - Train Loss: 0.0717, Val Loss: 0.0697\n",
      "Epoch 28/300 - Train Loss: 0.0725, Val Loss: 0.0673\n",
      "Epoch 29/300 - Train Loss: 0.0733, Val Loss: 0.0651\n",
      "Epoch 30/300 - Train Loss: 0.0728, Val Loss: 0.0661\n",
      "Epoch 31/300 - Train Loss: 0.0716, Val Loss: 0.0682\n",
      "Epoch 32/300 - Train Loss: 0.0717, Val Loss: 0.0676\n",
      "Epoch 33/300 - Train Loss: 0.0741, Val Loss: 0.0707\n",
      "Epoch 34/300 - Train Loss: 0.0744, Val Loss: 0.0682\n",
      "Epoch 35/300 - Train Loss: 0.0738, Val Loss: 0.0757\n",
      "Epoch 36/300 - Train Loss: 0.0761, Val Loss: 0.0670\n",
      "Epoch 37/300 - Train Loss: 0.0740, Val Loss: 0.0747\n",
      "Epoch 38/300 - Train Loss: 0.0733, Val Loss: 0.0715\n",
      "Epoch 39/300 - Train Loss: 0.0761, Val Loss: 0.0696\n",
      "Epoch 40/300 - Train Loss: 0.0750, Val Loss: 0.0790\n",
      "Epoch 41/300 - Train Loss: 0.0745, Val Loss: 0.0740\n",
      "Epoch 42/300 - Train Loss: 0.0757, Val Loss: 0.0692\n",
      "Epoch 43/300 - Train Loss: 0.0732, Val Loss: 0.0675\n",
      "Epoch 44/300 - Train Loss: 0.0747, Val Loss: 0.0673\n",
      "Epoch 45/300 - Train Loss: 0.0770, Val Loss: 0.0732\n",
      "Epoch 46/300 - Train Loss: 0.0729, Val Loss: 0.0765\n",
      "Epoch 47/300 - Train Loss: 0.0759, Val Loss: 0.0786\n",
      "Epoch 48/300 - Train Loss: 0.0751, Val Loss: 0.0686\n",
      "Epoch 49/300 - Train Loss: 0.0751, Val Loss: 0.0863\n",
      "Epoch 50/300 - Train Loss: 0.0766, Val Loss: 0.0691\n",
      "Epoch 51/300 - Train Loss: 0.0753, Val Loss: 0.0772\n",
      "Epoch 52/300 - Train Loss: 0.0768, Val Loss: 0.0710\n",
      "Epoch 53/300 - Train Loss: 0.0751, Val Loss: 0.0695\n",
      "Epoch 54/300 - Train Loss: 0.0762, Val Loss: 0.0756\n",
      "Epoch 55/300 - Train Loss: 0.0750, Val Loss: 0.1045\n",
      "Epoch 56/300 - Train Loss: 0.0776, Val Loss: 0.0774\n",
      "Epoch 57/300 - Train Loss: 0.0769, Val Loss: 0.1321\n",
      "Epoch 58/300 - Train Loss: 0.0760, Val Loss: 0.0710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:17:21,048] Trial 113 finished with value: 0.9657951465399838 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.134501553582562, 'learning_rate': 0.000362373846140375, 'batch_size': 256, 'weight_decay': 0.0025823038062115734}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300 - Train Loss: 0.0754, Val Loss: 0.1121\n",
      "Early stopping at epoch 59\n",
      "Macro F1 Score: 0.9658, Macro Precision: 0.9558, Macro Recall: 0.9768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 115\n",
      "Training with F1=16, F2=32, D=8, dropout=0.22950064215671723, LR=0.000151962119105182, BS=256, WD=0.00025842652879347203\n",
      "Epoch 1/300 - Train Loss: 0.4084, Val Loss: 0.2258\n",
      "Epoch 2/300 - Train Loss: 0.1854, Val Loss: 0.1569\n",
      "Epoch 3/300 - Train Loss: 0.1294, Val Loss: 0.1214\n",
      "Epoch 4/300 - Train Loss: 0.1043, Val Loss: 0.0928\n",
      "Epoch 5/300 - Train Loss: 0.0949, Val Loss: 0.0894\n",
      "Epoch 6/300 - Train Loss: 0.0912, Val Loss: 0.0808\n",
      "Epoch 7/300 - Train Loss: 0.0885, Val Loss: 0.0876\n",
      "Epoch 8/300 - Train Loss: 0.0864, Val Loss: 0.0851\n",
      "Epoch 9/300 - Train Loss: 0.0828, Val Loss: 0.0993\n",
      "Epoch 10/300 - Train Loss: 0.0827, Val Loss: 0.0721\n",
      "Epoch 11/300 - Train Loss: 0.0814, Val Loss: 0.0719\n",
      "Epoch 12/300 - Train Loss: 0.0787, Val Loss: 0.0718\n",
      "Epoch 13/300 - Train Loss: 0.0783, Val Loss: 0.0772\n",
      "Epoch 14/300 - Train Loss: 0.0776, Val Loss: 0.0717\n",
      "Epoch 15/300 - Train Loss: 0.0772, Val Loss: 0.0741\n",
      "Epoch 16/300 - Train Loss: 0.0775, Val Loss: 0.0765\n",
      "Epoch 17/300 - Train Loss: 0.0747, Val Loss: 0.0706\n",
      "Epoch 18/300 - Train Loss: 0.0743, Val Loss: 0.0784\n",
      "Epoch 19/300 - Train Loss: 0.0744, Val Loss: 0.0773\n",
      "Epoch 20/300 - Train Loss: 0.0732, Val Loss: 0.0726\n",
      "Epoch 21/300 - Train Loss: 0.0721, Val Loss: 0.0735\n",
      "Epoch 22/300 - Train Loss: 0.0707, Val Loss: 0.0787\n",
      "Epoch 23/300 - Train Loss: 0.0707, Val Loss: 0.0717\n",
      "Epoch 24/300 - Train Loss: 0.0705, Val Loss: 0.0701\n",
      "Epoch 25/300 - Train Loss: 0.0694, Val Loss: 0.0744\n",
      "Epoch 26/300 - Train Loss: 0.0689, Val Loss: 0.0751\n",
      "Epoch 27/300 - Train Loss: 0.0689, Val Loss: 0.0710\n",
      "Epoch 28/300 - Train Loss: 0.0684, Val Loss: 0.0684\n",
      "Epoch 29/300 - Train Loss: 0.0682, Val Loss: 0.0684\n",
      "Epoch 30/300 - Train Loss: 0.0683, Val Loss: 0.0738\n",
      "Epoch 31/300 - Train Loss: 0.0670, Val Loss: 0.0737\n",
      "Epoch 32/300 - Train Loss: 0.0661, Val Loss: 0.0723\n",
      "Epoch 33/300 - Train Loss: 0.0657, Val Loss: 0.0713\n",
      "Epoch 34/300 - Train Loss: 0.0658, Val Loss: 0.0698\n",
      "Epoch 35/300 - Train Loss: 0.0663, Val Loss: 0.0750\n",
      "Epoch 36/300 - Train Loss: 0.0643, Val Loss: 0.0673\n",
      "Epoch 37/300 - Train Loss: 0.0650, Val Loss: 0.0668\n",
      "Epoch 38/300 - Train Loss: 0.0643, Val Loss: 0.0662\n",
      "Epoch 39/300 - Train Loss: 0.0640, Val Loss: 0.0888\n",
      "Epoch 40/300 - Train Loss: 0.0644, Val Loss: 0.0662\n",
      "Epoch 41/300 - Train Loss: 0.0632, Val Loss: 0.0727\n",
      "Epoch 42/300 - Train Loss: 0.0638, Val Loss: 0.0676\n",
      "Epoch 43/300 - Train Loss: 0.0631, Val Loss: 0.0683\n",
      "Epoch 44/300 - Train Loss: 0.0636, Val Loss: 0.0663\n",
      "Epoch 45/300 - Train Loss: 0.0618, Val Loss: 0.0695\n",
      "Epoch 46/300 - Train Loss: 0.0620, Val Loss: 0.0697\n",
      "Epoch 47/300 - Train Loss: 0.0609, Val Loss: 0.0719\n",
      "Epoch 48/300 - Train Loss: 0.0632, Val Loss: 0.0635\n",
      "Epoch 49/300 - Train Loss: 0.0610, Val Loss: 0.0691\n",
      "Epoch 50/300 - Train Loss: 0.0618, Val Loss: 0.0716\n",
      "Epoch 51/300 - Train Loss: 0.0613, Val Loss: 0.0671\n",
      "Epoch 52/300 - Train Loss: 0.0611, Val Loss: 0.0661\n",
      "Epoch 53/300 - Train Loss: 0.0607, Val Loss: 0.0726\n",
      "Epoch 54/300 - Train Loss: 0.0595, Val Loss: 0.0667\n",
      "Epoch 55/300 - Train Loss: 0.0599, Val Loss: 0.0679\n",
      "Epoch 56/300 - Train Loss: 0.0602, Val Loss: 0.0688\n",
      "Epoch 57/300 - Train Loss: 0.0598, Val Loss: 0.0684\n",
      "Epoch 58/300 - Train Loss: 0.0585, Val Loss: 0.0699\n",
      "Epoch 59/300 - Train Loss: 0.0610, Val Loss: 0.0659\n",
      "Epoch 60/300 - Train Loss: 0.0585, Val Loss: 0.0722\n",
      "Epoch 61/300 - Train Loss: 0.0581, Val Loss: 0.0663\n",
      "Epoch 62/300 - Train Loss: 0.0570, Val Loss: 0.0708\n",
      "Epoch 63/300 - Train Loss: 0.0570, Val Loss: 0.0716\n",
      "Epoch 64/300 - Train Loss: 0.0555, Val Loss: 0.0674\n",
      "Epoch 65/300 - Train Loss: 0.0554, Val Loss: 0.0688\n",
      "Epoch 66/300 - Train Loss: 0.0558, Val Loss: 0.0673\n",
      "Epoch 67/300 - Train Loss: 0.0550, Val Loss: 0.0701\n",
      "Epoch 68/300 - Train Loss: 0.0556, Val Loss: 0.0667\n",
      "Epoch 69/300 - Train Loss: 0.0573, Val Loss: 0.0711\n",
      "Epoch 70/300 - Train Loss: 0.0559, Val Loss: 0.0715\n",
      "Epoch 71/300 - Train Loss: 0.0565, Val Loss: 0.0677\n",
      "Epoch 72/300 - Train Loss: 0.0555, Val Loss: 0.0689\n",
      "Epoch 73/300 - Train Loss: 0.0550, Val Loss: 0.0670\n",
      "Epoch 74/300 - Train Loss: 0.0550, Val Loss: 0.0690\n",
      "Epoch 75/300 - Train Loss: 0.0548, Val Loss: 0.0663\n",
      "Epoch 76/300 - Train Loss: 0.0552, Val Loss: 0.0683\n",
      "Epoch 77/300 - Train Loss: 0.0537, Val Loss: 0.0697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:21:21,596] Trial 114 finished with value: 0.9650021712451619 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.22950064215671723, 'learning_rate': 0.000151962119105182, 'batch_size': 256, 'weight_decay': 0.00025842652879347203}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300 - Train Loss: 0.0536, Val Loss: 0.0670\n",
      "Early stopping at epoch 78\n",
      "Macro F1 Score: 0.9650, Macro Precision: 0.9590, Macro Recall: 0.9714\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 116\n",
      "Training with F1=16, F2=32, D=8, dropout=0.35281802513767124, LR=0.00012534178698187496, BS=32, WD=0.006255088133426896\n",
      "Epoch 1/300 - Train Loss: 0.2658, Val Loss: 0.1417\n",
      "Epoch 2/300 - Train Loss: 0.1274, Val Loss: 0.0838\n",
      "Epoch 3/300 - Train Loss: 0.1095, Val Loss: 0.0799\n",
      "Epoch 4/300 - Train Loss: 0.1027, Val Loss: 0.0785\n",
      "Epoch 5/300 - Train Loss: 0.1013, Val Loss: 0.0729\n",
      "Epoch 6/300 - Train Loss: 0.1018, Val Loss: 0.0737\n",
      "Epoch 7/300 - Train Loss: 0.0992, Val Loss: 0.0741\n",
      "Epoch 8/300 - Train Loss: 0.1014, Val Loss: 0.0700\n",
      "Epoch 9/300 - Train Loss: 0.0977, Val Loss: 0.0771\n",
      "Epoch 10/300 - Train Loss: 0.0989, Val Loss: 0.0747\n",
      "Epoch 11/300 - Train Loss: 0.1008, Val Loss: 0.0737\n",
      "Epoch 12/300 - Train Loss: 0.1001, Val Loss: 0.0758\n",
      "Epoch 13/300 - Train Loss: 0.1028, Val Loss: 0.0732\n",
      "Epoch 14/300 - Train Loss: 0.1027, Val Loss: 0.0805\n",
      "Epoch 15/300 - Train Loss: 0.1014, Val Loss: 0.0823\n",
      "Epoch 16/300 - Train Loss: 0.1043, Val Loss: 0.0761\n",
      "Epoch 17/300 - Train Loss: 0.1033, Val Loss: 0.0741\n",
      "Epoch 18/300 - Train Loss: 0.1045, Val Loss: 0.0773\n",
      "Epoch 19/300 - Train Loss: 0.1049, Val Loss: 0.0735\n",
      "Epoch 20/300 - Train Loss: 0.1003, Val Loss: 0.0852\n",
      "Epoch 21/300 - Train Loss: 0.1044, Val Loss: 0.0752\n",
      "Epoch 22/300 - Train Loss: 0.1048, Val Loss: 0.0830\n",
      "Epoch 23/300 - Train Loss: 0.1086, Val Loss: 0.0730\n",
      "Epoch 24/300 - Train Loss: 0.1057, Val Loss: 0.0785\n",
      "Epoch 25/300 - Train Loss: 0.1059, Val Loss: 0.0804\n",
      "Epoch 26/300 - Train Loss: 0.1049, Val Loss: 0.0878\n",
      "Epoch 27/300 - Train Loss: 0.1077, Val Loss: 0.0737\n",
      "Epoch 28/300 - Train Loss: 0.1067, Val Loss: 0.0773\n",
      "Epoch 29/300 - Train Loss: 0.1054, Val Loss: 0.0752\n",
      "Epoch 30/300 - Train Loss: 0.1068, Val Loss: 0.0839\n",
      "Epoch 31/300 - Train Loss: 0.1072, Val Loss: 0.0794\n",
      "Epoch 32/300 - Train Loss: 0.1050, Val Loss: 0.0816\n",
      "Epoch 33/300 - Train Loss: 0.1068, Val Loss: 0.0806\n",
      "Epoch 34/300 - Train Loss: 0.1085, Val Loss: 0.0748\n",
      "Epoch 35/300 - Train Loss: 0.1089, Val Loss: 0.0820\n",
      "Epoch 36/300 - Train Loss: 0.1100, Val Loss: 0.0722\n",
      "Epoch 37/300 - Train Loss: 0.1098, Val Loss: 0.0815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:23:47,741] Trial 115 finished with value: 0.9718235850324414 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.35281802513767124, 'learning_rate': 0.00012534178698187496, 'batch_size': 32, 'weight_decay': 0.006255088133426896}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300 - Train Loss: 0.1072, Val Loss: 0.0825\n",
      "Early stopping at epoch 38\n",
      "Macro F1 Score: 0.9718, Macro Precision: 0.9732, Macro Recall: 0.9707\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 117\n",
      "Training with F1=8, F2=32, D=8, dropout=0.35444860394488575, LR=9.481061836069519e-05, BS=32, WD=0.0074970107348787595\n",
      "Epoch 1/300 - Train Loss: 0.3190, Val Loss: 0.1563\n",
      "Epoch 2/300 - Train Loss: 0.1427, Val Loss: 0.0995\n",
      "Epoch 3/300 - Train Loss: 0.1226, Val Loss: 0.1073\n",
      "Epoch 4/300 - Train Loss: 0.1139, Val Loss: 0.0884\n",
      "Epoch 5/300 - Train Loss: 0.1091, Val Loss: 0.0834\n",
      "Epoch 6/300 - Train Loss: 0.1049, Val Loss: 0.0798\n",
      "Epoch 7/300 - Train Loss: 0.1051, Val Loss: 0.0831\n",
      "Epoch 8/300 - Train Loss: 0.1028, Val Loss: 0.0748\n",
      "Epoch 9/300 - Train Loss: 0.1053, Val Loss: 0.0853\n",
      "Epoch 10/300 - Train Loss: 0.1042, Val Loss: 0.0782\n",
      "Epoch 11/300 - Train Loss: 0.1046, Val Loss: 0.0766\n",
      "Epoch 12/300 - Train Loss: 0.1041, Val Loss: 0.0760\n",
      "Epoch 13/300 - Train Loss: 0.1026, Val Loss: 0.0726\n",
      "Epoch 14/300 - Train Loss: 0.1046, Val Loss: 0.0742\n",
      "Epoch 15/300 - Train Loss: 0.1003, Val Loss: 0.0748\n",
      "Epoch 16/300 - Train Loss: 0.1040, Val Loss: 0.0776\n",
      "Epoch 17/300 - Train Loss: 0.1079, Val Loss: 0.0787\n",
      "Epoch 18/300 - Train Loss: 0.1042, Val Loss: 0.0762\n",
      "Epoch 19/300 - Train Loss: 0.1040, Val Loss: 0.0736\n",
      "Epoch 20/300 - Train Loss: 0.1060, Val Loss: 0.0752\n",
      "Epoch 21/300 - Train Loss: 0.1042, Val Loss: 0.0834\n",
      "Epoch 22/300 - Train Loss: 0.1026, Val Loss: 0.0830\n",
      "Epoch 23/300 - Train Loss: 0.1065, Val Loss: 0.0827\n",
      "Epoch 24/300 - Train Loss: 0.1065, Val Loss: 0.0740\n",
      "Epoch 25/300 - Train Loss: 0.1064, Val Loss: 0.0741\n",
      "Epoch 26/300 - Train Loss: 0.1052, Val Loss: 0.0733\n",
      "Epoch 27/300 - Train Loss: 0.1058, Val Loss: 0.0757\n",
      "Epoch 28/300 - Train Loss: 0.1068, Val Loss: 0.0729\n",
      "Epoch 29/300 - Train Loss: 0.1072, Val Loss: 0.0797\n",
      "Epoch 30/300 - Train Loss: 0.1053, Val Loss: 0.0828\n",
      "Epoch 31/300 - Train Loss: 0.1096, Val Loss: 0.0748\n",
      "Epoch 32/300 - Train Loss: 0.1093, Val Loss: 0.0851\n",
      "Epoch 33/300 - Train Loss: 0.1063, Val Loss: 0.0932\n",
      "Epoch 34/300 - Train Loss: 0.1083, Val Loss: 0.0726\n",
      "Epoch 35/300 - Train Loss: 0.1118, Val Loss: 0.0744\n",
      "Epoch 36/300 - Train Loss: 0.1082, Val Loss: 0.0742\n",
      "Epoch 37/300 - Train Loss: 0.1085, Val Loss: 0.0733\n",
      "Epoch 38/300 - Train Loss: 0.1075, Val Loss: 0.0705\n",
      "Epoch 39/300 - Train Loss: 0.1091, Val Loss: 0.0736\n",
      "Epoch 40/300 - Train Loss: 0.1077, Val Loss: 0.0718\n",
      "Epoch 41/300 - Train Loss: 0.1073, Val Loss: 0.0768\n",
      "Epoch 42/300 - Train Loss: 0.1105, Val Loss: 0.0935\n",
      "Epoch 43/300 - Train Loss: 0.1080, Val Loss: 0.0763\n",
      "Epoch 44/300 - Train Loss: 0.1069, Val Loss: 0.0905\n",
      "Epoch 45/300 - Train Loss: 0.1089, Val Loss: 0.0830\n",
      "Epoch 46/300 - Train Loss: 0.1075, Val Loss: 0.0785\n",
      "Epoch 47/300 - Train Loss: 0.1092, Val Loss: 0.0855\n",
      "Epoch 48/300 - Train Loss: 0.1075, Val Loss: 0.0759\n",
      "Epoch 49/300 - Train Loss: 0.1109, Val Loss: 0.0752\n",
      "Epoch 50/300 - Train Loss: 0.1096, Val Loss: 0.0744\n",
      "Epoch 51/300 - Train Loss: 0.1123, Val Loss: 0.0792\n",
      "Epoch 52/300 - Train Loss: 0.1091, Val Loss: 0.0755\n",
      "Epoch 53/300 - Train Loss: 0.1104, Val Loss: 0.0766\n",
      "Epoch 54/300 - Train Loss: 0.1125, Val Loss: 0.0770\n",
      "Epoch 55/300 - Train Loss: 0.1097, Val Loss: 0.0743\n",
      "Epoch 56/300 - Train Loss: 0.1084, Val Loss: 0.0765\n",
      "Epoch 57/300 - Train Loss: 0.1113, Val Loss: 0.0757\n",
      "Epoch 58/300 - Train Loss: 0.1104, Val Loss: 0.0749\n",
      "Epoch 59/300 - Train Loss: 0.1110, Val Loss: 0.0799\n",
      "Epoch 60/300 - Train Loss: 0.1087, Val Loss: 0.0740\n",
      "Epoch 61/300 - Train Loss: 0.1100, Val Loss: 0.0848\n",
      "Epoch 62/300 - Train Loss: 0.1096, Val Loss: 0.0803\n",
      "Epoch 63/300 - Train Loss: 0.1101, Val Loss: 0.0766\n",
      "Epoch 64/300 - Train Loss: 0.1107, Val Loss: 0.0748\n",
      "Epoch 65/300 - Train Loss: 0.1099, Val Loss: 0.0776\n",
      "Epoch 66/300 - Train Loss: 0.1105, Val Loss: 0.0736\n",
      "Epoch 67/300 - Train Loss: 0.1103, Val Loss: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:26:30,838] Trial 116 finished with value: 0.9677604749611 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.35444860394488575, 'learning_rate': 9.481061836069519e-05, 'batch_size': 32, 'weight_decay': 0.0074970107348787595}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/300 - Train Loss: 0.1108, Val Loss: 0.0897\n",
      "Early stopping at epoch 68\n",
      "Macro F1 Score: 0.9678, Macro Precision: 0.9767, Macro Recall: 0.9595\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       789\n",
      "           1       0.97      0.92      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 118\n",
      "Training with F1=16, F2=32, D=8, dropout=0.3650637346810893, LR=8.70831765343598e-05, BS=32, WD=0.008918115259619198\n",
      "Epoch 1/300 - Train Loss: 0.3052, Val Loss: 0.1285\n",
      "Epoch 2/300 - Train Loss: 0.1284, Val Loss: 0.0935\n",
      "Epoch 3/300 - Train Loss: 0.1118, Val Loss: 0.0832\n",
      "Epoch 4/300 - Train Loss: 0.1094, Val Loss: 0.0837\n",
      "Epoch 5/300 - Train Loss: 0.1049, Val Loss: 0.0812\n",
      "Epoch 6/300 - Train Loss: 0.1041, Val Loss: 0.0819\n",
      "Epoch 7/300 - Train Loss: 0.1024, Val Loss: 0.0797\n",
      "Epoch 8/300 - Train Loss: 0.1014, Val Loss: 0.0750\n",
      "Epoch 9/300 - Train Loss: 0.1016, Val Loss: 0.0745\n",
      "Epoch 10/300 - Train Loss: 0.1026, Val Loss: 0.0741\n",
      "Epoch 11/300 - Train Loss: 0.1029, Val Loss: 0.0877\n",
      "Epoch 12/300 - Train Loss: 0.1018, Val Loss: 0.0757\n",
      "Epoch 13/300 - Train Loss: 0.1038, Val Loss: 0.0741\n",
      "Epoch 14/300 - Train Loss: 0.1033, Val Loss: 0.0758\n",
      "Epoch 15/300 - Train Loss: 0.1044, Val Loss: 0.0748\n",
      "Epoch 16/300 - Train Loss: 0.1052, Val Loss: 0.0741\n",
      "Epoch 17/300 - Train Loss: 0.1047, Val Loss: 0.0766\n",
      "Epoch 18/300 - Train Loss: 0.1041, Val Loss: 0.0763\n",
      "Epoch 19/300 - Train Loss: 0.1062, Val Loss: 0.0776\n",
      "Epoch 20/300 - Train Loss: 0.1056, Val Loss: 0.0737\n",
      "Epoch 21/300 - Train Loss: 0.1056, Val Loss: 0.0735\n",
      "Epoch 22/300 - Train Loss: 0.1074, Val Loss: 0.0752\n",
      "Epoch 23/300 - Train Loss: 0.1094, Val Loss: 0.0743\n",
      "Epoch 24/300 - Train Loss: 0.1079, Val Loss: 0.0803\n",
      "Epoch 25/300 - Train Loss: 0.1085, Val Loss: 0.0863\n",
      "Epoch 26/300 - Train Loss: 0.1104, Val Loss: 0.0927\n",
      "Epoch 27/300 - Train Loss: 0.1068, Val Loss: 0.0738\n",
      "Epoch 28/300 - Train Loss: 0.1068, Val Loss: 0.0750\n",
      "Epoch 29/300 - Train Loss: 0.1082, Val Loss: 0.0777\n",
      "Epoch 30/300 - Train Loss: 0.1105, Val Loss: 0.0799\n",
      "Epoch 31/300 - Train Loss: 0.1114, Val Loss: 0.0767\n",
      "Epoch 32/300 - Train Loss: 0.1086, Val Loss: 0.0815\n",
      "Epoch 33/300 - Train Loss: 0.1120, Val Loss: 0.0744\n",
      "Epoch 34/300 - Train Loss: 0.1120, Val Loss: 0.1022\n",
      "Epoch 35/300 - Train Loss: 0.1084, Val Loss: 0.0761\n",
      "Epoch 36/300 - Train Loss: 0.1112, Val Loss: 0.0812\n",
      "Epoch 37/300 - Train Loss: 0.1118, Val Loss: 0.0756\n",
      "Epoch 38/300 - Train Loss: 0.1121, Val Loss: 0.0823\n",
      "Epoch 39/300 - Train Loss: 0.1133, Val Loss: 0.0753\n",
      "Epoch 40/300 - Train Loss: 0.1135, Val Loss: 0.0918\n",
      "Epoch 41/300 - Train Loss: 0.1132, Val Loss: 0.0849\n",
      "Epoch 42/300 - Train Loss: 0.1107, Val Loss: 0.0746\n",
      "Epoch 43/300 - Train Loss: 0.1130, Val Loss: 0.0805\n",
      "Epoch 44/300 - Train Loss: 0.1117, Val Loss: 0.0778\n",
      "Epoch 45/300 - Train Loss: 0.1132, Val Loss: 0.0815\n",
      "Epoch 46/300 - Train Loss: 0.1083, Val Loss: 0.0796\n",
      "Epoch 47/300 - Train Loss: 0.1104, Val Loss: 0.0773\n",
      "Epoch 48/300 - Train Loss: 0.1107, Val Loss: 0.0796\n",
      "Epoch 49/300 - Train Loss: 0.1124, Val Loss: 0.0784\n",
      "Epoch 50/300 - Train Loss: 0.1154, Val Loss: 0.0793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:29:46,875] Trial 117 finished with value: 0.9602727928266739 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.3650637346810893, 'learning_rate': 8.70831765343598e-05, 'batch_size': 32, 'weight_decay': 0.008918115259619198}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300 - Train Loss: 0.1111, Val Loss: 0.0790\n",
      "Early stopping at epoch 51\n",
      "Macro F1 Score: 0.9603, Macro Precision: 0.9527, Macro Recall: 0.9686\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 119\n",
      "Training with F1=16, F2=32, D=8, dropout=0.3181965496962345, LR=0.0002360169707564294, BS=32, WD=0.006418668487309296\n",
      "Epoch 1/300 - Train Loss: 0.1822, Val Loss: 0.0889\n",
      "Epoch 2/300 - Train Loss: 0.1082, Val Loss: 0.0851\n",
      "Epoch 3/300 - Train Loss: 0.1043, Val Loss: 0.0782\n",
      "Epoch 4/300 - Train Loss: 0.1041, Val Loss: 0.0701\n",
      "Epoch 5/300 - Train Loss: 0.1039, Val Loss: 0.0804\n",
      "Epoch 6/300 - Train Loss: 0.1046, Val Loss: 0.0751\n",
      "Epoch 7/300 - Train Loss: 0.1039, Val Loss: 0.0736\n",
      "Epoch 8/300 - Train Loss: 0.1078, Val Loss: 0.0753\n",
      "Epoch 9/300 - Train Loss: 0.1074, Val Loss: 0.0895\n",
      "Epoch 10/300 - Train Loss: 0.1068, Val Loss: 0.0840\n",
      "Epoch 11/300 - Train Loss: 0.1070, Val Loss: 0.0788\n",
      "Epoch 12/300 - Train Loss: 0.1072, Val Loss: 0.0890\n",
      "Epoch 13/300 - Train Loss: 0.1100, Val Loss: 0.0859\n",
      "Epoch 14/300 - Train Loss: 0.1075, Val Loss: 0.0938\n",
      "Epoch 15/300 - Train Loss: 0.1095, Val Loss: 0.1266\n",
      "Epoch 16/300 - Train Loss: 0.1098, Val Loss: 0.0814\n",
      "Epoch 17/300 - Train Loss: 0.1092, Val Loss: 0.0987\n",
      "Epoch 18/300 - Train Loss: 0.1115, Val Loss: 0.0950\n",
      "Epoch 19/300 - Train Loss: 0.1103, Val Loss: 0.0764\n",
      "Epoch 20/300 - Train Loss: 0.1114, Val Loss: 0.0791\n",
      "Epoch 21/300 - Train Loss: 0.1092, Val Loss: 0.0856\n",
      "Epoch 22/300 - Train Loss: 0.1089, Val Loss: 0.0988\n",
      "Epoch 23/300 - Train Loss: 0.1109, Val Loss: 0.0874\n",
      "Epoch 24/300 - Train Loss: 0.1109, Val Loss: 0.0806\n",
      "Epoch 25/300 - Train Loss: 0.1102, Val Loss: 0.0851\n",
      "Epoch 26/300 - Train Loss: 0.1121, Val Loss: 0.0929\n",
      "Epoch 27/300 - Train Loss: 0.1120, Val Loss: 0.0822\n",
      "Epoch 28/300 - Train Loss: 0.1089, Val Loss: 0.0733\n",
      "Epoch 29/300 - Train Loss: 0.1090, Val Loss: 0.0981\n",
      "Epoch 30/300 - Train Loss: 0.1087, Val Loss: 0.0953\n",
      "Epoch 31/300 - Train Loss: 0.1118, Val Loss: 0.0993\n",
      "Epoch 32/300 - Train Loss: 0.1122, Val Loss: 0.0750\n",
      "Epoch 33/300 - Train Loss: 0.1098, Val Loss: 0.0765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:31:57,677] Trial 118 finished with value: 0.968762090438038 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.3181965496962345, 'learning_rate': 0.0002360169707564294, 'batch_size': 32, 'weight_decay': 0.006418668487309296}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300 - Train Loss: 0.1086, Val Loss: 0.0791\n",
      "Early stopping at epoch 34\n",
      "Macro F1 Score: 0.9688, Macro Precision: 0.9698, Macro Recall: 0.9679\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 120\n",
      "Training with F1=32, F2=32, D=8, dropout=0.38935553748256635, LR=7.486867279741827e-05, BS=32, WD=0.005241336657921066\n",
      "Epoch 1/300 - Train Loss: 0.2884, Val Loss: 0.1154\n",
      "Epoch 2/300 - Train Loss: 0.1242, Val Loss: 0.0886\n",
      "Epoch 3/300 - Train Loss: 0.1135, Val Loss: 0.0804\n",
      "Epoch 4/300 - Train Loss: 0.1020, Val Loss: 0.0795\n",
      "Epoch 5/300 - Train Loss: 0.1002, Val Loss: 0.0730\n",
      "Epoch 6/300 - Train Loss: 0.0959, Val Loss: 0.0765\n",
      "Epoch 7/300 - Train Loss: 0.0970, Val Loss: 0.0736\n",
      "Epoch 8/300 - Train Loss: 0.0972, Val Loss: 0.0719\n",
      "Epoch 9/300 - Train Loss: 0.0974, Val Loss: 0.0832\n",
      "Epoch 10/300 - Train Loss: 0.0954, Val Loss: 0.0757\n",
      "Epoch 11/300 - Train Loss: 0.0963, Val Loss: 0.0801\n",
      "Epoch 12/300 - Train Loss: 0.0952, Val Loss: 0.0763\n",
      "Epoch 13/300 - Train Loss: 0.0958, Val Loss: 0.0812\n",
      "Epoch 14/300 - Train Loss: 0.0953, Val Loss: 0.0728\n",
      "Epoch 15/300 - Train Loss: 0.0983, Val Loss: 0.0716\n",
      "Epoch 16/300 - Train Loss: 0.0963, Val Loss: 0.0729\n",
      "Epoch 17/300 - Train Loss: 0.0972, Val Loss: 0.0882\n",
      "Epoch 18/300 - Train Loss: 0.0954, Val Loss: 0.0776\n",
      "Epoch 19/300 - Train Loss: 0.0975, Val Loss: 0.0711\n",
      "Epoch 20/300 - Train Loss: 0.0955, Val Loss: 0.0738\n",
      "Epoch 21/300 - Train Loss: 0.0994, Val Loss: 0.0734\n",
      "Epoch 22/300 - Train Loss: 0.0967, Val Loss: 0.0719\n",
      "Epoch 23/300 - Train Loss: 0.0991, Val Loss: 0.0701\n",
      "Epoch 24/300 - Train Loss: 0.0972, Val Loss: 0.0739\n",
      "Epoch 25/300 - Train Loss: 0.1003, Val Loss: 0.0704\n",
      "Epoch 26/300 - Train Loss: 0.1008, Val Loss: 0.0731\n",
      "Epoch 27/300 - Train Loss: 0.0997, Val Loss: 0.0792\n",
      "Epoch 28/300 - Train Loss: 0.1014, Val Loss: 0.0681\n",
      "Epoch 29/300 - Train Loss: 0.1005, Val Loss: 0.0871\n",
      "Epoch 30/300 - Train Loss: 0.1018, Val Loss: 0.0694\n",
      "Epoch 31/300 - Train Loss: 0.1009, Val Loss: 0.0707\n",
      "Epoch 32/300 - Train Loss: 0.0999, Val Loss: 0.0807\n",
      "Epoch 33/300 - Train Loss: 0.1003, Val Loss: 0.0739\n",
      "Epoch 34/300 - Train Loss: 0.1003, Val Loss: 0.0785\n",
      "Epoch 35/300 - Train Loss: 0.1027, Val Loss: 0.0695\n",
      "Epoch 36/300 - Train Loss: 0.1013, Val Loss: 0.0713\n",
      "Epoch 37/300 - Train Loss: 0.1033, Val Loss: 0.0698\n",
      "Epoch 38/300 - Train Loss: 0.1024, Val Loss: 0.0756\n",
      "Epoch 39/300 - Train Loss: 0.1011, Val Loss: 0.0747\n",
      "Epoch 40/300 - Train Loss: 0.1040, Val Loss: 0.0728\n",
      "Epoch 41/300 - Train Loss: 0.1021, Val Loss: 0.0712\n",
      "Epoch 42/300 - Train Loss: 0.1009, Val Loss: 0.0709\n",
      "Epoch 43/300 - Train Loss: 0.0999, Val Loss: 0.0748\n",
      "Epoch 44/300 - Train Loss: 0.1047, Val Loss: 0.0709\n",
      "Epoch 45/300 - Train Loss: 0.1054, Val Loss: 0.0878\n",
      "Epoch 46/300 - Train Loss: 0.1075, Val Loss: 0.0730\n",
      "Epoch 47/300 - Train Loss: 0.1025, Val Loss: 0.0706\n",
      "Epoch 48/300 - Train Loss: 0.1041, Val Loss: 0.0721\n",
      "Epoch 49/300 - Train Loss: 0.1030, Val Loss: 0.0696\n",
      "Epoch 50/300 - Train Loss: 0.1045, Val Loss: 0.0741\n",
      "Epoch 51/300 - Train Loss: 0.1051, Val Loss: 0.0708\n",
      "Epoch 52/300 - Train Loss: 0.1012, Val Loss: 0.0739\n",
      "Epoch 53/300 - Train Loss: 0.1081, Val Loss: 0.0921\n",
      "Epoch 54/300 - Train Loss: 0.1054, Val Loss: 0.0750\n",
      "Epoch 55/300 - Train Loss: 0.1040, Val Loss: 0.0812\n",
      "Epoch 56/300 - Train Loss: 0.1040, Val Loss: 0.0778\n",
      "Epoch 57/300 - Train Loss: 0.1062, Val Loss: 0.0726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:38:14,729] Trial 119 finished with value: 0.9667050917878949 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.38935553748256635, 'learning_rate': 7.486867279741827e-05, 'batch_size': 32, 'weight_decay': 0.005241336657921066}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300 - Train Loss: 0.1060, Val Loss: 0.0713\n",
      "Early stopping at epoch 58\n",
      "Macro F1 Score: 0.9667, Macro Precision: 0.9635, Macro Recall: 0.9703\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 121\n",
      "Training with F1=8, F2=32, D=8, dropout=0.3360736439584697, LR=0.0001235099901283453, BS=32, WD=0.0001720130617220759\n",
      "Epoch 1/300 - Train Loss: 0.2648, Val Loss: 0.1150\n",
      "Epoch 2/300 - Train Loss: 0.1166, Val Loss: 0.0832\n",
      "Epoch 3/300 - Train Loss: 0.1069, Val Loss: 0.0839\n",
      "Epoch 4/300 - Train Loss: 0.1030, Val Loss: 0.0767\n",
      "Epoch 5/300 - Train Loss: 0.0985, Val Loss: 0.0774\n",
      "Epoch 6/300 - Train Loss: 0.0955, Val Loss: 0.0801\n",
      "Epoch 7/300 - Train Loss: 0.0921, Val Loss: 0.0713\n",
      "Epoch 8/300 - Train Loss: 0.0936, Val Loss: 0.0710\n",
      "Epoch 9/300 - Train Loss: 0.0888, Val Loss: 0.0733\n",
      "Epoch 10/300 - Train Loss: 0.0892, Val Loss: 0.0733\n",
      "Epoch 11/300 - Train Loss: 0.0889, Val Loss: 0.0717\n",
      "Epoch 12/300 - Train Loss: 0.0877, Val Loss: 0.0723\n",
      "Epoch 13/300 - Train Loss: 0.0869, Val Loss: 0.0704\n",
      "Epoch 14/300 - Train Loss: 0.0853, Val Loss: 0.0714\n",
      "Epoch 15/300 - Train Loss: 0.0868, Val Loss: 0.0802\n",
      "Epoch 16/300 - Train Loss: 0.0845, Val Loss: 0.0757\n",
      "Epoch 17/300 - Train Loss: 0.0854, Val Loss: 0.0718\n",
      "Epoch 18/300 - Train Loss: 0.0815, Val Loss: 0.0689\n",
      "Epoch 19/300 - Train Loss: 0.0841, Val Loss: 0.0711\n",
      "Epoch 20/300 - Train Loss: 0.0823, Val Loss: 0.0713\n",
      "Epoch 21/300 - Train Loss: 0.0808, Val Loss: 0.0712\n",
      "Epoch 22/300 - Train Loss: 0.0823, Val Loss: 0.0678\n",
      "Epoch 23/300 - Train Loss: 0.0816, Val Loss: 0.0729\n",
      "Epoch 24/300 - Train Loss: 0.0812, Val Loss: 0.0677\n",
      "Epoch 25/300 - Train Loss: 0.0780, Val Loss: 0.0696\n",
      "Epoch 26/300 - Train Loss: 0.0812, Val Loss: 0.0673\n",
      "Epoch 27/300 - Train Loss: 0.0783, Val Loss: 0.0680\n",
      "Epoch 28/300 - Train Loss: 0.0782, Val Loss: 0.0702\n",
      "Epoch 29/300 - Train Loss: 0.0794, Val Loss: 0.0691\n",
      "Epoch 30/300 - Train Loss: 0.0794, Val Loss: 0.0705\n",
      "Epoch 31/300 - Train Loss: 0.0787, Val Loss: 0.0701\n",
      "Epoch 32/300 - Train Loss: 0.0770, Val Loss: 0.0732\n",
      "Epoch 33/300 - Train Loss: 0.0777, Val Loss: 0.0758\n",
      "Epoch 34/300 - Train Loss: 0.0773, Val Loss: 0.0756\n",
      "Epoch 35/300 - Train Loss: 0.0771, Val Loss: 0.0693\n",
      "Epoch 36/300 - Train Loss: 0.0777, Val Loss: 0.0757\n",
      "Epoch 37/300 - Train Loss: 0.0752, Val Loss: 0.0667\n",
      "Epoch 38/300 - Train Loss: 0.0754, Val Loss: 0.0719\n",
      "Epoch 39/300 - Train Loss: 0.0742, Val Loss: 0.0694\n",
      "Epoch 40/300 - Train Loss: 0.0749, Val Loss: 0.0714\n",
      "Epoch 41/300 - Train Loss: 0.0743, Val Loss: 0.0701\n",
      "Epoch 42/300 - Train Loss: 0.0774, Val Loss: 0.0697\n",
      "Epoch 43/300 - Train Loss: 0.0729, Val Loss: 0.0710\n",
      "Epoch 44/300 - Train Loss: 0.0745, Val Loss: 0.0700\n",
      "Epoch 45/300 - Train Loss: 0.0738, Val Loss: 0.0706\n",
      "Epoch 46/300 - Train Loss: 0.0717, Val Loss: 0.0692\n",
      "Epoch 47/300 - Train Loss: 0.0751, Val Loss: 0.0754\n",
      "Epoch 48/300 - Train Loss: 0.0723, Val Loss: 0.0750\n",
      "Epoch 49/300 - Train Loss: 0.0701, Val Loss: 0.0665\n",
      "Epoch 50/300 - Train Loss: 0.0716, Val Loss: 0.0777\n",
      "Epoch 51/300 - Train Loss: 0.0708, Val Loss: 0.0715\n",
      "Epoch 52/300 - Train Loss: 0.0698, Val Loss: 0.0744\n",
      "Epoch 53/300 - Train Loss: 0.0726, Val Loss: 0.0700\n",
      "Epoch 54/300 - Train Loss: 0.0703, Val Loss: 0.0704\n",
      "Epoch 55/300 - Train Loss: 0.0707, Val Loss: 0.0691\n",
      "Epoch 56/300 - Train Loss: 0.0707, Val Loss: 0.0716\n",
      "Epoch 57/300 - Train Loss: 0.0695, Val Loss: 0.0703\n",
      "Epoch 58/300 - Train Loss: 0.0704, Val Loss: 0.0737\n",
      "Epoch 59/300 - Train Loss: 0.0691, Val Loss: 0.0716\n",
      "Epoch 60/300 - Train Loss: 0.0693, Val Loss: 0.0709\n",
      "Epoch 61/300 - Train Loss: 0.0698, Val Loss: 0.0759\n",
      "Epoch 62/300 - Train Loss: 0.0678, Val Loss: 0.0682\n",
      "Epoch 63/300 - Train Loss: 0.0697, Val Loss: 0.0749\n",
      "Epoch 64/300 - Train Loss: 0.0709, Val Loss: 0.0697\n",
      "Epoch 65/300 - Train Loss: 0.0696, Val Loss: 0.0686\n",
      "Epoch 66/300 - Train Loss: 0.0685, Val Loss: 0.0685\n",
      "Epoch 67/300 - Train Loss: 0.0685, Val Loss: 0.0732\n",
      "Epoch 68/300 - Train Loss: 0.0678, Val Loss: 0.0726\n",
      "Epoch 69/300 - Train Loss: 0.0680, Val Loss: 0.0695\n",
      "Epoch 70/300 - Train Loss: 0.0683, Val Loss: 0.0714\n",
      "Epoch 71/300 - Train Loss: 0.0693, Val Loss: 0.0701\n",
      "Epoch 72/300 - Train Loss: 0.0676, Val Loss: 0.0681\n",
      "Epoch 73/300 - Train Loss: 0.0663, Val Loss: 0.0678\n",
      "Epoch 74/300 - Train Loss: 0.0680, Val Loss: 0.0732\n",
      "Epoch 75/300 - Train Loss: 0.0672, Val Loss: 0.0712\n",
      "Epoch 76/300 - Train Loss: 0.0676, Val Loss: 0.0712\n",
      "Epoch 77/300 - Train Loss: 0.0679, Val Loss: 0.0702\n",
      "Epoch 78/300 - Train Loss: 0.0671, Val Loss: 0.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:41:24,177] Trial 120 finished with value: 0.9688622499147151 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.3360736439584697, 'learning_rate': 0.0001235099901283453, 'batch_size': 32, 'weight_decay': 0.0001720130617220759}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/300 - Train Loss: 0.0657, Val Loss: 0.0713\n",
      "Early stopping at epoch 79\n",
      "Macro F1 Score: 0.9689, Macro Precision: 0.9569, Macro Recall: 0.9823\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 122\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2420871293828398, LR=0.0001736980010986167, BS=32, WD=0.00012479030351678818\n",
      "Epoch 1/300 - Train Loss: 0.2167, Val Loss: 0.0857\n",
      "Epoch 2/300 - Train Loss: 0.1037, Val Loss: 0.0725\n",
      "Epoch 3/300 - Train Loss: 0.0961, Val Loss: 0.0716\n",
      "Epoch 4/300 - Train Loss: 0.0945, Val Loss: 0.0862\n",
      "Epoch 5/300 - Train Loss: 0.0900, Val Loss: 0.0736\n",
      "Epoch 6/300 - Train Loss: 0.0872, Val Loss: 0.0740\n",
      "Epoch 7/300 - Train Loss: 0.0838, Val Loss: 0.0749\n",
      "Epoch 8/300 - Train Loss: 0.0805, Val Loss: 0.0731\n",
      "Epoch 9/300 - Train Loss: 0.0799, Val Loss: 0.0697\n",
      "Epoch 10/300 - Train Loss: 0.0844, Val Loss: 0.0710\n",
      "Epoch 11/300 - Train Loss: 0.0793, Val Loss: 0.0754\n",
      "Epoch 12/300 - Train Loss: 0.0787, Val Loss: 0.0730\n",
      "Epoch 13/300 - Train Loss: 0.0771, Val Loss: 0.0695\n",
      "Epoch 14/300 - Train Loss: 0.0782, Val Loss: 0.0681\n",
      "Epoch 15/300 - Train Loss: 0.0755, Val Loss: 0.0685\n",
      "Epoch 16/300 - Train Loss: 0.0773, Val Loss: 0.0707\n",
      "Epoch 17/300 - Train Loss: 0.0752, Val Loss: 0.0701\n",
      "Epoch 18/300 - Train Loss: 0.0720, Val Loss: 0.0707\n",
      "Epoch 19/300 - Train Loss: 0.0742, Val Loss: 0.0714\n",
      "Epoch 20/300 - Train Loss: 0.0741, Val Loss: 0.0706\n",
      "Epoch 21/300 - Train Loss: 0.0707, Val Loss: 0.0730\n",
      "Epoch 22/300 - Train Loss: 0.0707, Val Loss: 0.0693\n",
      "Epoch 23/300 - Train Loss: 0.0739, Val Loss: 0.0762\n",
      "Epoch 24/300 - Train Loss: 0.0712, Val Loss: 0.0720\n",
      "Epoch 25/300 - Train Loss: 0.0690, Val Loss: 0.0679\n",
      "Epoch 26/300 - Train Loss: 0.0697, Val Loss: 0.0734\n",
      "Epoch 27/300 - Train Loss: 0.0679, Val Loss: 0.0690\n",
      "Epoch 28/300 - Train Loss: 0.0675, Val Loss: 0.0684\n",
      "Epoch 29/300 - Train Loss: 0.0673, Val Loss: 0.0660\n",
      "Epoch 30/300 - Train Loss: 0.0651, Val Loss: 0.0701\n",
      "Epoch 31/300 - Train Loss: 0.0665, Val Loss: 0.0725\n",
      "Epoch 32/300 - Train Loss: 0.0630, Val Loss: 0.0740\n",
      "Epoch 33/300 - Train Loss: 0.0629, Val Loss: 0.0720\n",
      "Epoch 34/300 - Train Loss: 0.0632, Val Loss: 0.0723\n",
      "Epoch 35/300 - Train Loss: 0.0617, Val Loss: 0.0728\n",
      "Epoch 36/300 - Train Loss: 0.0636, Val Loss: 0.0717\n",
      "Epoch 37/300 - Train Loss: 0.0640, Val Loss: 0.0678\n",
      "Epoch 38/300 - Train Loss: 0.0627, Val Loss: 0.0696\n",
      "Epoch 39/300 - Train Loss: 0.0618, Val Loss: 0.0690\n",
      "Epoch 40/300 - Train Loss: 0.0601, Val Loss: 0.0697\n",
      "Epoch 41/300 - Train Loss: 0.0595, Val Loss: 0.0716\n",
      "Epoch 42/300 - Train Loss: 0.0597, Val Loss: 0.0712\n",
      "Epoch 43/300 - Train Loss: 0.0573, Val Loss: 0.0659\n",
      "Epoch 44/300 - Train Loss: 0.0597, Val Loss: 0.0688\n",
      "Epoch 45/300 - Train Loss: 0.0598, Val Loss: 0.0691\n",
      "Epoch 46/300 - Train Loss: 0.0595, Val Loss: 0.0702\n",
      "Epoch 47/300 - Train Loss: 0.0581, Val Loss: 0.0660\n",
      "Epoch 48/300 - Train Loss: 0.0573, Val Loss: 0.0704\n",
      "Epoch 49/300 - Train Loss: 0.0584, Val Loss: 0.0740\n",
      "Epoch 50/300 - Train Loss: 0.0558, Val Loss: 0.0694\n",
      "Epoch 51/300 - Train Loss: 0.0585, Val Loss: 0.0724\n",
      "Epoch 52/300 - Train Loss: 0.0575, Val Loss: 0.0690\n",
      "Epoch 53/300 - Train Loss: 0.0557, Val Loss: 0.0683\n",
      "Epoch 54/300 - Train Loss: 0.0612, Val Loss: 0.0779\n",
      "Epoch 55/300 - Train Loss: 0.0529, Val Loss: 0.0688\n",
      "Epoch 56/300 - Train Loss: 0.0556, Val Loss: 0.0720\n",
      "Epoch 57/300 - Train Loss: 0.0555, Val Loss: 0.0716\n",
      "Epoch 58/300 - Train Loss: 0.0561, Val Loss: 0.0708\n",
      "Epoch 59/300 - Train Loss: 0.0534, Val Loss: 0.0676\n",
      "Epoch 60/300 - Train Loss: 0.0524, Val Loss: 0.0682\n",
      "Epoch 61/300 - Train Loss: 0.0512, Val Loss: 0.0698\n",
      "Epoch 62/300 - Train Loss: 0.0534, Val Loss: 0.0678\n",
      "Epoch 63/300 - Train Loss: 0.0518, Val Loss: 0.0689\n",
      "Epoch 64/300 - Train Loss: 0.0523, Val Loss: 0.0698\n",
      "Epoch 65/300 - Train Loss: 0.0518, Val Loss: 0.0711\n",
      "Epoch 66/300 - Train Loss: 0.0530, Val Loss: 0.0723\n",
      "Epoch 67/300 - Train Loss: 0.0517, Val Loss: 0.0731\n",
      "Epoch 68/300 - Train Loss: 0.0501, Val Loss: 0.0689\n",
      "Epoch 69/300 - Train Loss: 0.0519, Val Loss: 0.0725\n",
      "Epoch 70/300 - Train Loss: 0.0514, Val Loss: 0.0700\n",
      "Epoch 71/300 - Train Loss: 0.0499, Val Loss: 0.0707\n",
      "Epoch 72/300 - Train Loss: 0.0512, Val Loss: 0.0766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:46:04,628] Trial 121 finished with value: 0.9698398821147807 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2420871293828398, 'learning_rate': 0.0001736980010986167, 'batch_size': 32, 'weight_decay': 0.00012479030351678818}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/300 - Train Loss: 0.0515, Val Loss: 0.0718\n",
      "Early stopping at epoch 73\n",
      "Macro F1 Score: 0.9698, Macro Precision: 0.9638, Macro Recall: 0.9763\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 123\n",
      "Training with F1=16, F2=32, D=8, dropout=0.276404177549481, LR=0.0007852930932768647, BS=128, WD=0.006030777777322273\n",
      "Epoch 1/300 - Train Loss: 0.1888, Val Loss: 0.0915\n",
      "Epoch 2/300 - Train Loss: 0.0986, Val Loss: 0.1159\n",
      "Epoch 3/300 - Train Loss: 0.0935, Val Loss: 0.0810\n",
      "Epoch 4/300 - Train Loss: 0.0939, Val Loss: 0.0818\n",
      "Epoch 5/300 - Train Loss: 0.0943, Val Loss: 0.0771\n",
      "Epoch 6/300 - Train Loss: 0.0945, Val Loss: 0.0875\n",
      "Epoch 7/300 - Train Loss: 0.0954, Val Loss: 0.0875\n",
      "Epoch 8/300 - Train Loss: 0.1002, Val Loss: 0.1228\n",
      "Epoch 9/300 - Train Loss: 0.0985, Val Loss: 0.0922\n",
      "Epoch 10/300 - Train Loss: 0.0991, Val Loss: 0.1049\n",
      "Epoch 11/300 - Train Loss: 0.0999, Val Loss: 0.0837\n",
      "Epoch 12/300 - Train Loss: 0.1002, Val Loss: 0.1039\n",
      "Epoch 13/300 - Train Loss: 0.0993, Val Loss: 0.1511\n",
      "Epoch 14/300 - Train Loss: 0.1008, Val Loss: 0.0872\n",
      "Epoch 15/300 - Train Loss: 0.1018, Val Loss: 0.0825\n",
      "Epoch 16/300 - Train Loss: 0.1012, Val Loss: 0.3604\n",
      "Epoch 17/300 - Train Loss: 0.1002, Val Loss: 0.1060\n",
      "Epoch 18/300 - Train Loss: 0.0997, Val Loss: 0.0978\n",
      "Epoch 19/300 - Train Loss: 0.1012, Val Loss: 0.1110\n",
      "Epoch 20/300 - Train Loss: 0.0998, Val Loss: 0.0841\n",
      "Epoch 21/300 - Train Loss: 0.1016, Val Loss: 0.1156\n",
      "Epoch 22/300 - Train Loss: 0.1010, Val Loss: 0.1048\n",
      "Epoch 23/300 - Train Loss: 0.0998, Val Loss: 0.1042\n",
      "Epoch 24/300 - Train Loss: 0.1008, Val Loss: 0.1057\n",
      "Epoch 25/300 - Train Loss: 0.1001, Val Loss: 0.0807\n",
      "Epoch 26/300 - Train Loss: 0.1022, Val Loss: 0.0844\n",
      "Epoch 27/300 - Train Loss: 0.1009, Val Loss: 0.1117\n",
      "Epoch 28/300 - Train Loss: 0.1021, Val Loss: 0.1017\n",
      "Epoch 29/300 - Train Loss: 0.1001, Val Loss: 0.1255\n",
      "Epoch 30/300 - Train Loss: 0.1000, Val Loss: 0.1199\n",
      "Epoch 31/300 - Train Loss: 0.0994, Val Loss: 0.1565\n",
      "Epoch 32/300 - Train Loss: 0.0999, Val Loss: 0.0970\n",
      "Epoch 33/300 - Train Loss: 0.1001, Val Loss: 0.0934\n",
      "Epoch 34/300 - Train Loss: 0.1000, Val Loss: 0.0923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:47:51,971] Trial 122 finished with value: 0.9639142761380888 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.276404177549481, 'learning_rate': 0.0007852930932768647, 'batch_size': 128, 'weight_decay': 0.006030777777322273}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/300 - Train Loss: 0.0981, Val Loss: 0.1617\n",
      "Early stopping at epoch 35\n",
      "Macro F1 Score: 0.9639, Macro Precision: 0.9589, Macro Recall: 0.9696\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       1.00      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 124\n",
      "Training with F1=16, F2=32, D=8, dropout=0.25814031734543813, LR=0.00012884280552182025, BS=256, WD=0.0031415532144085934\n",
      "Epoch 1/300 - Train Loss: 0.4539, Val Loss: 0.2335\n",
      "Epoch 2/300 - Train Loss: 0.2049, Val Loss: 0.1829\n",
      "Epoch 3/300 - Train Loss: 0.1596, Val Loss: 0.1277\n",
      "Epoch 4/300 - Train Loss: 0.1236, Val Loss: 0.1067\n",
      "Epoch 5/300 - Train Loss: 0.1050, Val Loss: 0.1000\n",
      "Epoch 6/300 - Train Loss: 0.0972, Val Loss: 0.0887\n",
      "Epoch 7/300 - Train Loss: 0.0932, Val Loss: 0.0918\n",
      "Epoch 8/300 - Train Loss: 0.0891, Val Loss: 0.0811\n",
      "Epoch 9/300 - Train Loss: 0.0886, Val Loss: 0.0777\n",
      "Epoch 10/300 - Train Loss: 0.0882, Val Loss: 0.0756\n",
      "Epoch 11/300 - Train Loss: 0.0855, Val Loss: 0.0798\n",
      "Epoch 12/300 - Train Loss: 0.0854, Val Loss: 0.0739\n",
      "Epoch 13/300 - Train Loss: 0.0832, Val Loss: 0.0744\n",
      "Epoch 14/300 - Train Loss: 0.0827, Val Loss: 0.0704\n",
      "Epoch 15/300 - Train Loss: 0.0818, Val Loss: 0.0740\n",
      "Epoch 16/300 - Train Loss: 0.0799, Val Loss: 0.0804\n",
      "Epoch 17/300 - Train Loss: 0.0795, Val Loss: 0.0751\n",
      "Epoch 18/300 - Train Loss: 0.0806, Val Loss: 0.0783\n",
      "Epoch 19/300 - Train Loss: 0.0803, Val Loss: 0.0793\n",
      "Epoch 20/300 - Train Loss: 0.0815, Val Loss: 0.0757\n",
      "Epoch 21/300 - Train Loss: 0.0779, Val Loss: 0.0801\n",
      "Epoch 22/300 - Train Loss: 0.0791, Val Loss: 0.0736\n",
      "Epoch 23/300 - Train Loss: 0.0775, Val Loss: 0.0699\n",
      "Epoch 24/300 - Train Loss: 0.0775, Val Loss: 0.0698\n",
      "Epoch 25/300 - Train Loss: 0.0789, Val Loss: 0.0721\n",
      "Epoch 26/300 - Train Loss: 0.0762, Val Loss: 0.0728\n",
      "Epoch 27/300 - Train Loss: 0.0772, Val Loss: 0.0716\n",
      "Epoch 28/300 - Train Loss: 0.0758, Val Loss: 0.0714\n",
      "Epoch 29/300 - Train Loss: 0.0773, Val Loss: 0.0715\n",
      "Epoch 30/300 - Train Loss: 0.0752, Val Loss: 0.0716\n",
      "Epoch 31/300 - Train Loss: 0.0756, Val Loss: 0.0712\n",
      "Epoch 32/300 - Train Loss: 0.0751, Val Loss: 0.0701\n",
      "Epoch 33/300 - Train Loss: 0.0744, Val Loss: 0.0737\n",
      "Epoch 34/300 - Train Loss: 0.0753, Val Loss: 0.0738\n",
      "Epoch 35/300 - Train Loss: 0.0760, Val Loss: 0.0681\n",
      "Epoch 36/300 - Train Loss: 0.0755, Val Loss: 0.0742\n",
      "Epoch 37/300 - Train Loss: 0.0755, Val Loss: 0.0762\n",
      "Epoch 38/300 - Train Loss: 0.0749, Val Loss: 0.0688\n",
      "Epoch 39/300 - Train Loss: 0.0748, Val Loss: 0.0719\n",
      "Epoch 40/300 - Train Loss: 0.0748, Val Loss: 0.0691\n",
      "Epoch 41/300 - Train Loss: 0.0738, Val Loss: 0.0667\n",
      "Epoch 42/300 - Train Loss: 0.0746, Val Loss: 0.0699\n",
      "Epoch 43/300 - Train Loss: 0.0752, Val Loss: 0.0685\n",
      "Epoch 44/300 - Train Loss: 0.0754, Val Loss: 0.0654\n",
      "Epoch 45/300 - Train Loss: 0.0739, Val Loss: 0.0665\n",
      "Epoch 46/300 - Train Loss: 0.0741, Val Loss: 0.0684\n",
      "Epoch 47/300 - Train Loss: 0.0750, Val Loss: 0.0713\n",
      "Epoch 48/300 - Train Loss: 0.0739, Val Loss: 0.0677\n",
      "Epoch 49/300 - Train Loss: 0.0774, Val Loss: 0.0720\n",
      "Epoch 50/300 - Train Loss: 0.0749, Val Loss: 0.0690\n",
      "Epoch 51/300 - Train Loss: 0.0733, Val Loss: 0.0695\n",
      "Epoch 52/300 - Train Loss: 0.0764, Val Loss: 0.0675\n",
      "Epoch 53/300 - Train Loss: 0.0755, Val Loss: 0.0705\n",
      "Epoch 54/300 - Train Loss: 0.0762, Val Loss: 0.0679\n",
      "Epoch 55/300 - Train Loss: 0.0746, Val Loss: 0.0655\n",
      "Epoch 56/300 - Train Loss: 0.0745, Val Loss: 0.0693\n",
      "Epoch 57/300 - Train Loss: 0.0746, Val Loss: 0.0674\n",
      "Epoch 58/300 - Train Loss: 0.0750, Val Loss: 0.0671\n",
      "Epoch 59/300 - Train Loss: 0.0759, Val Loss: 0.0694\n",
      "Epoch 60/300 - Train Loss: 0.0779, Val Loss: 0.0688\n",
      "Epoch 61/300 - Train Loss: 0.0749, Val Loss: 0.0668\n",
      "Epoch 62/300 - Train Loss: 0.0738, Val Loss: 0.0695\n",
      "Epoch 63/300 - Train Loss: 0.0746, Val Loss: 0.0675\n",
      "Epoch 64/300 - Train Loss: 0.0753, Val Loss: 0.0675\n",
      "Epoch 65/300 - Train Loss: 0.0756, Val Loss: 0.0682\n",
      "Epoch 66/300 - Train Loss: 0.0743, Val Loss: 0.0679\n",
      "Epoch 67/300 - Train Loss: 0.0757, Val Loss: 0.0674\n",
      "Epoch 68/300 - Train Loss: 0.0746, Val Loss: 0.0668\n",
      "Epoch 69/300 - Train Loss: 0.0746, Val Loss: 0.0677\n",
      "Epoch 70/300 - Train Loss: 0.0748, Val Loss: 0.0671\n",
      "Epoch 71/300 - Train Loss: 0.0754, Val Loss: 0.0694\n",
      "Epoch 72/300 - Train Loss: 0.0758, Val Loss: 0.0678\n",
      "Epoch 73/300 - Train Loss: 0.0766, Val Loss: 0.0704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:51:40,230] Trial 123 finished with value: 0.9694624160448041 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.25814031734543813, 'learning_rate': 0.00012884280552182025, 'batch_size': 256, 'weight_decay': 0.0031415532144085934}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300 - Train Loss: 0.0748, Val Loss: 0.0696\n",
      "Early stopping at epoch 74\n",
      "Macro F1 Score: 0.9695, Macro Precision: 0.9681, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 125\n",
      "Training with F1=16, F2=32, D=8, dropout=0.21699083008237632, LR=3.7577667068091455e-05, BS=32, WD=0.004771582514963943\n",
      "Epoch 1/300 - Train Loss: 0.3946, Val Loss: 0.1892\n",
      "Epoch 2/300 - Train Loss: 0.1878, Val Loss: 0.1447\n",
      "Epoch 3/300 - Train Loss: 0.1434, Val Loss: 0.1120\n",
      "Epoch 4/300 - Train Loss: 0.1191, Val Loss: 0.0943\n",
      "Epoch 5/300 - Train Loss: 0.1082, Val Loss: 0.0985\n",
      "Epoch 6/300 - Train Loss: 0.1067, Val Loss: 0.0785\n",
      "Epoch 7/300 - Train Loss: 0.1015, Val Loss: 0.0830\n",
      "Epoch 8/300 - Train Loss: 0.0998, Val Loss: 0.0754\n",
      "Epoch 9/300 - Train Loss: 0.0966, Val Loss: 0.0741\n",
      "Epoch 10/300 - Train Loss: 0.0951, Val Loss: 0.0742\n",
      "Epoch 11/300 - Train Loss: 0.0951, Val Loss: 0.0757\n",
      "Epoch 12/300 - Train Loss: 0.0948, Val Loss: 0.0756\n",
      "Epoch 13/300 - Train Loss: 0.0952, Val Loss: 0.0762\n",
      "Epoch 14/300 - Train Loss: 0.0938, Val Loss: 0.0755\n",
      "Epoch 15/300 - Train Loss: 0.0896, Val Loss: 0.0745\n",
      "Epoch 16/300 - Train Loss: 0.0907, Val Loss: 0.0756\n",
      "Epoch 17/300 - Train Loss: 0.0911, Val Loss: 0.0731\n",
      "Epoch 18/300 - Train Loss: 0.0907, Val Loss: 0.0728\n",
      "Epoch 19/300 - Train Loss: 0.0888, Val Loss: 0.0692\n",
      "Epoch 20/300 - Train Loss: 0.0908, Val Loss: 0.0793\n",
      "Epoch 21/300 - Train Loss: 0.0881, Val Loss: 0.0710\n",
      "Epoch 22/300 - Train Loss: 0.0892, Val Loss: 0.0714\n",
      "Epoch 23/300 - Train Loss: 0.0891, Val Loss: 0.0696\n",
      "Epoch 24/300 - Train Loss: 0.0891, Val Loss: 0.0781\n",
      "Epoch 25/300 - Train Loss: 0.0883, Val Loss: 0.0709\n",
      "Epoch 26/300 - Train Loss: 0.0872, Val Loss: 0.0692\n",
      "Epoch 27/300 - Train Loss: 0.0868, Val Loss: 0.0713\n",
      "Epoch 28/300 - Train Loss: 0.0895, Val Loss: 0.0670\n",
      "Epoch 29/300 - Train Loss: 0.0883, Val Loss: 0.0700\n",
      "Epoch 30/300 - Train Loss: 0.0891, Val Loss: 0.0687\n",
      "Epoch 31/300 - Train Loss: 0.0901, Val Loss: 0.0698\n",
      "Epoch 32/300 - Train Loss: 0.0868, Val Loss: 0.0713\n",
      "Epoch 33/300 - Train Loss: 0.0872, Val Loss: 0.0699\n",
      "Epoch 34/300 - Train Loss: 0.0900, Val Loss: 0.0795\n",
      "Epoch 35/300 - Train Loss: 0.0895, Val Loss: 0.0731\n",
      "Epoch 36/300 - Train Loss: 0.0898, Val Loss: 0.0714\n",
      "Epoch 37/300 - Train Loss: 0.0893, Val Loss: 0.0829\n",
      "Epoch 38/300 - Train Loss: 0.0887, Val Loss: 0.0695\n",
      "Epoch 39/300 - Train Loss: 0.0877, Val Loss: 0.0766\n",
      "Epoch 40/300 - Train Loss: 0.0912, Val Loss: 0.0723\n",
      "Epoch 41/300 - Train Loss: 0.0896, Val Loss: 0.0701\n",
      "Epoch 42/300 - Train Loss: 0.0892, Val Loss: 0.0717\n",
      "Epoch 43/300 - Train Loss: 0.0898, Val Loss: 0.0733\n",
      "Epoch 44/300 - Train Loss: 0.0885, Val Loss: 0.0732\n",
      "Epoch 45/300 - Train Loss: 0.0906, Val Loss: 0.0685\n",
      "Epoch 46/300 - Train Loss: 0.0903, Val Loss: 0.0723\n",
      "Epoch 47/300 - Train Loss: 0.0903, Val Loss: 0.0682\n",
      "Epoch 48/300 - Train Loss: 0.0902, Val Loss: 0.0681\n",
      "Epoch 49/300 - Train Loss: 0.0893, Val Loss: 0.0686\n",
      "Epoch 50/300 - Train Loss: 0.0900, Val Loss: 0.0760\n",
      "Epoch 51/300 - Train Loss: 0.0901, Val Loss: 0.0707\n",
      "Epoch 52/300 - Train Loss: 0.0903, Val Loss: 0.0687\n",
      "Epoch 53/300 - Train Loss: 0.0897, Val Loss: 0.0719\n",
      "Epoch 54/300 - Train Loss: 0.0888, Val Loss: 0.0830\n",
      "Epoch 55/300 - Train Loss: 0.0910, Val Loss: 0.0691\n",
      "Epoch 56/300 - Train Loss: 0.0899, Val Loss: 0.0767\n",
      "Epoch 57/300 - Train Loss: 0.0890, Val Loss: 0.0689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:55:23,102] Trial 124 finished with value: 0.9691694313598501 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.21699083008237632, 'learning_rate': 3.7577667068091455e-05, 'batch_size': 32, 'weight_decay': 0.004771582514963943}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300 - Train Loss: 0.0919, Val Loss: 0.0709\n",
      "Early stopping at epoch 58\n",
      "Macro F1 Score: 0.9692, Macro Precision: 0.9721, Macro Recall: 0.9663\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 126\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2933981788660601, LR=0.00019990009964907, BS=32, WD=0.002283552038863246\n",
      "Epoch 1/300 - Train Loss: 0.2002, Val Loss: 0.0968\n",
      "Epoch 2/300 - Train Loss: 0.1046, Val Loss: 0.0757\n",
      "Epoch 3/300 - Train Loss: 0.0989, Val Loss: 0.0709\n",
      "Epoch 4/300 - Train Loss: 0.0953, Val Loss: 0.0837\n",
      "Epoch 5/300 - Train Loss: 0.0923, Val Loss: 0.0713\n",
      "Epoch 6/300 - Train Loss: 0.0926, Val Loss: 0.0754\n",
      "Epoch 7/300 - Train Loss: 0.0919, Val Loss: 0.0721\n",
      "Epoch 8/300 - Train Loss: 0.0917, Val Loss: 0.0769\n",
      "Epoch 9/300 - Train Loss: 0.0916, Val Loss: 0.0707\n",
      "Epoch 10/300 - Train Loss: 0.0902, Val Loss: 0.0698\n",
      "Epoch 11/300 - Train Loss: 0.0898, Val Loss: 0.0710\n",
      "Epoch 12/300 - Train Loss: 0.0916, Val Loss: 0.0748\n",
      "Epoch 13/300 - Train Loss: 0.0923, Val Loss: 0.0675\n",
      "Epoch 14/300 - Train Loss: 0.0913, Val Loss: 0.0682\n",
      "Epoch 15/300 - Train Loss: 0.0898, Val Loss: 0.0739\n",
      "Epoch 16/300 - Train Loss: 0.0906, Val Loss: 0.0699\n",
      "Epoch 17/300 - Train Loss: 0.0907, Val Loss: 0.0718\n",
      "Epoch 18/300 - Train Loss: 0.0894, Val Loss: 0.0690\n",
      "Epoch 19/300 - Train Loss: 0.0898, Val Loss: 0.0665\n",
      "Epoch 20/300 - Train Loss: 0.0916, Val Loss: 0.0693\n",
      "Epoch 21/300 - Train Loss: 0.0913, Val Loss: 0.0697\n",
      "Epoch 22/300 - Train Loss: 0.0905, Val Loss: 0.0756\n",
      "Epoch 23/300 - Train Loss: 0.0899, Val Loss: 0.0748\n",
      "Epoch 24/300 - Train Loss: 0.0902, Val Loss: 0.0775\n",
      "Epoch 25/300 - Train Loss: 0.0905, Val Loss: 0.0742\n",
      "Epoch 26/300 - Train Loss: 0.0911, Val Loss: 0.0698\n",
      "Epoch 27/300 - Train Loss: 0.0932, Val Loss: 0.0720\n",
      "Epoch 28/300 - Train Loss: 0.0949, Val Loss: 0.0707\n",
      "Epoch 29/300 - Train Loss: 0.0933, Val Loss: 0.0700\n",
      "Epoch 30/300 - Train Loss: 0.0943, Val Loss: 0.0730\n",
      "Epoch 31/300 - Train Loss: 0.0920, Val Loss: 0.0770\n",
      "Epoch 32/300 - Train Loss: 0.0940, Val Loss: 0.0724\n",
      "Epoch 33/300 - Train Loss: 0.0936, Val Loss: 0.0735\n",
      "Epoch 34/300 - Train Loss: 0.0917, Val Loss: 0.0895\n",
      "Epoch 35/300 - Train Loss: 0.0920, Val Loss: 0.0705\n",
      "Epoch 36/300 - Train Loss: 0.0943, Val Loss: 0.0673\n",
      "Epoch 37/300 - Train Loss: 0.0924, Val Loss: 0.0788\n",
      "Epoch 38/300 - Train Loss: 0.0903, Val Loss: 0.0749\n",
      "Epoch 39/300 - Train Loss: 0.0926, Val Loss: 0.0668\n",
      "Epoch 40/300 - Train Loss: 0.0939, Val Loss: 0.0751\n",
      "Epoch 41/300 - Train Loss: 0.0951, Val Loss: 0.0727\n",
      "Epoch 42/300 - Train Loss: 0.0941, Val Loss: 0.0814\n",
      "Epoch 43/300 - Train Loss: 0.0951, Val Loss: 0.0674\n",
      "Epoch 44/300 - Train Loss: 0.0952, Val Loss: 0.0774\n",
      "Epoch 45/300 - Train Loss: 0.0940, Val Loss: 0.0925\n",
      "Epoch 46/300 - Train Loss: 0.0946, Val Loss: 0.1024\n",
      "Epoch 47/300 - Train Loss: 0.0955, Val Loss: 0.0733\n",
      "Epoch 48/300 - Train Loss: 0.0954, Val Loss: 0.0834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 06:58:31,563] Trial 125 finished with value: 0.9691647345825132 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2933981788660601, 'learning_rate': 0.00019990009964907, 'batch_size': 32, 'weight_decay': 0.002283552038863246}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300 - Train Loss: 0.0965, Val Loss: 0.0737\n",
      "Early stopping at epoch 49\n",
      "Macro F1 Score: 0.9692, Macro Precision: 0.9722, Macro Recall: 0.9662\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 127\n",
      "Training with F1=16, F2=32, D=8, dropout=0.42453622671859487, LR=5.984281284670512e-05, BS=256, WD=0.00814022775293754\n",
      "Epoch 1/300 - Train Loss: 0.6781, Val Loss: 0.4078\n",
      "Epoch 2/300 - Train Loss: 0.3316, Val Loss: 0.2997\n",
      "Epoch 3/300 - Train Loss: 0.2495, Val Loss: 0.2347\n",
      "Epoch 4/300 - Train Loss: 0.2106, Val Loss: 0.2067\n",
      "Epoch 5/300 - Train Loss: 0.1839, Val Loss: 0.1681\n",
      "Epoch 6/300 - Train Loss: 0.1563, Val Loss: 0.1411\n",
      "Epoch 7/300 - Train Loss: 0.1354, Val Loss: 0.1291\n",
      "Epoch 8/300 - Train Loss: 0.1187, Val Loss: 0.1216\n",
      "Epoch 9/300 - Train Loss: 0.1129, Val Loss: 0.1020\n",
      "Epoch 10/300 - Train Loss: 0.1067, Val Loss: 0.1074\n",
      "Epoch 11/300 - Train Loss: 0.1017, Val Loss: 0.0942\n",
      "Epoch 12/300 - Train Loss: 0.0980, Val Loss: 0.0977\n",
      "Epoch 13/300 - Train Loss: 0.0964, Val Loss: 0.0917\n",
      "Epoch 14/300 - Train Loss: 0.0948, Val Loss: 0.0846\n",
      "Epoch 15/300 - Train Loss: 0.0944, Val Loss: 0.0911\n",
      "Epoch 16/300 - Train Loss: 0.0949, Val Loss: 0.0838\n",
      "Epoch 17/300 - Train Loss: 0.0935, Val Loss: 0.0904\n",
      "Epoch 18/300 - Train Loss: 0.0919, Val Loss: 0.0854\n",
      "Epoch 19/300 - Train Loss: 0.0927, Val Loss: 0.0797\n",
      "Epoch 20/300 - Train Loss: 0.0911, Val Loss: 0.0859\n",
      "Epoch 21/300 - Train Loss: 0.0892, Val Loss: 0.0814\n",
      "Epoch 22/300 - Train Loss: 0.0893, Val Loss: 0.0788\n",
      "Epoch 23/300 - Train Loss: 0.0896, Val Loss: 0.0838\n",
      "Epoch 24/300 - Train Loss: 0.0886, Val Loss: 0.0773\n",
      "Epoch 25/300 - Train Loss: 0.0894, Val Loss: 0.0835\n",
      "Epoch 26/300 - Train Loss: 0.0865, Val Loss: 0.0808\n",
      "Epoch 27/300 - Train Loss: 0.0870, Val Loss: 0.0823\n",
      "Epoch 28/300 - Train Loss: 0.0861, Val Loss: 0.0832\n",
      "Epoch 29/300 - Train Loss: 0.0875, Val Loss: 0.0777\n",
      "Epoch 30/300 - Train Loss: 0.0902, Val Loss: 0.0796\n",
      "Epoch 31/300 - Train Loss: 0.0880, Val Loss: 0.0844\n",
      "Epoch 32/300 - Train Loss: 0.0861, Val Loss: 0.0761\n",
      "Epoch 33/300 - Train Loss: 0.0848, Val Loss: 0.0822\n",
      "Epoch 34/300 - Train Loss: 0.0859, Val Loss: 0.0780\n",
      "Epoch 35/300 - Train Loss: 0.0848, Val Loss: 0.0766\n",
      "Epoch 36/300 - Train Loss: 0.0841, Val Loss: 0.0764\n",
      "Epoch 37/300 - Train Loss: 0.0850, Val Loss: 0.0735\n",
      "Epoch 38/300 - Train Loss: 0.0876, Val Loss: 0.0797\n",
      "Epoch 39/300 - Train Loss: 0.0832, Val Loss: 0.0773\n",
      "Epoch 40/300 - Train Loss: 0.0848, Val Loss: 0.0784\n",
      "Epoch 41/300 - Train Loss: 0.0841, Val Loss: 0.0803\n",
      "Epoch 42/300 - Train Loss: 0.0854, Val Loss: 0.0788\n",
      "Epoch 43/300 - Train Loss: 0.0846, Val Loss: 0.0768\n",
      "Epoch 44/300 - Train Loss: 0.0857, Val Loss: 0.0823\n",
      "Epoch 45/300 - Train Loss: 0.0853, Val Loss: 0.0798\n",
      "Epoch 46/300 - Train Loss: 0.0838, Val Loss: 0.0787\n",
      "Epoch 47/300 - Train Loss: 0.0844, Val Loss: 0.0790\n",
      "Epoch 48/300 - Train Loss: 0.0855, Val Loss: 0.0769\n",
      "Epoch 49/300 - Train Loss: 0.0837, Val Loss: 0.0789\n",
      "Epoch 50/300 - Train Loss: 0.0844, Val Loss: 0.0718\n",
      "Epoch 51/300 - Train Loss: 0.0844, Val Loss: 0.0788\n",
      "Epoch 52/300 - Train Loss: 0.0828, Val Loss: 0.0745\n",
      "Epoch 53/300 - Train Loss: 0.0843, Val Loss: 0.0784\n",
      "Epoch 54/300 - Train Loss: 0.0841, Val Loss: 0.0757\n",
      "Epoch 55/300 - Train Loss: 0.0857, Val Loss: 0.0767\n",
      "Epoch 56/300 - Train Loss: 0.0822, Val Loss: 0.0734\n",
      "Epoch 57/300 - Train Loss: 0.0831, Val Loss: 0.0744\n",
      "Epoch 58/300 - Train Loss: 0.0840, Val Loss: 0.0763\n",
      "Epoch 59/300 - Train Loss: 0.0848, Val Loss: 0.0740\n",
      "Epoch 60/300 - Train Loss: 0.0846, Val Loss: 0.0721\n",
      "Epoch 61/300 - Train Loss: 0.0834, Val Loss: 0.0723\n",
      "Epoch 62/300 - Train Loss: 0.0854, Val Loss: 0.0749\n",
      "Epoch 63/300 - Train Loss: 0.0827, Val Loss: 0.0713\n",
      "Epoch 64/300 - Train Loss: 0.0838, Val Loss: 0.0720\n",
      "Epoch 65/300 - Train Loss: 0.0840, Val Loss: 0.0722\n",
      "Epoch 66/300 - Train Loss: 0.0843, Val Loss: 0.0740\n",
      "Epoch 67/300 - Train Loss: 0.0860, Val Loss: 0.0769\n",
      "Epoch 68/300 - Train Loss: 0.0827, Val Loss: 0.0731\n",
      "Epoch 69/300 - Train Loss: 0.0837, Val Loss: 0.0722\n",
      "Epoch 70/300 - Train Loss: 0.0831, Val Loss: 0.0769\n",
      "Epoch 71/300 - Train Loss: 0.0849, Val Loss: 0.0719\n",
      "Epoch 72/300 - Train Loss: 0.0835, Val Loss: 0.0726\n",
      "Epoch 73/300 - Train Loss: 0.0842, Val Loss: 0.0773\n",
      "Epoch 74/300 - Train Loss: 0.0831, Val Loss: 0.0718\n",
      "Epoch 75/300 - Train Loss: 0.0846, Val Loss: 0.0732\n",
      "Epoch 76/300 - Train Loss: 0.0827, Val Loss: 0.0756\n",
      "Epoch 77/300 - Train Loss: 0.0859, Val Loss: 0.0744\n",
      "Epoch 78/300 - Train Loss: 0.0838, Val Loss: 0.0731\n",
      "Epoch 79/300 - Train Loss: 0.0859, Val Loss: 0.0774\n",
      "Epoch 80/300 - Train Loss: 0.0848, Val Loss: 0.0744\n",
      "Epoch 81/300 - Train Loss: 0.0838, Val Loss: 0.0722\n",
      "Epoch 82/300 - Train Loss: 0.0847, Val Loss: 0.0749\n",
      "Epoch 83/300 - Train Loss: 0.0849, Val Loss: 0.0741\n",
      "Epoch 84/300 - Train Loss: 0.0850, Val Loss: 0.0729\n",
      "Epoch 85/300 - Train Loss: 0.0837, Val Loss: 0.0721\n",
      "Epoch 86/300 - Train Loss: 0.0856, Val Loss: 0.0735\n",
      "Epoch 87/300 - Train Loss: 0.0833, Val Loss: 0.0734\n",
      "Epoch 88/300 - Train Loss: 0.0863, Val Loss: 0.0722\n",
      "Epoch 89/300 - Train Loss: 0.0868, Val Loss: 0.0742\n",
      "Epoch 90/300 - Train Loss: 0.0857, Val Loss: 0.0741\n",
      "Epoch 91/300 - Train Loss: 0.0857, Val Loss: 0.0730\n",
      "Epoch 92/300 - Train Loss: 0.0850, Val Loss: 0.0761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:03:18,548] Trial 126 finished with value: 0.9658109315461081 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.42453622671859487, 'learning_rate': 5.984281284670512e-05, 'batch_size': 256, 'weight_decay': 0.00814022775293754}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/300 - Train Loss: 0.0861, Val Loss: 0.0746\n",
      "Early stopping at epoch 93\n",
      "Macro F1 Score: 0.9658, Macro Precision: 0.9665, Macro Recall: 0.9652\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 128\n",
      "Training with F1=4, F2=32, D=8, dropout=0.18328022519162446, LR=9.850521467173122e-05, BS=32, WD=0.0017444019915207269\n",
      "Epoch 1/300 - Train Loss: 0.3434, Val Loss: 0.1874\n",
      "Epoch 2/300 - Train Loss: 0.2035, Val Loss: 0.1555\n",
      "Epoch 3/300 - Train Loss: 0.1702, Val Loss: 0.1361\n",
      "Epoch 4/300 - Train Loss: 0.1403, Val Loss: 0.0944\n",
      "Epoch 5/300 - Train Loss: 0.1132, Val Loss: 0.0853\n",
      "Epoch 6/300 - Train Loss: 0.1017, Val Loss: 0.0816\n",
      "Epoch 7/300 - Train Loss: 0.1000, Val Loss: 0.0749\n",
      "Epoch 8/300 - Train Loss: 0.0974, Val Loss: 0.0759\n",
      "Epoch 9/300 - Train Loss: 0.0964, Val Loss: 0.0776\n",
      "Epoch 10/300 - Train Loss: 0.0939, Val Loss: 0.0775\n",
      "Epoch 11/300 - Train Loss: 0.0933, Val Loss: 0.0719\n",
      "Epoch 12/300 - Train Loss: 0.0931, Val Loss: 0.0747\n",
      "Epoch 13/300 - Train Loss: 0.0916, Val Loss: 0.0712\n",
      "Epoch 14/300 - Train Loss: 0.0910, Val Loss: 0.0715\n",
      "Epoch 15/300 - Train Loss: 0.0906, Val Loss: 0.0742\n",
      "Epoch 16/300 - Train Loss: 0.0891, Val Loss: 0.0790\n",
      "Epoch 17/300 - Train Loss: 0.0895, Val Loss: 0.0791\n",
      "Epoch 18/300 - Train Loss: 0.0908, Val Loss: 0.0855\n",
      "Epoch 19/300 - Train Loss: 0.0884, Val Loss: 0.0717\n",
      "Epoch 20/300 - Train Loss: 0.0898, Val Loss: 0.0712\n",
      "Epoch 21/300 - Train Loss: 0.0879, Val Loss: 0.0777\n",
      "Epoch 22/300 - Train Loss: 0.0870, Val Loss: 0.0733\n",
      "Epoch 23/300 - Train Loss: 0.0864, Val Loss: 0.0721\n",
      "Epoch 24/300 - Train Loss: 0.0869, Val Loss: 0.0755\n",
      "Epoch 25/300 - Train Loss: 0.0892, Val Loss: 0.0732\n",
      "Epoch 26/300 - Train Loss: 0.0891, Val Loss: 0.0753\n",
      "Epoch 27/300 - Train Loss: 0.0868, Val Loss: 0.0761\n",
      "Epoch 28/300 - Train Loss: 0.0852, Val Loss: 0.0719\n",
      "Epoch 29/300 - Train Loss: 0.0894, Val Loss: 0.0801\n",
      "Epoch 30/300 - Train Loss: 0.0892, Val Loss: 0.0701\n",
      "Epoch 31/300 - Train Loss: 0.0869, Val Loss: 0.0767\n",
      "Epoch 32/300 - Train Loss: 0.0860, Val Loss: 0.0723\n",
      "Epoch 33/300 - Train Loss: 0.0891, Val Loss: 0.0723\n",
      "Epoch 34/300 - Train Loss: 0.0838, Val Loss: 0.0762\n",
      "Epoch 35/300 - Train Loss: 0.0846, Val Loss: 0.0723\n",
      "Epoch 36/300 - Train Loss: 0.0854, Val Loss: 0.0715\n",
      "Epoch 37/300 - Train Loss: 0.0848, Val Loss: 0.0728\n",
      "Epoch 38/300 - Train Loss: 0.0853, Val Loss: 0.0702\n",
      "Epoch 39/300 - Train Loss: 0.0861, Val Loss: 0.0718\n",
      "Epoch 40/300 - Train Loss: 0.0853, Val Loss: 0.0715\n",
      "Epoch 41/300 - Train Loss: 0.0853, Val Loss: 0.0681\n",
      "Epoch 42/300 - Train Loss: 0.0856, Val Loss: 0.0742\n",
      "Epoch 43/300 - Train Loss: 0.0862, Val Loss: 0.0762\n",
      "Epoch 44/300 - Train Loss: 0.0844, Val Loss: 0.0785\n",
      "Epoch 45/300 - Train Loss: 0.0868, Val Loss: 0.0814\n",
      "Epoch 46/300 - Train Loss: 0.0843, Val Loss: 0.0755\n",
      "Epoch 47/300 - Train Loss: 0.0851, Val Loss: 0.0747\n",
      "Epoch 48/300 - Train Loss: 0.0825, Val Loss: 0.0787\n",
      "Epoch 49/300 - Train Loss: 0.0882, Val Loss: 0.0765\n",
      "Epoch 50/300 - Train Loss: 0.0882, Val Loss: 0.0769\n",
      "Epoch 51/300 - Train Loss: 0.0841, Val Loss: 0.0696\n",
      "Epoch 52/300 - Train Loss: 0.0857, Val Loss: 0.0734\n",
      "Epoch 53/300 - Train Loss: 0.0847, Val Loss: 0.0742\n",
      "Epoch 54/300 - Train Loss: 0.0851, Val Loss: 0.0749\n",
      "Epoch 55/300 - Train Loss: 0.0876, Val Loss: 0.0729\n",
      "Epoch 56/300 - Train Loss: 0.0850, Val Loss: 0.0718\n",
      "Epoch 57/300 - Train Loss: 0.0843, Val Loss: 0.0742\n",
      "Epoch 58/300 - Train Loss: 0.0852, Val Loss: 0.0773\n",
      "Epoch 59/300 - Train Loss: 0.0850, Val Loss: 0.0751\n",
      "Epoch 60/300 - Train Loss: 0.0840, Val Loss: 0.0722\n",
      "Epoch 61/300 - Train Loss: 0.0874, Val Loss: 0.0815\n",
      "Epoch 62/300 - Train Loss: 0.0850, Val Loss: 0.0739\n",
      "Epoch 63/300 - Train Loss: 0.0858, Val Loss: 0.0751\n",
      "Epoch 64/300 - Train Loss: 0.0845, Val Loss: 0.0702\n",
      "Epoch 65/300 - Train Loss: 0.0851, Val Loss: 0.0727\n",
      "Epoch 66/300 - Train Loss: 0.0862, Val Loss: 0.0779\n",
      "Epoch 67/300 - Train Loss: 0.0868, Val Loss: 0.0723\n",
      "Epoch 68/300 - Train Loss: 0.0861, Val Loss: 0.0703\n",
      "Epoch 69/300 - Train Loss: 0.0876, Val Loss: 0.0707\n",
      "Epoch 70/300 - Train Loss: 0.0871, Val Loss: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:05:49,785] Trial 127 finished with value: 0.9627423393661069 and parameters: {'F1': 4, 'F2': 32, 'D': 8, 'dropout': 0.18328022519162446, 'learning_rate': 9.850521467173122e-05, 'batch_size': 32, 'weight_decay': 0.0017444019915207269}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300 - Train Loss: 0.0838, Val Loss: 0.0708\n",
      "Early stopping at epoch 71\n",
      "Macro F1 Score: 0.9627, Macro Precision: 0.9553, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 129\n",
      "Training with F1=8, F2=32, D=2, dropout=0.30799253380179353, LR=0.00011640043355967004, BS=32, WD=0.0004727549232138438\n",
      "Epoch 1/300 - Train Loss: 0.3474, Val Loss: 0.1929\n",
      "Epoch 2/300 - Train Loss: 0.1787, Val Loss: 0.1233\n",
      "Epoch 3/300 - Train Loss: 0.1364, Val Loss: 0.1057\n",
      "Epoch 4/300 - Train Loss: 0.1206, Val Loss: 0.0921\n",
      "Epoch 5/300 - Train Loss: 0.1100, Val Loss: 0.0919\n",
      "Epoch 6/300 - Train Loss: 0.1053, Val Loss: 0.0831\n",
      "Epoch 7/300 - Train Loss: 0.1057, Val Loss: 0.0813\n",
      "Epoch 8/300 - Train Loss: 0.0996, Val Loss: 0.0847\n",
      "Epoch 9/300 - Train Loss: 0.0981, Val Loss: 0.0836\n",
      "Epoch 10/300 - Train Loss: 0.0985, Val Loss: 0.0802\n",
      "Epoch 11/300 - Train Loss: 0.0974, Val Loss: 0.0805\n",
      "Epoch 12/300 - Train Loss: 0.0983, Val Loss: 0.0781\n",
      "Epoch 13/300 - Train Loss: 0.0988, Val Loss: 0.0779\n",
      "Epoch 14/300 - Train Loss: 0.0965, Val Loss: 0.0758\n",
      "Epoch 15/300 - Train Loss: 0.0962, Val Loss: 0.0830\n",
      "Epoch 16/300 - Train Loss: 0.0932, Val Loss: 0.0829\n",
      "Epoch 17/300 - Train Loss: 0.0928, Val Loss: 0.0779\n",
      "Epoch 18/300 - Train Loss: 0.0902, Val Loss: 0.0753\n",
      "Epoch 19/300 - Train Loss: 0.0942, Val Loss: 0.0767\n",
      "Epoch 20/300 - Train Loss: 0.0928, Val Loss: 0.0778\n",
      "Epoch 21/300 - Train Loss: 0.0903, Val Loss: 0.0804\n",
      "Epoch 22/300 - Train Loss: 0.0944, Val Loss: 0.0817\n",
      "Epoch 23/300 - Train Loss: 0.0904, Val Loss: 0.0758\n",
      "Epoch 24/300 - Train Loss: 0.0902, Val Loss: 0.0760\n",
      "Epoch 25/300 - Train Loss: 0.0888, Val Loss: 0.0763\n",
      "Epoch 26/300 - Train Loss: 0.0914, Val Loss: 0.0786\n",
      "Epoch 27/300 - Train Loss: 0.0923, Val Loss: 0.0765\n",
      "Epoch 28/300 - Train Loss: 0.0905, Val Loss: 0.0797\n",
      "Epoch 29/300 - Train Loss: 0.0891, Val Loss: 0.0775\n",
      "Epoch 30/300 - Train Loss: 0.0858, Val Loss: 0.0737\n",
      "Epoch 31/300 - Train Loss: 0.0912, Val Loss: 0.0850\n",
      "Epoch 32/300 - Train Loss: 0.0884, Val Loss: 0.0743\n",
      "Epoch 33/300 - Train Loss: 0.0882, Val Loss: 0.0747\n",
      "Epoch 34/300 - Train Loss: 0.0867, Val Loss: 0.0811\n",
      "Epoch 35/300 - Train Loss: 0.0870, Val Loss: 0.0750\n",
      "Epoch 36/300 - Train Loss: 0.0853, Val Loss: 0.0724\n",
      "Epoch 37/300 - Train Loss: 0.0861, Val Loss: 0.0758\n",
      "Epoch 38/300 - Train Loss: 0.0848, Val Loss: 0.0754\n",
      "Epoch 39/300 - Train Loss: 0.0860, Val Loss: 0.0775\n",
      "Epoch 40/300 - Train Loss: 0.0837, Val Loss: 0.0734\n",
      "Epoch 41/300 - Train Loss: 0.0832, Val Loss: 0.0752\n",
      "Epoch 42/300 - Train Loss: 0.0831, Val Loss: 0.0739\n",
      "Epoch 43/300 - Train Loss: 0.0842, Val Loss: 0.0752\n",
      "Epoch 44/300 - Train Loss: 0.0867, Val Loss: 0.0716\n",
      "Epoch 45/300 - Train Loss: 0.0828, Val Loss: 0.0724\n",
      "Epoch 46/300 - Train Loss: 0.0838, Val Loss: 0.0728\n",
      "Epoch 47/300 - Train Loss: 0.0819, Val Loss: 0.0726\n",
      "Epoch 48/300 - Train Loss: 0.0827, Val Loss: 0.0792\n",
      "Epoch 49/300 - Train Loss: 0.0841, Val Loss: 0.0728\n",
      "Epoch 50/300 - Train Loss: 0.0827, Val Loss: 0.0740\n",
      "Epoch 51/300 - Train Loss: 0.0820, Val Loss: 0.0757\n",
      "Epoch 52/300 - Train Loss: 0.0846, Val Loss: 0.0737\n",
      "Epoch 53/300 - Train Loss: 0.0857, Val Loss: 0.0702\n",
      "Epoch 54/300 - Train Loss: 0.0819, Val Loss: 0.0765\n",
      "Epoch 55/300 - Train Loss: 0.0809, Val Loss: 0.0744\n",
      "Epoch 56/300 - Train Loss: 0.0802, Val Loss: 0.0740\n",
      "Epoch 57/300 - Train Loss: 0.0818, Val Loss: 0.0750\n",
      "Epoch 58/300 - Train Loss: 0.0824, Val Loss: 0.0700\n",
      "Epoch 59/300 - Train Loss: 0.0813, Val Loss: 0.0720\n",
      "Epoch 60/300 - Train Loss: 0.0831, Val Loss: 0.0739\n",
      "Epoch 61/300 - Train Loss: 0.0795, Val Loss: 0.0727\n",
      "Epoch 62/300 - Train Loss: 0.0788, Val Loss: 0.0721\n",
      "Epoch 63/300 - Train Loss: 0.0823, Val Loss: 0.0777\n",
      "Epoch 64/300 - Train Loss: 0.0802, Val Loss: 0.0736\n",
      "Epoch 65/300 - Train Loss: 0.0793, Val Loss: 0.0741\n",
      "Epoch 66/300 - Train Loss: 0.0806, Val Loss: 0.0705\n",
      "Epoch 67/300 - Train Loss: 0.0805, Val Loss: 0.0701\n",
      "Epoch 68/300 - Train Loss: 0.0803, Val Loss: 0.0704\n",
      "Epoch 69/300 - Train Loss: 0.0831, Val Loss: 0.0686\n",
      "Epoch 70/300 - Train Loss: 0.0788, Val Loss: 0.0750\n",
      "Epoch 71/300 - Train Loss: 0.0795, Val Loss: 0.0712\n",
      "Epoch 72/300 - Train Loss: 0.0802, Val Loss: 0.0745\n",
      "Epoch 73/300 - Train Loss: 0.0801, Val Loss: 0.0726\n",
      "Epoch 74/300 - Train Loss: 0.0804, Val Loss: 0.0739\n",
      "Epoch 75/300 - Train Loss: 0.0791, Val Loss: 0.0708\n",
      "Epoch 76/300 - Train Loss: 0.0813, Val Loss: 0.0746\n",
      "Epoch 77/300 - Train Loss: 0.0769, Val Loss: 0.0716\n",
      "Epoch 78/300 - Train Loss: 0.0801, Val Loss: 0.0720\n",
      "Epoch 79/300 - Train Loss: 0.0780, Val Loss: 0.0706\n",
      "Epoch 80/300 - Train Loss: 0.0793, Val Loss: 0.0759\n",
      "Epoch 81/300 - Train Loss: 0.0810, Val Loss: 0.0703\n",
      "Epoch 82/300 - Train Loss: 0.0801, Val Loss: 0.0696\n",
      "Epoch 83/300 - Train Loss: 0.0774, Val Loss: 0.0684\n",
      "Epoch 84/300 - Train Loss: 0.0786, Val Loss: 0.0735\n",
      "Epoch 85/300 - Train Loss: 0.0796, Val Loss: 0.0688\n",
      "Epoch 86/300 - Train Loss: 0.0780, Val Loss: 0.0699\n",
      "Epoch 87/300 - Train Loss: 0.0789, Val Loss: 0.0724\n",
      "Epoch 88/300 - Train Loss: 0.0774, Val Loss: 0.0691\n",
      "Epoch 89/300 - Train Loss: 0.0792, Val Loss: 0.0702\n",
      "Epoch 90/300 - Train Loss: 0.0797, Val Loss: 0.0712\n",
      "Epoch 91/300 - Train Loss: 0.0795, Val Loss: 0.0705\n",
      "Epoch 92/300 - Train Loss: 0.0768, Val Loss: 0.0729\n",
      "Epoch 93/300 - Train Loss: 0.0770, Val Loss: 0.0746\n",
      "Epoch 94/300 - Train Loss: 0.0781, Val Loss: 0.0689\n",
      "Epoch 95/300 - Train Loss: 0.0784, Val Loss: 0.0790\n",
      "Epoch 96/300 - Train Loss: 0.0784, Val Loss: 0.0717\n",
      "Epoch 97/300 - Train Loss: 0.0796, Val Loss: 0.0719\n",
      "Epoch 98/300 - Train Loss: 0.0792, Val Loss: 0.0682\n",
      "Epoch 99/300 - Train Loss: 0.0800, Val Loss: 0.0683\n",
      "Epoch 100/300 - Train Loss: 0.0793, Val Loss: 0.0691\n",
      "Epoch 101/300 - Train Loss: 0.0783, Val Loss: 0.0690\n",
      "Epoch 102/300 - Train Loss: 0.0803, Val Loss: 0.0678\n",
      "Epoch 103/300 - Train Loss: 0.0791, Val Loss: 0.0699\n",
      "Epoch 104/300 - Train Loss: 0.0769, Val Loss: 0.0716\n",
      "Epoch 105/300 - Train Loss: 0.0766, Val Loss: 0.0724\n",
      "Epoch 106/300 - Train Loss: 0.0773, Val Loss: 0.0715\n",
      "Epoch 107/300 - Train Loss: 0.0775, Val Loss: 0.0709\n",
      "Epoch 108/300 - Train Loss: 0.0789, Val Loss: 0.0721\n",
      "Epoch 109/300 - Train Loss: 0.0793, Val Loss: 0.0700\n",
      "Epoch 110/300 - Train Loss: 0.0811, Val Loss: 0.0727\n",
      "Epoch 111/300 - Train Loss: 0.0792, Val Loss: 0.0692\n",
      "Epoch 112/300 - Train Loss: 0.0779, Val Loss: 0.0678\n",
      "Epoch 113/300 - Train Loss: 0.0802, Val Loss: 0.0690\n",
      "Epoch 114/300 - Train Loss: 0.0779, Val Loss: 0.0682\n",
      "Epoch 115/300 - Train Loss: 0.0770, Val Loss: 0.0689\n",
      "Epoch 116/300 - Train Loss: 0.0756, Val Loss: 0.0730\n",
      "Epoch 117/300 - Train Loss: 0.0789, Val Loss: 0.0711\n",
      "Epoch 118/300 - Train Loss: 0.0778, Val Loss: 0.0742\n",
      "Epoch 119/300 - Train Loss: 0.0770, Val Loss: 0.0682\n",
      "Epoch 120/300 - Train Loss: 0.0767, Val Loss: 0.0789\n",
      "Epoch 121/300 - Train Loss: 0.0763, Val Loss: 0.0720\n",
      "Epoch 122/300 - Train Loss: 0.0769, Val Loss: 0.0730\n",
      "Epoch 123/300 - Train Loss: 0.0777, Val Loss: 0.0755\n",
      "Epoch 124/300 - Train Loss: 0.0764, Val Loss: 0.0735\n",
      "Epoch 125/300 - Train Loss: 0.0774, Val Loss: 0.0799\n",
      "Epoch 126/300 - Train Loss: 0.0789, Val Loss: 0.0719\n",
      "Epoch 127/300 - Train Loss: 0.0764, Val Loss: 0.0700\n",
      "Epoch 128/300 - Train Loss: 0.0761, Val Loss: 0.0689\n",
      "Epoch 129/300 - Train Loss: 0.0754, Val Loss: 0.0696\n",
      "Epoch 130/300 - Train Loss: 0.0759, Val Loss: 0.0733\n",
      "Epoch 131/300 - Train Loss: 0.0741, Val Loss: 0.0770\n",
      "Epoch 132/300 - Train Loss: 0.0772, Val Loss: 0.0692\n",
      "Epoch 133/300 - Train Loss: 0.0767, Val Loss: 0.0732\n",
      "Epoch 134/300 - Train Loss: 0.0752, Val Loss: 0.0704\n",
      "Epoch 135/300 - Train Loss: 0.0758, Val Loss: 0.0726\n",
      "Epoch 136/300 - Train Loss: 0.0762, Val Loss: 0.0734\n",
      "Epoch 137/300 - Train Loss: 0.0758, Val Loss: 0.0703\n",
      "Epoch 138/300 - Train Loss: 0.0759, Val Loss: 0.0687\n",
      "Epoch 139/300 - Train Loss: 0.0767, Val Loss: 0.0665\n",
      "Epoch 140/300 - Train Loss: 0.0729, Val Loss: 0.0719\n",
      "Epoch 141/300 - Train Loss: 0.0764, Val Loss: 0.0729\n",
      "Epoch 142/300 - Train Loss: 0.0745, Val Loss: 0.0790\n",
      "Epoch 143/300 - Train Loss: 0.0754, Val Loss: 0.0695\n",
      "Epoch 144/300 - Train Loss: 0.0779, Val Loss: 0.0693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0758, Val Loss: 0.0675\n",
      "Epoch 146/300 - Train Loss: 0.0754, Val Loss: 0.0681\n",
      "Epoch 147/300 - Train Loss: 0.0745, Val Loss: 0.0695\n",
      "Epoch 148/300 - Train Loss: 0.0752, Val Loss: 0.0696\n",
      "Epoch 149/300 - Train Loss: 0.0752, Val Loss: 0.0755\n",
      "Epoch 150/300 - Train Loss: 0.0786, Val Loss: 0.0709\n",
      "Epoch 151/300 - Train Loss: 0.0755, Val Loss: 0.0699\n",
      "Epoch 152/300 - Train Loss: 0.0749, Val Loss: 0.0727\n",
      "Epoch 153/300 - Train Loss: 0.0756, Val Loss: 0.0676\n",
      "Epoch 154/300 - Train Loss: 0.0754, Val Loss: 0.0745\n",
      "Epoch 155/300 - Train Loss: 0.0755, Val Loss: 0.0681\n",
      "Epoch 156/300 - Train Loss: 0.0741, Val Loss: 0.0700\n",
      "Epoch 157/300 - Train Loss: 0.0770, Val Loss: 0.0693\n",
      "Epoch 158/300 - Train Loss: 0.0745, Val Loss: 0.0697\n",
      "Epoch 159/300 - Train Loss: 0.0735, Val Loss: 0.0708\n",
      "Epoch 160/300 - Train Loss: 0.0743, Val Loss: 0.0728\n",
      "Epoch 161/300 - Train Loss: 0.0744, Val Loss: 0.0700\n",
      "Epoch 162/300 - Train Loss: 0.0755, Val Loss: 0.0716\n",
      "Epoch 163/300 - Train Loss: 0.0758, Val Loss: 0.0706\n",
      "Epoch 164/300 - Train Loss: 0.0753, Val Loss: 0.0749\n",
      "Epoch 165/300 - Train Loss: 0.0773, Val Loss: 0.0755\n",
      "Epoch 166/300 - Train Loss: 0.0731, Val Loss: 0.0675\n",
      "Epoch 167/300 - Train Loss: 0.0743, Val Loss: 0.0686\n",
      "Epoch 168/300 - Train Loss: 0.0753, Val Loss: 0.0679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:11:11,455] Trial 128 finished with value: 0.9664846378167006 and parameters: {'F1': 8, 'F2': 32, 'D': 2, 'dropout': 0.30799253380179353, 'learning_rate': 0.00011640043355967004, 'batch_size': 32, 'weight_decay': 0.0004727549232138438}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 169/300 - Train Loss: 0.0774, Val Loss: 0.0713\n",
      "Early stopping at epoch 169\n",
      "Macro F1 Score: 0.9665, Macro Precision: 0.9604, Macro Recall: 0.9729\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 130\n",
      "Training with F1=16, F2=32, D=8, dropout=0.27313246930375956, LR=0.00014342450844806744, BS=32, WD=0.0010708072436975296\n",
      "Epoch 1/300 - Train Loss: 0.2279, Val Loss: 0.0942\n",
      "Epoch 2/300 - Train Loss: 0.1087, Val Loss: 0.0833\n",
      "Epoch 3/300 - Train Loss: 0.0984, Val Loss: 0.0735\n",
      "Epoch 4/300 - Train Loss: 0.0953, Val Loss: 0.0798\n",
      "Epoch 5/300 - Train Loss: 0.0911, Val Loss: 0.0710\n",
      "Epoch 6/300 - Train Loss: 0.0909, Val Loss: 0.0714\n",
      "Epoch 7/300 - Train Loss: 0.0902, Val Loss: 0.0709\n",
      "Epoch 8/300 - Train Loss: 0.0856, Val Loss: 0.0812\n",
      "Epoch 9/300 - Train Loss: 0.0850, Val Loss: 0.0713\n",
      "Epoch 10/300 - Train Loss: 0.0861, Val Loss: 0.0868\n",
      "Epoch 11/300 - Train Loss: 0.0841, Val Loss: 0.0697\n",
      "Epoch 12/300 - Train Loss: 0.0873, Val Loss: 0.0737\n",
      "Epoch 13/300 - Train Loss: 0.0821, Val Loss: 0.0695\n",
      "Epoch 14/300 - Train Loss: 0.0852, Val Loss: 0.0701\n",
      "Epoch 15/300 - Train Loss: 0.0851, Val Loss: 0.0754\n",
      "Epoch 16/300 - Train Loss: 0.0828, Val Loss: 0.0717\n",
      "Epoch 17/300 - Train Loss: 0.0822, Val Loss: 0.0742\n",
      "Epoch 18/300 - Train Loss: 0.0833, Val Loss: 0.0685\n",
      "Epoch 19/300 - Train Loss: 0.0846, Val Loss: 0.0673\n",
      "Epoch 20/300 - Train Loss: 0.0842, Val Loss: 0.0670\n",
      "Epoch 21/300 - Train Loss: 0.0803, Val Loss: 0.0708\n",
      "Epoch 22/300 - Train Loss: 0.0823, Val Loss: 0.0713\n",
      "Epoch 23/300 - Train Loss: 0.0825, Val Loss: 0.0687\n",
      "Epoch 24/300 - Train Loss: 0.0822, Val Loss: 0.0662\n",
      "Epoch 25/300 - Train Loss: 0.0808, Val Loss: 0.0692\n",
      "Epoch 26/300 - Train Loss: 0.0811, Val Loss: 0.0702\n",
      "Epoch 27/300 - Train Loss: 0.0816, Val Loss: 0.0667\n",
      "Epoch 28/300 - Train Loss: 0.0796, Val Loss: 0.0723\n",
      "Epoch 29/300 - Train Loss: 0.0846, Val Loss: 0.0705\n",
      "Epoch 30/300 - Train Loss: 0.0799, Val Loss: 0.0689\n",
      "Epoch 31/300 - Train Loss: 0.0819, Val Loss: 0.0682\n",
      "Epoch 32/300 - Train Loss: 0.0804, Val Loss: 0.0722\n",
      "Epoch 33/300 - Train Loss: 0.0825, Val Loss: 0.0684\n",
      "Epoch 34/300 - Train Loss: 0.0787, Val Loss: 0.0685\n",
      "Epoch 35/300 - Train Loss: 0.0801, Val Loss: 0.0739\n",
      "Epoch 36/300 - Train Loss: 0.0819, Val Loss: 0.0732\n",
      "Epoch 37/300 - Train Loss: 0.0809, Val Loss: 0.0688\n",
      "Epoch 38/300 - Train Loss: 0.0837, Val Loss: 0.0710\n",
      "Epoch 39/300 - Train Loss: 0.0803, Val Loss: 0.0710\n",
      "Epoch 40/300 - Train Loss: 0.0788, Val Loss: 0.0683\n",
      "Epoch 41/300 - Train Loss: 0.0817, Val Loss: 0.0750\n",
      "Epoch 42/300 - Train Loss: 0.0823, Val Loss: 0.0714\n",
      "Epoch 43/300 - Train Loss: 0.0826, Val Loss: 0.0743\n",
      "Epoch 44/300 - Train Loss: 0.0802, Val Loss: 0.0695\n",
      "Epoch 45/300 - Train Loss: 0.0818, Val Loss: 0.0688\n",
      "Epoch 46/300 - Train Loss: 0.0805, Val Loss: 0.0707\n",
      "Epoch 47/300 - Train Loss: 0.0791, Val Loss: 0.0746\n",
      "Epoch 48/300 - Train Loss: 0.0814, Val Loss: 0.0708\n",
      "Epoch 49/300 - Train Loss: 0.0823, Val Loss: 0.0720\n",
      "Epoch 50/300 - Train Loss: 0.0801, Val Loss: 0.0718\n",
      "Epoch 51/300 - Train Loss: 0.0799, Val Loss: 0.0691\n",
      "Epoch 52/300 - Train Loss: 0.0792, Val Loss: 0.0707\n",
      "Epoch 53/300 - Train Loss: 0.0812, Val Loss: 0.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:14:39,695] Trial 129 finished with value: 0.9654975655673734 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.27313246930375956, 'learning_rate': 0.00014342450844806744, 'batch_size': 32, 'weight_decay': 0.0010708072436975296}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300 - Train Loss: 0.0822, Val Loss: 0.0835\n",
      "Early stopping at epoch 54\n",
      "Macro F1 Score: 0.9655, Macro Precision: 0.9595, Macro Recall: 0.9719\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 131\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2530284140882641, LR=2.7449638825963175e-05, BS=256, WD=0.0014047767643694585\n",
      "Epoch 1/300 - Train Loss: 0.8425, Val Loss: 0.5931\n",
      "Epoch 2/300 - Train Loss: 0.4527, Val Loss: 0.3571\n",
      "Epoch 3/300 - Train Loss: 0.3130, Val Loss: 0.2741\n",
      "Epoch 4/300 - Train Loss: 0.2530, Val Loss: 0.2389\n",
      "Epoch 5/300 - Train Loss: 0.2224, Val Loss: 0.2095\n",
      "Epoch 6/300 - Train Loss: 0.1981, Val Loss: 0.1902\n",
      "Epoch 7/300 - Train Loss: 0.1794, Val Loss: 0.1678\n",
      "Epoch 8/300 - Train Loss: 0.1634, Val Loss: 0.1550\n",
      "Epoch 9/300 - Train Loss: 0.1521, Val Loss: 0.1444\n",
      "Epoch 10/300 - Train Loss: 0.1407, Val Loss: 0.1319\n",
      "Epoch 11/300 - Train Loss: 0.1322, Val Loss: 0.1248\n",
      "Epoch 12/300 - Train Loss: 0.1252, Val Loss: 0.1172\n",
      "Epoch 13/300 - Train Loss: 0.1201, Val Loss: 0.1142\n",
      "Epoch 14/300 - Train Loss: 0.1148, Val Loss: 0.1120\n",
      "Epoch 15/300 - Train Loss: 0.1133, Val Loss: 0.1050\n",
      "Epoch 16/300 - Train Loss: 0.1095, Val Loss: 0.1023\n",
      "Epoch 17/300 - Train Loss: 0.1094, Val Loss: 0.1013\n",
      "Epoch 18/300 - Train Loss: 0.1038, Val Loss: 0.1002\n",
      "Epoch 19/300 - Train Loss: 0.1015, Val Loss: 0.0979\n",
      "Epoch 20/300 - Train Loss: 0.1009, Val Loss: 0.1001\n",
      "Epoch 21/300 - Train Loss: 0.0982, Val Loss: 0.0974\n",
      "Epoch 22/300 - Train Loss: 0.0982, Val Loss: 0.0955\n",
      "Epoch 23/300 - Train Loss: 0.0972, Val Loss: 0.1020\n",
      "Epoch 24/300 - Train Loss: 0.0950, Val Loss: 0.0868\n",
      "Epoch 25/300 - Train Loss: 0.0943, Val Loss: 0.0964\n",
      "Epoch 26/300 - Train Loss: 0.0929, Val Loss: 0.0903\n",
      "Epoch 27/300 - Train Loss: 0.0919, Val Loss: 0.0887\n",
      "Epoch 28/300 - Train Loss: 0.0913, Val Loss: 0.0857\n",
      "Epoch 29/300 - Train Loss: 0.0898, Val Loss: 0.0876\n",
      "Epoch 30/300 - Train Loss: 0.0890, Val Loss: 0.0869\n",
      "Epoch 31/300 - Train Loss: 0.0885, Val Loss: 0.0829\n",
      "Epoch 32/300 - Train Loss: 0.0898, Val Loss: 0.0911\n",
      "Epoch 33/300 - Train Loss: 0.0885, Val Loss: 0.0861\n",
      "Epoch 34/300 - Train Loss: 0.0878, Val Loss: 0.0850\n",
      "Epoch 35/300 - Train Loss: 0.0861, Val Loss: 0.0863\n",
      "Epoch 36/300 - Train Loss: 0.0857, Val Loss: 0.0854\n",
      "Epoch 37/300 - Train Loss: 0.0852, Val Loss: 0.0817\n",
      "Epoch 38/300 - Train Loss: 0.0860, Val Loss: 0.0848\n",
      "Epoch 39/300 - Train Loss: 0.0835, Val Loss: 0.0812\n",
      "Epoch 40/300 - Train Loss: 0.0832, Val Loss: 0.0817\n",
      "Epoch 41/300 - Train Loss: 0.0843, Val Loss: 0.0801\n",
      "Epoch 42/300 - Train Loss: 0.0844, Val Loss: 0.0800\n",
      "Epoch 43/300 - Train Loss: 0.0830, Val Loss: 0.0794\n",
      "Epoch 44/300 - Train Loss: 0.0806, Val Loss: 0.0839\n",
      "Epoch 45/300 - Train Loss: 0.0829, Val Loss: 0.0786\n",
      "Epoch 46/300 - Train Loss: 0.0817, Val Loss: 0.0811\n",
      "Epoch 47/300 - Train Loss: 0.0816, Val Loss: 0.0801\n",
      "Epoch 48/300 - Train Loss: 0.0827, Val Loss: 0.0793\n",
      "Epoch 49/300 - Train Loss: 0.0799, Val Loss: 0.0803\n",
      "Epoch 50/300 - Train Loss: 0.0820, Val Loss: 0.0797\n",
      "Epoch 51/300 - Train Loss: 0.0804, Val Loss: 0.0789\n",
      "Epoch 52/300 - Train Loss: 0.0794, Val Loss: 0.0794\n",
      "Epoch 53/300 - Train Loss: 0.0788, Val Loss: 0.0777\n",
      "Epoch 54/300 - Train Loss: 0.0790, Val Loss: 0.0792\n",
      "Epoch 55/300 - Train Loss: 0.0786, Val Loss: 0.0764\n",
      "Epoch 56/300 - Train Loss: 0.0791, Val Loss: 0.0758\n",
      "Epoch 57/300 - Train Loss: 0.0784, Val Loss: 0.0757\n",
      "Epoch 58/300 - Train Loss: 0.0784, Val Loss: 0.0757\n",
      "Epoch 59/300 - Train Loss: 0.0778, Val Loss: 0.0777\n",
      "Epoch 60/300 - Train Loss: 0.0768, Val Loss: 0.0768\n",
      "Epoch 61/300 - Train Loss: 0.0779, Val Loss: 0.0793\n",
      "Epoch 62/300 - Train Loss: 0.0782, Val Loss: 0.0783\n",
      "Epoch 63/300 - Train Loss: 0.0756, Val Loss: 0.0762\n",
      "Epoch 64/300 - Train Loss: 0.0768, Val Loss: 0.0769\n",
      "Epoch 65/300 - Train Loss: 0.0754, Val Loss: 0.0777\n",
      "Epoch 66/300 - Train Loss: 0.0784, Val Loss: 0.0764\n",
      "Epoch 67/300 - Train Loss: 0.0756, Val Loss: 0.0767\n",
      "Epoch 68/300 - Train Loss: 0.0789, Val Loss: 0.0739\n",
      "Epoch 69/300 - Train Loss: 0.0768, Val Loss: 0.0780\n",
      "Epoch 70/300 - Train Loss: 0.0769, Val Loss: 0.0750\n",
      "Epoch 71/300 - Train Loss: 0.0746, Val Loss: 0.0744\n",
      "Epoch 72/300 - Train Loss: 0.0752, Val Loss: 0.0773\n",
      "Epoch 73/300 - Train Loss: 0.0740, Val Loss: 0.0760\n",
      "Epoch 74/300 - Train Loss: 0.0755, Val Loss: 0.0781\n",
      "Epoch 75/300 - Train Loss: 0.0753, Val Loss: 0.0747\n",
      "Epoch 76/300 - Train Loss: 0.0740, Val Loss: 0.0776\n",
      "Epoch 77/300 - Train Loss: 0.0726, Val Loss: 0.0745\n",
      "Epoch 78/300 - Train Loss: 0.0742, Val Loss: 0.0741\n",
      "Epoch 79/300 - Train Loss: 0.0738, Val Loss: 0.0740\n",
      "Epoch 80/300 - Train Loss: 0.0743, Val Loss: 0.0756\n",
      "Epoch 81/300 - Train Loss: 0.0742, Val Loss: 0.0770\n",
      "Epoch 82/300 - Train Loss: 0.0745, Val Loss: 0.0754\n",
      "Epoch 83/300 - Train Loss: 0.0735, Val Loss: 0.0740\n",
      "Epoch 84/300 - Train Loss: 0.0724, Val Loss: 0.0754\n",
      "Epoch 85/300 - Train Loss: 0.0724, Val Loss: 0.0775\n",
      "Epoch 86/300 - Train Loss: 0.0723, Val Loss: 0.0722\n",
      "Epoch 87/300 - Train Loss: 0.0752, Val Loss: 0.0729\n",
      "Epoch 88/300 - Train Loss: 0.0722, Val Loss: 0.0755\n",
      "Epoch 89/300 - Train Loss: 0.0725, Val Loss: 0.0753\n",
      "Epoch 90/300 - Train Loss: 0.0722, Val Loss: 0.0764\n",
      "Epoch 91/300 - Train Loss: 0.0724, Val Loss: 0.0741\n",
      "Epoch 92/300 - Train Loss: 0.0729, Val Loss: 0.0759\n",
      "Epoch 93/300 - Train Loss: 0.0710, Val Loss: 0.0723\n",
      "Epoch 94/300 - Train Loss: 0.0730, Val Loss: 0.0745\n",
      "Epoch 95/300 - Train Loss: 0.0716, Val Loss: 0.0715\n",
      "Epoch 96/300 - Train Loss: 0.0702, Val Loss: 0.0750\n",
      "Epoch 97/300 - Train Loss: 0.0718, Val Loss: 0.0723\n",
      "Epoch 98/300 - Train Loss: 0.0728, Val Loss: 0.0773\n",
      "Epoch 99/300 - Train Loss: 0.0737, Val Loss: 0.0748\n",
      "Epoch 100/300 - Train Loss: 0.0689, Val Loss: 0.0737\n",
      "Epoch 101/300 - Train Loss: 0.0718, Val Loss: 0.0732\n",
      "Epoch 102/300 - Train Loss: 0.0715, Val Loss: 0.0776\n",
      "Epoch 103/300 - Train Loss: 0.0706, Val Loss: 0.0720\n",
      "Epoch 104/300 - Train Loss: 0.0714, Val Loss: 0.0729\n",
      "Epoch 105/300 - Train Loss: 0.0713, Val Loss: 0.0734\n",
      "Epoch 106/300 - Train Loss: 0.0725, Val Loss: 0.0749\n",
      "Epoch 107/300 - Train Loss: 0.0696, Val Loss: 0.0747\n",
      "Epoch 108/300 - Train Loss: 0.0701, Val Loss: 0.0728\n",
      "Epoch 109/300 - Train Loss: 0.0687, Val Loss: 0.0731\n",
      "Epoch 110/300 - Train Loss: 0.0698, Val Loss: 0.0707\n",
      "Epoch 111/300 - Train Loss: 0.0692, Val Loss: 0.0748\n",
      "Epoch 112/300 - Train Loss: 0.0702, Val Loss: 0.0733\n",
      "Epoch 113/300 - Train Loss: 0.0689, Val Loss: 0.0718\n",
      "Epoch 114/300 - Train Loss: 0.0694, Val Loss: 0.0781\n",
      "Epoch 115/300 - Train Loss: 0.0693, Val Loss: 0.0700\n",
      "Epoch 116/300 - Train Loss: 0.0705, Val Loss: 0.0701\n",
      "Epoch 117/300 - Train Loss: 0.0707, Val Loss: 0.0730\n",
      "Epoch 118/300 - Train Loss: 0.0690, Val Loss: 0.0775\n",
      "Epoch 119/300 - Train Loss: 0.0704, Val Loss: 0.0712\n",
      "Epoch 120/300 - Train Loss: 0.0694, Val Loss: 0.0722\n",
      "Epoch 121/300 - Train Loss: 0.0688, Val Loss: 0.0732\n",
      "Epoch 122/300 - Train Loss: 0.0705, Val Loss: 0.0722\n",
      "Epoch 123/300 - Train Loss: 0.0685, Val Loss: 0.0719\n",
      "Epoch 124/300 - Train Loss: 0.0699, Val Loss: 0.0743\n",
      "Epoch 125/300 - Train Loss: 0.0684, Val Loss: 0.0743\n",
      "Epoch 126/300 - Train Loss: 0.0696, Val Loss: 0.0737\n",
      "Epoch 127/300 - Train Loss: 0.0691, Val Loss: 0.0723\n",
      "Epoch 128/300 - Train Loss: 0.0681, Val Loss: 0.0734\n",
      "Epoch 129/300 - Train Loss: 0.0686, Val Loss: 0.0719\n",
      "Epoch 130/300 - Train Loss: 0.0684, Val Loss: 0.0734\n",
      "Epoch 131/300 - Train Loss: 0.0696, Val Loss: 0.0719\n",
      "Epoch 132/300 - Train Loss: 0.0680, Val Loss: 0.0719\n",
      "Epoch 133/300 - Train Loss: 0.0682, Val Loss: 0.0713\n",
      "Epoch 134/300 - Train Loss: 0.0681, Val Loss: 0.0710\n",
      "Epoch 135/300 - Train Loss: 0.0677, Val Loss: 0.0692\n",
      "Epoch 136/300 - Train Loss: 0.0689, Val Loss: 0.0712\n",
      "Epoch 137/300 - Train Loss: 0.0685, Val Loss: 0.0727\n",
      "Epoch 138/300 - Train Loss: 0.0698, Val Loss: 0.0724\n",
      "Epoch 139/300 - Train Loss: 0.0676, Val Loss: 0.0716\n",
      "Epoch 140/300 - Train Loss: 0.0667, Val Loss: 0.0726\n",
      "Epoch 141/300 - Train Loss: 0.0674, Val Loss: 0.0718\n",
      "Epoch 142/300 - Train Loss: 0.0669, Val Loss: 0.0720\n",
      "Epoch 143/300 - Train Loss: 0.0672, Val Loss: 0.0690\n",
      "Epoch 144/300 - Train Loss: 0.0681, Val Loss: 0.0717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0666, Val Loss: 0.0704\n",
      "Epoch 146/300 - Train Loss: 0.0669, Val Loss: 0.0696\n",
      "Epoch 147/300 - Train Loss: 0.0682, Val Loss: 0.0702\n",
      "Epoch 148/300 - Train Loss: 0.0679, Val Loss: 0.0699\n",
      "Epoch 149/300 - Train Loss: 0.0685, Val Loss: 0.0721\n",
      "Epoch 150/300 - Train Loss: 0.0661, Val Loss: 0.0686\n",
      "Epoch 151/300 - Train Loss: 0.0662, Val Loss: 0.0694\n",
      "Epoch 152/300 - Train Loss: 0.0667, Val Loss: 0.0718\n",
      "Epoch 153/300 - Train Loss: 0.0657, Val Loss: 0.0699\n",
      "Epoch 154/300 - Train Loss: 0.0666, Val Loss: 0.0699\n",
      "Epoch 155/300 - Train Loss: 0.0677, Val Loss: 0.0711\n",
      "Epoch 156/300 - Train Loss: 0.0668, Val Loss: 0.0744\n",
      "Epoch 157/300 - Train Loss: 0.0664, Val Loss: 0.0723\n",
      "Epoch 158/300 - Train Loss: 0.0659, Val Loss: 0.0714\n",
      "Epoch 159/300 - Train Loss: 0.0684, Val Loss: 0.0704\n",
      "Epoch 160/300 - Train Loss: 0.0665, Val Loss: 0.0726\n",
      "Epoch 161/300 - Train Loss: 0.0657, Val Loss: 0.0699\n",
      "Epoch 162/300 - Train Loss: 0.0657, Val Loss: 0.0693\n",
      "Epoch 163/300 - Train Loss: 0.0656, Val Loss: 0.0701\n",
      "Epoch 164/300 - Train Loss: 0.0663, Val Loss: 0.0714\n",
      "Epoch 165/300 - Train Loss: 0.0659, Val Loss: 0.0702\n",
      "Epoch 166/300 - Train Loss: 0.0673, Val Loss: 0.0724\n",
      "Epoch 167/300 - Train Loss: 0.0666, Val Loss: 0.0709\n",
      "Epoch 168/300 - Train Loss: 0.0644, Val Loss: 0.0733\n",
      "Epoch 169/300 - Train Loss: 0.0652, Val Loss: 0.0694\n",
      "Epoch 170/300 - Train Loss: 0.0660, Val Loss: 0.0706\n",
      "Epoch 171/300 - Train Loss: 0.0654, Val Loss: 0.0742\n",
      "Epoch 172/300 - Train Loss: 0.0662, Val Loss: 0.0700\n",
      "Epoch 173/300 - Train Loss: 0.0656, Val Loss: 0.0725\n",
      "Epoch 174/300 - Train Loss: 0.0648, Val Loss: 0.0704\n",
      "Epoch 175/300 - Train Loss: 0.0661, Val Loss: 0.0730\n",
      "Epoch 176/300 - Train Loss: 0.0664, Val Loss: 0.0701\n",
      "Epoch 177/300 - Train Loss: 0.0649, Val Loss: 0.0710\n",
      "Epoch 178/300 - Train Loss: 0.0651, Val Loss: 0.0730\n",
      "Epoch 179/300 - Train Loss: 0.0649, Val Loss: 0.0710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:23:54,744] Trial 130 finished with value: 0.9638809366846267 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2530284140882641, 'learning_rate': 2.7449638825963175e-05, 'batch_size': 256, 'weight_decay': 0.0014047767643694585}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 180/300 - Train Loss: 0.0643, Val Loss: 0.0700\n",
      "Early stopping at epoch 180\n",
      "Macro F1 Score: 0.9639, Macro Precision: 0.9542, Macro Recall: 0.9746\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 132\n",
      "Training with F1=16, F2=32, D=8, dropout=0.32248040715033066, LR=6.844116636564234e-05, BS=32, WD=0.007936486365536703\n",
      "Epoch 1/300 - Train Loss: 0.3179, Val Loss: 0.1639\n",
      "Epoch 2/300 - Train Loss: 0.1404, Val Loss: 0.0956\n",
      "Epoch 3/300 - Train Loss: 0.1146, Val Loss: 0.0931\n",
      "Epoch 4/300 - Train Loss: 0.1071, Val Loss: 0.0967\n",
      "Epoch 5/300 - Train Loss: 0.1028, Val Loss: 0.0751\n",
      "Epoch 6/300 - Train Loss: 0.1006, Val Loss: 0.0822\n",
      "Epoch 7/300 - Train Loss: 0.1009, Val Loss: 0.0800\n",
      "Epoch 8/300 - Train Loss: 0.1025, Val Loss: 0.0828\n",
      "Epoch 9/300 - Train Loss: 0.1019, Val Loss: 0.0776\n",
      "Epoch 10/300 - Train Loss: 0.1001, Val Loss: 0.0756\n",
      "Epoch 11/300 - Train Loss: 0.0984, Val Loss: 0.0767\n",
      "Epoch 12/300 - Train Loss: 0.0971, Val Loss: 0.0761\n",
      "Epoch 13/300 - Train Loss: 0.1004, Val Loss: 0.0765\n",
      "Epoch 14/300 - Train Loss: 0.0991, Val Loss: 0.0723\n",
      "Epoch 15/300 - Train Loss: 0.1031, Val Loss: 0.0756\n",
      "Epoch 16/300 - Train Loss: 0.1009, Val Loss: 0.0741\n",
      "Epoch 17/300 - Train Loss: 0.1030, Val Loss: 0.0800\n",
      "Epoch 18/300 - Train Loss: 0.1008, Val Loss: 0.0735\n",
      "Epoch 19/300 - Train Loss: 0.1026, Val Loss: 0.0757\n",
      "Epoch 20/300 - Train Loss: 0.1024, Val Loss: 0.0750\n",
      "Epoch 21/300 - Train Loss: 0.1029, Val Loss: 0.0719\n",
      "Epoch 22/300 - Train Loss: 0.1012, Val Loss: 0.0768\n",
      "Epoch 23/300 - Train Loss: 0.1031, Val Loss: 0.0733\n",
      "Epoch 24/300 - Train Loss: 0.1057, Val Loss: 0.0742\n",
      "Epoch 25/300 - Train Loss: 0.1052, Val Loss: 0.0726\n",
      "Epoch 26/300 - Train Loss: 0.1040, Val Loss: 0.0782\n",
      "Epoch 27/300 - Train Loss: 0.1058, Val Loss: 0.0780\n",
      "Epoch 28/300 - Train Loss: 0.1035, Val Loss: 0.0731\n",
      "Epoch 29/300 - Train Loss: 0.1027, Val Loss: 0.0739\n",
      "Epoch 30/300 - Train Loss: 0.1055, Val Loss: 0.0735\n",
      "Epoch 31/300 - Train Loss: 0.1050, Val Loss: 0.0752\n",
      "Epoch 32/300 - Train Loss: 0.1035, Val Loss: 0.0737\n",
      "Epoch 33/300 - Train Loss: 0.1068, Val Loss: 0.0858\n",
      "Epoch 34/300 - Train Loss: 0.1054, Val Loss: 0.0741\n",
      "Epoch 35/300 - Train Loss: 0.1065, Val Loss: 0.0825\n",
      "Epoch 36/300 - Train Loss: 0.1059, Val Loss: 0.0748\n",
      "Epoch 37/300 - Train Loss: 0.1058, Val Loss: 0.0796\n",
      "Epoch 38/300 - Train Loss: 0.1045, Val Loss: 0.0753\n",
      "Epoch 39/300 - Train Loss: 0.1082, Val Loss: 0.0830\n",
      "Epoch 40/300 - Train Loss: 0.1051, Val Loss: 0.0740\n",
      "Epoch 41/300 - Train Loss: 0.1069, Val Loss: 0.0765\n",
      "Epoch 42/300 - Train Loss: 0.1078, Val Loss: 0.0769\n",
      "Epoch 43/300 - Train Loss: 0.1069, Val Loss: 0.0779\n",
      "Epoch 44/300 - Train Loss: 0.1086, Val Loss: 0.0762\n",
      "Epoch 45/300 - Train Loss: 0.1067, Val Loss: 0.0796\n",
      "Epoch 46/300 - Train Loss: 0.1083, Val Loss: 0.0863\n",
      "Epoch 47/300 - Train Loss: 0.1058, Val Loss: 0.0747\n",
      "Epoch 48/300 - Train Loss: 0.1081, Val Loss: 0.0775\n",
      "Epoch 49/300 - Train Loss: 0.1079, Val Loss: 0.0766\n",
      "Epoch 50/300 - Train Loss: 0.1087, Val Loss: 0.0762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:27:10,829] Trial 131 finished with value: 0.9713270000362085 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.32248040715033066, 'learning_rate': 6.844116636564234e-05, 'batch_size': 32, 'weight_decay': 0.007936486365536703}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300 - Train Loss: 0.1100, Val Loss: 0.0787\n",
      "Early stopping at epoch 51\n",
      "Macro F1 Score: 0.9713, Macro Precision: 0.9728, Macro Recall: 0.9701\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 133\n",
      "Training with F1=16, F2=32, D=8, dropout=0.32322358157419584, LR=4.657517765090502e-05, BS=32, WD=0.00997559205104477\n",
      "Epoch 1/300 - Train Loss: 0.3947, Val Loss: 0.2042\n",
      "Epoch 2/300 - Train Loss: 0.1951, Val Loss: 0.1450\n",
      "Epoch 3/300 - Train Loss: 0.1471, Val Loss: 0.0993\n",
      "Epoch 4/300 - Train Loss: 0.1155, Val Loss: 0.1120\n",
      "Epoch 5/300 - Train Loss: 0.1105, Val Loss: 0.1114\n",
      "Epoch 6/300 - Train Loss: 0.1033, Val Loss: 0.0846\n",
      "Epoch 7/300 - Train Loss: 0.1034, Val Loss: 0.0800\n",
      "Epoch 8/300 - Train Loss: 0.1003, Val Loss: 0.0785\n",
      "Epoch 9/300 - Train Loss: 0.1003, Val Loss: 0.0804\n",
      "Epoch 10/300 - Train Loss: 0.1009, Val Loss: 0.0793\n",
      "Epoch 11/300 - Train Loss: 0.1020, Val Loss: 0.0784\n",
      "Epoch 12/300 - Train Loss: 0.1011, Val Loss: 0.0790\n",
      "Epoch 13/300 - Train Loss: 0.1011, Val Loss: 0.0924\n",
      "Epoch 14/300 - Train Loss: 0.0986, Val Loss: 0.0756\n",
      "Epoch 15/300 - Train Loss: 0.0986, Val Loss: 0.0767\n",
      "Epoch 16/300 - Train Loss: 0.1003, Val Loss: 0.0779\n",
      "Epoch 17/300 - Train Loss: 0.0998, Val Loss: 0.0836\n",
      "Epoch 18/300 - Train Loss: 0.0992, Val Loss: 0.0786\n",
      "Epoch 19/300 - Train Loss: 0.0995, Val Loss: 0.0754\n",
      "Epoch 20/300 - Train Loss: 0.1007, Val Loss: 0.0799\n",
      "Epoch 21/300 - Train Loss: 0.1023, Val Loss: 0.0753\n",
      "Epoch 22/300 - Train Loss: 0.1002, Val Loss: 0.0769\n",
      "Epoch 23/300 - Train Loss: 0.1008, Val Loss: 0.0735\n",
      "Epoch 24/300 - Train Loss: 0.1020, Val Loss: 0.0737\n",
      "Epoch 25/300 - Train Loss: 0.1034, Val Loss: 0.0789\n",
      "Epoch 26/300 - Train Loss: 0.1037, Val Loss: 0.0733\n",
      "Epoch 27/300 - Train Loss: 0.1034, Val Loss: 0.0751\n",
      "Epoch 28/300 - Train Loss: 0.1034, Val Loss: 0.0735\n",
      "Epoch 29/300 - Train Loss: 0.1040, Val Loss: 0.0739\n",
      "Epoch 30/300 - Train Loss: 0.1036, Val Loss: 0.0736\n",
      "Epoch 31/300 - Train Loss: 0.1022, Val Loss: 0.0753\n",
      "Epoch 32/300 - Train Loss: 0.1036, Val Loss: 0.0728\n",
      "Epoch 33/300 - Train Loss: 0.1038, Val Loss: 0.0722\n",
      "Epoch 34/300 - Train Loss: 0.1069, Val Loss: 0.0740\n",
      "Epoch 35/300 - Train Loss: 0.1065, Val Loss: 0.0743\n",
      "Epoch 36/300 - Train Loss: 0.1052, Val Loss: 0.0735\n",
      "Epoch 37/300 - Train Loss: 0.1060, Val Loss: 0.0748\n",
      "Epoch 38/300 - Train Loss: 0.1044, Val Loss: 0.0731\n",
      "Epoch 39/300 - Train Loss: 0.1061, Val Loss: 0.0803\n",
      "Epoch 40/300 - Train Loss: 0.1062, Val Loss: 0.0750\n",
      "Epoch 41/300 - Train Loss: 0.1032, Val Loss: 0.0769\n",
      "Epoch 42/300 - Train Loss: 0.1063, Val Loss: 0.0745\n",
      "Epoch 43/300 - Train Loss: 0.1107, Val Loss: 0.0791\n",
      "Epoch 44/300 - Train Loss: 0.1083, Val Loss: 0.0741\n",
      "Epoch 45/300 - Train Loss: 0.1059, Val Loss: 0.0789\n",
      "Epoch 46/300 - Train Loss: 0.1067, Val Loss: 0.0753\n",
      "Epoch 47/300 - Train Loss: 0.1075, Val Loss: 0.0900\n",
      "Epoch 48/300 - Train Loss: 0.1078, Val Loss: 0.0788\n",
      "Epoch 49/300 - Train Loss: 0.1106, Val Loss: 0.0771\n",
      "Epoch 50/300 - Train Loss: 0.1085, Val Loss: 0.0817\n",
      "Epoch 51/300 - Train Loss: 0.1081, Val Loss: 0.0796\n",
      "Epoch 52/300 - Train Loss: 0.1091, Val Loss: 0.0797\n",
      "Epoch 53/300 - Train Loss: 0.1096, Val Loss: 0.0773\n",
      "Epoch 54/300 - Train Loss: 0.1080, Val Loss: 0.0819\n",
      "Epoch 55/300 - Train Loss: 0.1086, Val Loss: 0.0764\n",
      "Epoch 56/300 - Train Loss: 0.1104, Val Loss: 0.0779\n",
      "Epoch 57/300 - Train Loss: 0.1086, Val Loss: 0.0778\n",
      "Epoch 58/300 - Train Loss: 0.1113, Val Loss: 0.0768\n",
      "Epoch 59/300 - Train Loss: 0.1103, Val Loss: 0.0839\n",
      "Epoch 60/300 - Train Loss: 0.1091, Val Loss: 0.0809\n",
      "Epoch 61/300 - Train Loss: 0.1129, Val Loss: 0.0784\n",
      "Epoch 62/300 - Train Loss: 0.1150, Val Loss: 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:31:12,874] Trial 132 finished with value: 0.9598428975723333 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.32322358157419584, 'learning_rate': 4.657517765090502e-05, 'batch_size': 32, 'weight_decay': 0.00997559205104477}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300 - Train Loss: 0.1104, Val Loss: 0.0740\n",
      "Early stopping at epoch 63\n",
      "Macro F1 Score: 0.9598, Macro Precision: 0.9568, Macro Recall: 0.9632\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.90      0.93      0.92        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 134\n",
      "Training with F1=16, F2=32, D=8, dropout=0.3475885476821202, LR=8.65339274400041e-05, BS=32, WD=0.00709188835836483\n",
      "Epoch 1/300 - Train Loss: 0.2944, Val Loss: 0.1403\n",
      "Epoch 2/300 - Train Loss: 0.1320, Val Loss: 0.1062\n",
      "Epoch 3/300 - Train Loss: 0.1115, Val Loss: 0.1086\n",
      "Epoch 4/300 - Train Loss: 0.1082, Val Loss: 0.0845\n",
      "Epoch 5/300 - Train Loss: 0.1052, Val Loss: 0.0965\n",
      "Epoch 6/300 - Train Loss: 0.1014, Val Loss: 0.0792\n",
      "Epoch 7/300 - Train Loss: 0.1004, Val Loss: 0.0731\n",
      "Epoch 8/300 - Train Loss: 0.1000, Val Loss: 0.0729\n",
      "Epoch 9/300 - Train Loss: 0.1001, Val Loss: 0.0735\n",
      "Epoch 10/300 - Train Loss: 0.1015, Val Loss: 0.0742\n",
      "Epoch 11/300 - Train Loss: 0.0975, Val Loss: 0.0731\n",
      "Epoch 12/300 - Train Loss: 0.0972, Val Loss: 0.0723\n",
      "Epoch 13/300 - Train Loss: 0.1004, Val Loss: 0.0717\n",
      "Epoch 14/300 - Train Loss: 0.1002, Val Loss: 0.0748\n",
      "Epoch 15/300 - Train Loss: 0.0998, Val Loss: 0.0764\n",
      "Epoch 16/300 - Train Loss: 0.1002, Val Loss: 0.0750\n",
      "Epoch 17/300 - Train Loss: 0.1008, Val Loss: 0.0729\n",
      "Epoch 18/300 - Train Loss: 0.1014, Val Loss: 0.0756\n",
      "Epoch 19/300 - Train Loss: 0.1027, Val Loss: 0.0814\n",
      "Epoch 20/300 - Train Loss: 0.1032, Val Loss: 0.0764\n",
      "Epoch 21/300 - Train Loss: 0.1034, Val Loss: 0.0738\n",
      "Epoch 22/300 - Train Loss: 0.1043, Val Loss: 0.0806\n",
      "Epoch 23/300 - Train Loss: 0.1041, Val Loss: 0.0751\n",
      "Epoch 24/300 - Train Loss: 0.1028, Val Loss: 0.0765\n",
      "Epoch 25/300 - Train Loss: 0.1046, Val Loss: 0.0731\n",
      "Epoch 26/300 - Train Loss: 0.1032, Val Loss: 0.0776\n",
      "Epoch 27/300 - Train Loss: 0.1035, Val Loss: 0.0739\n",
      "Epoch 28/300 - Train Loss: 0.1071, Val Loss: 0.0790\n",
      "Epoch 29/300 - Train Loss: 0.1060, Val Loss: 0.0762\n",
      "Epoch 30/300 - Train Loss: 0.1063, Val Loss: 0.0788\n",
      "Epoch 31/300 - Train Loss: 0.1062, Val Loss: 0.0814\n",
      "Epoch 32/300 - Train Loss: 0.1059, Val Loss: 0.0770\n",
      "Epoch 33/300 - Train Loss: 0.1075, Val Loss: 0.0836\n",
      "Epoch 34/300 - Train Loss: 0.1076, Val Loss: 0.0755\n",
      "Epoch 35/300 - Train Loss: 0.1043, Val Loss: 0.0823\n",
      "Epoch 36/300 - Train Loss: 0.1069, Val Loss: 0.0733\n",
      "Epoch 37/300 - Train Loss: 0.1079, Val Loss: 0.0809\n",
      "Epoch 38/300 - Train Loss: 0.1074, Val Loss: 0.0845\n",
      "Epoch 39/300 - Train Loss: 0.1106, Val Loss: 0.0795\n",
      "Epoch 40/300 - Train Loss: 0.1070, Val Loss: 0.0737\n",
      "Epoch 41/300 - Train Loss: 0.1074, Val Loss: 0.0796\n",
      "Epoch 42/300 - Train Loss: 0.1089, Val Loss: 0.0755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:33:58,196] Trial 133 finished with value: 0.9657163061988184 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.3475885476821202, 'learning_rate': 8.65339274400041e-05, 'batch_size': 32, 'weight_decay': 0.00709188835836483}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/300 - Train Loss: 0.1120, Val Loss: 0.0766\n",
      "Early stopping at epoch 43\n",
      "Macro F1 Score: 0.9657, Macro Precision: 0.9625, Macro Recall: 0.9693\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 135\n",
      "Training with F1=16, F2=32, D=8, dropout=0.22869597528768817, LR=7.123251992397339e-05, BS=32, WD=0.005733319436303367\n",
      "Epoch 1/300 - Train Loss: 0.2964, Val Loss: 0.1391\n",
      "Epoch 2/300 - Train Loss: 0.1313, Val Loss: 0.0992\n",
      "Epoch 3/300 - Train Loss: 0.1076, Val Loss: 0.0892\n",
      "Epoch 4/300 - Train Loss: 0.1027, Val Loss: 0.0796\n",
      "Epoch 5/300 - Train Loss: 0.0964, Val Loss: 0.0789\n",
      "Epoch 6/300 - Train Loss: 0.0948, Val Loss: 0.0853\n",
      "Epoch 7/300 - Train Loss: 0.0954, Val Loss: 0.0856\n",
      "Epoch 8/300 - Train Loss: 0.0941, Val Loss: 0.0723\n",
      "Epoch 9/300 - Train Loss: 0.0963, Val Loss: 0.0760\n",
      "Epoch 10/300 - Train Loss: 0.0946, Val Loss: 0.0797\n",
      "Epoch 11/300 - Train Loss: 0.0949, Val Loss: 0.0750\n",
      "Epoch 12/300 - Train Loss: 0.0936, Val Loss: 0.0799\n",
      "Epoch 13/300 - Train Loss: 0.0918, Val Loss: 0.0779\n",
      "Epoch 14/300 - Train Loss: 0.0936, Val Loss: 0.0783\n",
      "Epoch 15/300 - Train Loss: 0.0932, Val Loss: 0.0751\n",
      "Epoch 16/300 - Train Loss: 0.0930, Val Loss: 0.0805\n",
      "Epoch 17/300 - Train Loss: 0.0939, Val Loss: 0.0744\n",
      "Epoch 18/300 - Train Loss: 0.0930, Val Loss: 0.0722\n",
      "Epoch 19/300 - Train Loss: 0.0937, Val Loss: 0.0778\n",
      "Epoch 20/300 - Train Loss: 0.0942, Val Loss: 0.0728\n",
      "Epoch 21/300 - Train Loss: 0.0962, Val Loss: 0.0751\n",
      "Epoch 22/300 - Train Loss: 0.0949, Val Loss: 0.0756\n",
      "Epoch 23/300 - Train Loss: 0.0940, Val Loss: 0.0811\n",
      "Epoch 24/300 - Train Loss: 0.0957, Val Loss: 0.0750\n",
      "Epoch 25/300 - Train Loss: 0.0953, Val Loss: 0.0750\n",
      "Epoch 26/300 - Train Loss: 0.0974, Val Loss: 0.0755\n",
      "Epoch 27/300 - Train Loss: 0.0961, Val Loss: 0.0744\n",
      "Epoch 28/300 - Train Loss: 0.0936, Val Loss: 0.0830\n",
      "Epoch 29/300 - Train Loss: 0.0967, Val Loss: 0.0824\n",
      "Epoch 30/300 - Train Loss: 0.0974, Val Loss: 0.0775\n",
      "Epoch 31/300 - Train Loss: 0.0966, Val Loss: 0.0744\n",
      "Epoch 32/300 - Train Loss: 0.0969, Val Loss: 0.0738\n",
      "Epoch 33/300 - Train Loss: 0.0948, Val Loss: 0.0723\n",
      "Epoch 34/300 - Train Loss: 0.0972, Val Loss: 0.0801\n",
      "Epoch 35/300 - Train Loss: 0.0973, Val Loss: 0.0779\n",
      "Epoch 36/300 - Train Loss: 0.0973, Val Loss: 0.0733\n",
      "Epoch 37/300 - Train Loss: 0.0972, Val Loss: 0.0748\n",
      "Epoch 38/300 - Train Loss: 0.0993, Val Loss: 0.0749\n",
      "Epoch 39/300 - Train Loss: 0.0975, Val Loss: 0.0752\n",
      "Epoch 40/300 - Train Loss: 0.0968, Val Loss: 0.0752\n",
      "Epoch 41/300 - Train Loss: 0.1002, Val Loss: 0.0733\n",
      "Epoch 42/300 - Train Loss: 0.0987, Val Loss: 0.0771\n",
      "Epoch 43/300 - Train Loss: 0.0990, Val Loss: 0.0718\n",
      "Epoch 44/300 - Train Loss: 0.0993, Val Loss: 0.0798\n",
      "Epoch 45/300 - Train Loss: 0.1024, Val Loss: 0.0896\n",
      "Epoch 46/300 - Train Loss: 0.1000, Val Loss: 0.0830\n",
      "Epoch 47/300 - Train Loss: 0.1003, Val Loss: 0.0739\n",
      "Epoch 48/300 - Train Loss: 0.0977, Val Loss: 0.0772\n",
      "Epoch 49/300 - Train Loss: 0.0974, Val Loss: 0.0748\n",
      "Epoch 50/300 - Train Loss: 0.0981, Val Loss: 0.0813\n",
      "Epoch 51/300 - Train Loss: 0.0983, Val Loss: 0.0749\n",
      "Epoch 52/300 - Train Loss: 0.1008, Val Loss: 0.0734\n",
      "Epoch 53/300 - Train Loss: 0.0977, Val Loss: 0.0888\n",
      "Epoch 54/300 - Train Loss: 0.0980, Val Loss: 0.0737\n",
      "Epoch 55/300 - Train Loss: 0.0994, Val Loss: 0.0751\n",
      "Epoch 56/300 - Train Loss: 0.1016, Val Loss: 0.0851\n",
      "Epoch 57/300 - Train Loss: 0.0981, Val Loss: 0.0732\n",
      "Epoch 58/300 - Train Loss: 0.1021, Val Loss: 0.0738\n",
      "Epoch 59/300 - Train Loss: 0.0996, Val Loss: 0.0850\n",
      "Epoch 60/300 - Train Loss: 0.0984, Val Loss: 0.0756\n",
      "Epoch 61/300 - Train Loss: 0.0998, Val Loss: 0.0730\n",
      "Epoch 62/300 - Train Loss: 0.1025, Val Loss: 0.0721\n",
      "Epoch 63/300 - Train Loss: 0.1006, Val Loss: 0.0720\n",
      "Epoch 64/300 - Train Loss: 0.1012, Val Loss: 0.0727\n",
      "Epoch 65/300 - Train Loss: 0.1014, Val Loss: 0.0778\n",
      "Epoch 66/300 - Train Loss: 0.1005, Val Loss: 0.0744\n",
      "Epoch 67/300 - Train Loss: 0.0990, Val Loss: 0.0852\n",
      "Epoch 68/300 - Train Loss: 0.1013, Val Loss: 0.0775\n",
      "Epoch 69/300 - Train Loss: 0.1012, Val Loss: 0.0784\n",
      "Epoch 70/300 - Train Loss: 0.0989, Val Loss: 0.0775\n",
      "Epoch 71/300 - Train Loss: 0.1024, Val Loss: 0.0745\n",
      "Epoch 72/300 - Train Loss: 0.0994, Val Loss: 0.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:38:38,881] Trial 134 finished with value: 0.961251772379328 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.22869597528768817, 'learning_rate': 7.123251992397339e-05, 'batch_size': 32, 'weight_decay': 0.005733319436303367}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/300 - Train Loss: 0.0999, Val Loss: 0.0736\n",
      "Early stopping at epoch 73\n",
      "Macro F1 Score: 0.9613, Macro Precision: 0.9541, Macro Recall: 0.9693\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 136\n",
      "Training with F1=16, F2=32, D=8, dropout=0.3760505274561682, LR=0.00010269469311323069, BS=32, WD=0.008505431749479283\n",
      "Epoch 1/300 - Train Loss: 0.2690, Val Loss: 0.1010\n",
      "Epoch 2/300 - Train Loss: 0.1193, Val Loss: 0.0899\n",
      "Epoch 3/300 - Train Loss: 0.1093, Val Loss: 0.0919\n",
      "Epoch 4/300 - Train Loss: 0.1038, Val Loss: 0.0808\n",
      "Epoch 5/300 - Train Loss: 0.1021, Val Loss: 0.0732\n",
      "Epoch 6/300 - Train Loss: 0.1017, Val Loss: 0.0756\n",
      "Epoch 7/300 - Train Loss: 0.1018, Val Loss: 0.0771\n",
      "Epoch 8/300 - Train Loss: 0.1049, Val Loss: 0.0812\n",
      "Epoch 9/300 - Train Loss: 0.1011, Val Loss: 0.0740\n",
      "Epoch 10/300 - Train Loss: 0.1045, Val Loss: 0.0783\n",
      "Epoch 11/300 - Train Loss: 0.1031, Val Loss: 0.0769\n",
      "Epoch 12/300 - Train Loss: 0.1052, Val Loss: 0.0753\n",
      "Epoch 13/300 - Train Loss: 0.1058, Val Loss: 0.0806\n",
      "Epoch 14/300 - Train Loss: 0.1036, Val Loss: 0.0745\n",
      "Epoch 15/300 - Train Loss: 0.1063, Val Loss: 0.0742\n",
      "Epoch 16/300 - Train Loss: 0.1053, Val Loss: 0.0748\n",
      "Epoch 17/300 - Train Loss: 0.1069, Val Loss: 0.0737\n",
      "Epoch 18/300 - Train Loss: 0.1097, Val Loss: 0.0815\n",
      "Epoch 19/300 - Train Loss: 0.1071, Val Loss: 0.0867\n",
      "Epoch 20/300 - Train Loss: 0.1054, Val Loss: 0.0770\n",
      "Epoch 21/300 - Train Loss: 0.1097, Val Loss: 0.0880\n",
      "Epoch 22/300 - Train Loss: 0.1108, Val Loss: 0.0734\n",
      "Epoch 23/300 - Train Loss: 0.1100, Val Loss: 0.0760\n",
      "Epoch 24/300 - Train Loss: 0.1091, Val Loss: 0.0777\n",
      "Epoch 25/300 - Train Loss: 0.1084, Val Loss: 0.0783\n",
      "Epoch 26/300 - Train Loss: 0.1069, Val Loss: 0.0920\n",
      "Epoch 27/300 - Train Loss: 0.1109, Val Loss: 0.0846\n",
      "Epoch 28/300 - Train Loss: 0.1110, Val Loss: 0.0849\n",
      "Epoch 29/300 - Train Loss: 0.1134, Val Loss: 0.0830\n",
      "Epoch 30/300 - Train Loss: 0.1128, Val Loss: 0.0798\n",
      "Epoch 31/300 - Train Loss: 0.1101, Val Loss: 0.0830\n",
      "Epoch 32/300 - Train Loss: 0.1125, Val Loss: 0.0769\n",
      "Epoch 33/300 - Train Loss: 0.1106, Val Loss: 0.0931\n",
      "Epoch 34/300 - Train Loss: 0.1085, Val Loss: 0.0757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:40:53,634] Trial 135 finished with value: 0.9566863716930819 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.3760505274561682, 'learning_rate': 0.00010269469311323069, 'batch_size': 32, 'weight_decay': 0.008505431749479283}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/300 - Train Loss: 0.1144, Val Loss: 0.0862\n",
      "Early stopping at epoch 35\n",
      "Macro F1 Score: 0.9567, Macro Precision: 0.9515, Macro Recall: 0.9624\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.89      0.93      0.91        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 137\n",
      "Training with F1=16, F2=32, D=8, dropout=0.20228520594359906, LR=7.666705109793598e-05, BS=32, WD=0.004320867200154299\n",
      "Epoch 1/300 - Train Loss: 0.2974, Val Loss: 0.1508\n",
      "Epoch 2/300 - Train Loss: 0.1225, Val Loss: 0.1124\n",
      "Epoch 3/300 - Train Loss: 0.1036, Val Loss: 0.0889\n",
      "Epoch 4/300 - Train Loss: 0.0978, Val Loss: 0.0847\n",
      "Epoch 5/300 - Train Loss: 0.0980, Val Loss: 0.0808\n",
      "Epoch 6/300 - Train Loss: 0.0925, Val Loss: 0.0804\n",
      "Epoch 7/300 - Train Loss: 0.0917, Val Loss: 0.0703\n",
      "Epoch 8/300 - Train Loss: 0.0914, Val Loss: 0.0719\n",
      "Epoch 9/300 - Train Loss: 0.0895, Val Loss: 0.0721\n",
      "Epoch 10/300 - Train Loss: 0.0883, Val Loss: 0.0810\n",
      "Epoch 11/300 - Train Loss: 0.0878, Val Loss: 0.0701\n",
      "Epoch 12/300 - Train Loss: 0.0916, Val Loss: 0.0776\n",
      "Epoch 13/300 - Train Loss: 0.0876, Val Loss: 0.0756\n",
      "Epoch 14/300 - Train Loss: 0.0874, Val Loss: 0.0687\n",
      "Epoch 15/300 - Train Loss: 0.0900, Val Loss: 0.0793\n",
      "Epoch 16/300 - Train Loss: 0.0895, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.0870, Val Loss: 0.0710\n",
      "Epoch 18/300 - Train Loss: 0.0905, Val Loss: 0.0719\n",
      "Epoch 19/300 - Train Loss: 0.0892, Val Loss: 0.0723\n",
      "Epoch 20/300 - Train Loss: 0.0908, Val Loss: 0.0688\n",
      "Epoch 21/300 - Train Loss: 0.0906, Val Loss: 0.0707\n",
      "Epoch 22/300 - Train Loss: 0.0905, Val Loss: 0.0732\n",
      "Epoch 23/300 - Train Loss: 0.0885, Val Loss: 0.0737\n",
      "Epoch 24/300 - Train Loss: 0.0880, Val Loss: 0.0713\n",
      "Epoch 25/300 - Train Loss: 0.0899, Val Loss: 0.0685\n",
      "Epoch 26/300 - Train Loss: 0.0927, Val Loss: 0.0667\n",
      "Epoch 27/300 - Train Loss: 0.0890, Val Loss: 0.0747\n",
      "Epoch 28/300 - Train Loss: 0.0882, Val Loss: 0.0709\n",
      "Epoch 29/300 - Train Loss: 0.0888, Val Loss: 0.0722\n",
      "Epoch 30/300 - Train Loss: 0.0893, Val Loss: 0.0679\n",
      "Epoch 31/300 - Train Loss: 0.0900, Val Loss: 0.0682\n",
      "Epoch 32/300 - Train Loss: 0.0932, Val Loss: 0.0756\n",
      "Epoch 33/300 - Train Loss: 0.0909, Val Loss: 0.0713\n",
      "Epoch 34/300 - Train Loss: 0.0903, Val Loss: 0.0712\n",
      "Epoch 35/300 - Train Loss: 0.0907, Val Loss: 0.0709\n",
      "Epoch 36/300 - Train Loss: 0.0909, Val Loss: 0.0716\n",
      "Epoch 37/300 - Train Loss: 0.0917, Val Loss: 0.0729\n",
      "Epoch 38/300 - Train Loss: 0.0937, Val Loss: 0.0706\n",
      "Epoch 39/300 - Train Loss: 0.0906, Val Loss: 0.0729\n",
      "Epoch 40/300 - Train Loss: 0.0917, Val Loss: 0.0761\n",
      "Epoch 41/300 - Train Loss: 0.0903, Val Loss: 0.0702\n",
      "Epoch 42/300 - Train Loss: 0.0941, Val Loss: 0.0738\n",
      "Epoch 43/300 - Train Loss: 0.0911, Val Loss: 0.0760\n",
      "Epoch 44/300 - Train Loss: 0.0918, Val Loss: 0.0750\n",
      "Epoch 45/300 - Train Loss: 0.0911, Val Loss: 0.0716\n",
      "Epoch 46/300 - Train Loss: 0.0907, Val Loss: 0.0688\n",
      "Epoch 47/300 - Train Loss: 0.0939, Val Loss: 0.0740\n",
      "Epoch 48/300 - Train Loss: 0.0920, Val Loss: 0.0710\n",
      "Epoch 49/300 - Train Loss: 0.0957, Val Loss: 0.0771\n",
      "Epoch 50/300 - Train Loss: 0.0921, Val Loss: 0.0692\n",
      "Epoch 51/300 - Train Loss: 0.0913, Val Loss: 0.0765\n",
      "Epoch 52/300 - Train Loss: 0.0946, Val Loss: 0.0764\n",
      "Epoch 53/300 - Train Loss: 0.0929, Val Loss: 0.0800\n",
      "Epoch 54/300 - Train Loss: 0.0936, Val Loss: 0.0732\n",
      "Epoch 55/300 - Train Loss: 0.0949, Val Loss: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:44:29,247] Trial 136 finished with value: 0.9718282267249952 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.20228520594359906, 'learning_rate': 7.666705109793598e-05, 'batch_size': 32, 'weight_decay': 0.004320867200154299}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300 - Train Loss: 0.0975, Val Loss: 0.0781\n",
      "Early stopping at epoch 56\n",
      "Macro F1 Score: 0.9718, Macro Precision: 0.9730, Macro Recall: 0.9708\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 138\n",
      "Training with F1=8, F2=32, D=4, dropout=0.2009067008725446, LR=6.662259303129071e-05, BS=32, WD=0.004236325488315733\n",
      "Epoch 1/300 - Train Loss: 0.3523, Val Loss: 0.1878\n",
      "Epoch 2/300 - Train Loss: 0.1815, Val Loss: 0.1263\n",
      "Epoch 3/300 - Train Loss: 0.1347, Val Loss: 0.1005\n",
      "Epoch 4/300 - Train Loss: 0.1152, Val Loss: 0.0915\n",
      "Epoch 5/300 - Train Loss: 0.1087, Val Loss: 0.0857\n",
      "Epoch 6/300 - Train Loss: 0.1090, Val Loss: 0.0897\n",
      "Epoch 7/300 - Train Loss: 0.1018, Val Loss: 0.0827\n",
      "Epoch 8/300 - Train Loss: 0.1039, Val Loss: 0.0801\n",
      "Epoch 9/300 - Train Loss: 0.1002, Val Loss: 0.0796\n",
      "Epoch 10/300 - Train Loss: 0.1026, Val Loss: 0.0764\n",
      "Epoch 11/300 - Train Loss: 0.0996, Val Loss: 0.0780\n",
      "Epoch 12/300 - Train Loss: 0.0965, Val Loss: 0.0743\n",
      "Epoch 13/300 - Train Loss: 0.0990, Val Loss: 0.0751\n",
      "Epoch 14/300 - Train Loss: 0.0959, Val Loss: 0.0760\n",
      "Epoch 15/300 - Train Loss: 0.0947, Val Loss: 0.0763\n",
      "Epoch 16/300 - Train Loss: 0.0951, Val Loss: 0.0732\n",
      "Epoch 17/300 - Train Loss: 0.0960, Val Loss: 0.0744\n",
      "Epoch 18/300 - Train Loss: 0.0973, Val Loss: 0.0784\n",
      "Epoch 19/300 - Train Loss: 0.0949, Val Loss: 0.0711\n",
      "Epoch 20/300 - Train Loss: 0.0956, Val Loss: 0.0726\n",
      "Epoch 21/300 - Train Loss: 0.0958, Val Loss: 0.0754\n",
      "Epoch 22/300 - Train Loss: 0.0914, Val Loss: 0.0722\n",
      "Epoch 23/300 - Train Loss: 0.0924, Val Loss: 0.0781\n",
      "Epoch 24/300 - Train Loss: 0.0920, Val Loss: 0.0715\n",
      "Epoch 25/300 - Train Loss: 0.0935, Val Loss: 0.0721\n",
      "Epoch 26/300 - Train Loss: 0.0938, Val Loss: 0.0722\n",
      "Epoch 27/300 - Train Loss: 0.0941, Val Loss: 0.0710\n",
      "Epoch 28/300 - Train Loss: 0.0922, Val Loss: 0.0707\n",
      "Epoch 29/300 - Train Loss: 0.0931, Val Loss: 0.0730\n",
      "Epoch 30/300 - Train Loss: 0.0908, Val Loss: 0.0752\n",
      "Epoch 31/300 - Train Loss: 0.0903, Val Loss: 0.0713\n",
      "Epoch 32/300 - Train Loss: 0.0925, Val Loss: 0.0695\n",
      "Epoch 33/300 - Train Loss: 0.0943, Val Loss: 0.0718\n",
      "Epoch 34/300 - Train Loss: 0.0906, Val Loss: 0.0731\n",
      "Epoch 35/300 - Train Loss: 0.0956, Val Loss: 0.0704\n",
      "Epoch 36/300 - Train Loss: 0.0913, Val Loss: 0.0753\n",
      "Epoch 37/300 - Train Loss: 0.0935, Val Loss: 0.0720\n",
      "Epoch 38/300 - Train Loss: 0.0926, Val Loss: 0.0701\n",
      "Epoch 39/300 - Train Loss: 0.0926, Val Loss: 0.0722\n",
      "Epoch 40/300 - Train Loss: 0.0923, Val Loss: 0.0717\n",
      "Epoch 41/300 - Train Loss: 0.0930, Val Loss: 0.0707\n",
      "Epoch 42/300 - Train Loss: 0.0900, Val Loss: 0.0729\n",
      "Epoch 43/300 - Train Loss: 0.0942, Val Loss: 0.0746\n",
      "Epoch 44/300 - Train Loss: 0.0911, Val Loss: 0.0717\n",
      "Epoch 45/300 - Train Loss: 0.0925, Val Loss: 0.0764\n",
      "Epoch 46/300 - Train Loss: 0.0922, Val Loss: 0.0718\n",
      "Epoch 47/300 - Train Loss: 0.0932, Val Loss: 0.0697\n",
      "Epoch 48/300 - Train Loss: 0.0922, Val Loss: 0.0707\n",
      "Epoch 49/300 - Train Loss: 0.0960, Val Loss: 0.0756\n",
      "Epoch 50/300 - Train Loss: 0.0942, Val Loss: 0.0694\n",
      "Epoch 51/300 - Train Loss: 0.0955, Val Loss: 0.0715\n",
      "Epoch 52/300 - Train Loss: 0.0953, Val Loss: 0.0714\n",
      "Epoch 53/300 - Train Loss: 0.0922, Val Loss: 0.0692\n",
      "Epoch 54/300 - Train Loss: 0.0953, Val Loss: 0.0705\n",
      "Epoch 55/300 - Train Loss: 0.0958, Val Loss: 0.0759\n",
      "Epoch 56/300 - Train Loss: 0.0910, Val Loss: 0.0735\n",
      "Epoch 57/300 - Train Loss: 0.0925, Val Loss: 0.0712\n",
      "Epoch 58/300 - Train Loss: 0.0960, Val Loss: 0.0766\n",
      "Epoch 59/300 - Train Loss: 0.0924, Val Loss: 0.0738\n",
      "Epoch 60/300 - Train Loss: 0.0930, Val Loss: 0.0730\n",
      "Epoch 61/300 - Train Loss: 0.0955, Val Loss: 0.0714\n",
      "Epoch 62/300 - Train Loss: 0.0948, Val Loss: 0.0704\n",
      "Epoch 63/300 - Train Loss: 0.0955, Val Loss: 0.0749\n",
      "Epoch 64/300 - Train Loss: 0.0950, Val Loss: 0.0691\n",
      "Epoch 65/300 - Train Loss: 0.0980, Val Loss: 0.0731\n",
      "Epoch 66/300 - Train Loss: 0.0927, Val Loss: 0.0775\n",
      "Epoch 67/300 - Train Loss: 0.0931, Val Loss: 0.0717\n",
      "Epoch 68/300 - Train Loss: 0.0940, Val Loss: 0.0736\n",
      "Epoch 69/300 - Train Loss: 0.0937, Val Loss: 0.0720\n",
      "Epoch 70/300 - Train Loss: 0.0961, Val Loss: 0.0777\n",
      "Epoch 71/300 - Train Loss: 0.0960, Val Loss: 0.0687\n",
      "Epoch 72/300 - Train Loss: 0.0934, Val Loss: 0.0700\n",
      "Epoch 73/300 - Train Loss: 0.0946, Val Loss: 0.0742\n",
      "Epoch 74/300 - Train Loss: 0.0928, Val Loss: 0.0701\n",
      "Epoch 75/300 - Train Loss: 0.0966, Val Loss: 0.0730\n",
      "Epoch 76/300 - Train Loss: 0.0933, Val Loss: 0.0743\n",
      "Epoch 77/300 - Train Loss: 0.0945, Val Loss: 0.0697\n",
      "Epoch 78/300 - Train Loss: 0.0929, Val Loss: 0.0726\n",
      "Epoch 79/300 - Train Loss: 0.0961, Val Loss: 0.0752\n",
      "Epoch 80/300 - Train Loss: 0.0955, Val Loss: 0.0714\n",
      "Epoch 81/300 - Train Loss: 0.0927, Val Loss: 0.0760\n",
      "Epoch 82/300 - Train Loss: 0.0943, Val Loss: 0.0798\n",
      "Epoch 83/300 - Train Loss: 0.0982, Val Loss: 0.0781\n",
      "Epoch 84/300 - Train Loss: 0.0947, Val Loss: 0.0717\n",
      "Epoch 85/300 - Train Loss: 0.0962, Val Loss: 0.0739\n",
      "Epoch 86/300 - Train Loss: 0.0966, Val Loss: 0.0735\n",
      "Epoch 87/300 - Train Loss: 0.0970, Val Loss: 0.0752\n",
      "Epoch 88/300 - Train Loss: 0.0984, Val Loss: 0.0723\n",
      "Epoch 89/300 - Train Loss: 0.0954, Val Loss: 0.0695\n",
      "Epoch 90/300 - Train Loss: 0.0980, Val Loss: 0.0741\n",
      "Epoch 91/300 - Train Loss: 0.0951, Val Loss: 0.0762\n",
      "Epoch 92/300 - Train Loss: 0.0976, Val Loss: 0.0728\n",
      "Epoch 93/300 - Train Loss: 0.0949, Val Loss: 0.0742\n",
      "Epoch 94/300 - Train Loss: 0.0949, Val Loss: 0.0795\n",
      "Epoch 95/300 - Train Loss: 0.0948, Val Loss: 0.0819\n",
      "Epoch 96/300 - Train Loss: 0.0959, Val Loss: 0.0698\n",
      "Epoch 97/300 - Train Loss: 0.0949, Val Loss: 0.0730\n",
      "Epoch 98/300 - Train Loss: 0.0920, Val Loss: 0.0722\n",
      "Epoch 99/300 - Train Loss: 0.0960, Val Loss: 0.0794\n",
      "Epoch 100/300 - Train Loss: 0.0951, Val Loss: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:47:57,765] Trial 137 finished with value: 0.965794370252143 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.2009067008725446, 'learning_rate': 6.662259303129071e-05, 'batch_size': 32, 'weight_decay': 0.004236325488315733}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/300 - Train Loss: 0.0945, Val Loss: 0.0709\n",
      "Early stopping at epoch 101\n",
      "Macro F1 Score: 0.9658, Macro Precision: 0.9625, Macro Recall: 0.9694\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 139\n",
      "Training with F1=16, F2=32, D=8, dropout=0.6877548412433722, LR=5.076656825997491e-05, BS=32, WD=0.0035554358068192377\n",
      "Epoch 1/300 - Train Loss: 0.4954, Val Loss: 0.2919\n",
      "Epoch 2/300 - Train Loss: 0.2480, Val Loss: 0.2037\n",
      "Epoch 3/300 - Train Loss: 0.1960, Val Loss: 0.1404\n",
      "Epoch 4/300 - Train Loss: 0.1550, Val Loss: 0.1222\n",
      "Epoch 5/300 - Train Loss: 0.1367, Val Loss: 0.1161\n",
      "Epoch 6/300 - Train Loss: 0.1248, Val Loss: 0.0930\n",
      "Epoch 7/300 - Train Loss: 0.1224, Val Loss: 0.0940\n",
      "Epoch 8/300 - Train Loss: 0.1209, Val Loss: 0.1043\n",
      "Epoch 9/300 - Train Loss: 0.1166, Val Loss: 0.0818\n",
      "Epoch 10/300 - Train Loss: 0.1171, Val Loss: 0.0828\n",
      "Epoch 11/300 - Train Loss: 0.1139, Val Loss: 0.0842\n",
      "Epoch 12/300 - Train Loss: 0.1117, Val Loss: 0.0851\n",
      "Epoch 13/300 - Train Loss: 0.1099, Val Loss: 0.0830\n",
      "Epoch 14/300 - Train Loss: 0.1103, Val Loss: 0.0815\n",
      "Epoch 15/300 - Train Loss: 0.1089, Val Loss: 0.0793\n",
      "Epoch 16/300 - Train Loss: 0.1079, Val Loss: 0.0774\n",
      "Epoch 17/300 - Train Loss: 0.1088, Val Loss: 0.0779\n",
      "Epoch 18/300 - Train Loss: 0.1067, Val Loss: 0.0766\n",
      "Epoch 19/300 - Train Loss: 0.1086, Val Loss: 0.0743\n",
      "Epoch 20/300 - Train Loss: 0.1055, Val Loss: 0.0752\n",
      "Epoch 21/300 - Train Loss: 0.1086, Val Loss: 0.0801\n",
      "Epoch 22/300 - Train Loss: 0.1028, Val Loss: 0.0723\n",
      "Epoch 23/300 - Train Loss: 0.1084, Val Loss: 0.0755\n",
      "Epoch 24/300 - Train Loss: 0.1085, Val Loss: 0.0750\n",
      "Epoch 25/300 - Train Loss: 0.1059, Val Loss: 0.0804\n",
      "Epoch 26/300 - Train Loss: 0.1063, Val Loss: 0.0748\n",
      "Epoch 27/300 - Train Loss: 0.1076, Val Loss: 0.0753\n",
      "Epoch 28/300 - Train Loss: 0.1075, Val Loss: 0.0739\n",
      "Epoch 29/300 - Train Loss: 0.1045, Val Loss: 0.0742\n",
      "Epoch 30/300 - Train Loss: 0.1063, Val Loss: 0.0724\n",
      "Epoch 31/300 - Train Loss: 0.1041, Val Loss: 0.0729\n",
      "Epoch 32/300 - Train Loss: 0.1070, Val Loss: 0.0721\n",
      "Epoch 33/300 - Train Loss: 0.1040, Val Loss: 0.0729\n",
      "Epoch 34/300 - Train Loss: 0.1101, Val Loss: 0.0727\n",
      "Epoch 35/300 - Train Loss: 0.1063, Val Loss: 0.0833\n",
      "Epoch 36/300 - Train Loss: 0.1105, Val Loss: 0.0728\n",
      "Epoch 37/300 - Train Loss: 0.1076, Val Loss: 0.0726\n",
      "Epoch 38/300 - Train Loss: 0.1074, Val Loss: 0.0726\n",
      "Epoch 39/300 - Train Loss: 0.1096, Val Loss: 0.0707\n",
      "Epoch 40/300 - Train Loss: 0.1103, Val Loss: 0.0736\n",
      "Epoch 41/300 - Train Loss: 0.1081, Val Loss: 0.0746\n",
      "Epoch 42/300 - Train Loss: 0.1110, Val Loss: 0.0795\n",
      "Epoch 43/300 - Train Loss: 0.1082, Val Loss: 0.0761\n",
      "Epoch 44/300 - Train Loss: 0.1071, Val Loss: 0.0713\n",
      "Epoch 45/300 - Train Loss: 0.1093, Val Loss: 0.0720\n",
      "Epoch 46/300 - Train Loss: 0.1095, Val Loss: 0.0733\n",
      "Epoch 47/300 - Train Loss: 0.1089, Val Loss: 0.0746\n",
      "Epoch 48/300 - Train Loss: 0.1105, Val Loss: 0.0773\n",
      "Epoch 49/300 - Train Loss: 0.1071, Val Loss: 0.0737\n",
      "Epoch 50/300 - Train Loss: 0.1090, Val Loss: 0.0742\n",
      "Epoch 51/300 - Train Loss: 0.1090, Val Loss: 0.0734\n",
      "Epoch 52/300 - Train Loss: 0.1094, Val Loss: 0.0729\n",
      "Epoch 53/300 - Train Loss: 0.1112, Val Loss: 0.0715\n",
      "Epoch 54/300 - Train Loss: 0.1089, Val Loss: 0.0760\n",
      "Epoch 55/300 - Train Loss: 0.1112, Val Loss: 0.0739\n",
      "Epoch 56/300 - Train Loss: 0.1099, Val Loss: 0.0740\n",
      "Epoch 57/300 - Train Loss: 0.1100, Val Loss: 0.0748\n",
      "Epoch 58/300 - Train Loss: 0.1135, Val Loss: 0.0723\n",
      "Epoch 59/300 - Train Loss: 0.1135, Val Loss: 0.0721\n",
      "Epoch 60/300 - Train Loss: 0.1102, Val Loss: 0.0726\n",
      "Epoch 61/300 - Train Loss: 0.1115, Val Loss: 0.0722\n",
      "Epoch 62/300 - Train Loss: 0.1091, Val Loss: 0.0791\n",
      "Epoch 63/300 - Train Loss: 0.1129, Val Loss: 0.0751\n",
      "Epoch 64/300 - Train Loss: 0.1103, Val Loss: 0.0779\n",
      "Epoch 65/300 - Train Loss: 0.1121, Val Loss: 0.0726\n",
      "Epoch 66/300 - Train Loss: 0.1107, Val Loss: 0.0731\n",
      "Epoch 67/300 - Train Loss: 0.1124, Val Loss: 0.0734\n",
      "Epoch 68/300 - Train Loss: 0.1121, Val Loss: 0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:52:23,377] Trial 138 finished with value: 0.9690300164690574 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.6877548412433722, 'learning_rate': 5.076656825997491e-05, 'batch_size': 32, 'weight_decay': 0.0035554358068192377}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.1124, Val Loss: 0.0720\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9690, Macro Precision: 0.9682, Macro Recall: 0.9701\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       1.00      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 140\n",
      "Training with F1=16, F2=32, D=8, dropout=0.33318608980345316, LR=7.97994743120719e-05, BS=32, WD=0.0065188318296767285\n",
      "Epoch 1/300 - Train Loss: 0.2636, Val Loss: 0.1266\n",
      "Epoch 2/300 - Train Loss: 0.1240, Val Loss: 0.0889\n",
      "Epoch 3/300 - Train Loss: 0.1140, Val Loss: 0.0842\n",
      "Epoch 4/300 - Train Loss: 0.1072, Val Loss: 0.0828\n",
      "Epoch 5/300 - Train Loss: 0.1006, Val Loss: 0.0804\n",
      "Epoch 6/300 - Train Loss: 0.1014, Val Loss: 0.0750\n",
      "Epoch 7/300 - Train Loss: 0.0985, Val Loss: 0.0816\n",
      "Epoch 8/300 - Train Loss: 0.0975, Val Loss: 0.0821\n",
      "Epoch 9/300 - Train Loss: 0.0984, Val Loss: 0.0790\n",
      "Epoch 10/300 - Train Loss: 0.0945, Val Loss: 0.0757\n",
      "Epoch 11/300 - Train Loss: 0.0971, Val Loss: 0.0766\n",
      "Epoch 12/300 - Train Loss: 0.0975, Val Loss: 0.0754\n",
      "Epoch 13/300 - Train Loss: 0.0998, Val Loss: 0.0744\n",
      "Epoch 14/300 - Train Loss: 0.0993, Val Loss: 0.0734\n",
      "Epoch 15/300 - Train Loss: 0.1019, Val Loss: 0.0732\n",
      "Epoch 16/300 - Train Loss: 0.0989, Val Loss: 0.0757\n",
      "Epoch 17/300 - Train Loss: 0.0983, Val Loss: 0.0771\n",
      "Epoch 18/300 - Train Loss: 0.0988, Val Loss: 0.0718\n",
      "Epoch 19/300 - Train Loss: 0.0997, Val Loss: 0.0751\n",
      "Epoch 20/300 - Train Loss: 0.0986, Val Loss: 0.0744\n",
      "Epoch 21/300 - Train Loss: 0.0988, Val Loss: 0.0732\n",
      "Epoch 22/300 - Train Loss: 0.1004, Val Loss: 0.0727\n",
      "Epoch 23/300 - Train Loss: 0.1002, Val Loss: 0.0749\n",
      "Epoch 24/300 - Train Loss: 0.1018, Val Loss: 0.0759\n",
      "Epoch 25/300 - Train Loss: 0.1042, Val Loss: 0.0710\n",
      "Epoch 26/300 - Train Loss: 0.1010, Val Loss: 0.0739\n",
      "Epoch 27/300 - Train Loss: 0.1043, Val Loss: 0.0816\n",
      "Epoch 28/300 - Train Loss: 0.1043, Val Loss: 0.0737\n",
      "Epoch 29/300 - Train Loss: 0.1013, Val Loss: 0.0762\n",
      "Epoch 30/300 - Train Loss: 0.1011, Val Loss: 0.0750\n",
      "Epoch 31/300 - Train Loss: 0.1050, Val Loss: 0.0792\n",
      "Epoch 32/300 - Train Loss: 0.1021, Val Loss: 0.0791\n",
      "Epoch 33/300 - Train Loss: 0.1042, Val Loss: 0.0736\n",
      "Epoch 34/300 - Train Loss: 0.1064, Val Loss: 0.0753\n",
      "Epoch 35/300 - Train Loss: 0.1053, Val Loss: 0.0800\n",
      "Epoch 36/300 - Train Loss: 0.1035, Val Loss: 0.0724\n",
      "Epoch 37/300 - Train Loss: 0.1033, Val Loss: 0.0741\n",
      "Epoch 38/300 - Train Loss: 0.1074, Val Loss: 0.0718\n",
      "Epoch 39/300 - Train Loss: 0.1040, Val Loss: 0.0780\n",
      "Epoch 40/300 - Train Loss: 0.1047, Val Loss: 0.0740\n",
      "Epoch 41/300 - Train Loss: 0.1032, Val Loss: 0.0723\n",
      "Epoch 42/300 - Train Loss: 0.1043, Val Loss: 0.0811\n",
      "Epoch 43/300 - Train Loss: 0.1059, Val Loss: 0.0741\n",
      "Epoch 44/300 - Train Loss: 0.1048, Val Loss: 0.0747\n",
      "Epoch 45/300 - Train Loss: 0.1063, Val Loss: 0.0737\n",
      "Epoch 46/300 - Train Loss: 0.1048, Val Loss: 0.0958\n",
      "Epoch 47/300 - Train Loss: 0.1061, Val Loss: 0.0841\n",
      "Epoch 48/300 - Train Loss: 0.1054, Val Loss: 0.0810\n",
      "Epoch 49/300 - Train Loss: 0.1053, Val Loss: 0.0756\n",
      "Epoch 50/300 - Train Loss: 0.1060, Val Loss: 0.0753\n",
      "Epoch 51/300 - Train Loss: 0.1077, Val Loss: 0.0725\n",
      "Epoch 52/300 - Train Loss: 0.1056, Val Loss: 0.0733\n",
      "Epoch 53/300 - Train Loss: 0.1071, Val Loss: 0.0739\n",
      "Epoch 54/300 - Train Loss: 0.1049, Val Loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:55:55,120] Trial 139 finished with value: 0.961168797532434 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.33318608980345316, 'learning_rate': 7.97994743120719e-05, 'batch_size': 32, 'weight_decay': 0.0065188318296767285}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300 - Train Loss: 0.1059, Val Loss: 0.0819\n",
      "Early stopping at epoch 55\n",
      "Macro F1 Score: 0.9612, Macro Precision: 0.9648, Macro Recall: 0.9578\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.93      0.92      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 141\n",
      "Training with F1=8, F2=32, D=8, dropout=0.5150470880435541, LR=6.290844406179965e-05, BS=32, WD=0.005329375142469144\n",
      "Epoch 1/300 - Train Loss: 0.4280, Val Loss: 0.2100\n",
      "Epoch 2/300 - Train Loss: 0.1797, Val Loss: 0.1345\n",
      "Epoch 3/300 - Train Loss: 0.1390, Val Loss: 0.1176\n",
      "Epoch 4/300 - Train Loss: 0.1259, Val Loss: 0.1042\n",
      "Epoch 5/300 - Train Loss: 0.1177, Val Loss: 0.0918\n",
      "Epoch 6/300 - Train Loss: 0.1177, Val Loss: 0.0938\n",
      "Epoch 7/300 - Train Loss: 0.1133, Val Loss: 0.0871\n",
      "Epoch 8/300 - Train Loss: 0.1122, Val Loss: 0.0834\n",
      "Epoch 9/300 - Train Loss: 0.1104, Val Loss: 0.0822\n",
      "Epoch 10/300 - Train Loss: 0.1074, Val Loss: 0.0900\n",
      "Epoch 11/300 - Train Loss: 0.1083, Val Loss: 0.0806\n",
      "Epoch 12/300 - Train Loss: 0.1051, Val Loss: 0.0759\n",
      "Epoch 13/300 - Train Loss: 0.1060, Val Loss: 0.0815\n",
      "Epoch 14/300 - Train Loss: 0.1067, Val Loss: 0.0816\n",
      "Epoch 15/300 - Train Loss: 0.1061, Val Loss: 0.0804\n",
      "Epoch 16/300 - Train Loss: 0.1065, Val Loss: 0.0772\n",
      "Epoch 17/300 - Train Loss: 0.1059, Val Loss: 0.0817\n",
      "Epoch 18/300 - Train Loss: 0.1058, Val Loss: 0.0799\n",
      "Epoch 19/300 - Train Loss: 0.1072, Val Loss: 0.0779\n",
      "Epoch 20/300 - Train Loss: 0.1037, Val Loss: 0.0769\n",
      "Epoch 21/300 - Train Loss: 0.1046, Val Loss: 0.0748\n",
      "Epoch 22/300 - Train Loss: 0.1070, Val Loss: 0.0756\n",
      "Epoch 23/300 - Train Loss: 0.1064, Val Loss: 0.0756\n",
      "Epoch 24/300 - Train Loss: 0.1064, Val Loss: 0.0756\n",
      "Epoch 25/300 - Train Loss: 0.1043, Val Loss: 0.0795\n",
      "Epoch 26/300 - Train Loss: 0.1058, Val Loss: 0.0754\n",
      "Epoch 27/300 - Train Loss: 0.1024, Val Loss: 0.0747\n",
      "Epoch 28/300 - Train Loss: 0.1061, Val Loss: 0.0730\n",
      "Epoch 29/300 - Train Loss: 0.1063, Val Loss: 0.0717\n",
      "Epoch 30/300 - Train Loss: 0.1065, Val Loss: 0.0754\n",
      "Epoch 31/300 - Train Loss: 0.1043, Val Loss: 0.0753\n",
      "Epoch 32/300 - Train Loss: 0.1052, Val Loss: 0.0745\n",
      "Epoch 33/300 - Train Loss: 0.1071, Val Loss: 0.0726\n",
      "Epoch 34/300 - Train Loss: 0.1050, Val Loss: 0.0739\n",
      "Epoch 35/300 - Train Loss: 0.1051, Val Loss: 0.0779\n",
      "Epoch 36/300 - Train Loss: 0.1045, Val Loss: 0.0756\n",
      "Epoch 37/300 - Train Loss: 0.1067, Val Loss: 0.0758\n",
      "Epoch 38/300 - Train Loss: 0.1076, Val Loss: 0.0739\n",
      "Epoch 39/300 - Train Loss: 0.1073, Val Loss: 0.0751\n",
      "Epoch 40/300 - Train Loss: 0.1064, Val Loss: 0.0738\n",
      "Epoch 41/300 - Train Loss: 0.1089, Val Loss: 0.0808\n",
      "Epoch 42/300 - Train Loss: 0.1070, Val Loss: 0.0743\n",
      "Epoch 43/300 - Train Loss: 0.1072, Val Loss: 0.0766\n",
      "Epoch 44/300 - Train Loss: 0.1078, Val Loss: 0.0781\n",
      "Epoch 45/300 - Train Loss: 0.1102, Val Loss: 0.0750\n",
      "Epoch 46/300 - Train Loss: 0.1089, Val Loss: 0.0743\n",
      "Epoch 47/300 - Train Loss: 0.1082, Val Loss: 0.0775\n",
      "Epoch 48/300 - Train Loss: 0.1113, Val Loss: 0.0724\n",
      "Epoch 49/300 - Train Loss: 0.1072, Val Loss: 0.0736\n",
      "Epoch 50/300 - Train Loss: 0.1139, Val Loss: 0.0806\n",
      "Epoch 51/300 - Train Loss: 0.1103, Val Loss: 0.0810\n",
      "Epoch 52/300 - Train Loss: 0.1070, Val Loss: 0.0758\n",
      "Epoch 53/300 - Train Loss: 0.1092, Val Loss: 0.0835\n",
      "Epoch 54/300 - Train Loss: 0.1109, Val Loss: 0.0744\n",
      "Epoch 55/300 - Train Loss: 0.1069, Val Loss: 0.0762\n",
      "Epoch 56/300 - Train Loss: 0.1090, Val Loss: 0.0788\n",
      "Epoch 57/300 - Train Loss: 0.1081, Val Loss: 0.0768\n",
      "Epoch 58/300 - Train Loss: 0.1118, Val Loss: 0.0738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 07:58:17,010] Trial 140 finished with value: 0.965867414039864 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.5150470880435541, 'learning_rate': 6.290844406179965e-05, 'batch_size': 32, 'weight_decay': 0.005329375142469144}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300 - Train Loss: 0.1088, Val Loss: 0.0745\n",
      "Early stopping at epoch 59\n",
      "Macro F1 Score: 0.9659, Macro Precision: 0.9671, Macro Recall: 0.9648\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 142\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2403028127785303, LR=0.0001563932646070608, BS=32, WD=0.0003225260067905542\n",
      "Epoch 1/300 - Train Loss: 0.2059, Val Loss: 0.0887\n",
      "Epoch 2/300 - Train Loss: 0.1074, Val Loss: 0.0810\n",
      "Epoch 3/300 - Train Loss: 0.0990, Val Loss: 0.0783\n",
      "Epoch 4/300 - Train Loss: 0.0937, Val Loss: 0.0873\n",
      "Epoch 5/300 - Train Loss: 0.0921, Val Loss: 0.0753\n",
      "Epoch 6/300 - Train Loss: 0.0904, Val Loss: 0.0765\n",
      "Epoch 7/300 - Train Loss: 0.0882, Val Loss: 0.0717\n",
      "Epoch 8/300 - Train Loss: 0.0853, Val Loss: 0.0750\n",
      "Epoch 9/300 - Train Loss: 0.0838, Val Loss: 0.0671\n",
      "Epoch 10/300 - Train Loss: 0.0827, Val Loss: 0.0699\n",
      "Epoch 11/300 - Train Loss: 0.0805, Val Loss: 0.0779\n",
      "Epoch 12/300 - Train Loss: 0.0806, Val Loss: 0.0688\n",
      "Epoch 13/300 - Train Loss: 0.0794, Val Loss: 0.0694\n",
      "Epoch 14/300 - Train Loss: 0.0797, Val Loss: 0.0683\n",
      "Epoch 15/300 - Train Loss: 0.0779, Val Loss: 0.0791\n",
      "Epoch 16/300 - Train Loss: 0.0776, Val Loss: 0.0761\n",
      "Epoch 17/300 - Train Loss: 0.0789, Val Loss: 0.0724\n",
      "Epoch 18/300 - Train Loss: 0.0751, Val Loss: 0.0699\n",
      "Epoch 19/300 - Train Loss: 0.0753, Val Loss: 0.0720\n",
      "Epoch 20/300 - Train Loss: 0.0770, Val Loss: 0.0684\n",
      "Epoch 21/300 - Train Loss: 0.0743, Val Loss: 0.0693\n",
      "Epoch 22/300 - Train Loss: 0.0719, Val Loss: 0.0691\n",
      "Epoch 23/300 - Train Loss: 0.0725, Val Loss: 0.0672\n",
      "Epoch 24/300 - Train Loss: 0.0724, Val Loss: 0.0697\n",
      "Epoch 25/300 - Train Loss: 0.0700, Val Loss: 0.0720\n",
      "Epoch 26/300 - Train Loss: 0.0722, Val Loss: 0.0654\n",
      "Epoch 27/300 - Train Loss: 0.0690, Val Loss: 0.0674\n",
      "Epoch 28/300 - Train Loss: 0.0707, Val Loss: 0.0645\n",
      "Epoch 29/300 - Train Loss: 0.0718, Val Loss: 0.0677\n",
      "Epoch 30/300 - Train Loss: 0.0691, Val Loss: 0.0720\n",
      "Epoch 31/300 - Train Loss: 0.0684, Val Loss: 0.0644\n",
      "Epoch 32/300 - Train Loss: 0.0702, Val Loss: 0.0674\n",
      "Epoch 33/300 - Train Loss: 0.0700, Val Loss: 0.0692\n",
      "Epoch 34/300 - Train Loss: 0.0657, Val Loss: 0.0683\n",
      "Epoch 35/300 - Train Loss: 0.0693, Val Loss: 0.0695\n",
      "Epoch 36/300 - Train Loss: 0.0717, Val Loss: 0.0766\n",
      "Epoch 37/300 - Train Loss: 0.0665, Val Loss: 0.0703\n",
      "Epoch 38/300 - Train Loss: 0.0689, Val Loss: 0.0741\n",
      "Epoch 39/300 - Train Loss: 0.0671, Val Loss: 0.0697\n",
      "Epoch 40/300 - Train Loss: 0.0655, Val Loss: 0.0691\n",
      "Epoch 41/300 - Train Loss: 0.0689, Val Loss: 0.0699\n",
      "Epoch 42/300 - Train Loss: 0.0671, Val Loss: 0.0682\n",
      "Epoch 43/300 - Train Loss: 0.0657, Val Loss: 0.0658\n",
      "Epoch 44/300 - Train Loss: 0.0651, Val Loss: 0.0651\n",
      "Epoch 45/300 - Train Loss: 0.0630, Val Loss: 0.0689\n",
      "Epoch 46/300 - Train Loss: 0.0648, Val Loss: 0.0714\n",
      "Epoch 47/300 - Train Loss: 0.0637, Val Loss: 0.0709\n",
      "Epoch 48/300 - Train Loss: 0.0648, Val Loss: 0.0689\n",
      "Epoch 49/300 - Train Loss: 0.0648, Val Loss: 0.0672\n",
      "Epoch 50/300 - Train Loss: 0.0640, Val Loss: 0.0663\n",
      "Epoch 51/300 - Train Loss: 0.0640, Val Loss: 0.0685\n",
      "Epoch 52/300 - Train Loss: 0.0629, Val Loss: 0.0660\n",
      "Epoch 53/300 - Train Loss: 0.0627, Val Loss: 0.0690\n",
      "Epoch 54/300 - Train Loss: 0.0621, Val Loss: 0.0678\n",
      "Epoch 55/300 - Train Loss: 0.0612, Val Loss: 0.0696\n",
      "Epoch 56/300 - Train Loss: 0.0661, Val Loss: 0.0684\n",
      "Epoch 57/300 - Train Loss: 0.0614, Val Loss: 0.0684\n",
      "Epoch 58/300 - Train Loss: 0.0621, Val Loss: 0.0712\n",
      "Epoch 59/300 - Train Loss: 0.0621, Val Loss: 0.0691\n",
      "Epoch 60/300 - Train Loss: 0.0616, Val Loss: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:02:11,917] Trial 141 finished with value: 0.9646220161021875 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2403028127785303, 'learning_rate': 0.0001563932646070608, 'batch_size': 32, 'weight_decay': 0.0003225260067905542}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/300 - Train Loss: 0.0604, Val Loss: 0.0674\n",
      "Early stopping at epoch 61\n",
      "Macro F1 Score: 0.9646, Macro Precision: 0.9628, Macro Recall: 0.9665\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.93      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 143\n",
      "Training with F1=16, F2=32, D=8, dropout=0.26198702570444854, LR=5.55757899050671e-05, BS=32, WD=0.004535334992709867\n",
      "Epoch 1/300 - Train Loss: 0.3306, Val Loss: 0.1719\n",
      "Epoch 2/300 - Train Loss: 0.1580, Val Loss: 0.1185\n",
      "Epoch 3/300 - Train Loss: 0.1205, Val Loss: 0.0982\n",
      "Epoch 4/300 - Train Loss: 0.1079, Val Loss: 0.0913\n",
      "Epoch 5/300 - Train Loss: 0.1026, Val Loss: 0.0804\n",
      "Epoch 6/300 - Train Loss: 0.0979, Val Loss: 0.0802\n",
      "Epoch 7/300 - Train Loss: 0.0959, Val Loss: 0.0804\n",
      "Epoch 8/300 - Train Loss: 0.0943, Val Loss: 0.0818\n",
      "Epoch 9/300 - Train Loss: 0.0901, Val Loss: 0.0746\n",
      "Epoch 10/300 - Train Loss: 0.0951, Val Loss: 0.0847\n",
      "Epoch 11/300 - Train Loss: 0.0899, Val Loss: 0.0750\n",
      "Epoch 12/300 - Train Loss: 0.0866, Val Loss: 0.0725\n",
      "Epoch 13/300 - Train Loss: 0.0906, Val Loss: 0.0716\n",
      "Epoch 14/300 - Train Loss: 0.0900, Val Loss: 0.0680\n",
      "Epoch 15/300 - Train Loss: 0.0879, Val Loss: 0.0681\n",
      "Epoch 16/300 - Train Loss: 0.0878, Val Loss: 0.0696\n",
      "Epoch 17/300 - Train Loss: 0.0866, Val Loss: 0.0725\n",
      "Epoch 18/300 - Train Loss: 0.0892, Val Loss: 0.0703\n",
      "Epoch 19/300 - Train Loss: 0.0887, Val Loss: 0.0694\n",
      "Epoch 20/300 - Train Loss: 0.0868, Val Loss: 0.0690\n",
      "Epoch 21/300 - Train Loss: 0.0882, Val Loss: 0.0707\n",
      "Epoch 22/300 - Train Loss: 0.0910, Val Loss: 0.0707\n",
      "Epoch 23/300 - Train Loss: 0.0884, Val Loss: 0.0759\n",
      "Epoch 24/300 - Train Loss: 0.0884, Val Loss: 0.0681\n",
      "Epoch 25/300 - Train Loss: 0.0872, Val Loss: 0.0693\n",
      "Epoch 26/300 - Train Loss: 0.0870, Val Loss: 0.0688\n",
      "Epoch 27/300 - Train Loss: 0.0893, Val Loss: 0.0712\n",
      "Epoch 28/300 - Train Loss: 0.0897, Val Loss: 0.0680\n",
      "Epoch 29/300 - Train Loss: 0.0884, Val Loss: 0.0722\n",
      "Epoch 30/300 - Train Loss: 0.0889, Val Loss: 0.0765\n",
      "Epoch 31/300 - Train Loss: 0.0903, Val Loss: 0.0697\n",
      "Epoch 32/300 - Train Loss: 0.0920, Val Loss: 0.0687\n",
      "Epoch 33/300 - Train Loss: 0.0895, Val Loss: 0.0698\n",
      "Epoch 34/300 - Train Loss: 0.0906, Val Loss: 0.0690\n",
      "Epoch 35/300 - Train Loss: 0.0899, Val Loss: 0.0685\n",
      "Epoch 36/300 - Train Loss: 0.0879, Val Loss: 0.0710\n",
      "Epoch 37/300 - Train Loss: 0.0897, Val Loss: 0.0693\n",
      "Epoch 38/300 - Train Loss: 0.0920, Val Loss: 0.0683\n",
      "Epoch 39/300 - Train Loss: 0.0896, Val Loss: 0.0700\n",
      "Epoch 40/300 - Train Loss: 0.0903, Val Loss: 0.0698\n",
      "Epoch 41/300 - Train Loss: 0.0910, Val Loss: 0.0694\n",
      "Epoch 42/300 - Train Loss: 0.0922, Val Loss: 0.0686\n",
      "Epoch 43/300 - Train Loss: 0.0899, Val Loss: 0.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:05:01,774] Trial 142 finished with value: 0.9733158063386526 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.26198702570444854, 'learning_rate': 5.55757899050671e-05, 'batch_size': 32, 'weight_decay': 0.004535334992709867}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300 - Train Loss: 0.0925, Val Loss: 0.0709\n",
      "Early stopping at epoch 44\n",
      "Macro F1 Score: 0.9733, Macro Precision: 0.9742, Macro Recall: 0.9725\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 144\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2659556477997411, LR=6.034289256590833e-05, BS=32, WD=0.0047412507551525\n",
      "Epoch 1/300 - Train Loss: 0.3400, Val Loss: 0.1553\n",
      "Epoch 2/300 - Train Loss: 0.1374, Val Loss: 0.1040\n",
      "Epoch 3/300 - Train Loss: 0.1124, Val Loss: 0.0907\n",
      "Epoch 4/300 - Train Loss: 0.1041, Val Loss: 0.0829\n",
      "Epoch 5/300 - Train Loss: 0.0993, Val Loss: 0.0824\n",
      "Epoch 6/300 - Train Loss: 0.0998, Val Loss: 0.0807\n",
      "Epoch 7/300 - Train Loss: 0.0953, Val Loss: 0.0824\n",
      "Epoch 8/300 - Train Loss: 0.0931, Val Loss: 0.0793\n",
      "Epoch 9/300 - Train Loss: 0.0942, Val Loss: 0.0740\n",
      "Epoch 10/300 - Train Loss: 0.0930, Val Loss: 0.0828\n",
      "Epoch 11/300 - Train Loss: 0.0935, Val Loss: 0.0742\n",
      "Epoch 12/300 - Train Loss: 0.0923, Val Loss: 0.0761\n",
      "Epoch 13/300 - Train Loss: 0.0904, Val Loss: 0.0740\n",
      "Epoch 14/300 - Train Loss: 0.0915, Val Loss: 0.0729\n",
      "Epoch 15/300 - Train Loss: 0.0903, Val Loss: 0.0752\n",
      "Epoch 16/300 - Train Loss: 0.0917, Val Loss: 0.0742\n",
      "Epoch 17/300 - Train Loss: 0.0917, Val Loss: 0.0732\n",
      "Epoch 18/300 - Train Loss: 0.0923, Val Loss: 0.0740\n",
      "Epoch 19/300 - Train Loss: 0.0907, Val Loss: 0.0724\n",
      "Epoch 20/300 - Train Loss: 0.0918, Val Loss: 0.0756\n",
      "Epoch 21/300 - Train Loss: 0.0925, Val Loss: 0.0722\n",
      "Epoch 22/300 - Train Loss: 0.0904, Val Loss: 0.0722\n",
      "Epoch 23/300 - Train Loss: 0.0921, Val Loss: 0.0755\n",
      "Epoch 24/300 - Train Loss: 0.0891, Val Loss: 0.0728\n",
      "Epoch 25/300 - Train Loss: 0.0941, Val Loss: 0.0708\n",
      "Epoch 26/300 - Train Loss: 0.0897, Val Loss: 0.0708\n",
      "Epoch 27/300 - Train Loss: 0.0934, Val Loss: 0.0734\n",
      "Epoch 28/300 - Train Loss: 0.0942, Val Loss: 0.0755\n",
      "Epoch 29/300 - Train Loss: 0.0899, Val Loss: 0.0785\n",
      "Epoch 30/300 - Train Loss: 0.0907, Val Loss: 0.0720\n",
      "Epoch 31/300 - Train Loss: 0.0897, Val Loss: 0.0733\n",
      "Epoch 32/300 - Train Loss: 0.0905, Val Loss: 0.0731\n",
      "Epoch 33/300 - Train Loss: 0.0923, Val Loss: 0.0728\n",
      "Epoch 34/300 - Train Loss: 0.0950, Val Loss: 0.0784\n",
      "Epoch 35/300 - Train Loss: 0.0955, Val Loss: 0.0727\n",
      "Epoch 36/300 - Train Loss: 0.0957, Val Loss: 0.0712\n",
      "Epoch 37/300 - Train Loss: 0.0954, Val Loss: 0.0697\n",
      "Epoch 38/300 - Train Loss: 0.0931, Val Loss: 0.0765\n",
      "Epoch 39/300 - Train Loss: 0.0935, Val Loss: 0.0733\n",
      "Epoch 40/300 - Train Loss: 0.0940, Val Loss: 0.0753\n",
      "Epoch 41/300 - Train Loss: 0.0934, Val Loss: 0.0754\n",
      "Epoch 42/300 - Train Loss: 0.0926, Val Loss: 0.0770\n",
      "Epoch 43/300 - Train Loss: 0.0923, Val Loss: 0.0728\n",
      "Epoch 44/300 - Train Loss: 0.0936, Val Loss: 0.0740\n",
      "Epoch 45/300 - Train Loss: 0.0934, Val Loss: 0.0725\n",
      "Epoch 46/300 - Train Loss: 0.0951, Val Loss: 0.0689\n",
      "Epoch 47/300 - Train Loss: 0.0948, Val Loss: 0.0793\n",
      "Epoch 48/300 - Train Loss: 0.0982, Val Loss: 0.0704\n",
      "Epoch 49/300 - Train Loss: 0.0956, Val Loss: 0.0725\n",
      "Epoch 50/300 - Train Loss: 0.0946, Val Loss: 0.0729\n",
      "Epoch 51/300 - Train Loss: 0.0969, Val Loss: 0.0756\n",
      "Epoch 52/300 - Train Loss: 0.0969, Val Loss: 0.0718\n",
      "Epoch 53/300 - Train Loss: 0.0944, Val Loss: 0.0731\n",
      "Epoch 54/300 - Train Loss: 0.0957, Val Loss: 0.0717\n",
      "Epoch 55/300 - Train Loss: 0.0975, Val Loss: 0.0816\n",
      "Epoch 56/300 - Train Loss: 0.0963, Val Loss: 0.0721\n",
      "Epoch 57/300 - Train Loss: 0.0950, Val Loss: 0.0757\n",
      "Epoch 58/300 - Train Loss: 0.0959, Val Loss: 0.0728\n",
      "Epoch 59/300 - Train Loss: 0.0965, Val Loss: 0.0815\n",
      "Epoch 60/300 - Train Loss: 0.0957, Val Loss: 0.0704\n",
      "Epoch 61/300 - Train Loss: 0.0958, Val Loss: 0.0716\n",
      "Epoch 62/300 - Train Loss: 0.0980, Val Loss: 0.0729\n",
      "Epoch 63/300 - Train Loss: 0.0964, Val Loss: 0.0748\n",
      "Epoch 64/300 - Train Loss: 0.0957, Val Loss: 0.0755\n",
      "Epoch 65/300 - Train Loss: 0.0973, Val Loss: 0.0715\n",
      "Epoch 66/300 - Train Loss: 0.0962, Val Loss: 0.0732\n",
      "Epoch 67/300 - Train Loss: 0.0980, Val Loss: 0.0710\n",
      "Epoch 68/300 - Train Loss: 0.0975, Val Loss: 0.0721\n",
      "Epoch 69/300 - Train Loss: 0.0974, Val Loss: 0.0754\n",
      "Epoch 70/300 - Train Loss: 0.0969, Val Loss: 0.0852\n",
      "Epoch 71/300 - Train Loss: 0.0955, Val Loss: 0.0787\n",
      "Epoch 72/300 - Train Loss: 0.0972, Val Loss: 0.0756\n",
      "Epoch 73/300 - Train Loss: 0.0952, Val Loss: 0.0746\n",
      "Epoch 74/300 - Train Loss: 0.0958, Val Loss: 0.0710\n",
      "Epoch 75/300 - Train Loss: 0.0991, Val Loss: 0.0734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:09:54,944] Trial 143 finished with value: 0.9703385128364741 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2659556477997411, 'learning_rate': 6.034289256590833e-05, 'batch_size': 32, 'weight_decay': 0.0047412507551525}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300 - Train Loss: 0.0969, Val Loss: 0.0725\n",
      "Early stopping at epoch 76\n",
      "Macro F1 Score: 0.9703, Macro Precision: 0.9718, Macro Recall: 0.9691\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 145\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2871286426841912, LR=5.571564113175261e-05, BS=32, WD=0.007247135756164391\n",
      "Epoch 1/300 - Train Loss: 0.3371, Val Loss: 0.1706\n",
      "Epoch 2/300 - Train Loss: 0.1599, Val Loss: 0.1350\n",
      "Epoch 3/300 - Train Loss: 0.1203, Val Loss: 0.1055\n",
      "Epoch 4/300 - Train Loss: 0.1088, Val Loss: 0.1048\n",
      "Epoch 5/300 - Train Loss: 0.1051, Val Loss: 0.0858\n",
      "Epoch 6/300 - Train Loss: 0.1037, Val Loss: 0.0862\n",
      "Epoch 7/300 - Train Loss: 0.0987, Val Loss: 0.0775\n",
      "Epoch 8/300 - Train Loss: 0.0976, Val Loss: 0.0743\n",
      "Epoch 9/300 - Train Loss: 0.0957, Val Loss: 0.0845\n",
      "Epoch 10/300 - Train Loss: 0.0938, Val Loss: 0.0711\n",
      "Epoch 11/300 - Train Loss: 0.0958, Val Loss: 0.0771\n",
      "Epoch 12/300 - Train Loss: 0.0949, Val Loss: 0.0767\n",
      "Epoch 13/300 - Train Loss: 0.0947, Val Loss: 0.0776\n",
      "Epoch 14/300 - Train Loss: 0.0940, Val Loss: 0.0725\n",
      "Epoch 15/300 - Train Loss: 0.0941, Val Loss: 0.0720\n",
      "Epoch 16/300 - Train Loss: 0.0938, Val Loss: 0.0785\n",
      "Epoch 17/300 - Train Loss: 0.0935, Val Loss: 0.0747\n",
      "Epoch 18/300 - Train Loss: 0.0940, Val Loss: 0.0778\n",
      "Epoch 19/300 - Train Loss: 0.0959, Val Loss: 0.0709\n",
      "Epoch 20/300 - Train Loss: 0.0972, Val Loss: 0.0701\n",
      "Epoch 21/300 - Train Loss: 0.0938, Val Loss: 0.0821\n",
      "Epoch 22/300 - Train Loss: 0.0952, Val Loss: 0.0750\n",
      "Epoch 23/300 - Train Loss: 0.0942, Val Loss: 0.0720\n",
      "Epoch 24/300 - Train Loss: 0.0954, Val Loss: 0.0771\n",
      "Epoch 25/300 - Train Loss: 0.0953, Val Loss: 0.0741\n",
      "Epoch 26/300 - Train Loss: 0.0982, Val Loss: 0.0773\n",
      "Epoch 27/300 - Train Loss: 0.0984, Val Loss: 0.0707\n",
      "Epoch 28/300 - Train Loss: 0.0964, Val Loss: 0.0721\n",
      "Epoch 29/300 - Train Loss: 0.0965, Val Loss: 0.0771\n",
      "Epoch 30/300 - Train Loss: 0.0979, Val Loss: 0.0740\n",
      "Epoch 31/300 - Train Loss: 0.0950, Val Loss: 0.0712\n",
      "Epoch 32/300 - Train Loss: 0.0971, Val Loss: 0.0699\n",
      "Epoch 33/300 - Train Loss: 0.0987, Val Loss: 0.0711\n",
      "Epoch 34/300 - Train Loss: 0.0979, Val Loss: 0.0746\n",
      "Epoch 35/300 - Train Loss: 0.0987, Val Loss: 0.0747\n",
      "Epoch 36/300 - Train Loss: 0.0994, Val Loss: 0.0702\n",
      "Epoch 37/300 - Train Loss: 0.0966, Val Loss: 0.0745\n",
      "Epoch 38/300 - Train Loss: 0.0998, Val Loss: 0.0837\n",
      "Epoch 39/300 - Train Loss: 0.0999, Val Loss: 0.0712\n",
      "Epoch 40/300 - Train Loss: 0.1003, Val Loss: 0.0725\n",
      "Epoch 41/300 - Train Loss: 0.0992, Val Loss: 0.0744\n",
      "Epoch 42/300 - Train Loss: 0.1007, Val Loss: 0.0729\n",
      "Epoch 43/300 - Train Loss: 0.0987, Val Loss: 0.0701\n",
      "Epoch 44/300 - Train Loss: 0.0995, Val Loss: 0.0773\n",
      "Epoch 45/300 - Train Loss: 0.1021, Val Loss: 0.0871\n",
      "Epoch 46/300 - Train Loss: 0.1007, Val Loss: 0.0816\n",
      "Epoch 47/300 - Train Loss: 0.1022, Val Loss: 0.0769\n",
      "Epoch 48/300 - Train Loss: 0.0997, Val Loss: 0.0727\n",
      "Epoch 49/300 - Train Loss: 0.1022, Val Loss: 0.0720\n",
      "Epoch 50/300 - Train Loss: 0.1004, Val Loss: 0.0725\n",
      "Epoch 51/300 - Train Loss: 0.1017, Val Loss: 0.0756\n",
      "Epoch 52/300 - Train Loss: 0.1029, Val Loss: 0.0741\n",
      "Epoch 53/300 - Train Loss: 0.1040, Val Loss: 0.0751\n",
      "Epoch 54/300 - Train Loss: 0.1020, Val Loss: 0.0704\n",
      "Epoch 55/300 - Train Loss: 0.1027, Val Loss: 0.0743\n",
      "Epoch 56/300 - Train Loss: 0.1037, Val Loss: 0.0705\n",
      "Epoch 57/300 - Train Loss: 0.1042, Val Loss: 0.0703\n",
      "Epoch 58/300 - Train Loss: 0.1046, Val Loss: 0.0724\n",
      "Epoch 59/300 - Train Loss: 0.1026, Val Loss: 0.0741\n",
      "Epoch 60/300 - Train Loss: 0.1031, Val Loss: 0.0729\n",
      "Epoch 61/300 - Train Loss: 0.1045, Val Loss: 0.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:13:53,957] Trial 144 finished with value: 0.9741636280821234 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2871286426841912, 'learning_rate': 5.571564113175261e-05, 'batch_size': 32, 'weight_decay': 0.007247135756164391}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/300 - Train Loss: 0.1065, Val Loss: 0.0770\n",
      "Early stopping at epoch 62\n",
      "Macro F1 Score: 0.9742, Macro Precision: 0.9779, Macro Recall: 0.9707\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 146\n",
      "Training with F1=16, F2=16, D=8, dropout=0.304321222896987, LR=4.34751500724151e-05, BS=32, WD=0.007669711443890019\n",
      "Epoch 1/300 - Train Loss: 0.4801, Val Loss: 0.2351\n",
      "Epoch 2/300 - Train Loss: 0.2122, Val Loss: 0.1469\n",
      "Epoch 3/300 - Train Loss: 0.1518, Val Loss: 0.1081\n",
      "Epoch 4/300 - Train Loss: 0.1202, Val Loss: 0.0961\n",
      "Epoch 5/300 - Train Loss: 0.1149, Val Loss: 0.0924\n",
      "Epoch 6/300 - Train Loss: 0.1094, Val Loss: 0.0879\n",
      "Epoch 7/300 - Train Loss: 0.1065, Val Loss: 0.0923\n",
      "Epoch 8/300 - Train Loss: 0.1034, Val Loss: 0.0794\n",
      "Epoch 9/300 - Train Loss: 0.1017, Val Loss: 0.0754\n",
      "Epoch 10/300 - Train Loss: 0.1000, Val Loss: 0.0818\n",
      "Epoch 11/300 - Train Loss: 0.1005, Val Loss: 0.0786\n",
      "Epoch 12/300 - Train Loss: 0.0986, Val Loss: 0.0772\n",
      "Epoch 13/300 - Train Loss: 0.1014, Val Loss: 0.0764\n",
      "Epoch 14/300 - Train Loss: 0.1011, Val Loss: 0.0743\n",
      "Epoch 15/300 - Train Loss: 0.0960, Val Loss: 0.0776\n",
      "Epoch 16/300 - Train Loss: 0.0967, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.1000, Val Loss: 0.0744\n",
      "Epoch 18/300 - Train Loss: 0.0977, Val Loss: 0.0885\n",
      "Epoch 19/300 - Train Loss: 0.0969, Val Loss: 0.0732\n",
      "Epoch 20/300 - Train Loss: 0.0969, Val Loss: 0.0738\n",
      "Epoch 21/300 - Train Loss: 0.0985, Val Loss: 0.0792\n",
      "Epoch 22/300 - Train Loss: 0.0968, Val Loss: 0.0731\n",
      "Epoch 23/300 - Train Loss: 0.0992, Val Loss: 0.0774\n",
      "Epoch 24/300 - Train Loss: 0.0993, Val Loss: 0.0738\n",
      "Epoch 25/300 - Train Loss: 0.0978, Val Loss: 0.0797\n",
      "Epoch 26/300 - Train Loss: 0.1005, Val Loss: 0.0726\n",
      "Epoch 27/300 - Train Loss: 0.0971, Val Loss: 0.0719\n",
      "Epoch 28/300 - Train Loss: 0.0994, Val Loss: 0.0727\n",
      "Epoch 29/300 - Train Loss: 0.0996, Val Loss: 0.0731\n",
      "Epoch 30/300 - Train Loss: 0.1022, Val Loss: 0.0720\n",
      "Epoch 31/300 - Train Loss: 0.0982, Val Loss: 0.0742\n",
      "Epoch 32/300 - Train Loss: 0.0995, Val Loss: 0.0785\n",
      "Epoch 33/300 - Train Loss: 0.0984, Val Loss: 0.0779\n",
      "Epoch 34/300 - Train Loss: 0.1001, Val Loss: 0.0789\n",
      "Epoch 35/300 - Train Loss: 0.0989, Val Loss: 0.0762\n",
      "Epoch 36/300 - Train Loss: 0.0981, Val Loss: 0.0743\n",
      "Epoch 37/300 - Train Loss: 0.1007, Val Loss: 0.0736\n",
      "Epoch 38/300 - Train Loss: 0.0995, Val Loss: 0.0829\n",
      "Epoch 39/300 - Train Loss: 0.1004, Val Loss: 0.0821\n",
      "Epoch 40/300 - Train Loss: 0.1003, Val Loss: 0.0751\n",
      "Epoch 41/300 - Train Loss: 0.1009, Val Loss: 0.0735\n",
      "Epoch 42/300 - Train Loss: 0.1029, Val Loss: 0.0724\n",
      "Epoch 43/300 - Train Loss: 0.1007, Val Loss: 0.0706\n",
      "Epoch 44/300 - Train Loss: 0.1016, Val Loss: 0.0733\n",
      "Epoch 45/300 - Train Loss: 0.1014, Val Loss: 0.0723\n",
      "Epoch 46/300 - Train Loss: 0.0991, Val Loss: 0.0724\n",
      "Epoch 47/300 - Train Loss: 0.1007, Val Loss: 0.0695\n",
      "Epoch 48/300 - Train Loss: 0.1019, Val Loss: 0.0724\n",
      "Epoch 49/300 - Train Loss: 0.1014, Val Loss: 0.0794\n",
      "Epoch 50/300 - Train Loss: 0.1013, Val Loss: 0.0735\n",
      "Epoch 51/300 - Train Loss: 0.1029, Val Loss: 0.0758\n",
      "Epoch 52/300 - Train Loss: 0.1013, Val Loss: 0.0728\n",
      "Epoch 53/300 - Train Loss: 0.1047, Val Loss: 0.0700\n",
      "Epoch 54/300 - Train Loss: 0.1025, Val Loss: 0.0750\n",
      "Epoch 55/300 - Train Loss: 0.1007, Val Loss: 0.0712\n",
      "Epoch 56/300 - Train Loss: 0.1031, Val Loss: 0.0797\n",
      "Epoch 57/300 - Train Loss: 0.0998, Val Loss: 0.0712\n",
      "Epoch 58/300 - Train Loss: 0.1029, Val Loss: 0.0728\n",
      "Epoch 59/300 - Train Loss: 0.1012, Val Loss: 0.0732\n",
      "Epoch 60/300 - Train Loss: 0.1014, Val Loss: 0.0752\n",
      "Epoch 61/300 - Train Loss: 0.1032, Val Loss: 0.0751\n",
      "Epoch 62/300 - Train Loss: 0.1042, Val Loss: 0.0749\n",
      "Epoch 63/300 - Train Loss: 0.1046, Val Loss: 0.0704\n",
      "Epoch 64/300 - Train Loss: 0.1041, Val Loss: 0.0731\n",
      "Epoch 65/300 - Train Loss: 0.1024, Val Loss: 0.0739\n",
      "Epoch 66/300 - Train Loss: 0.1039, Val Loss: 0.0720\n",
      "Epoch 67/300 - Train Loss: 0.1056, Val Loss: 0.0726\n",
      "Epoch 68/300 - Train Loss: 0.1023, Val Loss: 0.0738\n",
      "Epoch 69/300 - Train Loss: 0.1045, Val Loss: 0.0722\n",
      "Epoch 70/300 - Train Loss: 0.1026, Val Loss: 0.0762\n",
      "Epoch 71/300 - Train Loss: 0.1037, Val Loss: 0.0749\n",
      "Epoch 72/300 - Train Loss: 0.1049, Val Loss: 0.0723\n",
      "Epoch 73/300 - Train Loss: 0.1044, Val Loss: 0.0724\n",
      "Epoch 74/300 - Train Loss: 0.1076, Val Loss: 0.0762\n",
      "Epoch 75/300 - Train Loss: 0.1036, Val Loss: 0.0747\n",
      "Epoch 76/300 - Train Loss: 0.1022, Val Loss: 0.0779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:18:39,149] Trial 145 finished with value: 0.9703438592962833 and parameters: {'F1': 16, 'F2': 16, 'D': 8, 'dropout': 0.304321222896987, 'learning_rate': 4.34751500724151e-05, 'batch_size': 32, 'weight_decay': 0.007669711443890019}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300 - Train Loss: 0.1060, Val Loss: 0.0801\n",
      "Early stopping at epoch 77\n",
      "Macro F1 Score: 0.9703, Macro Precision: 0.9716, Macro Recall: 0.9693\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 147\n",
      "Training with F1=16, F2=32, D=4, dropout=0.2850950601982045, LR=5.3481772088573195e-05, BS=32, WD=0.0008306200519630769\n",
      "Epoch 1/300 - Train Loss: 0.3801, Val Loss: 0.1893\n",
      "Epoch 2/300 - Train Loss: 0.1903, Val Loss: 0.1407\n",
      "Epoch 3/300 - Train Loss: 0.1501, Val Loss: 0.1149\n",
      "Epoch 4/300 - Train Loss: 0.1274, Val Loss: 0.1018\n",
      "Epoch 5/300 - Train Loss: 0.1170, Val Loss: 0.0897\n",
      "Epoch 6/300 - Train Loss: 0.1071, Val Loss: 0.0839\n",
      "Epoch 7/300 - Train Loss: 0.1053, Val Loss: 0.0806\n",
      "Epoch 8/300 - Train Loss: 0.1007, Val Loss: 0.0918\n",
      "Epoch 9/300 - Train Loss: 0.0999, Val Loss: 0.0869\n",
      "Epoch 10/300 - Train Loss: 0.0972, Val Loss: 0.0747\n",
      "Epoch 11/300 - Train Loss: 0.0930, Val Loss: 0.0759\n",
      "Epoch 12/300 - Train Loss: 0.0943, Val Loss: 0.0771\n",
      "Epoch 13/300 - Train Loss: 0.0927, Val Loss: 0.0712\n",
      "Epoch 14/300 - Train Loss: 0.0899, Val Loss: 0.0749\n",
      "Epoch 15/300 - Train Loss: 0.0913, Val Loss: 0.0727\n",
      "Epoch 16/300 - Train Loss: 0.0902, Val Loss: 0.0776\n",
      "Epoch 17/300 - Train Loss: 0.0884, Val Loss: 0.0699\n",
      "Epoch 18/300 - Train Loss: 0.0879, Val Loss: 0.0716\n",
      "Epoch 19/300 - Train Loss: 0.0889, Val Loss: 0.0742\n",
      "Epoch 20/300 - Train Loss: 0.0864, Val Loss: 0.0779\n",
      "Epoch 21/300 - Train Loss: 0.0864, Val Loss: 0.0708\n",
      "Epoch 22/300 - Train Loss: 0.0863, Val Loss: 0.0747\n",
      "Epoch 23/300 - Train Loss: 0.0847, Val Loss: 0.0725\n",
      "Epoch 24/300 - Train Loss: 0.0828, Val Loss: 0.0678\n",
      "Epoch 25/300 - Train Loss: 0.0845, Val Loss: 0.0761\n",
      "Epoch 26/300 - Train Loss: 0.0844, Val Loss: 0.0712\n",
      "Epoch 27/300 - Train Loss: 0.0813, Val Loss: 0.0699\n",
      "Epoch 28/300 - Train Loss: 0.0832, Val Loss: 0.0676\n",
      "Epoch 29/300 - Train Loss: 0.0802, Val Loss: 0.0751\n",
      "Epoch 30/300 - Train Loss: 0.0840, Val Loss: 0.0743\n",
      "Epoch 31/300 - Train Loss: 0.0861, Val Loss: 0.0671\n",
      "Epoch 32/300 - Train Loss: 0.0845, Val Loss: 0.0724\n",
      "Epoch 33/300 - Train Loss: 0.0816, Val Loss: 0.0826\n",
      "Epoch 34/300 - Train Loss: 0.0805, Val Loss: 0.0676\n",
      "Epoch 35/300 - Train Loss: 0.0823, Val Loss: 0.0676\n",
      "Epoch 36/300 - Train Loss: 0.0803, Val Loss: 0.0688\n",
      "Epoch 37/300 - Train Loss: 0.0805, Val Loss: 0.0673\n",
      "Epoch 38/300 - Train Loss: 0.0804, Val Loss: 0.0707\n",
      "Epoch 39/300 - Train Loss: 0.0802, Val Loss: 0.0710\n",
      "Epoch 40/300 - Train Loss: 0.0801, Val Loss: 0.0696\n",
      "Epoch 41/300 - Train Loss: 0.0833, Val Loss: 0.0725\n",
      "Epoch 42/300 - Train Loss: 0.0772, Val Loss: 0.0781\n",
      "Epoch 43/300 - Train Loss: 0.0784, Val Loss: 0.0710\n",
      "Epoch 44/300 - Train Loss: 0.0777, Val Loss: 0.0677\n",
      "Epoch 45/300 - Train Loss: 0.0793, Val Loss: 0.0679\n",
      "Epoch 46/300 - Train Loss: 0.0790, Val Loss: 0.0681\n",
      "Epoch 47/300 - Train Loss: 0.0780, Val Loss: 0.0659\n",
      "Epoch 48/300 - Train Loss: 0.0763, Val Loss: 0.0699\n",
      "Epoch 49/300 - Train Loss: 0.0794, Val Loss: 0.0708\n",
      "Epoch 50/300 - Train Loss: 0.0777, Val Loss: 0.0699\n",
      "Epoch 51/300 - Train Loss: 0.0791, Val Loss: 0.0697\n",
      "Epoch 52/300 - Train Loss: 0.0778, Val Loss: 0.0665\n",
      "Epoch 53/300 - Train Loss: 0.0760, Val Loss: 0.0645\n",
      "Epoch 54/300 - Train Loss: 0.0758, Val Loss: 0.0698\n",
      "Epoch 55/300 - Train Loss: 0.0760, Val Loss: 0.0675\n",
      "Epoch 56/300 - Train Loss: 0.0751, Val Loss: 0.0702\n",
      "Epoch 57/300 - Train Loss: 0.0793, Val Loss: 0.0696\n",
      "Epoch 58/300 - Train Loss: 0.0758, Val Loss: 0.0698\n",
      "Epoch 59/300 - Train Loss: 0.0771, Val Loss: 0.0687\n",
      "Epoch 60/300 - Train Loss: 0.0766, Val Loss: 0.0697\n",
      "Epoch 61/300 - Train Loss: 0.0761, Val Loss: 0.0660\n",
      "Epoch 62/300 - Train Loss: 0.0762, Val Loss: 0.0729\n",
      "Epoch 63/300 - Train Loss: 0.0762, Val Loss: 0.0673\n",
      "Epoch 64/300 - Train Loss: 0.0767, Val Loss: 0.0655\n",
      "Epoch 65/300 - Train Loss: 0.0751, Val Loss: 0.0681\n",
      "Epoch 66/300 - Train Loss: 0.0754, Val Loss: 0.0687\n",
      "Epoch 67/300 - Train Loss: 0.0756, Val Loss: 0.0681\n",
      "Epoch 68/300 - Train Loss: 0.0730, Val Loss: 0.0652\n",
      "Epoch 69/300 - Train Loss: 0.0771, Val Loss: 0.0831\n",
      "Epoch 70/300 - Train Loss: 0.0753, Val Loss: 0.0653\n",
      "Epoch 71/300 - Train Loss: 0.0742, Val Loss: 0.0676\n",
      "Epoch 72/300 - Train Loss: 0.0741, Val Loss: 0.0722\n",
      "Epoch 73/300 - Train Loss: 0.0734, Val Loss: 0.0712\n",
      "Epoch 74/300 - Train Loss: 0.0746, Val Loss: 0.0693\n",
      "Epoch 75/300 - Train Loss: 0.0747, Val Loss: 0.0674\n",
      "Epoch 76/300 - Train Loss: 0.0733, Val Loss: 0.0693\n",
      "Epoch 77/300 - Train Loss: 0.0735, Val Loss: 0.0716\n",
      "Epoch 78/300 - Train Loss: 0.0755, Val Loss: 0.0680\n",
      "Epoch 79/300 - Train Loss: 0.0746, Val Loss: 0.0707\n",
      "Epoch 80/300 - Train Loss: 0.0744, Val Loss: 0.0701\n",
      "Epoch 81/300 - Train Loss: 0.0742, Val Loss: 0.0730\n",
      "Epoch 82/300 - Train Loss: 0.0729, Val Loss: 0.0646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:22:07,265] Trial 146 finished with value: 0.9682056844620511 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.2850950601982045, 'learning_rate': 5.3481772088573195e-05, 'batch_size': 32, 'weight_decay': 0.0008306200519630769}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/300 - Train Loss: 0.0724, Val Loss: 0.0691\n",
      "Early stopping at epoch 83\n",
      "Macro F1 Score: 0.9682, Macro Precision: 0.9643, Macro Recall: 0.9724\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 148\n",
      "Training with F1=16, F2=32, D=8, dropout=0.36107466353158724, LR=6.762460159579936e-05, BS=32, WD=0.004253724063442024\n",
      "Epoch 1/300 - Train Loss: 0.3204, Val Loss: 0.1678\n",
      "Epoch 2/300 - Train Loss: 0.1569, Val Loss: 0.1020\n",
      "Epoch 3/300 - Train Loss: 0.1185, Val Loss: 0.0933\n",
      "Epoch 4/300 - Train Loss: 0.1095, Val Loss: 0.0860\n",
      "Epoch 5/300 - Train Loss: 0.1019, Val Loss: 0.0887\n",
      "Epoch 6/300 - Train Loss: 0.0982, Val Loss: 0.0838\n",
      "Epoch 7/300 - Train Loss: 0.0997, Val Loss: 0.0753\n",
      "Epoch 8/300 - Train Loss: 0.0963, Val Loss: 0.0771\n",
      "Epoch 9/300 - Train Loss: 0.0968, Val Loss: 0.0771\n",
      "Epoch 10/300 - Train Loss: 0.0976, Val Loss: 0.0694\n",
      "Epoch 11/300 - Train Loss: 0.0944, Val Loss: 0.0740\n",
      "Epoch 12/300 - Train Loss: 0.0949, Val Loss: 0.0789\n",
      "Epoch 13/300 - Train Loss: 0.0920, Val Loss: 0.0707\n",
      "Epoch 14/300 - Train Loss: 0.0929, Val Loss: 0.0713\n",
      "Epoch 15/300 - Train Loss: 0.0937, Val Loss: 0.0710\n",
      "Epoch 16/300 - Train Loss: 0.0917, Val Loss: 0.0706\n",
      "Epoch 17/300 - Train Loss: 0.0940, Val Loss: 0.0706\n",
      "Epoch 18/300 - Train Loss: 0.0932, Val Loss: 0.0806\n",
      "Epoch 19/300 - Train Loss: 0.0961, Val Loss: 0.0751\n",
      "Epoch 20/300 - Train Loss: 0.0956, Val Loss: 0.0700\n",
      "Epoch 21/300 - Train Loss: 0.0947, Val Loss: 0.0724\n",
      "Epoch 22/300 - Train Loss: 0.0935, Val Loss: 0.0716\n",
      "Epoch 23/300 - Train Loss: 0.0930, Val Loss: 0.0764\n",
      "Epoch 24/300 - Train Loss: 0.0958, Val Loss: 0.0722\n",
      "Epoch 25/300 - Train Loss: 0.0950, Val Loss: 0.0701\n",
      "Epoch 26/300 - Train Loss: 0.0937, Val Loss: 0.0734\n",
      "Epoch 27/300 - Train Loss: 0.0928, Val Loss: 0.0716\n",
      "Epoch 28/300 - Train Loss: 0.0971, Val Loss: 0.0709\n",
      "Epoch 29/300 - Train Loss: 0.0940, Val Loss: 0.0719\n",
      "Epoch 30/300 - Train Loss: 0.0952, Val Loss: 0.0692\n",
      "Epoch 31/300 - Train Loss: 0.0966, Val Loss: 0.0806\n",
      "Epoch 32/300 - Train Loss: 0.0967, Val Loss: 0.0719\n",
      "Epoch 33/300 - Train Loss: 0.0962, Val Loss: 0.0716\n",
      "Epoch 34/300 - Train Loss: 0.0963, Val Loss: 0.0748\n",
      "Epoch 35/300 - Train Loss: 0.0974, Val Loss: 0.0840\n",
      "Epoch 36/300 - Train Loss: 0.0973, Val Loss: 0.0725\n",
      "Epoch 37/300 - Train Loss: 0.0973, Val Loss: 0.0724\n",
      "Epoch 38/300 - Train Loss: 0.0970, Val Loss: 0.0744\n",
      "Epoch 39/300 - Train Loss: 0.0990, Val Loss: 0.0733\n",
      "Epoch 40/300 - Train Loss: 0.0980, Val Loss: 0.0753\n",
      "Epoch 41/300 - Train Loss: 0.0959, Val Loss: 0.0690\n",
      "Epoch 42/300 - Train Loss: 0.0989, Val Loss: 0.0708\n",
      "Epoch 43/300 - Train Loss: 0.0976, Val Loss: 0.0748\n",
      "Epoch 44/300 - Train Loss: 0.0969, Val Loss: 0.0717\n",
      "Epoch 45/300 - Train Loss: 0.0984, Val Loss: 0.0728\n",
      "Epoch 46/300 - Train Loss: 0.0998, Val Loss: 0.0727\n",
      "Epoch 47/300 - Train Loss: 0.0981, Val Loss: 0.0690\n",
      "Epoch 48/300 - Train Loss: 0.0956, Val Loss: 0.0731\n",
      "Epoch 49/300 - Train Loss: 0.0995, Val Loss: 0.0722\n",
      "Epoch 50/300 - Train Loss: 0.0997, Val Loss: 0.0708\n",
      "Epoch 51/300 - Train Loss: 0.0999, Val Loss: 0.0748\n",
      "Epoch 52/300 - Train Loss: 0.0981, Val Loss: 0.0740\n",
      "Epoch 53/300 - Train Loss: 0.1015, Val Loss: 0.0723\n",
      "Epoch 54/300 - Train Loss: 0.0992, Val Loss: 0.0727\n",
      "Epoch 55/300 - Train Loss: 0.1021, Val Loss: 0.0723\n",
      "Epoch 56/300 - Train Loss: 0.1003, Val Loss: 0.0700\n",
      "Epoch 57/300 - Train Loss: 0.1007, Val Loss: 0.0832\n",
      "Epoch 58/300 - Train Loss: 0.0991, Val Loss: 0.0720\n",
      "Epoch 59/300 - Train Loss: 0.0988, Val Loss: 0.0736\n",
      "Epoch 60/300 - Train Loss: 0.0997, Val Loss: 0.0736\n",
      "Epoch 61/300 - Train Loss: 0.0982, Val Loss: 0.0801\n",
      "Epoch 62/300 - Train Loss: 0.0997, Val Loss: 0.0727\n",
      "Epoch 63/300 - Train Loss: 0.1022, Val Loss: 0.0729\n",
      "Epoch 64/300 - Train Loss: 0.0996, Val Loss: 0.0809\n",
      "Epoch 65/300 - Train Loss: 0.1015, Val Loss: 0.0740\n",
      "Epoch 66/300 - Train Loss: 0.1017, Val Loss: 0.0734\n",
      "Epoch 67/300 - Train Loss: 0.0994, Val Loss: 0.0772\n",
      "Epoch 68/300 - Train Loss: 0.1028, Val Loss: 0.0712\n",
      "Epoch 69/300 - Train Loss: 0.1021, Val Loss: 0.0753\n",
      "Epoch 70/300 - Train Loss: 0.1009, Val Loss: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:26:40,882] Trial 147 finished with value: 0.9684641547298704 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.36107466353158724, 'learning_rate': 6.762460159579936e-05, 'batch_size': 32, 'weight_decay': 0.004253724063442024}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300 - Train Loss: 0.1026, Val Loss: 0.0794\n",
      "Early stopping at epoch 71\n",
      "Macro F1 Score: 0.9685, Macro Precision: 0.9675, Macro Recall: 0.9697\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 149\n",
      "Training with F1=32, F2=32, D=8, dropout=0.266697132499663, LR=5.747200150044786e-05, BS=128, WD=0.0006406809570958958\n",
      "Epoch 1/300 - Train Loss: 0.4271, Val Loss: 0.2329\n",
      "Epoch 2/300 - Train Loss: 0.1888, Val Loss: 0.1558\n",
      "Epoch 3/300 - Train Loss: 0.1342, Val Loss: 0.1123\n",
      "Epoch 4/300 - Train Loss: 0.1075, Val Loss: 0.1049\n",
      "Epoch 5/300 - Train Loss: 0.0990, Val Loss: 0.0911\n",
      "Epoch 6/300 - Train Loss: 0.0939, Val Loss: 0.0952\n",
      "Epoch 7/300 - Train Loss: 0.0908, Val Loss: 0.0821\n",
      "Epoch 8/300 - Train Loss: 0.0880, Val Loss: 0.0864\n",
      "Epoch 9/300 - Train Loss: 0.0845, Val Loss: 0.0813\n",
      "Epoch 10/300 - Train Loss: 0.0841, Val Loss: 0.0779\n",
      "Epoch 11/300 - Train Loss: 0.0838, Val Loss: 0.0841\n",
      "Epoch 12/300 - Train Loss: 0.0825, Val Loss: 0.0821\n",
      "Epoch 13/300 - Train Loss: 0.0803, Val Loss: 0.0821\n",
      "Epoch 14/300 - Train Loss: 0.0801, Val Loss: 0.0865\n",
      "Epoch 15/300 - Train Loss: 0.0798, Val Loss: 0.0780\n",
      "Epoch 16/300 - Train Loss: 0.0791, Val Loss: 0.0776\n",
      "Epoch 17/300 - Train Loss: 0.0778, Val Loss: 0.0881\n",
      "Epoch 18/300 - Train Loss: 0.0760, Val Loss: 0.0762\n",
      "Epoch 19/300 - Train Loss: 0.0774, Val Loss: 0.0813\n",
      "Epoch 20/300 - Train Loss: 0.0757, Val Loss: 0.0759\n",
      "Epoch 21/300 - Train Loss: 0.0757, Val Loss: 0.0772\n",
      "Epoch 22/300 - Train Loss: 0.0743, Val Loss: 0.0779\n",
      "Epoch 23/300 - Train Loss: 0.0763, Val Loss: 0.0795\n",
      "Epoch 24/300 - Train Loss: 0.0739, Val Loss: 0.0796\n",
      "Epoch 25/300 - Train Loss: 0.0753, Val Loss: 0.0745\n",
      "Epoch 26/300 - Train Loss: 0.0730, Val Loss: 0.0714\n",
      "Epoch 27/300 - Train Loss: 0.0720, Val Loss: 0.0764\n",
      "Epoch 28/300 - Train Loss: 0.0741, Val Loss: 0.0748\n",
      "Epoch 29/300 - Train Loss: 0.0727, Val Loss: 0.0752\n",
      "Epoch 30/300 - Train Loss: 0.0715, Val Loss: 0.0744\n",
      "Epoch 31/300 - Train Loss: 0.0715, Val Loss: 0.0743\n",
      "Epoch 32/300 - Train Loss: 0.0716, Val Loss: 0.0713\n",
      "Epoch 33/300 - Train Loss: 0.0714, Val Loss: 0.0729\n",
      "Epoch 34/300 - Train Loss: 0.0711, Val Loss: 0.0718\n",
      "Epoch 35/300 - Train Loss: 0.0692, Val Loss: 0.0707\n",
      "Epoch 36/300 - Train Loss: 0.0692, Val Loss: 0.0726\n",
      "Epoch 37/300 - Train Loss: 0.0699, Val Loss: 0.0724\n",
      "Epoch 38/300 - Train Loss: 0.0689, Val Loss: 0.0699\n",
      "Epoch 39/300 - Train Loss: 0.0701, Val Loss: 0.0727\n",
      "Epoch 40/300 - Train Loss: 0.0687, Val Loss: 0.0701\n",
      "Epoch 41/300 - Train Loss: 0.0684, Val Loss: 0.0747\n",
      "Epoch 42/300 - Train Loss: 0.0679, Val Loss: 0.0694\n",
      "Epoch 43/300 - Train Loss: 0.0675, Val Loss: 0.0744\n",
      "Epoch 44/300 - Train Loss: 0.0679, Val Loss: 0.0721\n",
      "Epoch 45/300 - Train Loss: 0.0671, Val Loss: 0.0709\n",
      "Epoch 46/300 - Train Loss: 0.0671, Val Loss: 0.0734\n",
      "Epoch 47/300 - Train Loss: 0.0672, Val Loss: 0.0714\n",
      "Epoch 48/300 - Train Loss: 0.0654, Val Loss: 0.0697\n",
      "Epoch 49/300 - Train Loss: 0.0666, Val Loss: 0.0708\n",
      "Epoch 50/300 - Train Loss: 0.0676, Val Loss: 0.0697\n",
      "Epoch 51/300 - Train Loss: 0.0653, Val Loss: 0.0683\n",
      "Epoch 52/300 - Train Loss: 0.0660, Val Loss: 0.0688\n",
      "Epoch 53/300 - Train Loss: 0.0656, Val Loss: 0.0696\n",
      "Epoch 54/300 - Train Loss: 0.0660, Val Loss: 0.0705\n",
      "Epoch 55/300 - Train Loss: 0.0651, Val Loss: 0.0757\n",
      "Epoch 56/300 - Train Loss: 0.0659, Val Loss: 0.0689\n",
      "Epoch 57/300 - Train Loss: 0.0648, Val Loss: 0.0675\n",
      "Epoch 58/300 - Train Loss: 0.0652, Val Loss: 0.0694\n",
      "Epoch 59/300 - Train Loss: 0.0655, Val Loss: 0.0674\n",
      "Epoch 60/300 - Train Loss: 0.0641, Val Loss: 0.0701\n",
      "Epoch 61/300 - Train Loss: 0.0636, Val Loss: 0.0742\n",
      "Epoch 62/300 - Train Loss: 0.0643, Val Loss: 0.0680\n",
      "Epoch 63/300 - Train Loss: 0.0639, Val Loss: 0.0696\n",
      "Epoch 64/300 - Train Loss: 0.0637, Val Loss: 0.0712\n",
      "Epoch 65/300 - Train Loss: 0.0641, Val Loss: 0.0686\n",
      "Epoch 66/300 - Train Loss: 0.0630, Val Loss: 0.0703\n",
      "Epoch 67/300 - Train Loss: 0.0630, Val Loss: 0.0667\n",
      "Epoch 68/300 - Train Loss: 0.0636, Val Loss: 0.0661\n",
      "Epoch 69/300 - Train Loss: 0.0638, Val Loss: 0.0706\n",
      "Epoch 70/300 - Train Loss: 0.0632, Val Loss: 0.0679\n",
      "Epoch 71/300 - Train Loss: 0.0625, Val Loss: 0.0691\n",
      "Epoch 72/300 - Train Loss: 0.0626, Val Loss: 0.0676\n",
      "Epoch 73/300 - Train Loss: 0.0622, Val Loss: 0.0719\n",
      "Epoch 74/300 - Train Loss: 0.0616, Val Loss: 0.0669\n",
      "Epoch 75/300 - Train Loss: 0.0626, Val Loss: 0.0695\n",
      "Epoch 76/300 - Train Loss: 0.0619, Val Loss: 0.0668\n",
      "Epoch 77/300 - Train Loss: 0.0611, Val Loss: 0.0675\n",
      "Epoch 78/300 - Train Loss: 0.0627, Val Loss: 0.0676\n",
      "Epoch 79/300 - Train Loss: 0.0617, Val Loss: 0.0652\n",
      "Epoch 80/300 - Train Loss: 0.0622, Val Loss: 0.0662\n",
      "Epoch 81/300 - Train Loss: 0.0611, Val Loss: 0.0777\n",
      "Epoch 82/300 - Train Loss: 0.0613, Val Loss: 0.0667\n",
      "Epoch 83/300 - Train Loss: 0.0607, Val Loss: 0.0684\n",
      "Epoch 84/300 - Train Loss: 0.0606, Val Loss: 0.0651\n",
      "Epoch 85/300 - Train Loss: 0.0614, Val Loss: 0.0681\n",
      "Epoch 86/300 - Train Loss: 0.0616, Val Loss: 0.0652\n",
      "Epoch 87/300 - Train Loss: 0.0601, Val Loss: 0.0669\n",
      "Epoch 88/300 - Train Loss: 0.0606, Val Loss: 0.0672\n",
      "Epoch 89/300 - Train Loss: 0.0610, Val Loss: 0.0663\n",
      "Epoch 90/300 - Train Loss: 0.0606, Val Loss: 0.0650\n",
      "Epoch 91/300 - Train Loss: 0.0599, Val Loss: 0.0670\n",
      "Epoch 92/300 - Train Loss: 0.0609, Val Loss: 0.0683\n",
      "Epoch 93/300 - Train Loss: 0.0599, Val Loss: 0.0697\n",
      "Epoch 94/300 - Train Loss: 0.0595, Val Loss: 0.0657\n",
      "Epoch 95/300 - Train Loss: 0.0605, Val Loss: 0.0701\n",
      "Epoch 96/300 - Train Loss: 0.0575, Val Loss: 0.0678\n",
      "Epoch 97/300 - Train Loss: 0.0606, Val Loss: 0.0687\n",
      "Epoch 98/300 - Train Loss: 0.0599, Val Loss: 0.0663\n",
      "Epoch 99/300 - Train Loss: 0.0598, Val Loss: 0.0686\n",
      "Epoch 100/300 - Train Loss: 0.0596, Val Loss: 0.0666\n",
      "Epoch 101/300 - Train Loss: 0.0585, Val Loss: 0.0693\n",
      "Epoch 102/300 - Train Loss: 0.0580, Val Loss: 0.0677\n",
      "Epoch 103/300 - Train Loss: 0.0572, Val Loss: 0.0682\n",
      "Epoch 104/300 - Train Loss: 0.0593, Val Loss: 0.0670\n",
      "Epoch 105/300 - Train Loss: 0.0580, Val Loss: 0.0674\n",
      "Epoch 106/300 - Train Loss: 0.0583, Val Loss: 0.0693\n",
      "Epoch 107/300 - Train Loss: 0.0577, Val Loss: 0.0671\n",
      "Epoch 108/300 - Train Loss: 0.0584, Val Loss: 0.0672\n",
      "Epoch 109/300 - Train Loss: 0.0571, Val Loss: 0.0694\n",
      "Epoch 110/300 - Train Loss: 0.0584, Val Loss: 0.0698\n",
      "Epoch 111/300 - Train Loss: 0.0575, Val Loss: 0.0661\n",
      "Epoch 112/300 - Train Loss: 0.0575, Val Loss: 0.0669\n",
      "Epoch 113/300 - Train Loss: 0.0589, Val Loss: 0.0677\n",
      "Epoch 114/300 - Train Loss: 0.0580, Val Loss: 0.0698\n",
      "Epoch 115/300 - Train Loss: 0.0567, Val Loss: 0.0677\n",
      "Epoch 116/300 - Train Loss: 0.0584, Val Loss: 0.0685\n",
      "Epoch 117/300 - Train Loss: 0.0588, Val Loss: 0.0686\n",
      "Epoch 118/300 - Train Loss: 0.0578, Val Loss: 0.0685\n",
      "Epoch 119/300 - Train Loss: 0.0557, Val Loss: 0.0663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:37:49,772] Trial 148 finished with value: 0.9663648185471968 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.266697132499663, 'learning_rate': 5.747200150044786e-05, 'batch_size': 128, 'weight_decay': 0.0006406809570958958}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300 - Train Loss: 0.0571, Val Loss: 0.0661\n",
      "Early stopping at epoch 120\n",
      "Macro F1 Score: 0.9664, Macro Precision: 0.9562, Macro Recall: 0.9775\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 150\n",
      "Training with F1=16, F2=32, D=2, dropout=0.29743530800468054, LR=3.604509818225574e-05, BS=32, WD=0.006417885434106005\n",
      "Epoch 1/300 - Train Loss: 0.5213, Val Loss: 0.2569\n",
      "Epoch 2/300 - Train Loss: 0.2348, Val Loss: 0.1796\n",
      "Epoch 3/300 - Train Loss: 0.1762, Val Loss: 0.1392\n",
      "Epoch 4/300 - Train Loss: 0.1461, Val Loss: 0.1099\n",
      "Epoch 5/300 - Train Loss: 0.1301, Val Loss: 0.0949\n",
      "Epoch 6/300 - Train Loss: 0.1201, Val Loss: 0.0983\n",
      "Epoch 7/300 - Train Loss: 0.1130, Val Loss: 0.0920\n",
      "Epoch 8/300 - Train Loss: 0.1116, Val Loss: 0.0899\n",
      "Epoch 9/300 - Train Loss: 0.1062, Val Loss: 0.0840\n",
      "Epoch 10/300 - Train Loss: 0.1066, Val Loss: 0.0811\n",
      "Epoch 11/300 - Train Loss: 0.1050, Val Loss: 0.0864\n",
      "Epoch 12/300 - Train Loss: 0.1040, Val Loss: 0.0801\n",
      "Epoch 13/300 - Train Loss: 0.1037, Val Loss: 0.0819\n",
      "Epoch 14/300 - Train Loss: 0.1052, Val Loss: 0.0826\n",
      "Epoch 15/300 - Train Loss: 0.1019, Val Loss: 0.0813\n",
      "Epoch 16/300 - Train Loss: 0.1039, Val Loss: 0.0790\n",
      "Epoch 17/300 - Train Loss: 0.1023, Val Loss: 0.0776\n",
      "Epoch 18/300 - Train Loss: 0.1026, Val Loss: 0.0748\n",
      "Epoch 19/300 - Train Loss: 0.1009, Val Loss: 0.0740\n",
      "Epoch 20/300 - Train Loss: 0.0992, Val Loss: 0.0797\n",
      "Epoch 21/300 - Train Loss: 0.1001, Val Loss: 0.0761\n",
      "Epoch 22/300 - Train Loss: 0.0998, Val Loss: 0.0773\n",
      "Epoch 23/300 - Train Loss: 0.1017, Val Loss: 0.0726\n",
      "Epoch 24/300 - Train Loss: 0.1002, Val Loss: 0.0729\n",
      "Epoch 25/300 - Train Loss: 0.1002, Val Loss: 0.0754\n",
      "Epoch 26/300 - Train Loss: 0.1009, Val Loss: 0.0799\n",
      "Epoch 27/300 - Train Loss: 0.0982, Val Loss: 0.0756\n",
      "Epoch 28/300 - Train Loss: 0.1014, Val Loss: 0.0765\n",
      "Epoch 29/300 - Train Loss: 0.1028, Val Loss: 0.0730\n",
      "Epoch 30/300 - Train Loss: 0.0985, Val Loss: 0.0744\n",
      "Epoch 31/300 - Train Loss: 0.1010, Val Loss: 0.0725\n",
      "Epoch 32/300 - Train Loss: 0.0999, Val Loss: 0.0771\n",
      "Epoch 33/300 - Train Loss: 0.1012, Val Loss: 0.0754\n",
      "Epoch 34/300 - Train Loss: 0.1015, Val Loss: 0.0751\n",
      "Epoch 35/300 - Train Loss: 0.1004, Val Loss: 0.0727\n",
      "Epoch 36/300 - Train Loss: 0.0991, Val Loss: 0.0713\n",
      "Epoch 37/300 - Train Loss: 0.0990, Val Loss: 0.0746\n",
      "Epoch 38/300 - Train Loss: 0.1007, Val Loss: 0.0712\n",
      "Epoch 39/300 - Train Loss: 0.0985, Val Loss: 0.0740\n",
      "Epoch 40/300 - Train Loss: 0.1006, Val Loss: 0.0736\n",
      "Epoch 41/300 - Train Loss: 0.0995, Val Loss: 0.0699\n",
      "Epoch 42/300 - Train Loss: 0.0988, Val Loss: 0.0718\n",
      "Epoch 43/300 - Train Loss: 0.0955, Val Loss: 0.0757\n",
      "Epoch 44/300 - Train Loss: 0.1004, Val Loss: 0.0754\n",
      "Epoch 45/300 - Train Loss: 0.1002, Val Loss: 0.0712\n",
      "Epoch 46/300 - Train Loss: 0.0997, Val Loss: 0.0749\n",
      "Epoch 47/300 - Train Loss: 0.1008, Val Loss: 0.0909\n",
      "Epoch 48/300 - Train Loss: 0.0984, Val Loss: 0.0761\n",
      "Epoch 49/300 - Train Loss: 0.0979, Val Loss: 0.0713\n",
      "Epoch 50/300 - Train Loss: 0.1000, Val Loss: 0.0699\n",
      "Epoch 51/300 - Train Loss: 0.1005, Val Loss: 0.0758\n",
      "Epoch 52/300 - Train Loss: 0.0970, Val Loss: 0.0708\n",
      "Epoch 53/300 - Train Loss: 0.0986, Val Loss: 0.0698\n",
      "Epoch 54/300 - Train Loss: 0.0984, Val Loss: 0.0728\n",
      "Epoch 55/300 - Train Loss: 0.0995, Val Loss: 0.0704\n",
      "Epoch 56/300 - Train Loss: 0.0998, Val Loss: 0.0703\n",
      "Epoch 57/300 - Train Loss: 0.0982, Val Loss: 0.0723\n",
      "Epoch 58/300 - Train Loss: 0.1009, Val Loss: 0.0701\n",
      "Epoch 59/300 - Train Loss: 0.0981, Val Loss: 0.0699\n",
      "Epoch 60/300 - Train Loss: 0.1022, Val Loss: 0.0752\n",
      "Epoch 61/300 - Train Loss: 0.0981, Val Loss: 0.0733\n",
      "Epoch 62/300 - Train Loss: 0.0992, Val Loss: 0.0707\n",
      "Epoch 63/300 - Train Loss: 0.0998, Val Loss: 0.0723\n",
      "Epoch 64/300 - Train Loss: 0.0996, Val Loss: 0.0706\n",
      "Epoch 65/300 - Train Loss: 0.0996, Val Loss: 0.0723\n",
      "Epoch 66/300 - Train Loss: 0.0969, Val Loss: 0.0735\n",
      "Epoch 67/300 - Train Loss: 0.1002, Val Loss: 0.0708\n",
      "Epoch 68/300 - Train Loss: 0.1016, Val Loss: 0.0709\n",
      "Epoch 69/300 - Train Loss: 0.0998, Val Loss: 0.0703\n",
      "Epoch 70/300 - Train Loss: 0.0992, Val Loss: 0.0713\n",
      "Epoch 71/300 - Train Loss: 0.1003, Val Loss: 0.0716\n",
      "Epoch 72/300 - Train Loss: 0.1015, Val Loss: 0.0690\n",
      "Epoch 73/300 - Train Loss: 0.1050, Val Loss: 0.0693\n",
      "Epoch 74/300 - Train Loss: 0.1006, Val Loss: 0.0735\n",
      "Epoch 75/300 - Train Loss: 0.1025, Val Loss: 0.0692\n",
      "Epoch 76/300 - Train Loss: 0.0991, Val Loss: 0.0725\n",
      "Epoch 77/300 - Train Loss: 0.1006, Val Loss: 0.0716\n",
      "Epoch 78/300 - Train Loss: 0.1013, Val Loss: 0.0738\n",
      "Epoch 79/300 - Train Loss: 0.1006, Val Loss: 0.0734\n",
      "Epoch 80/300 - Train Loss: 0.0990, Val Loss: 0.0744\n",
      "Epoch 81/300 - Train Loss: 0.0987, Val Loss: 0.0717\n",
      "Epoch 82/300 - Train Loss: 0.1029, Val Loss: 0.0716\n",
      "Epoch 83/300 - Train Loss: 0.1017, Val Loss: 0.0763\n",
      "Epoch 84/300 - Train Loss: 0.1007, Val Loss: 0.0741\n",
      "Epoch 85/300 - Train Loss: 0.1022, Val Loss: 0.0805\n",
      "Epoch 86/300 - Train Loss: 0.1022, Val Loss: 0.0702\n",
      "Epoch 87/300 - Train Loss: 0.1021, Val Loss: 0.0717\n",
      "Epoch 88/300 - Train Loss: 0.0996, Val Loss: 0.0725\n",
      "Epoch 89/300 - Train Loss: 0.0983, Val Loss: 0.0711\n",
      "Epoch 90/300 - Train Loss: 0.1010, Val Loss: 0.0724\n",
      "Epoch 91/300 - Train Loss: 0.1035, Val Loss: 0.0733\n",
      "Epoch 92/300 - Train Loss: 0.1006, Val Loss: 0.0724\n",
      "Epoch 93/300 - Train Loss: 0.1014, Val Loss: 0.0718\n",
      "Epoch 94/300 - Train Loss: 0.1000, Val Loss: 0.0741\n",
      "Epoch 95/300 - Train Loss: 0.1004, Val Loss: 0.0723\n",
      "Epoch 96/300 - Train Loss: 0.1053, Val Loss: 0.0730\n",
      "Epoch 97/300 - Train Loss: 0.1002, Val Loss: 0.0695\n",
      "Epoch 98/300 - Train Loss: 0.1029, Val Loss: 0.0711\n",
      "Epoch 99/300 - Train Loss: 0.1011, Val Loss: 0.0734\n",
      "Epoch 100/300 - Train Loss: 0.1010, Val Loss: 0.0766\n",
      "Epoch 101/300 - Train Loss: 0.1043, Val Loss: 0.0699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:41:25,389] Trial 149 finished with value: 0.9648126226213245 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.29743530800468054, 'learning_rate': 3.604509818225574e-05, 'batch_size': 32, 'weight_decay': 0.006417885434106005}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/300 - Train Loss: 0.1011, Val Loss: 0.0734\n",
      "Early stopping at epoch 102\n",
      "Macro F1 Score: 0.9648, Macro Precision: 0.9658, Macro Recall: 0.9639\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 151\n",
      "Training with F1=8, F2=8, D=8, dropout=0.31674025714204235, LR=8.737449236256824e-05, BS=32, WD=0.00022049277406819488\n",
      "Epoch 1/300 - Train Loss: 0.4226, Val Loss: 0.1842\n",
      "Epoch 2/300 - Train Loss: 0.1882, Val Loss: 0.1310\n",
      "Epoch 3/300 - Train Loss: 0.1429, Val Loss: 0.1003\n",
      "Epoch 4/300 - Train Loss: 0.1248, Val Loss: 0.0939\n",
      "Epoch 5/300 - Train Loss: 0.1178, Val Loss: 0.0867\n",
      "Epoch 6/300 - Train Loss: 0.1135, Val Loss: 0.0845\n",
      "Epoch 7/300 - Train Loss: 0.1097, Val Loss: 0.0839\n",
      "Epoch 8/300 - Train Loss: 0.1064, Val Loss: 0.0850\n",
      "Epoch 9/300 - Train Loss: 0.1042, Val Loss: 0.0776\n",
      "Epoch 10/300 - Train Loss: 0.1032, Val Loss: 0.0850\n",
      "Epoch 11/300 - Train Loss: 0.1001, Val Loss: 0.0768\n",
      "Epoch 12/300 - Train Loss: 0.0979, Val Loss: 0.0738\n",
      "Epoch 13/300 - Train Loss: 0.0978, Val Loss: 0.0751\n",
      "Epoch 14/300 - Train Loss: 0.0936, Val Loss: 0.0718\n",
      "Epoch 15/300 - Train Loss: 0.0973, Val Loss: 0.0773\n",
      "Epoch 16/300 - Train Loss: 0.0933, Val Loss: 0.0710\n",
      "Epoch 17/300 - Train Loss: 0.0917, Val Loss: 0.0724\n",
      "Epoch 18/300 - Train Loss: 0.0923, Val Loss: 0.0709\n",
      "Epoch 19/300 - Train Loss: 0.0921, Val Loss: 0.0754\n",
      "Epoch 20/300 - Train Loss: 0.0917, Val Loss: 0.0709\n",
      "Epoch 21/300 - Train Loss: 0.0903, Val Loss: 0.0702\n",
      "Epoch 22/300 - Train Loss: 0.0887, Val Loss: 0.0682\n",
      "Epoch 23/300 - Train Loss: 0.0923, Val Loss: 0.0725\n",
      "Epoch 24/300 - Train Loss: 0.0891, Val Loss: 0.0703\n",
      "Epoch 25/300 - Train Loss: 0.0898, Val Loss: 0.0719\n",
      "Epoch 26/300 - Train Loss: 0.0877, Val Loss: 0.0685\n",
      "Epoch 27/300 - Train Loss: 0.0872, Val Loss: 0.0741\n",
      "Epoch 28/300 - Train Loss: 0.0900, Val Loss: 0.0716\n",
      "Epoch 29/300 - Train Loss: 0.0880, Val Loss: 0.0714\n",
      "Epoch 30/300 - Train Loss: 0.0900, Val Loss: 0.0813\n",
      "Epoch 31/300 - Train Loss: 0.0871, Val Loss: 0.0672\n",
      "Epoch 32/300 - Train Loss: 0.0883, Val Loss: 0.0697\n",
      "Epoch 33/300 - Train Loss: 0.0864, Val Loss: 0.0682\n",
      "Epoch 34/300 - Train Loss: 0.0863, Val Loss: 0.0696\n",
      "Epoch 35/300 - Train Loss: 0.0875, Val Loss: 0.0708\n",
      "Epoch 36/300 - Train Loss: 0.0861, Val Loss: 0.0695\n",
      "Epoch 37/300 - Train Loss: 0.0856, Val Loss: 0.0709\n",
      "Epoch 38/300 - Train Loss: 0.0847, Val Loss: 0.0703\n",
      "Epoch 39/300 - Train Loss: 0.0836, Val Loss: 0.0687\n",
      "Epoch 40/300 - Train Loss: 0.0836, Val Loss: 0.0710\n",
      "Epoch 41/300 - Train Loss: 0.0874, Val Loss: 0.0766\n",
      "Epoch 42/300 - Train Loss: 0.0850, Val Loss: 0.0699\n",
      "Epoch 43/300 - Train Loss: 0.0846, Val Loss: 0.0675\n",
      "Epoch 44/300 - Train Loss: 0.0832, Val Loss: 0.0715\n",
      "Epoch 45/300 - Train Loss: 0.0823, Val Loss: 0.0685\n",
      "Epoch 46/300 - Train Loss: 0.0861, Val Loss: 0.0674\n",
      "Epoch 47/300 - Train Loss: 0.0841, Val Loss: 0.0695\n",
      "Epoch 48/300 - Train Loss: 0.0833, Val Loss: 0.0660\n",
      "Epoch 49/300 - Train Loss: 0.0855, Val Loss: 0.0661\n",
      "Epoch 50/300 - Train Loss: 0.0852, Val Loss: 0.0687\n",
      "Epoch 51/300 - Train Loss: 0.0862, Val Loss: 0.0713\n",
      "Epoch 52/300 - Train Loss: 0.0826, Val Loss: 0.0679\n",
      "Epoch 53/300 - Train Loss: 0.0825, Val Loss: 0.0706\n",
      "Epoch 54/300 - Train Loss: 0.0853, Val Loss: 0.0716\n",
      "Epoch 55/300 - Train Loss: 0.0826, Val Loss: 0.0682\n",
      "Epoch 56/300 - Train Loss: 0.0817, Val Loss: 0.0679\n",
      "Epoch 57/300 - Train Loss: 0.0861, Val Loss: 0.0729\n",
      "Epoch 58/300 - Train Loss: 0.0817, Val Loss: 0.0697\n",
      "Epoch 59/300 - Train Loss: 0.0834, Val Loss: 0.0683\n",
      "Epoch 60/300 - Train Loss: 0.0823, Val Loss: 0.0662\n",
      "Epoch 61/300 - Train Loss: 0.0837, Val Loss: 0.0702\n",
      "Epoch 62/300 - Train Loss: 0.0822, Val Loss: 0.0701\n",
      "Epoch 63/300 - Train Loss: 0.0814, Val Loss: 0.0676\n",
      "Epoch 64/300 - Train Loss: 0.0816, Val Loss: 0.0661\n",
      "Epoch 65/300 - Train Loss: 0.0826, Val Loss: 0.0702\n",
      "Epoch 66/300 - Train Loss: 0.0807, Val Loss: 0.0670\n",
      "Epoch 67/300 - Train Loss: 0.0813, Val Loss: 0.0645\n",
      "Epoch 68/300 - Train Loss: 0.0830, Val Loss: 0.0709\n",
      "Epoch 69/300 - Train Loss: 0.0853, Val Loss: 0.0673\n",
      "Epoch 70/300 - Train Loss: 0.0796, Val Loss: 0.0672\n",
      "Epoch 71/300 - Train Loss: 0.0811, Val Loss: 0.0729\n",
      "Epoch 72/300 - Train Loss: 0.0802, Val Loss: 0.0718\n",
      "Epoch 73/300 - Train Loss: 0.0809, Val Loss: 0.0687\n",
      "Epoch 74/300 - Train Loss: 0.0821, Val Loss: 0.0666\n",
      "Epoch 75/300 - Train Loss: 0.0820, Val Loss: 0.0670\n",
      "Epoch 76/300 - Train Loss: 0.0810, Val Loss: 0.0658\n",
      "Epoch 77/300 - Train Loss: 0.0838, Val Loss: 0.0666\n",
      "Epoch 78/300 - Train Loss: 0.0807, Val Loss: 0.0687\n",
      "Epoch 79/300 - Train Loss: 0.0800, Val Loss: 0.0682\n",
      "Epoch 80/300 - Train Loss: 0.0799, Val Loss: 0.0731\n",
      "Epoch 81/300 - Train Loss: 0.0788, Val Loss: 0.0721\n",
      "Epoch 82/300 - Train Loss: 0.0812, Val Loss: 0.0671\n",
      "Epoch 83/300 - Train Loss: 0.0805, Val Loss: 0.0653\n",
      "Epoch 84/300 - Train Loss: 0.0823, Val Loss: 0.0708\n",
      "Epoch 85/300 - Train Loss: 0.0810, Val Loss: 0.0678\n",
      "Epoch 86/300 - Train Loss: 0.0818, Val Loss: 0.0664\n",
      "Epoch 87/300 - Train Loss: 0.0791, Val Loss: 0.0643\n",
      "Epoch 88/300 - Train Loss: 0.0811, Val Loss: 0.0710\n",
      "Epoch 89/300 - Train Loss: 0.0789, Val Loss: 0.0703\n",
      "Epoch 90/300 - Train Loss: 0.0801, Val Loss: 0.0684\n",
      "Epoch 91/300 - Train Loss: 0.0792, Val Loss: 0.0698\n",
      "Epoch 92/300 - Train Loss: 0.0821, Val Loss: 0.0693\n",
      "Epoch 93/300 - Train Loss: 0.0797, Val Loss: 0.0683\n",
      "Epoch 94/300 - Train Loss: 0.0798, Val Loss: 0.0667\n",
      "Epoch 95/300 - Train Loss: 0.0799, Val Loss: 0.0687\n",
      "Epoch 96/300 - Train Loss: 0.0797, Val Loss: 0.0655\n",
      "Epoch 97/300 - Train Loss: 0.0829, Val Loss: 0.0680\n",
      "Epoch 98/300 - Train Loss: 0.0819, Val Loss: 0.0699\n",
      "Epoch 99/300 - Train Loss: 0.0805, Val Loss: 0.0660\n",
      "Epoch 100/300 - Train Loss: 0.0779, Val Loss: 0.0646\n",
      "Epoch 101/300 - Train Loss: 0.0797, Val Loss: 0.0649\n",
      "Epoch 102/300 - Train Loss: 0.0804, Val Loss: 0.0718\n",
      "Epoch 103/300 - Train Loss: 0.0778, Val Loss: 0.0665\n",
      "Epoch 104/300 - Train Loss: 0.0793, Val Loss: 0.0673\n",
      "Epoch 105/300 - Train Loss: 0.0767, Val Loss: 0.0700\n",
      "Epoch 106/300 - Train Loss: 0.0792, Val Loss: 0.0671\n",
      "Epoch 107/300 - Train Loss: 0.0797, Val Loss: 0.0669\n",
      "Epoch 108/300 - Train Loss: 0.0799, Val Loss: 0.0662\n",
      "Epoch 109/300 - Train Loss: 0.0779, Val Loss: 0.0684\n",
      "Epoch 110/300 - Train Loss: 0.0785, Val Loss: 0.0672\n",
      "Epoch 111/300 - Train Loss: 0.0792, Val Loss: 0.0687\n",
      "Epoch 112/300 - Train Loss: 0.0753, Val Loss: 0.0765\n",
      "Epoch 113/300 - Train Loss: 0.0781, Val Loss: 0.0668\n",
      "Epoch 114/300 - Train Loss: 0.0775, Val Loss: 0.0666\n",
      "Epoch 115/300 - Train Loss: 0.0781, Val Loss: 0.0696\n",
      "Epoch 116/300 - Train Loss: 0.0767, Val Loss: 0.0734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:45:53,950] Trial 150 finished with value: 0.9698398821147807 and parameters: {'F1': 8, 'F2': 8, 'D': 8, 'dropout': 0.31674025714204235, 'learning_rate': 8.737449236256824e-05, 'batch_size': 32, 'weight_decay': 0.00022049277406819488}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/300 - Train Loss: 0.0772, Val Loss: 0.0665\n",
      "Early stopping at epoch 117\n",
      "Macro F1 Score: 0.9698, Macro Precision: 0.9638, Macro Recall: 0.9763\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 152\n",
      "Training with F1=16, F2=32, D=8, dropout=0.27772084816098286, LR=7.58329757663527e-05, BS=32, WD=0.009216025713414077\n",
      "Epoch 1/300 - Train Loss: 0.3059, Val Loss: 0.1720\n",
      "Epoch 2/300 - Train Loss: 0.1484, Val Loss: 0.1179\n",
      "Epoch 3/300 - Train Loss: 0.1099, Val Loss: 0.0860\n",
      "Epoch 4/300 - Train Loss: 0.1025, Val Loss: 0.0786\n",
      "Epoch 5/300 - Train Loss: 0.0990, Val Loss: 0.0762\n",
      "Epoch 6/300 - Train Loss: 0.0958, Val Loss: 0.0779\n",
      "Epoch 7/300 - Train Loss: 0.0970, Val Loss: 0.0816\n",
      "Epoch 8/300 - Train Loss: 0.0971, Val Loss: 0.0882\n",
      "Epoch 9/300 - Train Loss: 0.0956, Val Loss: 0.0800\n",
      "Epoch 10/300 - Train Loss: 0.0993, Val Loss: 0.0770\n",
      "Epoch 11/300 - Train Loss: 0.0959, Val Loss: 0.0742\n",
      "Epoch 12/300 - Train Loss: 0.0976, Val Loss: 0.0791\n",
      "Epoch 13/300 - Train Loss: 0.0970, Val Loss: 0.0748\n",
      "Epoch 14/300 - Train Loss: 0.0999, Val Loss: 0.0750\n",
      "Epoch 15/300 - Train Loss: 0.1008, Val Loss: 0.0721\n",
      "Epoch 16/300 - Train Loss: 0.0996, Val Loss: 0.0780\n",
      "Epoch 17/300 - Train Loss: 0.0996, Val Loss: 0.0750\n",
      "Epoch 18/300 - Train Loss: 0.1004, Val Loss: 0.0833\n",
      "Epoch 19/300 - Train Loss: 0.0993, Val Loss: 0.0719\n",
      "Epoch 20/300 - Train Loss: 0.1016, Val Loss: 0.0758\n",
      "Epoch 21/300 - Train Loss: 0.1013, Val Loss: 0.0757\n",
      "Epoch 22/300 - Train Loss: 0.1030, Val Loss: 0.0732\n",
      "Epoch 23/300 - Train Loss: 0.1025, Val Loss: 0.0749\n",
      "Epoch 24/300 - Train Loss: 0.1049, Val Loss: 0.0792\n",
      "Epoch 25/300 - Train Loss: 0.1030, Val Loss: 0.0747\n",
      "Epoch 26/300 - Train Loss: 0.1024, Val Loss: 0.0778\n",
      "Epoch 27/300 - Train Loss: 0.1028, Val Loss: 0.0812\n",
      "Epoch 28/300 - Train Loss: 0.1062, Val Loss: 0.0755\n",
      "Epoch 29/300 - Train Loss: 0.1053, Val Loss: 0.0780\n",
      "Epoch 30/300 - Train Loss: 0.1038, Val Loss: 0.0751\n",
      "Epoch 31/300 - Train Loss: 0.1044, Val Loss: 0.0728\n",
      "Epoch 32/300 - Train Loss: 0.1054, Val Loss: 0.0738\n",
      "Epoch 33/300 - Train Loss: 0.1066, Val Loss: 0.0782\n",
      "Epoch 34/300 - Train Loss: 0.1070, Val Loss: 0.0775\n",
      "Epoch 35/300 - Train Loss: 0.1051, Val Loss: 0.0807\n",
      "Epoch 36/300 - Train Loss: 0.1053, Val Loss: 0.0833\n",
      "Epoch 37/300 - Train Loss: 0.1063, Val Loss: 0.0747\n",
      "Epoch 38/300 - Train Loss: 0.1073, Val Loss: 0.0936\n",
      "Epoch 39/300 - Train Loss: 0.1105, Val Loss: 0.0765\n",
      "Epoch 40/300 - Train Loss: 0.1055, Val Loss: 0.0746\n",
      "Epoch 41/300 - Train Loss: 0.1092, Val Loss: 0.0762\n",
      "Epoch 42/300 - Train Loss: 0.1076, Val Loss: 0.0745\n",
      "Epoch 43/300 - Train Loss: 0.1083, Val Loss: 0.0916\n",
      "Epoch 44/300 - Train Loss: 0.1067, Val Loss: 0.0900\n",
      "Epoch 45/300 - Train Loss: 0.1114, Val Loss: 0.0768\n",
      "Epoch 46/300 - Train Loss: 0.1062, Val Loss: 0.0739\n",
      "Epoch 47/300 - Train Loss: 0.1130, Val Loss: 0.0748\n",
      "Epoch 48/300 - Train Loss: 0.1078, Val Loss: 0.0754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:49:02,909] Trial 151 finished with value: 0.9684587511163011 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.27772084816098286, 'learning_rate': 7.58329757663527e-05, 'batch_size': 32, 'weight_decay': 0.009216025713414077}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300 - Train Loss: 0.1086, Val Loss: 0.0764\n",
      "Early stopping at epoch 49\n",
      "Macro F1 Score: 0.9685, Macro Precision: 0.9677, Macro Recall: 0.9696\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 153\n",
      "Training with F1=16, F2=32, D=8, dropout=0.26106335881110554, LR=4.8375381026960793e-05, BS=32, WD=0.0077310662505103555\n",
      "Epoch 1/300 - Train Loss: 0.3544, Val Loss: 0.1441\n",
      "Epoch 2/300 - Train Loss: 0.1464, Val Loss: 0.1030\n",
      "Epoch 3/300 - Train Loss: 0.1200, Val Loss: 0.1143\n",
      "Epoch 4/300 - Train Loss: 0.1092, Val Loss: 0.0819\n",
      "Epoch 5/300 - Train Loss: 0.1059, Val Loss: 0.0828\n",
      "Epoch 6/300 - Train Loss: 0.1023, Val Loss: 0.0866\n",
      "Epoch 7/300 - Train Loss: 0.1010, Val Loss: 0.0820\n",
      "Epoch 8/300 - Train Loss: 0.1008, Val Loss: 0.0778\n",
      "Epoch 9/300 - Train Loss: 0.0976, Val Loss: 0.0823\n",
      "Epoch 10/300 - Train Loss: 0.0975, Val Loss: 0.0730\n",
      "Epoch 11/300 - Train Loss: 0.0970, Val Loss: 0.0743\n",
      "Epoch 12/300 - Train Loss: 0.0954, Val Loss: 0.0772\n",
      "Epoch 13/300 - Train Loss: 0.0959, Val Loss: 0.0737\n",
      "Epoch 14/300 - Train Loss: 0.0965, Val Loss: 0.0735\n",
      "Epoch 15/300 - Train Loss: 0.0964, Val Loss: 0.0711\n",
      "Epoch 16/300 - Train Loss: 0.0968, Val Loss: 0.0725\n",
      "Epoch 17/300 - Train Loss: 0.0970, Val Loss: 0.0776\n",
      "Epoch 18/300 - Train Loss: 0.0975, Val Loss: 0.0749\n",
      "Epoch 19/300 - Train Loss: 0.1001, Val Loss: 0.0713\n",
      "Epoch 20/300 - Train Loss: 0.0947, Val Loss: 0.0731\n",
      "Epoch 21/300 - Train Loss: 0.0949, Val Loss: 0.0722\n",
      "Epoch 22/300 - Train Loss: 0.0954, Val Loss: 0.0758\n",
      "Epoch 23/300 - Train Loss: 0.0962, Val Loss: 0.0736\n",
      "Epoch 24/300 - Train Loss: 0.0961, Val Loss: 0.0733\n",
      "Epoch 25/300 - Train Loss: 0.0993, Val Loss: 0.0719\n",
      "Epoch 26/300 - Train Loss: 0.0995, Val Loss: 0.0750\n",
      "Epoch 27/300 - Train Loss: 0.0962, Val Loss: 0.0826\n",
      "Epoch 28/300 - Train Loss: 0.0961, Val Loss: 0.0738\n",
      "Epoch 29/300 - Train Loss: 0.0984, Val Loss: 0.0719\n",
      "Epoch 30/300 - Train Loss: 0.0967, Val Loss: 0.0745\n",
      "Epoch 31/300 - Train Loss: 0.0978, Val Loss: 0.0701\n",
      "Epoch 32/300 - Train Loss: 0.0954, Val Loss: 0.0702\n",
      "Epoch 33/300 - Train Loss: 0.0975, Val Loss: 0.0738\n",
      "Epoch 34/300 - Train Loss: 0.0984, Val Loss: 0.0707\n",
      "Epoch 35/300 - Train Loss: 0.0969, Val Loss: 0.0710\n",
      "Epoch 36/300 - Train Loss: 0.0977, Val Loss: 0.0704\n",
      "Epoch 37/300 - Train Loss: 0.1012, Val Loss: 0.0694\n",
      "Epoch 38/300 - Train Loss: 0.0997, Val Loss: 0.0705\n",
      "Epoch 39/300 - Train Loss: 0.0969, Val Loss: 0.0736\n",
      "Epoch 40/300 - Train Loss: 0.0991, Val Loss: 0.0709\n",
      "Epoch 41/300 - Train Loss: 0.0996, Val Loss: 0.0707\n",
      "Epoch 42/300 - Train Loss: 0.1007, Val Loss: 0.0711\n",
      "Epoch 43/300 - Train Loss: 0.0997, Val Loss: 0.0722\n",
      "Epoch 44/300 - Train Loss: 0.1007, Val Loss: 0.0719\n",
      "Epoch 45/300 - Train Loss: 0.0981, Val Loss: 0.0742\n",
      "Epoch 46/300 - Train Loss: 0.1015, Val Loss: 0.0726\n",
      "Epoch 47/300 - Train Loss: 0.0993, Val Loss: 0.0712\n",
      "Epoch 48/300 - Train Loss: 0.0996, Val Loss: 0.0774\n",
      "Epoch 49/300 - Train Loss: 0.1024, Val Loss: 0.0722\n",
      "Epoch 50/300 - Train Loss: 0.1006, Val Loss: 0.0753\n",
      "Epoch 51/300 - Train Loss: 0.1013, Val Loss: 0.0784\n",
      "Epoch 52/300 - Train Loss: 0.1014, Val Loss: 0.0776\n",
      "Epoch 53/300 - Train Loss: 0.0994, Val Loss: 0.0714\n",
      "Epoch 54/300 - Train Loss: 0.1021, Val Loss: 0.0702\n",
      "Epoch 55/300 - Train Loss: 0.0983, Val Loss: 0.0766\n",
      "Epoch 56/300 - Train Loss: 0.1025, Val Loss: 0.0683\n",
      "Epoch 57/300 - Train Loss: 0.1016, Val Loss: 0.0690\n",
      "Epoch 58/300 - Train Loss: 0.1010, Val Loss: 0.0700\n",
      "Epoch 59/300 - Train Loss: 0.1006, Val Loss: 0.0765\n",
      "Epoch 60/300 - Train Loss: 0.1012, Val Loss: 0.0712\n",
      "Epoch 61/300 - Train Loss: 0.1034, Val Loss: 0.0718\n",
      "Epoch 62/300 - Train Loss: 0.1010, Val Loss: 0.0728\n",
      "Epoch 63/300 - Train Loss: 0.1040, Val Loss: 0.0760\n",
      "Epoch 64/300 - Train Loss: 0.1019, Val Loss: 0.0734\n",
      "Epoch 65/300 - Train Loss: 0.1021, Val Loss: 0.0796\n",
      "Epoch 66/300 - Train Loss: 0.1026, Val Loss: 0.0746\n",
      "Epoch 67/300 - Train Loss: 0.1004, Val Loss: 0.0736\n",
      "Epoch 68/300 - Train Loss: 0.1014, Val Loss: 0.0763\n",
      "Epoch 69/300 - Train Loss: 0.1024, Val Loss: 0.0717\n",
      "Epoch 70/300 - Train Loss: 0.1022, Val Loss: 0.0703\n",
      "Epoch 71/300 - Train Loss: 0.1021, Val Loss: 0.0730\n",
      "Epoch 72/300 - Train Loss: 0.1028, Val Loss: 0.0724\n",
      "Epoch 73/300 - Train Loss: 0.1007, Val Loss: 0.0713\n",
      "Epoch 74/300 - Train Loss: 0.1038, Val Loss: 0.0712\n",
      "Epoch 75/300 - Train Loss: 0.1013, Val Loss: 0.0722\n",
      "Epoch 76/300 - Train Loss: 0.0987, Val Loss: 0.0764\n",
      "Epoch 77/300 - Train Loss: 0.1029, Val Loss: 0.0701\n",
      "Epoch 78/300 - Train Loss: 0.1034, Val Loss: 0.0753\n",
      "Epoch 79/300 - Train Loss: 0.1016, Val Loss: 0.0707\n",
      "Epoch 80/300 - Train Loss: 0.1017, Val Loss: 0.0776\n",
      "Epoch 81/300 - Train Loss: 0.1021, Val Loss: 0.0708\n",
      "Epoch 82/300 - Train Loss: 0.1036, Val Loss: 0.0704\n",
      "Epoch 83/300 - Train Loss: 0.1007, Val Loss: 0.0752\n",
      "Epoch 84/300 - Train Loss: 0.1039, Val Loss: 0.0740\n",
      "Epoch 85/300 - Train Loss: 0.1034, Val Loss: 0.0699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:54:33,897] Trial 152 finished with value: 0.965643330151354 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.26106335881110554, 'learning_rate': 4.8375381026960793e-05, 'batch_size': 32, 'weight_decay': 0.0077310662505103555}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/300 - Train Loss: 0.1055, Val Loss: 0.0754\n",
      "Early stopping at epoch 86\n",
      "Macro F1 Score: 0.9656, Macro Precision: 0.9624, Macro Recall: 0.9693\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 154\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2468698296244614, LR=7.914368724120601e-05, BS=32, WD=0.009865227291154145\n",
      "Epoch 1/300 - Train Loss: 0.2957, Val Loss: 0.1650\n",
      "Epoch 2/300 - Train Loss: 0.1289, Val Loss: 0.0956\n",
      "Epoch 3/300 - Train Loss: 0.1084, Val Loss: 0.0833\n",
      "Epoch 4/300 - Train Loss: 0.1042, Val Loss: 0.0998\n",
      "Epoch 5/300 - Train Loss: 0.0998, Val Loss: 0.0790\n",
      "Epoch 6/300 - Train Loss: 0.0973, Val Loss: 0.0920\n",
      "Epoch 7/300 - Train Loss: 0.0978, Val Loss: 0.0762\n",
      "Epoch 8/300 - Train Loss: 0.0985, Val Loss: 0.0838\n",
      "Epoch 9/300 - Train Loss: 0.0976, Val Loss: 0.0765\n",
      "Epoch 10/300 - Train Loss: 0.0971, Val Loss: 0.0781\n",
      "Epoch 11/300 - Train Loss: 0.1018, Val Loss: 0.0703\n",
      "Epoch 12/300 - Train Loss: 0.0970, Val Loss: 0.0706\n",
      "Epoch 13/300 - Train Loss: 0.1000, Val Loss: 0.0773\n",
      "Epoch 14/300 - Train Loss: 0.0998, Val Loss: 0.0743\n",
      "Epoch 15/300 - Train Loss: 0.1023, Val Loss: 0.0728\n",
      "Epoch 16/300 - Train Loss: 0.1031, Val Loss: 0.0790\n",
      "Epoch 17/300 - Train Loss: 0.1015, Val Loss: 0.0763\n",
      "Epoch 18/300 - Train Loss: 0.1009, Val Loss: 0.0722\n",
      "Epoch 19/300 - Train Loss: 0.1011, Val Loss: 0.0725\n",
      "Epoch 20/300 - Train Loss: 0.1013, Val Loss: 0.0828\n",
      "Epoch 21/300 - Train Loss: 0.1013, Val Loss: 0.0717\n",
      "Epoch 22/300 - Train Loss: 0.1016, Val Loss: 0.0732\n",
      "Epoch 23/300 - Train Loss: 0.1017, Val Loss: 0.0760\n",
      "Epoch 24/300 - Train Loss: 0.1021, Val Loss: 0.0807\n",
      "Epoch 25/300 - Train Loss: 0.1039, Val Loss: 0.0727\n",
      "Epoch 26/300 - Train Loss: 0.1029, Val Loss: 0.0719\n",
      "Epoch 27/300 - Train Loss: 0.1025, Val Loss: 0.0728\n",
      "Epoch 28/300 - Train Loss: 0.1032, Val Loss: 0.0757\n",
      "Epoch 29/300 - Train Loss: 0.1050, Val Loss: 0.0798\n",
      "Epoch 30/300 - Train Loss: 0.1053, Val Loss: 0.0858\n",
      "Epoch 31/300 - Train Loss: 0.1056, Val Loss: 0.0796\n",
      "Epoch 32/300 - Train Loss: 0.1027, Val Loss: 0.0746\n",
      "Epoch 33/300 - Train Loss: 0.1070, Val Loss: 0.0725\n",
      "Epoch 34/300 - Train Loss: 0.1082, Val Loss: 0.0750\n",
      "Epoch 35/300 - Train Loss: 0.1051, Val Loss: 0.0747\n",
      "Epoch 36/300 - Train Loss: 0.1054, Val Loss: 0.0761\n",
      "Epoch 37/300 - Train Loss: 0.1039, Val Loss: 0.0868\n",
      "Epoch 38/300 - Train Loss: 0.1065, Val Loss: 0.0864\n",
      "Epoch 39/300 - Train Loss: 0.1034, Val Loss: 0.0769\n",
      "Epoch 40/300 - Train Loss: 0.1069, Val Loss: 0.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 08:57:11,565] Trial 153 finished with value: 0.9694624160448041 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2468698296244614, 'learning_rate': 7.914368724120601e-05, 'batch_size': 32, 'weight_decay': 0.009865227291154145}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300 - Train Loss: 0.1073, Val Loss: 0.0758\n",
      "Early stopping at epoch 41\n",
      "Macro F1 Score: 0.9695, Macro Precision: 0.9681, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 155\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2912420597280357, LR=6.76879678087478e-05, BS=32, WD=0.005653685282685139\n",
      "Epoch 1/300 - Train Loss: 0.3163, Val Loss: 0.1764\n",
      "Epoch 2/300 - Train Loss: 0.1469, Val Loss: 0.0958\n",
      "Epoch 3/300 - Train Loss: 0.1125, Val Loss: 0.0873\n",
      "Epoch 4/300 - Train Loss: 0.1057, Val Loss: 0.0869\n",
      "Epoch 5/300 - Train Loss: 0.1023, Val Loss: 0.0792\n",
      "Epoch 6/300 - Train Loss: 0.0966, Val Loss: 0.0767\n",
      "Epoch 7/300 - Train Loss: 0.0942, Val Loss: 0.0749\n",
      "Epoch 8/300 - Train Loss: 0.0976, Val Loss: 0.0773\n",
      "Epoch 9/300 - Train Loss: 0.0935, Val Loss: 0.0700\n",
      "Epoch 10/300 - Train Loss: 0.0922, Val Loss: 0.0716\n",
      "Epoch 11/300 - Train Loss: 0.0943, Val Loss: 0.0748\n",
      "Epoch 12/300 - Train Loss: 0.0950, Val Loss: 0.0725\n",
      "Epoch 13/300 - Train Loss: 0.0946, Val Loss: 0.0694\n",
      "Epoch 14/300 - Train Loss: 0.0927, Val Loss: 0.0748\n",
      "Epoch 15/300 - Train Loss: 0.0933, Val Loss: 0.0682\n",
      "Epoch 16/300 - Train Loss: 0.0958, Val Loss: 0.0843\n",
      "Epoch 17/300 - Train Loss: 0.0945, Val Loss: 0.0710\n",
      "Epoch 18/300 - Train Loss: 0.0930, Val Loss: 0.0718\n",
      "Epoch 19/300 - Train Loss: 0.0948, Val Loss: 0.0711\n",
      "Epoch 20/300 - Train Loss: 0.0949, Val Loss: 0.0706\n",
      "Epoch 21/300 - Train Loss: 0.0971, Val Loss: 0.0699\n",
      "Epoch 22/300 - Train Loss: 0.0965, Val Loss: 0.0763\n",
      "Epoch 23/300 - Train Loss: 0.0964, Val Loss: 0.0758\n",
      "Epoch 24/300 - Train Loss: 0.0949, Val Loss: 0.0680\n",
      "Epoch 25/300 - Train Loss: 0.0963, Val Loss: 0.0692\n",
      "Epoch 26/300 - Train Loss: 0.0956, Val Loss: 0.0726\n",
      "Epoch 27/300 - Train Loss: 0.0970, Val Loss: 0.0696\n",
      "Epoch 28/300 - Train Loss: 0.0972, Val Loss: 0.0749\n",
      "Epoch 29/300 - Train Loss: 0.0965, Val Loss: 0.0709\n",
      "Epoch 30/300 - Train Loss: 0.0970, Val Loss: 0.0695\n",
      "Epoch 31/300 - Train Loss: 0.0973, Val Loss: 0.0706\n",
      "Epoch 32/300 - Train Loss: 0.0958, Val Loss: 0.0710\n",
      "Epoch 33/300 - Train Loss: 0.0968, Val Loss: 0.0752\n",
      "Epoch 34/300 - Train Loss: 0.0971, Val Loss: 0.0728\n",
      "Epoch 35/300 - Train Loss: 0.0959, Val Loss: 0.0724\n",
      "Epoch 36/300 - Train Loss: 0.0980, Val Loss: 0.0706\n",
      "Epoch 37/300 - Train Loss: 0.0994, Val Loss: 0.0745\n",
      "Epoch 38/300 - Train Loss: 0.0987, Val Loss: 0.0764\n",
      "Epoch 39/300 - Train Loss: 0.0992, Val Loss: 0.0721\n",
      "Epoch 40/300 - Train Loss: 0.0983, Val Loss: 0.0750\n",
      "Epoch 41/300 - Train Loss: 0.1003, Val Loss: 0.0811\n",
      "Epoch 42/300 - Train Loss: 0.0970, Val Loss: 0.0769\n",
      "Epoch 43/300 - Train Loss: 0.0973, Val Loss: 0.0724\n",
      "Epoch 44/300 - Train Loss: 0.0977, Val Loss: 0.0713\n",
      "Epoch 45/300 - Train Loss: 0.1016, Val Loss: 0.0859\n",
      "Epoch 46/300 - Train Loss: 0.0995, Val Loss: 0.0747\n",
      "Epoch 47/300 - Train Loss: 0.0985, Val Loss: 0.0846\n",
      "Epoch 48/300 - Train Loss: 0.0980, Val Loss: 0.0750\n",
      "Epoch 49/300 - Train Loss: 0.1001, Val Loss: 0.0782\n",
      "Epoch 50/300 - Train Loss: 0.0989, Val Loss: 0.0710\n",
      "Epoch 51/300 - Train Loss: 0.0992, Val Loss: 0.0742\n",
      "Epoch 52/300 - Train Loss: 0.1007, Val Loss: 0.0722\n",
      "Epoch 53/300 - Train Loss: 0.0990, Val Loss: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:00:39,066] Trial 154 finished with value: 0.9680459196817734 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2912420597280357, 'learning_rate': 6.76879678087478e-05, 'batch_size': 32, 'weight_decay': 0.005653685282685139}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300 - Train Loss: 0.0970, Val Loss: 0.0692\n",
      "Early stopping at epoch 54\n",
      "Macro F1 Score: 0.9680, Macro Precision: 0.9671, Macro Recall: 0.9693\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 156\n",
      "Training with F1=16, F2=32, D=8, dropout=0.23563193832889817, LR=5.4606025814114306e-05, BS=32, WD=0.0034416884686446414\n",
      "Epoch 1/300 - Train Loss: 0.3105, Val Loss: 0.1399\n",
      "Epoch 2/300 - Train Loss: 0.1407, Val Loss: 0.1162\n",
      "Epoch 3/300 - Train Loss: 0.1142, Val Loss: 0.0915\n",
      "Epoch 4/300 - Train Loss: 0.1054, Val Loss: 0.0856\n",
      "Epoch 5/300 - Train Loss: 0.1044, Val Loss: 0.0811\n",
      "Epoch 6/300 - Train Loss: 0.1003, Val Loss: 0.0819\n",
      "Epoch 7/300 - Train Loss: 0.0987, Val Loss: 0.0816\n",
      "Epoch 8/300 - Train Loss: 0.0936, Val Loss: 0.0783\n",
      "Epoch 9/300 - Train Loss: 0.0936, Val Loss: 0.0779\n",
      "Epoch 10/300 - Train Loss: 0.0929, Val Loss: 0.0825\n",
      "Epoch 11/300 - Train Loss: 0.0925, Val Loss: 0.0740\n",
      "Epoch 12/300 - Train Loss: 0.0889, Val Loss: 0.0856\n",
      "Epoch 13/300 - Train Loss: 0.0925, Val Loss: 0.0744\n",
      "Epoch 14/300 - Train Loss: 0.0910, Val Loss: 0.0814\n",
      "Epoch 15/300 - Train Loss: 0.0887, Val Loss: 0.0784\n",
      "Epoch 16/300 - Train Loss: 0.0873, Val Loss: 0.0778\n",
      "Epoch 17/300 - Train Loss: 0.0867, Val Loss: 0.0743\n",
      "Epoch 18/300 - Train Loss: 0.0876, Val Loss: 0.0760\n",
      "Epoch 19/300 - Train Loss: 0.0882, Val Loss: 0.0758\n",
      "Epoch 20/300 - Train Loss: 0.0889, Val Loss: 0.0721\n",
      "Epoch 21/300 - Train Loss: 0.0894, Val Loss: 0.0718\n",
      "Epoch 22/300 - Train Loss: 0.0865, Val Loss: 0.0726\n",
      "Epoch 23/300 - Train Loss: 0.0869, Val Loss: 0.0724\n",
      "Epoch 24/300 - Train Loss: 0.0880, Val Loss: 0.0732\n",
      "Epoch 25/300 - Train Loss: 0.0889, Val Loss: 0.0807\n",
      "Epoch 26/300 - Train Loss: 0.0880, Val Loss: 0.0712\n",
      "Epoch 27/300 - Train Loss: 0.0890, Val Loss: 0.0771\n",
      "Epoch 28/300 - Train Loss: 0.0888, Val Loss: 0.0769\n",
      "Epoch 29/300 - Train Loss: 0.0887, Val Loss: 0.0722\n",
      "Epoch 30/300 - Train Loss: 0.0859, Val Loss: 0.0764\n",
      "Epoch 31/300 - Train Loss: 0.0855, Val Loss: 0.0735\n",
      "Epoch 32/300 - Train Loss: 0.0853, Val Loss: 0.0685\n",
      "Epoch 33/300 - Train Loss: 0.0879, Val Loss: 0.0706\n",
      "Epoch 34/300 - Train Loss: 0.0880, Val Loss: 0.0735\n",
      "Epoch 35/300 - Train Loss: 0.0873, Val Loss: 0.0696\n",
      "Epoch 36/300 - Train Loss: 0.0872, Val Loss: 0.0691\n",
      "Epoch 37/300 - Train Loss: 0.0874, Val Loss: 0.0728\n",
      "Epoch 38/300 - Train Loss: 0.0872, Val Loss: 0.0735\n",
      "Epoch 39/300 - Train Loss: 0.0873, Val Loss: 0.0717\n",
      "Epoch 40/300 - Train Loss: 0.0865, Val Loss: 0.0718\n",
      "Epoch 41/300 - Train Loss: 0.0868, Val Loss: 0.0725\n",
      "Epoch 42/300 - Train Loss: 0.0875, Val Loss: 0.0724\n",
      "Epoch 43/300 - Train Loss: 0.0849, Val Loss: 0.0718\n",
      "Epoch 44/300 - Train Loss: 0.0883, Val Loss: 0.0696\n",
      "Epoch 45/300 - Train Loss: 0.0866, Val Loss: 0.0704\n",
      "Epoch 46/300 - Train Loss: 0.0898, Val Loss: 0.0683\n",
      "Epoch 47/300 - Train Loss: 0.0874, Val Loss: 0.0695\n",
      "Epoch 48/300 - Train Loss: 0.0907, Val Loss: 0.0765\n",
      "Epoch 49/300 - Train Loss: 0.0870, Val Loss: 0.0732\n",
      "Epoch 50/300 - Train Loss: 0.0872, Val Loss: 0.0720\n",
      "Epoch 51/300 - Train Loss: 0.0872, Val Loss: 0.0739\n",
      "Epoch 52/300 - Train Loss: 0.0893, Val Loss: 0.0724\n",
      "Epoch 53/300 - Train Loss: 0.0875, Val Loss: 0.0705\n",
      "Epoch 54/300 - Train Loss: 0.0900, Val Loss: 0.0722\n",
      "Epoch 55/300 - Train Loss: 0.0903, Val Loss: 0.0703\n",
      "Epoch 56/300 - Train Loss: 0.0872, Val Loss: 0.0747\n",
      "Epoch 57/300 - Train Loss: 0.0902, Val Loss: 0.0689\n",
      "Epoch 58/300 - Train Loss: 0.0868, Val Loss: 0.0737\n",
      "Epoch 59/300 - Train Loss: 0.0881, Val Loss: 0.0736\n",
      "Epoch 60/300 - Train Loss: 0.0874, Val Loss: 0.0700\n",
      "Epoch 61/300 - Train Loss: 0.0892, Val Loss: 0.0716\n",
      "Epoch 62/300 - Train Loss: 0.0905, Val Loss: 0.0735\n",
      "Epoch 63/300 - Train Loss: 0.0894, Val Loss: 0.0690\n",
      "Epoch 64/300 - Train Loss: 0.0911, Val Loss: 0.0712\n",
      "Epoch 65/300 - Train Loss: 0.0884, Val Loss: 0.0748\n",
      "Epoch 66/300 - Train Loss: 0.0872, Val Loss: 0.0728\n",
      "Epoch 67/300 - Train Loss: 0.0882, Val Loss: 0.0667\n",
      "Epoch 68/300 - Train Loss: 0.0879, Val Loss: 0.0768\n",
      "Epoch 69/300 - Train Loss: 0.0883, Val Loss: 0.0706\n",
      "Epoch 70/300 - Train Loss: 0.0899, Val Loss: 0.0716\n",
      "Epoch 71/300 - Train Loss: 0.0905, Val Loss: 0.0714\n",
      "Epoch 72/300 - Train Loss: 0.0889, Val Loss: 0.0763\n",
      "Epoch 73/300 - Train Loss: 0.0892, Val Loss: 0.0696\n",
      "Epoch 74/300 - Train Loss: 0.0877, Val Loss: 0.0700\n",
      "Epoch 75/300 - Train Loss: 0.0892, Val Loss: 0.0780\n",
      "Epoch 76/300 - Train Loss: 0.0884, Val Loss: 0.0711\n",
      "Epoch 77/300 - Train Loss: 0.0901, Val Loss: 0.0671\n",
      "Epoch 78/300 - Train Loss: 0.0902, Val Loss: 0.0751\n",
      "Epoch 79/300 - Train Loss: 0.0882, Val Loss: 0.0682\n",
      "Epoch 80/300 - Train Loss: 0.0895, Val Loss: 0.0706\n",
      "Epoch 81/300 - Train Loss: 0.0906, Val Loss: 0.0685\n",
      "Epoch 82/300 - Train Loss: 0.0908, Val Loss: 0.0695\n",
      "Epoch 83/300 - Train Loss: 0.0887, Val Loss: 0.0688\n",
      "Epoch 84/300 - Train Loss: 0.0886, Val Loss: 0.0698\n",
      "Epoch 85/300 - Train Loss: 0.0876, Val Loss: 0.0756\n",
      "Epoch 86/300 - Train Loss: 0.0885, Val Loss: 0.0704\n",
      "Epoch 87/300 - Train Loss: 0.0896, Val Loss: 0.0720\n",
      "Epoch 88/300 - Train Loss: 0.0895, Val Loss: 0.0731\n",
      "Epoch 89/300 - Train Loss: 0.0899, Val Loss: 0.0748\n",
      "Epoch 90/300 - Train Loss: 0.0909, Val Loss: 0.0753\n",
      "Epoch 91/300 - Train Loss: 0.0913, Val Loss: 0.0721\n",
      "Epoch 92/300 - Train Loss: 0.0926, Val Loss: 0.0733\n",
      "Epoch 93/300 - Train Loss: 0.0947, Val Loss: 0.0708\n",
      "Epoch 94/300 - Train Loss: 0.0910, Val Loss: 0.0686\n",
      "Epoch 95/300 - Train Loss: 0.0917, Val Loss: 0.0684\n",
      "Epoch 96/300 - Train Loss: 0.0886, Val Loss: 0.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:06:51,824] Trial 155 finished with value: 0.9643095778521659 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.23563193832889817, 'learning_rate': 5.4606025814114306e-05, 'batch_size': 32, 'weight_decay': 0.0034416884686446414}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300 - Train Loss: 0.0917, Val Loss: 0.0735\n",
      "Early stopping at epoch 97\n",
      "Macro F1 Score: 0.9643, Macro Precision: 0.9656, Macro Recall: 0.9632\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 157\n",
      "Training with F1=16, F2=32, D=4, dropout=0.2528123109001927, LR=9.214729813682272e-05, BS=32, WD=0.006974916167974811\n",
      "Epoch 1/300 - Train Loss: 0.2860, Val Loss: 0.1389\n",
      "Epoch 2/300 - Train Loss: 0.1285, Val Loss: 0.0933\n",
      "Epoch 3/300 - Train Loss: 0.1104, Val Loss: 0.0864\n",
      "Epoch 4/300 - Train Loss: 0.1031, Val Loss: 0.0774\n",
      "Epoch 5/300 - Train Loss: 0.1041, Val Loss: 0.0767\n",
      "Epoch 6/300 - Train Loss: 0.1021, Val Loss: 0.0805\n",
      "Epoch 7/300 - Train Loss: 0.0995, Val Loss: 0.0717\n",
      "Epoch 8/300 - Train Loss: 0.0992, Val Loss: 0.0742\n",
      "Epoch 9/300 - Train Loss: 0.1000, Val Loss: 0.0748\n",
      "Epoch 10/300 - Train Loss: 0.0975, Val Loss: 0.0737\n",
      "Epoch 11/300 - Train Loss: 0.0980, Val Loss: 0.0719\n",
      "Epoch 12/300 - Train Loss: 0.1005, Val Loss: 0.0742\n",
      "Epoch 13/300 - Train Loss: 0.0987, Val Loss: 0.0711\n",
      "Epoch 14/300 - Train Loss: 0.0976, Val Loss: 0.0714\n",
      "Epoch 15/300 - Train Loss: 0.0983, Val Loss: 0.0742\n",
      "Epoch 16/300 - Train Loss: 0.0988, Val Loss: 0.0717\n",
      "Epoch 17/300 - Train Loss: 0.1000, Val Loss: 0.0767\n",
      "Epoch 18/300 - Train Loss: 0.1006, Val Loss: 0.0698\n",
      "Epoch 19/300 - Train Loss: 0.0985, Val Loss: 0.0728\n",
      "Epoch 20/300 - Train Loss: 0.0983, Val Loss: 0.0774\n",
      "Epoch 21/300 - Train Loss: 0.0995, Val Loss: 0.0735\n",
      "Epoch 22/300 - Train Loss: 0.1011, Val Loss: 0.0712\n",
      "Epoch 23/300 - Train Loss: 0.1010, Val Loss: 0.0793\n",
      "Epoch 24/300 - Train Loss: 0.1032, Val Loss: 0.0812\n",
      "Epoch 25/300 - Train Loss: 0.1011, Val Loss: 0.0768\n",
      "Epoch 26/300 - Train Loss: 0.1023, Val Loss: 0.0714\n",
      "Epoch 27/300 - Train Loss: 0.1000, Val Loss: 0.0705\n",
      "Epoch 28/300 - Train Loss: 0.1011, Val Loss: 0.0720\n",
      "Epoch 29/300 - Train Loss: 0.0991, Val Loss: 0.0734\n",
      "Epoch 30/300 - Train Loss: 0.0975, Val Loss: 0.0700\n",
      "Epoch 31/300 - Train Loss: 0.1010, Val Loss: 0.0684\n",
      "Epoch 32/300 - Train Loss: 0.1029, Val Loss: 0.0734\n",
      "Epoch 33/300 - Train Loss: 0.1023, Val Loss: 0.0753\n",
      "Epoch 34/300 - Train Loss: 0.1031, Val Loss: 0.0705\n",
      "Epoch 35/300 - Train Loss: 0.1026, Val Loss: 0.0720\n",
      "Epoch 36/300 - Train Loss: 0.0995, Val Loss: 0.0719\n",
      "Epoch 37/300 - Train Loss: 0.1030, Val Loss: 0.0711\n",
      "Epoch 38/300 - Train Loss: 0.1033, Val Loss: 0.0813\n",
      "Epoch 39/300 - Train Loss: 0.1021, Val Loss: 0.0817\n",
      "Epoch 40/300 - Train Loss: 0.1020, Val Loss: 0.0727\n",
      "Epoch 41/300 - Train Loss: 0.1015, Val Loss: 0.0707\n",
      "Epoch 42/300 - Train Loss: 0.1033, Val Loss: 0.0736\n",
      "Epoch 43/300 - Train Loss: 0.1048, Val Loss: 0.0706\n",
      "Epoch 44/300 - Train Loss: 0.1037, Val Loss: 0.0776\n",
      "Epoch 45/300 - Train Loss: 0.1024, Val Loss: 0.0740\n",
      "Epoch 46/300 - Train Loss: 0.1049, Val Loss: 0.0735\n",
      "Epoch 47/300 - Train Loss: 0.1039, Val Loss: 0.0816\n",
      "Epoch 48/300 - Train Loss: 0.1034, Val Loss: 0.0740\n",
      "Epoch 49/300 - Train Loss: 0.1030, Val Loss: 0.0775\n",
      "Epoch 50/300 - Train Loss: 0.1020, Val Loss: 0.0840\n",
      "Epoch 51/300 - Train Loss: 0.1020, Val Loss: 0.0739\n",
      "Epoch 52/300 - Train Loss: 0.1044, Val Loss: 0.0947\n",
      "Epoch 53/300 - Train Loss: 0.1017, Val Loss: 0.0722\n",
      "Epoch 54/300 - Train Loss: 0.1005, Val Loss: 0.0774\n",
      "Epoch 55/300 - Train Loss: 0.1057, Val Loss: 0.0779\n",
      "Epoch 56/300 - Train Loss: 0.1070, Val Loss: 0.0709\n",
      "Epoch 57/300 - Train Loss: 0.1029, Val Loss: 0.0745\n",
      "Epoch 58/300 - Train Loss: 0.1035, Val Loss: 0.0723\n",
      "Epoch 59/300 - Train Loss: 0.1032, Val Loss: 0.0699\n",
      "Epoch 60/300 - Train Loss: 0.1036, Val Loss: 0.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:09:24,581] Trial 156 finished with value: 0.9741636280821234 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.2528123109001927, 'learning_rate': 9.214729813682272e-05, 'batch_size': 32, 'weight_decay': 0.006974916167974811}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/300 - Train Loss: 0.1029, Val Loss: 0.0756\n",
      "Early stopping at epoch 61\n",
      "Macro F1 Score: 0.9742, Macro Precision: 0.9779, Macro Recall: 0.9707\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 158\n",
      "Training with F1=4, F2=32, D=4, dropout=0.5750198608203206, LR=9.566207872135015e-05, BS=32, WD=0.007098066795290482\n",
      "Epoch 1/300 - Train Loss: 0.4847, Val Loss: 0.2089\n",
      "Epoch 2/300 - Train Loss: 0.1843, Val Loss: 0.1423\n",
      "Epoch 3/300 - Train Loss: 0.1634, Val Loss: 0.1342\n",
      "Epoch 4/300 - Train Loss: 0.1519, Val Loss: 0.1135\n",
      "Epoch 5/300 - Train Loss: 0.1486, Val Loss: 0.1102\n",
      "Epoch 6/300 - Train Loss: 0.1432, Val Loss: 0.1079\n",
      "Epoch 7/300 - Train Loss: 0.1423, Val Loss: 0.1073\n",
      "Epoch 8/300 - Train Loss: 0.1412, Val Loss: 0.1053\n",
      "Epoch 9/300 - Train Loss: 0.1412, Val Loss: 0.1011\n",
      "Epoch 10/300 - Train Loss: 0.1407, Val Loss: 0.0967\n",
      "Epoch 11/300 - Train Loss: 0.1373, Val Loss: 0.1005\n",
      "Epoch 12/300 - Train Loss: 0.1357, Val Loss: 0.0940\n",
      "Epoch 13/300 - Train Loss: 0.1369, Val Loss: 0.0952\n",
      "Epoch 14/300 - Train Loss: 0.1400, Val Loss: 0.1000\n",
      "Epoch 15/300 - Train Loss: 0.1422, Val Loss: 0.0951\n",
      "Epoch 16/300 - Train Loss: 0.1389, Val Loss: 0.1001\n",
      "Epoch 17/300 - Train Loss: 0.1377, Val Loss: 0.0969\n",
      "Epoch 18/300 - Train Loss: 0.1395, Val Loss: 0.0979\n",
      "Epoch 19/300 - Train Loss: 0.1419, Val Loss: 0.0961\n",
      "Epoch 20/300 - Train Loss: 0.1384, Val Loss: 0.0971\n",
      "Epoch 21/300 - Train Loss: 0.1406, Val Loss: 0.0951\n",
      "Epoch 22/300 - Train Loss: 0.1411, Val Loss: 0.1010\n",
      "Epoch 23/300 - Train Loss: 0.1398, Val Loss: 0.0964\n",
      "Epoch 24/300 - Train Loss: 0.1415, Val Loss: 0.0988\n",
      "Epoch 25/300 - Train Loss: 0.1409, Val Loss: 0.1113\n",
      "Epoch 26/300 - Train Loss: 0.1431, Val Loss: 0.1016\n",
      "Epoch 27/300 - Train Loss: 0.1417, Val Loss: 0.0954\n",
      "Epoch 28/300 - Train Loss: 0.1417, Val Loss: 0.0974\n",
      "Epoch 29/300 - Train Loss: 0.1430, Val Loss: 0.0983\n",
      "Epoch 30/300 - Train Loss: 0.1423, Val Loss: 0.0962\n",
      "Epoch 31/300 - Train Loss: 0.1449, Val Loss: 0.1043\n",
      "Epoch 32/300 - Train Loss: 0.1412, Val Loss: 0.0983\n",
      "Epoch 33/300 - Train Loss: 0.1419, Val Loss: 0.0951\n",
      "Epoch 34/300 - Train Loss: 0.1420, Val Loss: 0.1005\n",
      "Epoch 35/300 - Train Loss: 0.1427, Val Loss: 0.0967\n",
      "Epoch 36/300 - Train Loss: 0.1432, Val Loss: 0.0951\n",
      "Epoch 37/300 - Train Loss: 0.1459, Val Loss: 0.1129\n",
      "Epoch 38/300 - Train Loss: 0.1404, Val Loss: 0.1004\n",
      "Epoch 39/300 - Train Loss: 0.1465, Val Loss: 0.1086\n",
      "Epoch 40/300 - Train Loss: 0.1416, Val Loss: 0.0944\n",
      "Epoch 41/300 - Train Loss: 0.1450, Val Loss: 0.0961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:10:47,614] Trial 157 finished with value: 0.9540487198683403 and parameters: {'F1': 4, 'F2': 32, 'D': 4, 'dropout': 0.5750198608203206, 'learning_rate': 9.566207872135015e-05, 'batch_size': 32, 'weight_decay': 0.007098066795290482}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300 - Train Loss: 0.1447, Val Loss: 0.0967\n",
      "Early stopping at epoch 42\n",
      "Macro F1 Score: 0.9540, Macro Precision: 0.9541, Macro Recall: 0.9545\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      1.00      0.98       789\n",
      "           1       0.90      0.92      0.91        61\n",
      "           2       0.99      0.95      0.97       593\n",
      "\n",
      "    accuracy                           0.97      1443\n",
      "   macro avg       0.95      0.95      0.95      1443\n",
      "weighted avg       0.97      0.97      0.97      1443\n",
      "\n",
      "\n",
      "Trial 159\n",
      "Training with F1=16, F2=32, D=4, dropout=0.2106257755134311, LR=0.00011206541349544972, BS=32, WD=7.33109723172059e-05\n",
      "Epoch 1/300 - Train Loss: 0.2538, Val Loss: 0.1358\n",
      "Epoch 2/300 - Train Loss: 0.1146, Val Loss: 0.0871\n",
      "Epoch 3/300 - Train Loss: 0.1007, Val Loss: 0.0857\n",
      "Epoch 4/300 - Train Loss: 0.0982, Val Loss: 0.0854\n",
      "Epoch 5/300 - Train Loss: 0.0922, Val Loss: 0.0760\n",
      "Epoch 6/300 - Train Loss: 0.0926, Val Loss: 0.0789\n",
      "Epoch 7/300 - Train Loss: 0.0882, Val Loss: 0.0720\n",
      "Epoch 8/300 - Train Loss: 0.0849, Val Loss: 0.0751\n",
      "Epoch 9/300 - Train Loss: 0.0862, Val Loss: 0.0737\n",
      "Epoch 10/300 - Train Loss: 0.0844, Val Loss: 0.0726\n",
      "Epoch 11/300 - Train Loss: 0.0806, Val Loss: 0.0799\n",
      "Epoch 12/300 - Train Loss: 0.0812, Val Loss: 0.0719\n",
      "Epoch 13/300 - Train Loss: 0.0792, Val Loss: 0.0709\n",
      "Epoch 14/300 - Train Loss: 0.0814, Val Loss: 0.0759\n",
      "Epoch 15/300 - Train Loss: 0.0785, Val Loss: 0.0711\n",
      "Epoch 16/300 - Train Loss: 0.0772, Val Loss: 0.0695\n",
      "Epoch 17/300 - Train Loss: 0.0772, Val Loss: 0.0732\n",
      "Epoch 18/300 - Train Loss: 0.0747, Val Loss: 0.0680\n",
      "Epoch 19/300 - Train Loss: 0.0763, Val Loss: 0.0693\n",
      "Epoch 20/300 - Train Loss: 0.0756, Val Loss: 0.0700\n",
      "Epoch 21/300 - Train Loss: 0.0737, Val Loss: 0.0699\n",
      "Epoch 22/300 - Train Loss: 0.0735, Val Loss: 0.0700\n",
      "Epoch 23/300 - Train Loss: 0.0730, Val Loss: 0.0676\n",
      "Epoch 24/300 - Train Loss: 0.0707, Val Loss: 0.0688\n",
      "Epoch 25/300 - Train Loss: 0.0718, Val Loss: 0.0718\n",
      "Epoch 26/300 - Train Loss: 0.0733, Val Loss: 0.0700\n",
      "Epoch 27/300 - Train Loss: 0.0717, Val Loss: 0.0761\n",
      "Epoch 28/300 - Train Loss: 0.0700, Val Loss: 0.0681\n",
      "Epoch 29/300 - Train Loss: 0.0717, Val Loss: 0.0713\n",
      "Epoch 30/300 - Train Loss: 0.0697, Val Loss: 0.0685\n",
      "Epoch 31/300 - Train Loss: 0.0683, Val Loss: 0.0669\n",
      "Epoch 32/300 - Train Loss: 0.0686, Val Loss: 0.0703\n",
      "Epoch 33/300 - Train Loss: 0.0663, Val Loss: 0.0681\n",
      "Epoch 34/300 - Train Loss: 0.0663, Val Loss: 0.0681\n",
      "Epoch 35/300 - Train Loss: 0.0690, Val Loss: 0.0720\n",
      "Epoch 36/300 - Train Loss: 0.0660, Val Loss: 0.0688\n",
      "Epoch 37/300 - Train Loss: 0.0653, Val Loss: 0.0671\n",
      "Epoch 38/300 - Train Loss: 0.0654, Val Loss: 0.0718\n",
      "Epoch 39/300 - Train Loss: 0.0639, Val Loss: 0.0715\n",
      "Epoch 40/300 - Train Loss: 0.0638, Val Loss: 0.0666\n",
      "Epoch 41/300 - Train Loss: 0.0636, Val Loss: 0.0687\n",
      "Epoch 42/300 - Train Loss: 0.0627, Val Loss: 0.0643\n",
      "Epoch 43/300 - Train Loss: 0.0652, Val Loss: 0.0712\n",
      "Epoch 44/300 - Train Loss: 0.0642, Val Loss: 0.0678\n",
      "Epoch 45/300 - Train Loss: 0.0618, Val Loss: 0.0691\n",
      "Epoch 46/300 - Train Loss: 0.0657, Val Loss: 0.0657\n",
      "Epoch 47/300 - Train Loss: 0.0621, Val Loss: 0.0728\n",
      "Epoch 48/300 - Train Loss: 0.0641, Val Loss: 0.0669\n",
      "Epoch 49/300 - Train Loss: 0.0609, Val Loss: 0.0733\n",
      "Epoch 50/300 - Train Loss: 0.0614, Val Loss: 0.0686\n",
      "Epoch 51/300 - Train Loss: 0.0611, Val Loss: 0.0663\n",
      "Epoch 52/300 - Train Loss: 0.0609, Val Loss: 0.0674\n",
      "Epoch 53/300 - Train Loss: 0.0596, Val Loss: 0.0725\n",
      "Epoch 54/300 - Train Loss: 0.0585, Val Loss: 0.0692\n",
      "Epoch 55/300 - Train Loss: 0.0584, Val Loss: 0.0698\n",
      "Epoch 56/300 - Train Loss: 0.0570, Val Loss: 0.0690\n",
      "Epoch 57/300 - Train Loss: 0.0590, Val Loss: 0.0681\n",
      "Epoch 58/300 - Train Loss: 0.0604, Val Loss: 0.0700\n",
      "Epoch 59/300 - Train Loss: 0.0566, Val Loss: 0.0685\n",
      "Epoch 60/300 - Train Loss: 0.0589, Val Loss: 0.0696\n",
      "Epoch 61/300 - Train Loss: 0.0553, Val Loss: 0.0725\n",
      "Epoch 62/300 - Train Loss: 0.0543, Val Loss: 0.0677\n",
      "Epoch 63/300 - Train Loss: 0.0559, Val Loss: 0.0710\n",
      "Epoch 64/300 - Train Loss: 0.0565, Val Loss: 0.0716\n",
      "Epoch 65/300 - Train Loss: 0.0574, Val Loss: 0.0665\n",
      "Epoch 66/300 - Train Loss: 0.0564, Val Loss: 0.0681\n",
      "Epoch 67/300 - Train Loss: 0.0542, Val Loss: 0.0705\n",
      "Epoch 68/300 - Train Loss: 0.0567, Val Loss: 0.0706\n",
      "Epoch 69/300 - Train Loss: 0.0576, Val Loss: 0.0661\n",
      "Epoch 70/300 - Train Loss: 0.0536, Val Loss: 0.0692\n",
      "Epoch 71/300 - Train Loss: 0.0527, Val Loss: 0.0689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:13:48,077] Trial 158 finished with value: 0.9650021712451619 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.2106257755134311, 'learning_rate': 0.00011206541349544972, 'batch_size': 32, 'weight_decay': 7.33109723172059e-05}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300 - Train Loss: 0.0568, Val Loss: 0.0695\n",
      "Early stopping at epoch 72\n",
      "Macro F1 Score: 0.9650, Macro Precision: 0.9590, Macro Recall: 0.9714\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 160\n",
      "Training with F1=16, F2=32, D=4, dropout=0.3297476578261809, LR=9.098628020028013e-05, BS=32, WD=0.005135115893872556\n",
      "Epoch 1/300 - Train Loss: 0.3203, Val Loss: 0.1576\n",
      "Epoch 2/300 - Train Loss: 0.1418, Val Loss: 0.0972\n",
      "Epoch 3/300 - Train Loss: 0.1195, Val Loss: 0.0826\n",
      "Epoch 4/300 - Train Loss: 0.1100, Val Loss: 0.0953\n",
      "Epoch 5/300 - Train Loss: 0.1087, Val Loss: 0.0830\n",
      "Epoch 6/300 - Train Loss: 0.1065, Val Loss: 0.0894\n",
      "Epoch 7/300 - Train Loss: 0.1012, Val Loss: 0.0763\n",
      "Epoch 8/300 - Train Loss: 0.1016, Val Loss: 0.0759\n",
      "Epoch 9/300 - Train Loss: 0.1023, Val Loss: 0.0751\n",
      "Epoch 10/300 - Train Loss: 0.0996, Val Loss: 0.0754\n",
      "Epoch 11/300 - Train Loss: 0.1006, Val Loss: 0.0774\n",
      "Epoch 12/300 - Train Loss: 0.0995, Val Loss: 0.0738\n",
      "Epoch 13/300 - Train Loss: 0.1007, Val Loss: 0.0773\n",
      "Epoch 14/300 - Train Loss: 0.1005, Val Loss: 0.0735\n",
      "Epoch 15/300 - Train Loss: 0.0989, Val Loss: 0.0717\n",
      "Epoch 16/300 - Train Loss: 0.0960, Val Loss: 0.0716\n",
      "Epoch 17/300 - Train Loss: 0.0997, Val Loss: 0.0732\n",
      "Epoch 18/300 - Train Loss: 0.0977, Val Loss: 0.0697\n",
      "Epoch 19/300 - Train Loss: 0.0968, Val Loss: 0.0804\n",
      "Epoch 20/300 - Train Loss: 0.0979, Val Loss: 0.0730\n",
      "Epoch 21/300 - Train Loss: 0.1001, Val Loss: 0.0724\n",
      "Epoch 22/300 - Train Loss: 0.0986, Val Loss: 0.0741\n",
      "Epoch 23/300 - Train Loss: 0.1002, Val Loss: 0.0749\n",
      "Epoch 24/300 - Train Loss: 0.0992, Val Loss: 0.0811\n",
      "Epoch 25/300 - Train Loss: 0.1014, Val Loss: 0.0710\n",
      "Epoch 26/300 - Train Loss: 0.0989, Val Loss: 0.0707\n",
      "Epoch 27/300 - Train Loss: 0.1033, Val Loss: 0.0741\n",
      "Epoch 28/300 - Train Loss: 0.1014, Val Loss: 0.0745\n",
      "Epoch 29/300 - Train Loss: 0.0995, Val Loss: 0.0713\n",
      "Epoch 30/300 - Train Loss: 0.0997, Val Loss: 0.0705\n",
      "Epoch 31/300 - Train Loss: 0.1031, Val Loss: 0.0719\n",
      "Epoch 32/300 - Train Loss: 0.0991, Val Loss: 0.0752\n",
      "Epoch 33/300 - Train Loss: 0.1043, Val Loss: 0.0744\n",
      "Epoch 34/300 - Train Loss: 0.1015, Val Loss: 0.0791\n",
      "Epoch 35/300 - Train Loss: 0.1025, Val Loss: 0.0691\n",
      "Epoch 36/300 - Train Loss: 0.1021, Val Loss: 0.0715\n",
      "Epoch 37/300 - Train Loss: 0.1016, Val Loss: 0.0745\n",
      "Epoch 38/300 - Train Loss: 0.1005, Val Loss: 0.0730\n",
      "Epoch 39/300 - Train Loss: 0.1008, Val Loss: 0.0735\n",
      "Epoch 40/300 - Train Loss: 0.1012, Val Loss: 0.0717\n",
      "Epoch 41/300 - Train Loss: 0.1015, Val Loss: 0.0710\n",
      "Epoch 42/300 - Train Loss: 0.1013, Val Loss: 0.0719\n",
      "Epoch 43/300 - Train Loss: 0.0986, Val Loss: 0.0758\n",
      "Epoch 44/300 - Train Loss: 0.1032, Val Loss: 0.0753\n",
      "Epoch 45/300 - Train Loss: 0.1040, Val Loss: 0.0742\n",
      "Epoch 46/300 - Train Loss: 0.1019, Val Loss: 0.0733\n",
      "Epoch 47/300 - Train Loss: 0.1023, Val Loss: 0.0753\n",
      "Epoch 48/300 - Train Loss: 0.1009, Val Loss: 0.0727\n",
      "Epoch 49/300 - Train Loss: 0.1024, Val Loss: 0.0711\n",
      "Epoch 50/300 - Train Loss: 0.1024, Val Loss: 0.0745\n",
      "Epoch 51/300 - Train Loss: 0.1029, Val Loss: 0.0712\n",
      "Epoch 52/300 - Train Loss: 0.1046, Val Loss: 0.0752\n",
      "Epoch 53/300 - Train Loss: 0.1022, Val Loss: 0.0731\n",
      "Epoch 54/300 - Train Loss: 0.1039, Val Loss: 0.0761\n",
      "Epoch 55/300 - Train Loss: 0.0996, Val Loss: 0.0762\n",
      "Epoch 56/300 - Train Loss: 0.1033, Val Loss: 0.0764\n",
      "Epoch 57/300 - Train Loss: 0.1034, Val Loss: 0.0782\n",
      "Epoch 58/300 - Train Loss: 0.1019, Val Loss: 0.0718\n",
      "Epoch 59/300 - Train Loss: 0.1014, Val Loss: 0.0747\n",
      "Epoch 60/300 - Train Loss: 0.1042, Val Loss: 0.0730\n",
      "Epoch 61/300 - Train Loss: 0.1064, Val Loss: 0.0720\n",
      "Epoch 62/300 - Train Loss: 0.1045, Val Loss: 0.0732\n",
      "Epoch 63/300 - Train Loss: 0.1020, Val Loss: 0.0753\n",
      "Epoch 64/300 - Train Loss: 0.1070, Val Loss: 0.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:16:31,204] Trial 159 finished with value: 0.9714831951674058 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.3297476578261809, 'learning_rate': 9.098628020028013e-05, 'batch_size': 32, 'weight_decay': 0.005135115893872556}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300 - Train Loss: 0.1040, Val Loss: 0.0826\n",
      "Early stopping at epoch 65\n",
      "Macro F1 Score: 0.9715, Macro Precision: 0.9777, Macro Recall: 0.9656\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 161\n",
      "Training with F1=8, F2=32, D=4, dropout=0.3417008825127941, LR=9.19622354131915e-05, BS=32, WD=0.0047175257558416325\n",
      "Epoch 1/300 - Train Loss: 0.3615, Val Loss: 0.1614\n",
      "Epoch 2/300 - Train Loss: 0.1667, Val Loss: 0.1114\n",
      "Epoch 3/300 - Train Loss: 0.1363, Val Loss: 0.0977\n",
      "Epoch 4/300 - Train Loss: 0.1177, Val Loss: 0.0840\n",
      "Epoch 5/300 - Train Loss: 0.1091, Val Loss: 0.0773\n",
      "Epoch 6/300 - Train Loss: 0.1062, Val Loss: 0.0764\n",
      "Epoch 7/300 - Train Loss: 0.1045, Val Loss: 0.0774\n",
      "Epoch 8/300 - Train Loss: 0.1043, Val Loss: 0.0769\n",
      "Epoch 9/300 - Train Loss: 0.1038, Val Loss: 0.0795\n",
      "Epoch 10/300 - Train Loss: 0.1016, Val Loss: 0.0799\n",
      "Epoch 11/300 - Train Loss: 0.0999, Val Loss: 0.0775\n",
      "Epoch 12/300 - Train Loss: 0.1009, Val Loss: 0.0745\n",
      "Epoch 13/300 - Train Loss: 0.0994, Val Loss: 0.0797\n",
      "Epoch 14/300 - Train Loss: 0.1031, Val Loss: 0.0814\n",
      "Epoch 15/300 - Train Loss: 0.1003, Val Loss: 0.0812\n",
      "Epoch 16/300 - Train Loss: 0.1000, Val Loss: 0.0744\n",
      "Epoch 17/300 - Train Loss: 0.1010, Val Loss: 0.0757\n",
      "Epoch 18/300 - Train Loss: 0.1015, Val Loss: 0.0808\n",
      "Epoch 19/300 - Train Loss: 0.1000, Val Loss: 0.0765\n",
      "Epoch 20/300 - Train Loss: 0.0984, Val Loss: 0.0738\n",
      "Epoch 21/300 - Train Loss: 0.1010, Val Loss: 0.0806\n",
      "Epoch 22/300 - Train Loss: 0.1021, Val Loss: 0.0749\n",
      "Epoch 23/300 - Train Loss: 0.1016, Val Loss: 0.0730\n",
      "Epoch 24/300 - Train Loss: 0.1007, Val Loss: 0.0772\n",
      "Epoch 25/300 - Train Loss: 0.0994, Val Loss: 0.0745\n",
      "Epoch 26/300 - Train Loss: 0.1009, Val Loss: 0.0763\n",
      "Epoch 27/300 - Train Loss: 0.1014, Val Loss: 0.0707\n",
      "Epoch 28/300 - Train Loss: 0.1021, Val Loss: 0.0727\n",
      "Epoch 29/300 - Train Loss: 0.1006, Val Loss: 0.0751\n",
      "Epoch 30/300 - Train Loss: 0.1040, Val Loss: 0.0777\n",
      "Epoch 31/300 - Train Loss: 0.1040, Val Loss: 0.0748\n",
      "Epoch 32/300 - Train Loss: 0.0999, Val Loss: 0.0813\n",
      "Epoch 33/300 - Train Loss: 0.1017, Val Loss: 0.0742\n",
      "Epoch 34/300 - Train Loss: 0.1031, Val Loss: 0.0786\n",
      "Epoch 35/300 - Train Loss: 0.1038, Val Loss: 0.0749\n",
      "Epoch 36/300 - Train Loss: 0.1020, Val Loss: 0.0748\n",
      "Epoch 37/300 - Train Loss: 0.1019, Val Loss: 0.0827\n",
      "Epoch 38/300 - Train Loss: 0.1012, Val Loss: 0.0743\n",
      "Epoch 39/300 - Train Loss: 0.0994, Val Loss: 0.0772\n",
      "Epoch 40/300 - Train Loss: 0.1018, Val Loss: 0.0745\n",
      "Epoch 41/300 - Train Loss: 0.0998, Val Loss: 0.0754\n",
      "Epoch 42/300 - Train Loss: 0.1027, Val Loss: 0.0755\n",
      "Epoch 43/300 - Train Loss: 0.1046, Val Loss: 0.0804\n",
      "Epoch 44/300 - Train Loss: 0.1040, Val Loss: 0.0719\n",
      "Epoch 45/300 - Train Loss: 0.1059, Val Loss: 0.0725\n",
      "Epoch 46/300 - Train Loss: 0.1038, Val Loss: 0.0726\n",
      "Epoch 47/300 - Train Loss: 0.1012, Val Loss: 0.0759\n",
      "Epoch 48/300 - Train Loss: 0.1022, Val Loss: 0.0738\n",
      "Epoch 49/300 - Train Loss: 0.1046, Val Loss: 0.0781\n",
      "Epoch 50/300 - Train Loss: 0.1034, Val Loss: 0.0761\n",
      "Epoch 51/300 - Train Loss: 0.1022, Val Loss: 0.0826\n",
      "Epoch 52/300 - Train Loss: 0.1028, Val Loss: 0.0786\n",
      "Epoch 53/300 - Train Loss: 0.1021, Val Loss: 0.0774\n",
      "Epoch 54/300 - Train Loss: 0.1037, Val Loss: 0.0758\n",
      "Epoch 55/300 - Train Loss: 0.1032, Val Loss: 0.0737\n",
      "Epoch 56/300 - Train Loss: 0.1061, Val Loss: 0.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:18:28,190] Trial 160 finished with value: 0.9708404151546982 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.3417008825127941, 'learning_rate': 9.19622354131915e-05, 'batch_size': 32, 'weight_decay': 0.0047175257558416325}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/300 - Train Loss: 0.1023, Val Loss: 0.0729\n",
      "Early stopping at epoch 57\n",
      "Macro F1 Score: 0.9708, Macro Precision: 0.9720, Macro Recall: 0.9698\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 162\n",
      "Training with F1=16, F2=32, D=4, dropout=0.32262537794363727, LR=0.00010079512650049854, BS=32, WD=0.0039458006997866585\n",
      "Epoch 1/300 - Train Loss: 0.2909, Val Loss: 0.1575\n",
      "Epoch 2/300 - Train Loss: 0.1381, Val Loss: 0.0897\n",
      "Epoch 3/300 - Train Loss: 0.1133, Val Loss: 0.0873\n",
      "Epoch 4/300 - Train Loss: 0.1075, Val Loss: 0.0865\n",
      "Epoch 5/300 - Train Loss: 0.1028, Val Loss: 0.0910\n",
      "Epoch 6/300 - Train Loss: 0.1002, Val Loss: 0.0789\n",
      "Epoch 7/300 - Train Loss: 0.0990, Val Loss: 0.0868\n",
      "Epoch 8/300 - Train Loss: 0.0963, Val Loss: 0.0762\n",
      "Epoch 9/300 - Train Loss: 0.0957, Val Loss: 0.0736\n",
      "Epoch 10/300 - Train Loss: 0.0973, Val Loss: 0.0853\n",
      "Epoch 11/300 - Train Loss: 0.0977, Val Loss: 0.0815\n",
      "Epoch 12/300 - Train Loss: 0.0934, Val Loss: 0.0737\n",
      "Epoch 13/300 - Train Loss: 0.0943, Val Loss: 0.0777\n",
      "Epoch 14/300 - Train Loss: 0.0957, Val Loss: 0.0859\n",
      "Epoch 15/300 - Train Loss: 0.0936, Val Loss: 0.0731\n",
      "Epoch 16/300 - Train Loss: 0.0947, Val Loss: 0.0707\n",
      "Epoch 17/300 - Train Loss: 0.0923, Val Loss: 0.0705\n",
      "Epoch 18/300 - Train Loss: 0.0928, Val Loss: 0.0693\n",
      "Epoch 19/300 - Train Loss: 0.0936, Val Loss: 0.0719\n",
      "Epoch 20/300 - Train Loss: 0.0944, Val Loss: 0.0736\n",
      "Epoch 21/300 - Train Loss: 0.0950, Val Loss: 0.0747\n",
      "Epoch 22/300 - Train Loss: 0.0916, Val Loss: 0.0718\n",
      "Epoch 23/300 - Train Loss: 0.0942, Val Loss: 0.0739\n",
      "Epoch 24/300 - Train Loss: 0.0937, Val Loss: 0.0757\n",
      "Epoch 25/300 - Train Loss: 0.0946, Val Loss: 0.0752\n",
      "Epoch 26/300 - Train Loss: 0.0972, Val Loss: 0.0711\n",
      "Epoch 27/300 - Train Loss: 0.0921, Val Loss: 0.0754\n",
      "Epoch 28/300 - Train Loss: 0.0944, Val Loss: 0.0737\n",
      "Epoch 29/300 - Train Loss: 0.0942, Val Loss: 0.0778\n",
      "Epoch 30/300 - Train Loss: 0.0975, Val Loss: 0.0723\n",
      "Epoch 31/300 - Train Loss: 0.0916, Val Loss: 0.0679\n",
      "Epoch 32/300 - Train Loss: 0.0967, Val Loss: 0.0764\n",
      "Epoch 33/300 - Train Loss: 0.0948, Val Loss: 0.0727\n",
      "Epoch 34/300 - Train Loss: 0.0957, Val Loss: 0.0692\n",
      "Epoch 35/300 - Train Loss: 0.0959, Val Loss: 0.0716\n",
      "Epoch 36/300 - Train Loss: 0.0960, Val Loss: 0.0742\n",
      "Epoch 37/300 - Train Loss: 0.0939, Val Loss: 0.0795\n",
      "Epoch 38/300 - Train Loss: 0.0972, Val Loss: 0.0756\n",
      "Epoch 39/300 - Train Loss: 0.0953, Val Loss: 0.0697\n",
      "Epoch 40/300 - Train Loss: 0.0942, Val Loss: 0.0694\n",
      "Epoch 41/300 - Train Loss: 0.0978, Val Loss: 0.0710\n",
      "Epoch 42/300 - Train Loss: 0.1014, Val Loss: 0.0749\n",
      "Epoch 43/300 - Train Loss: 0.1003, Val Loss: 0.0675\n",
      "Epoch 44/300 - Train Loss: 0.0964, Val Loss: 0.0704\n",
      "Epoch 45/300 - Train Loss: 0.0989, Val Loss: 0.0706\n",
      "Epoch 46/300 - Train Loss: 0.0984, Val Loss: 0.0698\n",
      "Epoch 47/300 - Train Loss: 0.0976, Val Loss: 0.0708\n",
      "Epoch 48/300 - Train Loss: 0.0974, Val Loss: 0.0757\n",
      "Epoch 49/300 - Train Loss: 0.0990, Val Loss: 0.0717\n",
      "Epoch 50/300 - Train Loss: 0.0983, Val Loss: 0.0724\n",
      "Epoch 51/300 - Train Loss: 0.0987, Val Loss: 0.0740\n",
      "Epoch 52/300 - Train Loss: 0.0966, Val Loss: 0.0761\n",
      "Epoch 53/300 - Train Loss: 0.1010, Val Loss: 0.0696\n",
      "Epoch 54/300 - Train Loss: 0.0955, Val Loss: 0.0775\n",
      "Epoch 55/300 - Train Loss: 0.0982, Val Loss: 0.0756\n",
      "Epoch 56/300 - Train Loss: 0.0998, Val Loss: 0.0752\n",
      "Epoch 57/300 - Train Loss: 0.0996, Val Loss: 0.0705\n",
      "Epoch 58/300 - Train Loss: 0.1010, Val Loss: 0.0709\n",
      "Epoch 59/300 - Train Loss: 0.0982, Val Loss: 0.0797\n",
      "Epoch 60/300 - Train Loss: 0.0977, Val Loss: 0.0721\n",
      "Epoch 61/300 - Train Loss: 0.0951, Val Loss: 0.0740\n",
      "Epoch 62/300 - Train Loss: 0.0963, Val Loss: 0.0744\n",
      "Epoch 63/300 - Train Loss: 0.0977, Val Loss: 0.0709\n",
      "Epoch 64/300 - Train Loss: 0.1014, Val Loss: 0.0741\n",
      "Epoch 65/300 - Train Loss: 0.0968, Val Loss: 0.0761\n",
      "Epoch 66/300 - Train Loss: 0.0988, Val Loss: 0.0793\n",
      "Epoch 67/300 - Train Loss: 0.0977, Val Loss: 0.0735\n",
      "Epoch 68/300 - Train Loss: 0.0995, Val Loss: 0.0776\n",
      "Epoch 69/300 - Train Loss: 0.0991, Val Loss: 0.0719\n",
      "Epoch 70/300 - Train Loss: 0.0970, Val Loss: 0.0703\n",
      "Epoch 71/300 - Train Loss: 0.0976, Val Loss: 0.0749\n",
      "Epoch 72/300 - Train Loss: 0.0967, Val Loss: 0.0775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:21:31,281] Trial 161 finished with value: 0.965872251839027 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.32262537794363727, 'learning_rate': 0.00010079512650049854, 'batch_size': 32, 'weight_decay': 0.0039458006997866585}. Best is trial 62 with value: 0.9742503388691485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/300 - Train Loss: 0.0992, Val Loss: 0.0739\n",
      "Early stopping at epoch 73\n",
      "Macro F1 Score: 0.9659, Macro Precision: 0.9670, Macro Recall: 0.9649\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 163\n",
      "Training with F1=16, F2=32, D=4, dropout=0.3057340737191981, LR=7.889461667548952e-05, BS=32, WD=0.005962664841358983\n",
      "Epoch 1/300 - Train Loss: 0.3449, Val Loss: 0.1468\n",
      "Epoch 2/300 - Train Loss: 0.1390, Val Loss: 0.0939\n",
      "Epoch 3/300 - Train Loss: 0.1107, Val Loss: 0.0951\n",
      "Epoch 4/300 - Train Loss: 0.1046, Val Loss: 0.0803\n",
      "Epoch 5/300 - Train Loss: 0.0994, Val Loss: 0.0734\n",
      "Epoch 6/300 - Train Loss: 0.0979, Val Loss: 0.0748\n",
      "Epoch 7/300 - Train Loss: 0.0970, Val Loss: 0.0821\n",
      "Epoch 8/300 - Train Loss: 0.0981, Val Loss: 0.0791\n",
      "Epoch 9/300 - Train Loss: 0.0987, Val Loss: 0.0752\n",
      "Epoch 10/300 - Train Loss: 0.0964, Val Loss: 0.0722\n",
      "Epoch 11/300 - Train Loss: 0.0953, Val Loss: 0.0734\n",
      "Epoch 12/300 - Train Loss: 0.0944, Val Loss: 0.0758\n",
      "Epoch 13/300 - Train Loss: 0.0943, Val Loss: 0.0699\n",
      "Epoch 14/300 - Train Loss: 0.0938, Val Loss: 0.0696\n",
      "Epoch 15/300 - Train Loss: 0.0965, Val Loss: 0.0710\n",
      "Epoch 16/300 - Train Loss: 0.0954, Val Loss: 0.0724\n",
      "Epoch 17/300 - Train Loss: 0.0948, Val Loss: 0.0710\n",
      "Epoch 18/300 - Train Loss: 0.0964, Val Loss: 0.0718\n",
      "Epoch 19/300 - Train Loss: 0.0945, Val Loss: 0.0694\n",
      "Epoch 20/300 - Train Loss: 0.0966, Val Loss: 0.0693\n",
      "Epoch 21/300 - Train Loss: 0.0947, Val Loss: 0.0707\n",
      "Epoch 22/300 - Train Loss: 0.0956, Val Loss: 0.0722\n",
      "Epoch 23/300 - Train Loss: 0.0934, Val Loss: 0.0718\n",
      "Epoch 24/300 - Train Loss: 0.0955, Val Loss: 0.0711\n",
      "Epoch 25/300 - Train Loss: 0.0982, Val Loss: 0.0692\n",
      "Epoch 26/300 - Train Loss: 0.0992, Val Loss: 0.0700\n",
      "Epoch 27/300 - Train Loss: 0.0979, Val Loss: 0.0716\n",
      "Epoch 28/300 - Train Loss: 0.0999, Val Loss: 0.0761\n",
      "Epoch 29/300 - Train Loss: 0.0988, Val Loss: 0.0715\n",
      "Epoch 30/300 - Train Loss: 0.0973, Val Loss: 0.0691\n",
      "Epoch 31/300 - Train Loss: 0.0973, Val Loss: 0.0862\n",
      "Epoch 32/300 - Train Loss: 0.0969, Val Loss: 0.0712\n",
      "Epoch 33/300 - Train Loss: 0.0972, Val Loss: 0.0695\n",
      "Epoch 34/300 - Train Loss: 0.0974, Val Loss: 0.0672\n",
      "Epoch 35/300 - Train Loss: 0.0993, Val Loss: 0.0705\n",
      "Epoch 36/300 - Train Loss: 0.1006, Val Loss: 0.0721\n",
      "Epoch 37/300 - Train Loss: 0.0990, Val Loss: 0.0683\n",
      "Epoch 38/300 - Train Loss: 0.1000, Val Loss: 0.0762\n",
      "Epoch 39/300 - Train Loss: 0.0991, Val Loss: 0.0749\n",
      "Epoch 40/300 - Train Loss: 0.0990, Val Loss: 0.0804\n",
      "Epoch 41/300 - Train Loss: 0.0994, Val Loss: 0.0713\n",
      "Epoch 42/300 - Train Loss: 0.0985, Val Loss: 0.0762\n",
      "Epoch 43/300 - Train Loss: 0.1012, Val Loss: 0.0720\n",
      "Epoch 44/300 - Train Loss: 0.1009, Val Loss: 0.0736\n",
      "Epoch 45/300 - Train Loss: 0.1034, Val Loss: 0.0725\n",
      "Epoch 46/300 - Train Loss: 0.1008, Val Loss: 0.0735\n",
      "Epoch 47/300 - Train Loss: 0.1014, Val Loss: 0.0753\n",
      "Epoch 48/300 - Train Loss: 0.1004, Val Loss: 0.0691\n",
      "Epoch 49/300 - Train Loss: 0.1019, Val Loss: 0.0737\n",
      "Epoch 50/300 - Train Loss: 0.1013, Val Loss: 0.0754\n",
      "Epoch 51/300 - Train Loss: 0.1022, Val Loss: 0.0765\n",
      "Epoch 52/300 - Train Loss: 0.1032, Val Loss: 0.0776\n",
      "Epoch 53/300 - Train Loss: 0.1014, Val Loss: 0.0692\n",
      "Epoch 54/300 - Train Loss: 0.1021, Val Loss: 0.0734\n",
      "Epoch 55/300 - Train Loss: 0.1008, Val Loss: 0.0799\n",
      "Epoch 56/300 - Train Loss: 0.1002, Val Loss: 0.0687\n",
      "Epoch 57/300 - Train Loss: 0.1012, Val Loss: 0.0680\n",
      "Epoch 58/300 - Train Loss: 0.1020, Val Loss: 0.0693\n",
      "Epoch 59/300 - Train Loss: 0.1014, Val Loss: 0.0725\n",
      "Epoch 60/300 - Train Loss: 0.1033, Val Loss: 0.0717\n",
      "Epoch 61/300 - Train Loss: 0.1013, Val Loss: 0.0709\n",
      "Epoch 62/300 - Train Loss: 0.1011, Val Loss: 0.0730\n",
      "Epoch 63/300 - Train Loss: 0.1027, Val Loss: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:24:11,829] Trial 162 finished with value: 0.9761195541557947 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.3057340737191981, 'learning_rate': 7.889461667548952e-05, 'batch_size': 32, 'weight_decay': 0.005962664841358983}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300 - Train Loss: 0.1028, Val Loss: 0.0709\n",
      "Early stopping at epoch 64\n",
      "Macro F1 Score: 0.9761, Macro Precision: 0.9827, Macro Recall: 0.9700\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       789\n",
      "           1       0.98      0.95      0.97        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 164\n",
      "Training with F1=16, F2=32, D=4, dropout=0.3287104668625416, LR=8.111981446008971e-05, BS=32, WD=0.005132098232463278\n",
      "Epoch 1/300 - Train Loss: 0.3242, Val Loss: 0.1483\n",
      "Epoch 2/300 - Train Loss: 0.1474, Val Loss: 0.1093\n",
      "Epoch 3/300 - Train Loss: 0.1182, Val Loss: 0.0961\n",
      "Epoch 4/300 - Train Loss: 0.1073, Val Loss: 0.0857\n",
      "Epoch 5/300 - Train Loss: 0.1054, Val Loss: 0.0877\n",
      "Epoch 6/300 - Train Loss: 0.1032, Val Loss: 0.0861\n",
      "Epoch 7/300 - Train Loss: 0.1011, Val Loss: 0.0783\n",
      "Epoch 8/300 - Train Loss: 0.0990, Val Loss: 0.0753\n",
      "Epoch 9/300 - Train Loss: 0.0975, Val Loss: 0.0763\n",
      "Epoch 10/300 - Train Loss: 0.0976, Val Loss: 0.0757\n",
      "Epoch 11/300 - Train Loss: 0.0982, Val Loss: 0.0777\n",
      "Epoch 12/300 - Train Loss: 0.0979, Val Loss: 0.0749\n",
      "Epoch 13/300 - Train Loss: 0.0981, Val Loss: 0.0771\n",
      "Epoch 14/300 - Train Loss: 0.0975, Val Loss: 0.0727\n",
      "Epoch 15/300 - Train Loss: 0.0980, Val Loss: 0.0759\n",
      "Epoch 16/300 - Train Loss: 0.0982, Val Loss: 0.0703\n",
      "Epoch 17/300 - Train Loss: 0.0987, Val Loss: 0.0736\n",
      "Epoch 18/300 - Train Loss: 0.0988, Val Loss: 0.0717\n",
      "Epoch 19/300 - Train Loss: 0.0952, Val Loss: 0.0748\n",
      "Epoch 20/300 - Train Loss: 0.0981, Val Loss: 0.0696\n",
      "Epoch 21/300 - Train Loss: 0.0978, Val Loss: 0.0726\n",
      "Epoch 22/300 - Train Loss: 0.1001, Val Loss: 0.0709\n",
      "Epoch 23/300 - Train Loss: 0.0967, Val Loss: 0.0724\n",
      "Epoch 24/300 - Train Loss: 0.0997, Val Loss: 0.0722\n",
      "Epoch 25/300 - Train Loss: 0.0988, Val Loss: 0.0717\n",
      "Epoch 26/300 - Train Loss: 0.0955, Val Loss: 0.0718\n",
      "Epoch 27/300 - Train Loss: 0.0961, Val Loss: 0.0692\n",
      "Epoch 28/300 - Train Loss: 0.0980, Val Loss: 0.0766\n",
      "Epoch 29/300 - Train Loss: 0.0975, Val Loss: 0.0841\n",
      "Epoch 30/300 - Train Loss: 0.0983, Val Loss: 0.0762\n",
      "Epoch 31/300 - Train Loss: 0.0998, Val Loss: 0.0726\n",
      "Epoch 32/300 - Train Loss: 0.0977, Val Loss: 0.0706\n",
      "Epoch 33/300 - Train Loss: 0.0983, Val Loss: 0.0718\n",
      "Epoch 34/300 - Train Loss: 0.0992, Val Loss: 0.0697\n",
      "Epoch 35/300 - Train Loss: 0.1030, Val Loss: 0.0736\n",
      "Epoch 36/300 - Train Loss: 0.0979, Val Loss: 0.0722\n",
      "Epoch 37/300 - Train Loss: 0.1018, Val Loss: 0.0726\n",
      "Epoch 38/300 - Train Loss: 0.1006, Val Loss: 0.0742\n",
      "Epoch 39/300 - Train Loss: 0.1006, Val Loss: 0.0739\n",
      "Epoch 40/300 - Train Loss: 0.1011, Val Loss: 0.0707\n",
      "Epoch 41/300 - Train Loss: 0.1031, Val Loss: 0.0711\n",
      "Epoch 42/300 - Train Loss: 0.1000, Val Loss: 0.0776\n",
      "Epoch 43/300 - Train Loss: 0.0962, Val Loss: 0.0704\n",
      "Epoch 44/300 - Train Loss: 0.1017, Val Loss: 0.0733\n",
      "Epoch 45/300 - Train Loss: 0.0968, Val Loss: 0.0704\n",
      "Epoch 46/300 - Train Loss: 0.1009, Val Loss: 0.0716\n",
      "Epoch 47/300 - Train Loss: 0.0978, Val Loss: 0.0730\n",
      "Epoch 48/300 - Train Loss: 0.1000, Val Loss: 0.0756\n",
      "Epoch 49/300 - Train Loss: 0.0983, Val Loss: 0.0727\n",
      "Epoch 50/300 - Train Loss: 0.1003, Val Loss: 0.0759\n",
      "Epoch 51/300 - Train Loss: 0.0996, Val Loss: 0.0696\n",
      "Epoch 52/300 - Train Loss: 0.1047, Val Loss: 0.0699\n",
      "Epoch 53/300 - Train Loss: 0.1009, Val Loss: 0.0689\n",
      "Epoch 54/300 - Train Loss: 0.0994, Val Loss: 0.0717\n",
      "Epoch 55/300 - Train Loss: 0.0997, Val Loss: 0.0720\n",
      "Epoch 56/300 - Train Loss: 0.0989, Val Loss: 0.0701\n",
      "Epoch 57/300 - Train Loss: 0.1015, Val Loss: 0.0708\n",
      "Epoch 58/300 - Train Loss: 0.1014, Val Loss: 0.0695\n",
      "Epoch 59/300 - Train Loss: 0.1005, Val Loss: 0.0709\n",
      "Epoch 60/300 - Train Loss: 0.1049, Val Loss: 0.0708\n",
      "Epoch 61/300 - Train Loss: 0.1047, Val Loss: 0.0695\n",
      "Epoch 62/300 - Train Loss: 0.1024, Val Loss: 0.0726\n",
      "Epoch 63/300 - Train Loss: 0.1030, Val Loss: 0.0693\n",
      "Epoch 64/300 - Train Loss: 0.1025, Val Loss: 0.0731\n",
      "Epoch 65/300 - Train Loss: 0.1031, Val Loss: 0.0710\n",
      "Epoch 66/300 - Train Loss: 0.1044, Val Loss: 0.0711\n",
      "Epoch 67/300 - Train Loss: 0.1015, Val Loss: 0.0735\n",
      "Epoch 68/300 - Train Loss: 0.0993, Val Loss: 0.0767\n",
      "Epoch 69/300 - Train Loss: 0.1035, Val Loss: 0.0695\n",
      "Epoch 70/300 - Train Loss: 0.1028, Val Loss: 0.0712\n",
      "Epoch 71/300 - Train Loss: 0.1029, Val Loss: 0.0722\n",
      "Epoch 72/300 - Train Loss: 0.0992, Val Loss: 0.0713\n",
      "Epoch 73/300 - Train Loss: 0.0997, Val Loss: 0.0704\n",
      "Epoch 74/300 - Train Loss: 0.1016, Val Loss: 0.0728\n",
      "Epoch 75/300 - Train Loss: 0.1013, Val Loss: 0.0723\n",
      "Epoch 76/300 - Train Loss: 0.1044, Val Loss: 0.0709\n",
      "Epoch 77/300 - Train Loss: 0.1004, Val Loss: 0.0717\n",
      "Epoch 78/300 - Train Loss: 0.1012, Val Loss: 0.0726\n",
      "Epoch 79/300 - Train Loss: 0.1018, Val Loss: 0.0732\n",
      "Epoch 80/300 - Train Loss: 0.0990, Val Loss: 0.0704\n",
      "Epoch 81/300 - Train Loss: 0.1048, Val Loss: 0.0736\n",
      "Epoch 82/300 - Train Loss: 0.1018, Val Loss: 0.0754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:27:40,118] Trial 163 finished with value: 0.9709922482280325 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.3287104668625416, 'learning_rate': 8.111981446008971e-05, 'batch_size': 32, 'weight_decay': 0.005132098232463278}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/300 - Train Loss: 0.1026, Val Loss: 0.0846\n",
      "Early stopping at epoch 83\n",
      "Macro F1 Score: 0.9710, Macro Precision: 0.9771, Macro Recall: 0.9652\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 165\n",
      "Training with F1=16, F2=32, D=4, dropout=0.3046901135402996, LR=0.000681015510758135, BS=32, WD=0.006065041015143326\n",
      "Epoch 1/300 - Train Loss: 0.1627, Val Loss: 0.0977\n",
      "Epoch 2/300 - Train Loss: 0.1133, Val Loss: 0.0864\n",
      "Epoch 3/300 - Train Loss: 0.1151, Val Loss: 0.0813\n",
      "Epoch 4/300 - Train Loss: 0.1132, Val Loss: 0.0947\n",
      "Epoch 5/300 - Train Loss: 0.1149, Val Loss: 0.0948\n",
      "Epoch 6/300 - Train Loss: 0.1180, Val Loss: 0.0879\n",
      "Epoch 7/300 - Train Loss: 0.1157, Val Loss: 0.0933\n",
      "Epoch 8/300 - Train Loss: 0.1142, Val Loss: 0.0882\n",
      "Epoch 9/300 - Train Loss: 0.1186, Val Loss: 0.1566\n",
      "Epoch 10/300 - Train Loss: 0.1185, Val Loss: 0.0816\n",
      "Epoch 11/300 - Train Loss: 0.1150, Val Loss: 0.1054\n",
      "Epoch 12/300 - Train Loss: 0.1168, Val Loss: 0.0802\n",
      "Epoch 13/300 - Train Loss: 0.1200, Val Loss: 0.0819\n",
      "Epoch 14/300 - Train Loss: 0.1165, Val Loss: 0.0891\n",
      "Epoch 15/300 - Train Loss: 0.1144, Val Loss: 0.1090\n",
      "Epoch 16/300 - Train Loss: 0.1144, Val Loss: 0.1143\n",
      "Epoch 17/300 - Train Loss: 0.1147, Val Loss: 0.0840\n",
      "Epoch 18/300 - Train Loss: 0.1152, Val Loss: 0.1396\n",
      "Epoch 19/300 - Train Loss: 0.1148, Val Loss: 0.0826\n",
      "Epoch 20/300 - Train Loss: 0.1172, Val Loss: 0.1164\n",
      "Epoch 21/300 - Train Loss: 0.1147, Val Loss: 0.1030\n",
      "Epoch 22/300 - Train Loss: 0.1180, Val Loss: 0.0850\n",
      "Epoch 23/300 - Train Loss: 0.1124, Val Loss: 0.0877\n",
      "Epoch 24/300 - Train Loss: 0.1159, Val Loss: 0.0808\n",
      "Epoch 25/300 - Train Loss: 0.1153, Val Loss: 0.0836\n",
      "Epoch 26/300 - Train Loss: 0.1146, Val Loss: 0.0917\n",
      "Epoch 27/300 - Train Loss: 0.1150, Val Loss: 0.1350\n",
      "Epoch 28/300 - Train Loss: 0.1159, Val Loss: 0.0772\n",
      "Epoch 29/300 - Train Loss: 0.1144, Val Loss: 0.0950\n",
      "Epoch 30/300 - Train Loss: 0.1150, Val Loss: 0.0809\n",
      "Epoch 31/300 - Train Loss: 0.1169, Val Loss: 0.0831\n",
      "Epoch 32/300 - Train Loss: 0.1137, Val Loss: 0.0794\n",
      "Epoch 33/300 - Train Loss: 0.1140, Val Loss: 0.0814\n",
      "Epoch 34/300 - Train Loss: 0.1158, Val Loss: 0.0806\n",
      "Epoch 35/300 - Train Loss: 0.1146, Val Loss: 0.0774\n",
      "Epoch 36/300 - Train Loss: 0.1131, Val Loss: 0.0934\n",
      "Epoch 37/300 - Train Loss: 0.1152, Val Loss: 0.0795\n",
      "Epoch 38/300 - Train Loss: 0.1135, Val Loss: 0.0764\n",
      "Epoch 39/300 - Train Loss: 0.1148, Val Loss: 0.0861\n",
      "Epoch 40/300 - Train Loss: 0.1144, Val Loss: 0.0867\n",
      "Epoch 41/300 - Train Loss: 0.1123, Val Loss: 0.0886\n",
      "Epoch 42/300 - Train Loss: 0.1175, Val Loss: 0.0877\n",
      "Epoch 43/300 - Train Loss: 0.1159, Val Loss: 0.0797\n",
      "Epoch 44/300 - Train Loss: 0.1161, Val Loss: 0.0869\n",
      "Epoch 45/300 - Train Loss: 0.1158, Val Loss: 0.0840\n",
      "Epoch 46/300 - Train Loss: 0.1160, Val Loss: 0.0802\n",
      "Epoch 47/300 - Train Loss: 0.1153, Val Loss: 0.0917\n",
      "Epoch 48/300 - Train Loss: 0.1113, Val Loss: 0.0890\n",
      "Epoch 49/300 - Train Loss: 0.1150, Val Loss: 0.0967\n",
      "Epoch 50/300 - Train Loss: 0.1163, Val Loss: 0.0818\n",
      "Epoch 51/300 - Train Loss: 0.1142, Val Loss: 0.0851\n",
      "Epoch 52/300 - Train Loss: 0.1174, Val Loss: 0.0785\n",
      "Epoch 53/300 - Train Loss: 0.1148, Val Loss: 0.0792\n",
      "Epoch 54/300 - Train Loss: 0.1142, Val Loss: 0.0770\n",
      "Epoch 55/300 - Train Loss: 0.1137, Val Loss: 0.0890\n",
      "Epoch 56/300 - Train Loss: 0.1150, Val Loss: 0.0832\n",
      "Epoch 57/300 - Train Loss: 0.1123, Val Loss: 0.0780\n",
      "Epoch 58/300 - Train Loss: 0.1144, Val Loss: 0.0819\n",
      "Epoch 59/300 - Train Loss: 0.1156, Val Loss: 0.0836\n",
      "Epoch 60/300 - Train Loss: 0.1144, Val Loss: 0.1229\n",
      "Epoch 61/300 - Train Loss: 0.1135, Val Loss: 0.0853\n",
      "Epoch 62/300 - Train Loss: 0.1138, Val Loss: 0.0837\n",
      "Epoch 63/300 - Train Loss: 0.1149, Val Loss: 0.0769\n",
      "Epoch 64/300 - Train Loss: 0.1143, Val Loss: 0.0848\n",
      "Epoch 65/300 - Train Loss: 0.1136, Val Loss: 0.0804\n",
      "Epoch 66/300 - Train Loss: 0.1178, Val Loss: 0.0786\n",
      "Epoch 67/300 - Train Loss: 0.1118, Val Loss: 0.0999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:30:30,918] Trial 164 finished with value: 0.9655314351885798 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.3046901135402996, 'learning_rate': 0.000681015510758135, 'batch_size': 32, 'weight_decay': 0.006065041015143326}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/300 - Train Loss: 0.1155, Val Loss: 0.0927\n",
      "Early stopping at epoch 68\n",
      "Macro F1 Score: 0.9655, Macro Precision: 0.9801, Macro Recall: 0.9525\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       789\n",
      "           1       0.98      0.90      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.95      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 166\n",
      "Training with F1=16, F2=32, D=4, dropout=0.30968485193898065, LR=7.526605624959887e-05, BS=32, WD=0.004514922721735093\n",
      "Epoch 1/300 - Train Loss: 0.3261, Val Loss: 0.1281\n",
      "Epoch 2/300 - Train Loss: 0.1246, Val Loss: 0.0961\n",
      "Epoch 3/300 - Train Loss: 0.1098, Val Loss: 0.0902\n",
      "Epoch 4/300 - Train Loss: 0.1036, Val Loss: 0.0812\n",
      "Epoch 5/300 - Train Loss: 0.0987, Val Loss: 0.0866\n",
      "Epoch 6/300 - Train Loss: 0.0998, Val Loss: 0.0791\n",
      "Epoch 7/300 - Train Loss: 0.0968, Val Loss: 0.0818\n",
      "Epoch 8/300 - Train Loss: 0.0963, Val Loss: 0.0747\n",
      "Epoch 9/300 - Train Loss: 0.0964, Val Loss: 0.0744\n",
      "Epoch 10/300 - Train Loss: 0.0939, Val Loss: 0.0785\n",
      "Epoch 11/300 - Train Loss: 0.0958, Val Loss: 0.0789\n",
      "Epoch 12/300 - Train Loss: 0.0943, Val Loss: 0.0711\n",
      "Epoch 13/300 - Train Loss: 0.0936, Val Loss: 0.0818\n",
      "Epoch 14/300 - Train Loss: 0.0929, Val Loss: 0.0737\n",
      "Epoch 15/300 - Train Loss: 0.0940, Val Loss: 0.0743\n",
      "Epoch 16/300 - Train Loss: 0.0967, Val Loss: 0.0695\n",
      "Epoch 17/300 - Train Loss: 0.0949, Val Loss: 0.0743\n",
      "Epoch 18/300 - Train Loss: 0.0931, Val Loss: 0.0701\n",
      "Epoch 19/300 - Train Loss: 0.0937, Val Loss: 0.0714\n",
      "Epoch 20/300 - Train Loss: 0.0923, Val Loss: 0.0692\n",
      "Epoch 21/300 - Train Loss: 0.0940, Val Loss: 0.0719\n",
      "Epoch 22/300 - Train Loss: 0.0922, Val Loss: 0.0710\n",
      "Epoch 23/300 - Train Loss: 0.0932, Val Loss: 0.0734\n",
      "Epoch 24/300 - Train Loss: 0.0969, Val Loss: 0.0745\n",
      "Epoch 25/300 - Train Loss: 0.0925, Val Loss: 0.0712\n",
      "Epoch 26/300 - Train Loss: 0.0940, Val Loss: 0.0776\n",
      "Epoch 27/300 - Train Loss: 0.0923, Val Loss: 0.0717\n",
      "Epoch 28/300 - Train Loss: 0.0945, Val Loss: 0.0693\n",
      "Epoch 29/300 - Train Loss: 0.0927, Val Loss: 0.0705\n",
      "Epoch 30/300 - Train Loss: 0.0948, Val Loss: 0.0703\n",
      "Epoch 31/300 - Train Loss: 0.0939, Val Loss: 0.0698\n",
      "Epoch 32/300 - Train Loss: 0.0966, Val Loss: 0.0701\n",
      "Epoch 33/300 - Train Loss: 0.0951, Val Loss: 0.0737\n",
      "Epoch 34/300 - Train Loss: 0.0933, Val Loss: 0.0801\n",
      "Epoch 35/300 - Train Loss: 0.0972, Val Loss: 0.0707\n",
      "Epoch 36/300 - Train Loss: 0.0958, Val Loss: 0.0737\n",
      "Epoch 37/300 - Train Loss: 0.0958, Val Loss: 0.0738\n",
      "Epoch 38/300 - Train Loss: 0.0950, Val Loss: 0.0784\n",
      "Epoch 39/300 - Train Loss: 0.0949, Val Loss: 0.0720\n",
      "Epoch 40/300 - Train Loss: 0.0960, Val Loss: 0.0697\n",
      "Epoch 41/300 - Train Loss: 0.0980, Val Loss: 0.0749\n",
      "Epoch 42/300 - Train Loss: 0.0975, Val Loss: 0.0712\n",
      "Epoch 43/300 - Train Loss: 0.0972, Val Loss: 0.0701\n",
      "Epoch 44/300 - Train Loss: 0.0974, Val Loss: 0.0719\n",
      "Epoch 45/300 - Train Loss: 0.0981, Val Loss: 0.0708\n",
      "Epoch 46/300 - Train Loss: 0.0964, Val Loss: 0.0745\n",
      "Epoch 47/300 - Train Loss: 0.0967, Val Loss: 0.0709\n",
      "Epoch 48/300 - Train Loss: 0.0950, Val Loss: 0.0726\n",
      "Epoch 49/300 - Train Loss: 0.0980, Val Loss: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:32:36,445] Trial 165 finished with value: 0.9613339232287608 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.30968485193898065, 'learning_rate': 7.526605624959887e-05, 'batch_size': 32, 'weight_decay': 0.004514922721735093}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300 - Train Loss: 0.0965, Val Loss: 0.0733\n",
      "Early stopping at epoch 50\n",
      "Macro F1 Score: 0.9613, Macro Precision: 0.9539, Macro Recall: 0.9695\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 167\n",
      "Training with F1=16, F2=32, D=4, dropout=0.34420803069450306, LR=0.0005016059337618488, BS=32, WD=0.005367250621207009\n",
      "Epoch 1/300 - Train Loss: 0.1667, Val Loss: 0.0837\n",
      "Epoch 2/300 - Train Loss: 0.1110, Val Loss: 0.0747\n",
      "Epoch 3/300 - Train Loss: 0.1088, Val Loss: 0.0778\n",
      "Epoch 4/300 - Train Loss: 0.1077, Val Loss: 0.0811\n",
      "Epoch 5/300 - Train Loss: 0.1098, Val Loss: 0.0889\n",
      "Epoch 6/300 - Train Loss: 0.1102, Val Loss: 0.0870\n",
      "Epoch 7/300 - Train Loss: 0.1158, Val Loss: 0.0839\n",
      "Epoch 8/300 - Train Loss: 0.1147, Val Loss: 0.0828\n",
      "Epoch 9/300 - Train Loss: 0.1156, Val Loss: 0.0893\n",
      "Epoch 10/300 - Train Loss: 0.1156, Val Loss: 0.0886\n",
      "Epoch 11/300 - Train Loss: 0.1150, Val Loss: 0.0964\n",
      "Epoch 12/300 - Train Loss: 0.1151, Val Loss: 0.0939\n",
      "Epoch 13/300 - Train Loss: 0.1177, Val Loss: 0.0799\n",
      "Epoch 14/300 - Train Loss: 0.1124, Val Loss: 0.0856\n",
      "Epoch 15/300 - Train Loss: 0.1120, Val Loss: 0.0946\n",
      "Epoch 16/300 - Train Loss: 0.1177, Val Loss: 0.1033\n",
      "Epoch 17/300 - Train Loss: 0.1146, Val Loss: 0.0817\n",
      "Epoch 18/300 - Train Loss: 0.1174, Val Loss: 0.1329\n",
      "Epoch 19/300 - Train Loss: 0.1145, Val Loss: 0.0858\n",
      "Epoch 20/300 - Train Loss: 0.1171, Val Loss: 0.0837\n",
      "Epoch 21/300 - Train Loss: 0.1153, Val Loss: 0.0882\n",
      "Epoch 22/300 - Train Loss: 0.1151, Val Loss: 0.1008\n",
      "Epoch 23/300 - Train Loss: 0.1155, Val Loss: 0.0812\n",
      "Epoch 24/300 - Train Loss: 0.1157, Val Loss: 0.0979\n",
      "Epoch 25/300 - Train Loss: 0.1146, Val Loss: 0.0905\n",
      "Epoch 26/300 - Train Loss: 0.1128, Val Loss: 0.0819\n",
      "Epoch 27/300 - Train Loss: 0.1140, Val Loss: 0.0896\n",
      "Epoch 28/300 - Train Loss: 0.1115, Val Loss: 0.0815\n",
      "Epoch 29/300 - Train Loss: 0.1138, Val Loss: 0.0895\n",
      "Epoch 30/300 - Train Loss: 0.1152, Val Loss: 0.0793\n",
      "Epoch 31/300 - Train Loss: 0.1163, Val Loss: 0.0842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:33:56,738] Trial 166 finished with value: 0.9648898863163388 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.34420803069450306, 'learning_rate': 0.0005016059337618488, 'batch_size': 32, 'weight_decay': 0.005367250621207009}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/300 - Train Loss: 0.1123, Val Loss: 0.0759\n",
      "Early stopping at epoch 32\n",
      "Macro F1 Score: 0.9649, Macro Precision: 0.9658, Macro Recall: 0.9641\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 168\n",
      "Training with F1=16, F2=32, D=4, dropout=0.286975270262769, LR=7.110308128284101e-05, BS=32, WD=0.003038346083497517\n",
      "Epoch 1/300 - Train Loss: 0.3327, Val Loss: 0.1415\n",
      "Epoch 2/300 - Train Loss: 0.1435, Val Loss: 0.1090\n",
      "Epoch 3/300 - Train Loss: 0.1178, Val Loss: 0.0868\n",
      "Epoch 4/300 - Train Loss: 0.1091, Val Loss: 0.0886\n",
      "Epoch 5/300 - Train Loss: 0.1043, Val Loss: 0.0823\n",
      "Epoch 6/300 - Train Loss: 0.1005, Val Loss: 0.0802\n",
      "Epoch 7/300 - Train Loss: 0.0977, Val Loss: 0.0776\n",
      "Epoch 8/300 - Train Loss: 0.0980, Val Loss: 0.0750\n",
      "Epoch 9/300 - Train Loss: 0.0953, Val Loss: 0.0821\n",
      "Epoch 10/300 - Train Loss: 0.0974, Val Loss: 0.0828\n",
      "Epoch 11/300 - Train Loss: 0.0933, Val Loss: 0.0720\n",
      "Epoch 12/300 - Train Loss: 0.0927, Val Loss: 0.0777\n",
      "Epoch 13/300 - Train Loss: 0.0937, Val Loss: 0.0760\n",
      "Epoch 14/300 - Train Loss: 0.0934, Val Loss: 0.0760\n",
      "Epoch 15/300 - Train Loss: 0.0933, Val Loss: 0.0728\n",
      "Epoch 16/300 - Train Loss: 0.0940, Val Loss: 0.0704\n",
      "Epoch 17/300 - Train Loss: 0.0906, Val Loss: 0.0744\n",
      "Epoch 18/300 - Train Loss: 0.0922, Val Loss: 0.0825\n",
      "Epoch 19/300 - Train Loss: 0.0906, Val Loss: 0.0718\n",
      "Epoch 20/300 - Train Loss: 0.0917, Val Loss: 0.0748\n",
      "Epoch 21/300 - Train Loss: 0.0905, Val Loss: 0.0707\n",
      "Epoch 22/300 - Train Loss: 0.0902, Val Loss: 0.0720\n",
      "Epoch 23/300 - Train Loss: 0.0905, Val Loss: 0.0733\n",
      "Epoch 24/300 - Train Loss: 0.0913, Val Loss: 0.0721\n",
      "Epoch 25/300 - Train Loss: 0.0891, Val Loss: 0.0717\n",
      "Epoch 26/300 - Train Loss: 0.0902, Val Loss: 0.0740\n",
      "Epoch 27/300 - Train Loss: 0.0930, Val Loss: 0.0734\n",
      "Epoch 28/300 - Train Loss: 0.0902, Val Loss: 0.0796\n",
      "Epoch 29/300 - Train Loss: 0.0906, Val Loss: 0.0692\n",
      "Epoch 30/300 - Train Loss: 0.0913, Val Loss: 0.0719\n",
      "Epoch 31/300 - Train Loss: 0.0898, Val Loss: 0.0698\n",
      "Epoch 32/300 - Train Loss: 0.0889, Val Loss: 0.0735\n",
      "Epoch 33/300 - Train Loss: 0.0907, Val Loss: 0.0703\n",
      "Epoch 34/300 - Train Loss: 0.0878, Val Loss: 0.0712\n",
      "Epoch 35/300 - Train Loss: 0.0898, Val Loss: 0.0722\n",
      "Epoch 36/300 - Train Loss: 0.0875, Val Loss: 0.0710\n",
      "Epoch 37/300 - Train Loss: 0.0902, Val Loss: 0.0698\n",
      "Epoch 38/300 - Train Loss: 0.0909, Val Loss: 0.0690\n",
      "Epoch 39/300 - Train Loss: 0.0905, Val Loss: 0.0769\n",
      "Epoch 40/300 - Train Loss: 0.0896, Val Loss: 0.0712\n",
      "Epoch 41/300 - Train Loss: 0.0891, Val Loss: 0.0759\n",
      "Epoch 42/300 - Train Loss: 0.0911, Val Loss: 0.0717\n",
      "Epoch 43/300 - Train Loss: 0.0914, Val Loss: 0.0739\n",
      "Epoch 44/300 - Train Loss: 0.0921, Val Loss: 0.0769\n",
      "Epoch 45/300 - Train Loss: 0.0912, Val Loss: 0.0861\n",
      "Epoch 46/300 - Train Loss: 0.0905, Val Loss: 0.0719\n",
      "Epoch 47/300 - Train Loss: 0.0903, Val Loss: 0.0732\n",
      "Epoch 48/300 - Train Loss: 0.0902, Val Loss: 0.0734\n",
      "Epoch 49/300 - Train Loss: 0.0907, Val Loss: 0.0713\n",
      "Epoch 50/300 - Train Loss: 0.0902, Val Loss: 0.0713\n",
      "Epoch 51/300 - Train Loss: 0.0915, Val Loss: 0.0719\n",
      "Epoch 52/300 - Train Loss: 0.0905, Val Loss: 0.0717\n",
      "Epoch 53/300 - Train Loss: 0.0921, Val Loss: 0.0738\n",
      "Epoch 54/300 - Train Loss: 0.0884, Val Loss: 0.0693\n",
      "Epoch 55/300 - Train Loss: 0.0907, Val Loss: 0.0698\n",
      "Epoch 56/300 - Train Loss: 0.0920, Val Loss: 0.0736\n",
      "Epoch 57/300 - Train Loss: 0.0900, Val Loss: 0.0764\n",
      "Epoch 58/300 - Train Loss: 0.0913, Val Loss: 0.0739\n",
      "Epoch 59/300 - Train Loss: 0.0909, Val Loss: 0.0720\n",
      "Epoch 60/300 - Train Loss: 0.0904, Val Loss: 0.0789\n",
      "Epoch 61/300 - Train Loss: 0.0917, Val Loss: 0.0796\n",
      "Epoch 62/300 - Train Loss: 0.0935, Val Loss: 0.0700\n",
      "Epoch 63/300 - Train Loss: 0.0905, Val Loss: 0.0709\n",
      "Epoch 64/300 - Train Loss: 0.0924, Val Loss: 0.0778\n",
      "Epoch 65/300 - Train Loss: 0.0927, Val Loss: 0.0748\n",
      "Epoch 66/300 - Train Loss: 0.0926, Val Loss: 0.0750\n",
      "Epoch 67/300 - Train Loss: 0.0946, Val Loss: 0.0680\n",
      "Epoch 68/300 - Train Loss: 0.0921, Val Loss: 0.0727\n",
      "Epoch 69/300 - Train Loss: 0.0938, Val Loss: 0.0743\n",
      "Epoch 70/300 - Train Loss: 0.0935, Val Loss: 0.0722\n",
      "Epoch 71/300 - Train Loss: 0.0924, Val Loss: 0.0696\n",
      "Epoch 72/300 - Train Loss: 0.0920, Val Loss: 0.0716\n",
      "Epoch 73/300 - Train Loss: 0.0917, Val Loss: 0.0750\n",
      "Epoch 74/300 - Train Loss: 0.0932, Val Loss: 0.0886\n",
      "Epoch 75/300 - Train Loss: 0.0946, Val Loss: 0.0789\n",
      "Epoch 76/300 - Train Loss: 0.0902, Val Loss: 0.0700\n",
      "Epoch 77/300 - Train Loss: 0.0919, Val Loss: 0.0707\n",
      "Epoch 78/300 - Train Loss: 0.0930, Val Loss: 0.0714\n",
      "Epoch 79/300 - Train Loss: 0.0916, Val Loss: 0.0772\n",
      "Epoch 80/300 - Train Loss: 0.0920, Val Loss: 0.0731\n",
      "Epoch 81/300 - Train Loss: 0.0923, Val Loss: 0.0735\n",
      "Epoch 82/300 - Train Loss: 0.0924, Val Loss: 0.0714\n",
      "Epoch 83/300 - Train Loss: 0.0925, Val Loss: 0.0739\n",
      "Epoch 84/300 - Train Loss: 0.0966, Val Loss: 0.0714\n",
      "Epoch 85/300 - Train Loss: 0.0935, Val Loss: 0.0812\n",
      "Epoch 86/300 - Train Loss: 0.0937, Val Loss: 0.0697\n",
      "Epoch 87/300 - Train Loss: 0.0960, Val Loss: 0.0752\n",
      "Epoch 88/300 - Train Loss: 0.0943, Val Loss: 0.0788\n",
      "Epoch 89/300 - Train Loss: 0.0945, Val Loss: 0.0685\n",
      "Epoch 90/300 - Train Loss: 0.0928, Val Loss: 0.0787\n",
      "Epoch 91/300 - Train Loss: 0.0947, Val Loss: 0.0729\n",
      "Epoch 92/300 - Train Loss: 0.0948, Val Loss: 0.0761\n",
      "Epoch 93/300 - Train Loss: 0.0953, Val Loss: 0.0698\n",
      "Epoch 94/300 - Train Loss: 0.0941, Val Loss: 0.0780\n",
      "Epoch 95/300 - Train Loss: 0.0961, Val Loss: 0.0720\n",
      "Epoch 96/300 - Train Loss: 0.0925, Val Loss: 0.0740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:37:59,958] Trial 167 finished with value: 0.9702650362098001 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.286975270262769, 'learning_rate': 7.110308128284101e-05, 'batch_size': 32, 'weight_decay': 0.003038346083497517}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300 - Train Loss: 0.0950, Val Loss: 0.0721\n",
      "Early stopping at epoch 97\n",
      "Macro F1 Score: 0.9703, Macro Precision: 0.9716, Macro Recall: 0.9691\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 169\n",
      "Training with F1=16, F2=32, D=4, dropout=0.2816547510318649, LR=6.235275911995436e-05, BS=64, WD=0.006525353506461519\n",
      "Epoch 1/300 - Train Loss: 0.4391, Val Loss: 0.2107\n",
      "Epoch 2/300 - Train Loss: 0.1788, Val Loss: 0.1391\n",
      "Epoch 3/300 - Train Loss: 0.1321, Val Loss: 0.1123\n",
      "Epoch 4/300 - Train Loss: 0.1139, Val Loss: 0.1031\n",
      "Epoch 5/300 - Train Loss: 0.1050, Val Loss: 0.0913\n",
      "Epoch 6/300 - Train Loss: 0.1000, Val Loss: 0.0957\n",
      "Epoch 7/300 - Train Loss: 0.0966, Val Loss: 0.0897\n",
      "Epoch 8/300 - Train Loss: 0.0946, Val Loss: 0.0897\n",
      "Epoch 9/300 - Train Loss: 0.0950, Val Loss: 0.0821\n",
      "Epoch 10/300 - Train Loss: 0.0945, Val Loss: 0.0826\n",
      "Epoch 11/300 - Train Loss: 0.0915, Val Loss: 0.0805\n",
      "Epoch 12/300 - Train Loss: 0.0907, Val Loss: 0.0775\n",
      "Epoch 13/300 - Train Loss: 0.0907, Val Loss: 0.0806\n",
      "Epoch 14/300 - Train Loss: 0.0873, Val Loss: 0.0812\n",
      "Epoch 15/300 - Train Loss: 0.0882, Val Loss: 0.0755\n",
      "Epoch 16/300 - Train Loss: 0.0904, Val Loss: 0.0777\n",
      "Epoch 17/300 - Train Loss: 0.0890, Val Loss: 0.0820\n",
      "Epoch 18/300 - Train Loss: 0.0893, Val Loss: 0.0851\n",
      "Epoch 19/300 - Train Loss: 0.0885, Val Loss: 0.0789\n",
      "Epoch 20/300 - Train Loss: 0.0880, Val Loss: 0.0765\n",
      "Epoch 21/300 - Train Loss: 0.0879, Val Loss: 0.0757\n",
      "Epoch 22/300 - Train Loss: 0.0883, Val Loss: 0.0811\n",
      "Epoch 23/300 - Train Loss: 0.0896, Val Loss: 0.0783\n",
      "Epoch 24/300 - Train Loss: 0.0888, Val Loss: 0.0744\n",
      "Epoch 25/300 - Train Loss: 0.0885, Val Loss: 0.0758\n",
      "Epoch 26/300 - Train Loss: 0.0875, Val Loss: 0.0746\n",
      "Epoch 27/300 - Train Loss: 0.0894, Val Loss: 0.0760\n",
      "Epoch 28/300 - Train Loss: 0.0883, Val Loss: 0.0736\n",
      "Epoch 29/300 - Train Loss: 0.0875, Val Loss: 0.0759\n",
      "Epoch 30/300 - Train Loss: 0.0877, Val Loss: 0.0764\n",
      "Epoch 31/300 - Train Loss: 0.0890, Val Loss: 0.0748\n",
      "Epoch 32/300 - Train Loss: 0.0891, Val Loss: 0.0762\n",
      "Epoch 33/300 - Train Loss: 0.0902, Val Loss: 0.0777\n",
      "Epoch 34/300 - Train Loss: 0.0880, Val Loss: 0.0762\n",
      "Epoch 35/300 - Train Loss: 0.0888, Val Loss: 0.0737\n",
      "Epoch 36/300 - Train Loss: 0.0915, Val Loss: 0.0760\n",
      "Epoch 37/300 - Train Loss: 0.0897, Val Loss: 0.0755\n",
      "Epoch 38/300 - Train Loss: 0.0885, Val Loss: 0.0735\n",
      "Epoch 39/300 - Train Loss: 0.0891, Val Loss: 0.0772\n",
      "Epoch 40/300 - Train Loss: 0.0881, Val Loss: 0.0737\n",
      "Epoch 41/300 - Train Loss: 0.0898, Val Loss: 0.0737\n",
      "Epoch 42/300 - Train Loss: 0.0897, Val Loss: 0.0739\n",
      "Epoch 43/300 - Train Loss: 0.0898, Val Loss: 0.0748\n",
      "Epoch 44/300 - Train Loss: 0.0888, Val Loss: 0.0738\n",
      "Epoch 45/300 - Train Loss: 0.0910, Val Loss: 0.0757\n",
      "Epoch 46/300 - Train Loss: 0.0889, Val Loss: 0.0754\n",
      "Epoch 47/300 - Train Loss: 0.0904, Val Loss: 0.0736\n",
      "Epoch 48/300 - Train Loss: 0.0914, Val Loss: 0.0764\n",
      "Epoch 49/300 - Train Loss: 0.0907, Val Loss: 0.0741\n",
      "Epoch 50/300 - Train Loss: 0.0916, Val Loss: 0.0739\n",
      "Epoch 51/300 - Train Loss: 0.0923, Val Loss: 0.0762\n",
      "Epoch 52/300 - Train Loss: 0.0908, Val Loss: 0.0748\n",
      "Epoch 53/300 - Train Loss: 0.0926, Val Loss: 0.0752\n",
      "Epoch 54/300 - Train Loss: 0.0916, Val Loss: 0.0740\n",
      "Epoch 55/300 - Train Loss: 0.0902, Val Loss: 0.0751\n",
      "Epoch 56/300 - Train Loss: 0.0902, Val Loss: 0.0742\n",
      "Epoch 57/300 - Train Loss: 0.0907, Val Loss: 0.0742\n",
      "Epoch 58/300 - Train Loss: 0.0933, Val Loss: 0.0741\n",
      "Epoch 59/300 - Train Loss: 0.0934, Val Loss: 0.0730\n",
      "Epoch 60/300 - Train Loss: 0.0905, Val Loss: 0.0755\n",
      "Epoch 61/300 - Train Loss: 0.0936, Val Loss: 0.0756\n",
      "Epoch 62/300 - Train Loss: 0.0932, Val Loss: 0.0739\n",
      "Epoch 63/300 - Train Loss: 0.0926, Val Loss: 0.0756\n",
      "Epoch 64/300 - Train Loss: 0.0938, Val Loss: 0.0759\n",
      "Epoch 65/300 - Train Loss: 0.0920, Val Loss: 0.0724\n",
      "Epoch 66/300 - Train Loss: 0.0918, Val Loss: 0.0725\n",
      "Epoch 67/300 - Train Loss: 0.0929, Val Loss: 0.0736\n",
      "Epoch 68/300 - Train Loss: 0.0929, Val Loss: 0.0766\n",
      "Epoch 69/300 - Train Loss: 0.0925, Val Loss: 0.0738\n",
      "Epoch 70/300 - Train Loss: 0.0941, Val Loss: 0.0754\n",
      "Epoch 71/300 - Train Loss: 0.0937, Val Loss: 0.0732\n",
      "Epoch 72/300 - Train Loss: 0.0937, Val Loss: 0.0745\n",
      "Epoch 73/300 - Train Loss: 0.0936, Val Loss: 0.0738\n",
      "Epoch 74/300 - Train Loss: 0.0928, Val Loss: 0.0776\n",
      "Epoch 75/300 - Train Loss: 0.0937, Val Loss: 0.0752\n",
      "Epoch 76/300 - Train Loss: 0.0932, Val Loss: 0.0816\n",
      "Epoch 77/300 - Train Loss: 0.0949, Val Loss: 0.0728\n",
      "Epoch 78/300 - Train Loss: 0.0943, Val Loss: 0.0747\n",
      "Epoch 79/300 - Train Loss: 0.0939, Val Loss: 0.0753\n",
      "Epoch 80/300 - Train Loss: 0.0932, Val Loss: 0.0782\n",
      "Epoch 81/300 - Train Loss: 0.0938, Val Loss: 0.0738\n",
      "Epoch 82/300 - Train Loss: 0.0936, Val Loss: 0.0729\n",
      "Epoch 83/300 - Train Loss: 0.0952, Val Loss: 0.0740\n",
      "Epoch 84/300 - Train Loss: 0.0930, Val Loss: 0.0752\n",
      "Epoch 85/300 - Train Loss: 0.0941, Val Loss: 0.0740\n",
      "Epoch 86/300 - Train Loss: 0.0943, Val Loss: 0.0749\n",
      "Epoch 87/300 - Train Loss: 0.0951, Val Loss: 0.0747\n",
      "Epoch 88/300 - Train Loss: 0.0932, Val Loss: 0.0743\n",
      "Epoch 89/300 - Train Loss: 0.0945, Val Loss: 0.0838\n",
      "Epoch 90/300 - Train Loss: 0.0938, Val Loss: 0.0741\n",
      "Epoch 91/300 - Train Loss: 0.0940, Val Loss: 0.0758\n",
      "Epoch 92/300 - Train Loss: 0.0950, Val Loss: 0.0779\n",
      "Epoch 93/300 - Train Loss: 0.0944, Val Loss: 0.0725\n",
      "Epoch 94/300 - Train Loss: 0.0948, Val Loss: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:41:22,699] Trial 168 finished with value: 0.966779382197998 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.2816547510318649, 'learning_rate': 6.235275911995436e-05, 'batch_size': 64, 'weight_decay': 0.006525353506461519}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95/300 - Train Loss: 0.0946, Val Loss: 0.0753\n",
      "Early stopping at epoch 95\n",
      "Macro F1 Score: 0.9668, Macro Precision: 0.9756, Macro Recall: 0.9586\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.92      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 170\n",
      "Training with F1=8, F2=16, D=4, dropout=0.3281764666091506, LR=8.470140934555576e-05, BS=32, WD=0.0037811802753365863\n",
      "Epoch 1/300 - Train Loss: 0.4054, Val Loss: 0.1910\n",
      "Epoch 2/300 - Train Loss: 0.1778, Val Loss: 0.1272\n",
      "Epoch 3/300 - Train Loss: 0.1359, Val Loss: 0.1070\n",
      "Epoch 4/300 - Train Loss: 0.1208, Val Loss: 0.0893\n",
      "Epoch 5/300 - Train Loss: 0.1141, Val Loss: 0.0901\n",
      "Epoch 6/300 - Train Loss: 0.1126, Val Loss: 0.0850\n",
      "Epoch 7/300 - Train Loss: 0.1113, Val Loss: 0.0819\n",
      "Epoch 8/300 - Train Loss: 0.1066, Val Loss: 0.0851\n",
      "Epoch 9/300 - Train Loss: 0.1051, Val Loss: 0.0884\n",
      "Epoch 10/300 - Train Loss: 0.1055, Val Loss: 0.0801\n",
      "Epoch 11/300 - Train Loss: 0.1006, Val Loss: 0.0752\n",
      "Epoch 12/300 - Train Loss: 0.1004, Val Loss: 0.0755\n",
      "Epoch 13/300 - Train Loss: 0.0990, Val Loss: 0.0728\n",
      "Epoch 14/300 - Train Loss: 0.0989, Val Loss: 0.0757\n",
      "Epoch 15/300 - Train Loss: 0.0978, Val Loss: 0.0712\n",
      "Epoch 16/300 - Train Loss: 0.0992, Val Loss: 0.0736\n",
      "Epoch 17/300 - Train Loss: 0.0981, Val Loss: 0.0733\n",
      "Epoch 18/300 - Train Loss: 0.0988, Val Loss: 0.0733\n",
      "Epoch 19/300 - Train Loss: 0.0971, Val Loss: 0.0741\n",
      "Epoch 20/300 - Train Loss: 0.0986, Val Loss: 0.0725\n",
      "Epoch 21/300 - Train Loss: 0.0967, Val Loss: 0.0752\n",
      "Epoch 22/300 - Train Loss: 0.0957, Val Loss: 0.0718\n",
      "Epoch 23/300 - Train Loss: 0.1007, Val Loss: 0.0745\n",
      "Epoch 24/300 - Train Loss: 0.0967, Val Loss: 0.0757\n",
      "Epoch 25/300 - Train Loss: 0.0965, Val Loss: 0.0733\n",
      "Epoch 26/300 - Train Loss: 0.0978, Val Loss: 0.0730\n",
      "Epoch 27/300 - Train Loss: 0.0983, Val Loss: 0.0715\n",
      "Epoch 28/300 - Train Loss: 0.0981, Val Loss: 0.0739\n",
      "Epoch 29/300 - Train Loss: 0.0976, Val Loss: 0.0750\n",
      "Epoch 30/300 - Train Loss: 0.0971, Val Loss: 0.0791\n",
      "Epoch 31/300 - Train Loss: 0.0958, Val Loss: 0.0744\n",
      "Epoch 32/300 - Train Loss: 0.0949, Val Loss: 0.0715\n",
      "Epoch 33/300 - Train Loss: 0.0989, Val Loss: 0.0732\n",
      "Epoch 34/300 - Train Loss: 0.0949, Val Loss: 0.0752\n",
      "Epoch 35/300 - Train Loss: 0.0961, Val Loss: 0.0718\n",
      "Epoch 36/300 - Train Loss: 0.0973, Val Loss: 0.0724\n",
      "Epoch 37/300 - Train Loss: 0.1003, Val Loss: 0.0737\n",
      "Epoch 38/300 - Train Loss: 0.0984, Val Loss: 0.0760\n",
      "Epoch 39/300 - Train Loss: 0.0984, Val Loss: 0.0731\n",
      "Epoch 40/300 - Train Loss: 0.0992, Val Loss: 0.0725\n",
      "Epoch 41/300 - Train Loss: 0.0997, Val Loss: 0.0713\n",
      "Epoch 42/300 - Train Loss: 0.0974, Val Loss: 0.0727\n",
      "Epoch 43/300 - Train Loss: 0.0984, Val Loss: 0.0760\n",
      "Epoch 44/300 - Train Loss: 0.0960, Val Loss: 0.0694\n",
      "Epoch 45/300 - Train Loss: 0.0993, Val Loss: 0.0722\n",
      "Epoch 46/300 - Train Loss: 0.0986, Val Loss: 0.0771\n",
      "Epoch 47/300 - Train Loss: 0.0977, Val Loss: 0.0717\n",
      "Epoch 48/300 - Train Loss: 0.0972, Val Loss: 0.0813\n",
      "Epoch 49/300 - Train Loss: 0.0977, Val Loss: 0.0756\n",
      "Epoch 50/300 - Train Loss: 0.0999, Val Loss: 0.0738\n",
      "Epoch 51/300 - Train Loss: 0.0982, Val Loss: 0.0728\n",
      "Epoch 52/300 - Train Loss: 0.0983, Val Loss: 0.0725\n",
      "Epoch 53/300 - Train Loss: 0.0994, Val Loss: 0.0736\n",
      "Epoch 54/300 - Train Loss: 0.1008, Val Loss: 0.0738\n",
      "Epoch 55/300 - Train Loss: 0.0986, Val Loss: 0.0736\n",
      "Epoch 56/300 - Train Loss: 0.0979, Val Loss: 0.0717\n",
      "Epoch 57/300 - Train Loss: 0.1021, Val Loss: 0.0730\n",
      "Epoch 58/300 - Train Loss: 0.1010, Val Loss: 0.0793\n",
      "Epoch 59/300 - Train Loss: 0.0982, Val Loss: 0.0726\n",
      "Epoch 60/300 - Train Loss: 0.1008, Val Loss: 0.0792\n",
      "Epoch 61/300 - Train Loss: 0.0990, Val Loss: 0.0736\n",
      "Epoch 62/300 - Train Loss: 0.0988, Val Loss: 0.0731\n",
      "Epoch 63/300 - Train Loss: 0.0999, Val Loss: 0.0783\n",
      "Epoch 64/300 - Train Loss: 0.1004, Val Loss: 0.0730\n",
      "Epoch 65/300 - Train Loss: 0.1014, Val Loss: 0.0735\n",
      "Epoch 66/300 - Train Loss: 0.1003, Val Loss: 0.0730\n",
      "Epoch 67/300 - Train Loss: 0.1002, Val Loss: 0.0739\n",
      "Epoch 68/300 - Train Loss: 0.1018, Val Loss: 0.0736\n",
      "Epoch 69/300 - Train Loss: 0.0998, Val Loss: 0.0721\n",
      "Epoch 70/300 - Train Loss: 0.1024, Val Loss: 0.0730\n",
      "Epoch 71/300 - Train Loss: 0.1034, Val Loss: 0.0706\n",
      "Epoch 72/300 - Train Loss: 0.1017, Val Loss: 0.0710\n",
      "Epoch 73/300 - Train Loss: 0.0983, Val Loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:43:51,839] Trial 169 finished with value: 0.9703438592962833 and parameters: {'F1': 8, 'F2': 16, 'D': 4, 'dropout': 0.3281764666091506, 'learning_rate': 8.470140934555576e-05, 'batch_size': 32, 'weight_decay': 0.0037811802753365863}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300 - Train Loss: 0.1004, Val Loss: 0.0719\n",
      "Early stopping at epoch 74\n",
      "Macro F1 Score: 0.9703, Macro Precision: 0.9716, Macro Recall: 0.9693\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 171\n",
      "Training with F1=16, F2=32, D=4, dropout=0.3943817967107614, LR=3.169345836211809e-05, BS=32, WD=0.00769293992374734\n",
      "Epoch 1/300 - Train Loss: 0.5595, Val Loss: 0.2951\n",
      "Epoch 2/300 - Train Loss: 0.2428, Val Loss: 0.1836\n",
      "Epoch 3/300 - Train Loss: 0.1839, Val Loss: 0.1509\n",
      "Epoch 4/300 - Train Loss: 0.1557, Val Loss: 0.1176\n",
      "Epoch 5/300 - Train Loss: 0.1379, Val Loss: 0.1180\n",
      "Epoch 6/300 - Train Loss: 0.1258, Val Loss: 0.1029\n",
      "Epoch 7/300 - Train Loss: 0.1195, Val Loss: 0.1003\n",
      "Epoch 8/300 - Train Loss: 0.1145, Val Loss: 0.0931\n",
      "Epoch 9/300 - Train Loss: 0.1153, Val Loss: 0.0854\n",
      "Epoch 10/300 - Train Loss: 0.1132, Val Loss: 0.0926\n",
      "Epoch 11/300 - Train Loss: 0.1077, Val Loss: 0.0828\n",
      "Epoch 12/300 - Train Loss: 0.1096, Val Loss: 0.0871\n",
      "Epoch 13/300 - Train Loss: 0.1089, Val Loss: 0.0801\n",
      "Epoch 14/300 - Train Loss: 0.1049, Val Loss: 0.0850\n",
      "Epoch 15/300 - Train Loss: 0.1059, Val Loss: 0.0858\n",
      "Epoch 16/300 - Train Loss: 0.1039, Val Loss: 0.0839\n",
      "Epoch 17/300 - Train Loss: 0.1026, Val Loss: 0.0768\n",
      "Epoch 18/300 - Train Loss: 0.1035, Val Loss: 0.0810\n",
      "Epoch 19/300 - Train Loss: 0.1031, Val Loss: 0.0793\n",
      "Epoch 20/300 - Train Loss: 0.1048, Val Loss: 0.0781\n",
      "Epoch 21/300 - Train Loss: 0.1042, Val Loss: 0.0808\n",
      "Epoch 22/300 - Train Loss: 0.1037, Val Loss: 0.0804\n",
      "Epoch 23/300 - Train Loss: 0.0997, Val Loss: 0.0802\n",
      "Epoch 24/300 - Train Loss: 0.1023, Val Loss: 0.0792\n",
      "Epoch 25/300 - Train Loss: 0.1036, Val Loss: 0.0828\n",
      "Epoch 26/300 - Train Loss: 0.1004, Val Loss: 0.0798\n",
      "Epoch 27/300 - Train Loss: 0.1002, Val Loss: 0.0736\n",
      "Epoch 28/300 - Train Loss: 0.1004, Val Loss: 0.0801\n",
      "Epoch 29/300 - Train Loss: 0.1009, Val Loss: 0.0811\n",
      "Epoch 30/300 - Train Loss: 0.1014, Val Loss: 0.0773\n",
      "Epoch 31/300 - Train Loss: 0.1012, Val Loss: 0.0754\n",
      "Epoch 32/300 - Train Loss: 0.1029, Val Loss: 0.0749\n",
      "Epoch 33/300 - Train Loss: 0.1010, Val Loss: 0.0740\n",
      "Epoch 34/300 - Train Loss: 0.1017, Val Loss: 0.0752\n",
      "Epoch 35/300 - Train Loss: 0.1011, Val Loss: 0.0773\n",
      "Epoch 36/300 - Train Loss: 0.0989, Val Loss: 0.0784\n",
      "Epoch 37/300 - Train Loss: 0.1027, Val Loss: 0.0764\n",
      "Epoch 38/300 - Train Loss: 0.1038, Val Loss: 0.0773\n",
      "Epoch 39/300 - Train Loss: 0.1032, Val Loss: 0.0768\n",
      "Epoch 40/300 - Train Loss: 0.1026, Val Loss: 0.0846\n",
      "Epoch 41/300 - Train Loss: 0.1005, Val Loss: 0.0757\n",
      "Epoch 42/300 - Train Loss: 0.1017, Val Loss: 0.0746\n",
      "Epoch 43/300 - Train Loss: 0.1030, Val Loss: 0.0757\n",
      "Epoch 44/300 - Train Loss: 0.1023, Val Loss: 0.0754\n",
      "Epoch 45/300 - Train Loss: 0.1023, Val Loss: 0.0764\n",
      "Epoch 46/300 - Train Loss: 0.1029, Val Loss: 0.0735\n",
      "Epoch 47/300 - Train Loss: 0.1027, Val Loss: 0.0748\n",
      "Epoch 48/300 - Train Loss: 0.1003, Val Loss: 0.0754\n",
      "Epoch 49/300 - Train Loss: 0.1049, Val Loss: 0.0740\n",
      "Epoch 50/300 - Train Loss: 0.1064, Val Loss: 0.0759\n",
      "Epoch 51/300 - Train Loss: 0.1031, Val Loss: 0.0738\n",
      "Epoch 52/300 - Train Loss: 0.1023, Val Loss: 0.0754\n",
      "Epoch 53/300 - Train Loss: 0.1057, Val Loss: 0.0744\n",
      "Epoch 54/300 - Train Loss: 0.1016, Val Loss: 0.0782\n",
      "Epoch 55/300 - Train Loss: 0.1020, Val Loss: 0.0757\n",
      "Epoch 56/300 - Train Loss: 0.1058, Val Loss: 0.0767\n",
      "Epoch 57/300 - Train Loss: 0.1053, Val Loss: 0.0749\n",
      "Epoch 58/300 - Train Loss: 0.1033, Val Loss: 0.0739\n",
      "Epoch 59/300 - Train Loss: 0.1051, Val Loss: 0.0837\n",
      "Epoch 60/300 - Train Loss: 0.1037, Val Loss: 0.0763\n",
      "Epoch 61/300 - Train Loss: 0.1065, Val Loss: 0.0739\n",
      "Epoch 62/300 - Train Loss: 0.1054, Val Loss: 0.0766\n",
      "Epoch 63/300 - Train Loss: 0.1055, Val Loss: 0.0751\n",
      "Epoch 64/300 - Train Loss: 0.1084, Val Loss: 0.0786\n",
      "Epoch 65/300 - Train Loss: 0.1060, Val Loss: 0.0749\n",
      "Epoch 66/300 - Train Loss: 0.1053, Val Loss: 0.0739\n",
      "Epoch 67/300 - Train Loss: 0.1055, Val Loss: 0.0740\n",
      "Epoch 68/300 - Train Loss: 0.1054, Val Loss: 0.0750\n",
      "Epoch 69/300 - Train Loss: 0.1062, Val Loss: 0.0757\n",
      "Epoch 70/300 - Train Loss: 0.1039, Val Loss: 0.0785\n",
      "Epoch 71/300 - Train Loss: 0.1080, Val Loss: 0.0793\n",
      "Epoch 72/300 - Train Loss: 0.1069, Val Loss: 0.0759\n",
      "Epoch 73/300 - Train Loss: 0.1052, Val Loss: 0.0791\n",
      "Epoch 74/300 - Train Loss: 0.1066, Val Loss: 0.0732\n",
      "Epoch 75/300 - Train Loss: 0.1048, Val Loss: 0.0759\n",
      "Epoch 76/300 - Train Loss: 0.1083, Val Loss: 0.0751\n",
      "Epoch 77/300 - Train Loss: 0.1068, Val Loss: 0.0736\n",
      "Epoch 78/300 - Train Loss: 0.1072, Val Loss: 0.0762\n",
      "Epoch 79/300 - Train Loss: 0.1082, Val Loss: 0.0773\n",
      "Epoch 80/300 - Train Loss: 0.1098, Val Loss: 0.0805\n",
      "Epoch 81/300 - Train Loss: 0.1075, Val Loss: 0.0750\n",
      "Epoch 82/300 - Train Loss: 0.1084, Val Loss: 0.0764\n",
      "Epoch 83/300 - Train Loss: 0.1076, Val Loss: 0.0772\n",
      "Epoch 84/300 - Train Loss: 0.1096, Val Loss: 0.0775\n",
      "Epoch 85/300 - Train Loss: 0.1088, Val Loss: 0.0777\n",
      "Epoch 86/300 - Train Loss: 0.1087, Val Loss: 0.0751\n",
      "Epoch 87/300 - Train Loss: 0.1086, Val Loss: 0.0764\n",
      "Epoch 88/300 - Train Loss: 0.1072, Val Loss: 0.0758\n",
      "Epoch 89/300 - Train Loss: 0.1061, Val Loss: 0.0755\n",
      "Epoch 90/300 - Train Loss: 0.1070, Val Loss: 0.0750\n",
      "Epoch 91/300 - Train Loss: 0.1086, Val Loss: 0.0757\n",
      "Epoch 92/300 - Train Loss: 0.1060, Val Loss: 0.0767\n",
      "Epoch 93/300 - Train Loss: 0.1053, Val Loss: 0.0749\n",
      "Epoch 94/300 - Train Loss: 0.1070, Val Loss: 0.0802\n",
      "Epoch 95/300 - Train Loss: 0.1091, Val Loss: 0.0752\n",
      "Epoch 96/300 - Train Loss: 0.1136, Val Loss: 0.0747\n",
      "Epoch 97/300 - Train Loss: 0.1066, Val Loss: 0.0737\n",
      "Epoch 98/300 - Train Loss: 0.1075, Val Loss: 0.0843\n",
      "Epoch 99/300 - Train Loss: 0.1104, Val Loss: 0.0835\n",
      "Epoch 100/300 - Train Loss: 0.1091, Val Loss: 0.0759\n",
      "Epoch 101/300 - Train Loss: 0.1109, Val Loss: 0.0782\n",
      "Epoch 102/300 - Train Loss: 0.1101, Val Loss: 0.0786\n",
      "Epoch 103/300 - Train Loss: 0.1105, Val Loss: 0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:48:12,745] Trial 170 finished with value: 0.9588513540679394 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.3943817967107614, 'learning_rate': 3.169345836211809e-05, 'batch_size': 32, 'weight_decay': 0.00769293992374734}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/300 - Train Loss: 0.1094, Val Loss: 0.0769\n",
      "Early stopping at epoch 104\n",
      "Macro F1 Score: 0.9589, Macro Precision: 0.9599, Macro Recall: 0.9579\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.92      0.92      0.92        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 172\n",
      "Training with F1=16, F2=32, D=4, dropout=0.2574185041896004, LR=0.00010562334793381304, BS=32, WD=0.008487657263911225\n",
      "Epoch 1/300 - Train Loss: 0.2901, Val Loss: 0.1376\n",
      "Epoch 2/300 - Train Loss: 0.1346, Val Loss: 0.0892\n",
      "Epoch 3/300 - Train Loss: 0.1152, Val Loss: 0.0908\n",
      "Epoch 4/300 - Train Loss: 0.1069, Val Loss: 0.0826\n",
      "Epoch 5/300 - Train Loss: 0.1062, Val Loss: 0.0894\n",
      "Epoch 6/300 - Train Loss: 0.1042, Val Loss: 0.0800\n",
      "Epoch 7/300 - Train Loss: 0.1033, Val Loss: 0.0773\n",
      "Epoch 8/300 - Train Loss: 0.1017, Val Loss: 0.0746\n",
      "Epoch 9/300 - Train Loss: 0.1028, Val Loss: 0.0721\n",
      "Epoch 10/300 - Train Loss: 0.1022, Val Loss: 0.0774\n",
      "Epoch 11/300 - Train Loss: 0.1027, Val Loss: 0.0787\n",
      "Epoch 12/300 - Train Loss: 0.1001, Val Loss: 0.0849\n",
      "Epoch 13/300 - Train Loss: 0.1007, Val Loss: 0.0728\n",
      "Epoch 14/300 - Train Loss: 0.1035, Val Loss: 0.0750\n",
      "Epoch 15/300 - Train Loss: 0.1045, Val Loss: 0.0761\n",
      "Epoch 16/300 - Train Loss: 0.1034, Val Loss: 0.0839\n",
      "Epoch 17/300 - Train Loss: 0.1043, Val Loss: 0.0790\n",
      "Epoch 18/300 - Train Loss: 0.1048, Val Loss: 0.0749\n",
      "Epoch 19/300 - Train Loss: 0.1019, Val Loss: 0.0736\n",
      "Epoch 20/300 - Train Loss: 0.1033, Val Loss: 0.0762\n",
      "Epoch 21/300 - Train Loss: 0.1023, Val Loss: 0.0750\n",
      "Epoch 22/300 - Train Loss: 0.1048, Val Loss: 0.0714\n",
      "Epoch 23/300 - Train Loss: 0.1035, Val Loss: 0.0766\n",
      "Epoch 24/300 - Train Loss: 0.1056, Val Loss: 0.0808\n",
      "Epoch 25/300 - Train Loss: 0.1043, Val Loss: 0.0728\n",
      "Epoch 26/300 - Train Loss: 0.1057, Val Loss: 0.0727\n",
      "Epoch 27/300 - Train Loss: 0.1055, Val Loss: 0.0715\n",
      "Epoch 28/300 - Train Loss: 0.1053, Val Loss: 0.0765\n",
      "Epoch 29/300 - Train Loss: 0.1065, Val Loss: 0.0767\n",
      "Epoch 30/300 - Train Loss: 0.1068, Val Loss: 0.0818\n",
      "Epoch 31/300 - Train Loss: 0.1044, Val Loss: 0.0750\n",
      "Epoch 32/300 - Train Loss: 0.1065, Val Loss: 0.0738\n",
      "Epoch 33/300 - Train Loss: 0.1056, Val Loss: 0.0780\n",
      "Epoch 34/300 - Train Loss: 0.1037, Val Loss: 0.0747\n",
      "Epoch 35/300 - Train Loss: 0.1065, Val Loss: 0.0807\n",
      "Epoch 36/300 - Train Loss: 0.1054, Val Loss: 0.0758\n",
      "Epoch 37/300 - Train Loss: 0.1059, Val Loss: 0.0743\n",
      "Epoch 38/300 - Train Loss: 0.1060, Val Loss: 0.0886\n",
      "Epoch 39/300 - Train Loss: 0.1059, Val Loss: 0.0781\n",
      "Epoch 40/300 - Train Loss: 0.1073, Val Loss: 0.0856\n",
      "Epoch 41/300 - Train Loss: 0.1062, Val Loss: 0.0996\n",
      "Epoch 42/300 - Train Loss: 0.1067, Val Loss: 0.0771\n",
      "Epoch 43/300 - Train Loss: 0.1047, Val Loss: 0.0734\n",
      "Epoch 44/300 - Train Loss: 0.1046, Val Loss: 0.0749\n",
      "Epoch 45/300 - Train Loss: 0.1073, Val Loss: 0.0739\n",
      "Epoch 46/300 - Train Loss: 0.1079, Val Loss: 0.0790\n",
      "Epoch 47/300 - Train Loss: 0.1072, Val Loss: 0.0735\n",
      "Epoch 48/300 - Train Loss: 0.1069, Val Loss: 0.0949\n",
      "Epoch 49/300 - Train Loss: 0.1069, Val Loss: 0.0803\n",
      "Epoch 50/300 - Train Loss: 0.1086, Val Loss: 0.0819\n",
      "Epoch 51/300 - Train Loss: 0.1046, Val Loss: 0.0726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:50:23,278] Trial 171 finished with value: 0.972945788611263 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.2574185041896004, 'learning_rate': 0.00010562334793381304, 'batch_size': 32, 'weight_decay': 0.008487657263911225}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300 - Train Loss: 0.1096, Val Loss: 0.0834\n",
      "Early stopping at epoch 52\n",
      "Macro F1 Score: 0.9729, Macro Precision: 0.9821, Macro Recall: 0.9645\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       789\n",
      "           1       0.98      0.93      0.96        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 173\n",
      "Training with F1=16, F2=32, D=4, dropout=0.19039540814560968, LR=0.00038704238704172163, BS=32, WD=0.006833757146698847\n",
      "Epoch 1/300 - Train Loss: 0.1731, Val Loss: 0.0887\n",
      "Epoch 2/300 - Train Loss: 0.1041, Val Loss: 0.0848\n",
      "Epoch 3/300 - Train Loss: 0.1040, Val Loss: 0.0779\n",
      "Epoch 4/300 - Train Loss: 0.1005, Val Loss: 0.0753\n",
      "Epoch 5/300 - Train Loss: 0.1018, Val Loss: 0.0731\n",
      "Epoch 6/300 - Train Loss: 0.1056, Val Loss: 0.0858\n",
      "Epoch 7/300 - Train Loss: 0.1039, Val Loss: 0.0800\n",
      "Epoch 8/300 - Train Loss: 0.1047, Val Loss: 0.0747\n",
      "Epoch 9/300 - Train Loss: 0.1036, Val Loss: 0.0771\n",
      "Epoch 10/300 - Train Loss: 0.1040, Val Loss: 0.0944\n",
      "Epoch 11/300 - Train Loss: 0.1070, Val Loss: 0.0898\n",
      "Epoch 12/300 - Train Loss: 0.1079, Val Loss: 0.0784\n",
      "Epoch 13/300 - Train Loss: 0.1069, Val Loss: 0.0754\n",
      "Epoch 14/300 - Train Loss: 0.1076, Val Loss: 0.1055\n",
      "Epoch 15/300 - Train Loss: 0.1067, Val Loss: 0.0783\n",
      "Epoch 16/300 - Train Loss: 0.1074, Val Loss: 0.0886\n",
      "Epoch 17/300 - Train Loss: 0.1055, Val Loss: 0.0786\n",
      "Epoch 18/300 - Train Loss: 0.1069, Val Loss: 0.0803\n",
      "Epoch 19/300 - Train Loss: 0.1072, Val Loss: 0.1008\n",
      "Epoch 20/300 - Train Loss: 0.1063, Val Loss: 0.0836\n",
      "Epoch 21/300 - Train Loss: 0.1064, Val Loss: 0.0872\n",
      "Epoch 22/300 - Train Loss: 0.1092, Val Loss: 0.0845\n",
      "Epoch 23/300 - Train Loss: 0.1085, Val Loss: 0.0724\n",
      "Epoch 24/300 - Train Loss: 0.1082, Val Loss: 0.0799\n",
      "Epoch 25/300 - Train Loss: 0.1066, Val Loss: 0.0936\n",
      "Epoch 26/300 - Train Loss: 0.1070, Val Loss: 0.0770\n",
      "Epoch 27/300 - Train Loss: 0.1089, Val Loss: 0.1212\n",
      "Epoch 28/300 - Train Loss: 0.1097, Val Loss: 0.0786\n",
      "Epoch 29/300 - Train Loss: 0.1087, Val Loss: 0.0786\n",
      "Epoch 30/300 - Train Loss: 0.1075, Val Loss: 0.1037\n",
      "Epoch 31/300 - Train Loss: 0.1051, Val Loss: 0.0907\n",
      "Epoch 32/300 - Train Loss: 0.1058, Val Loss: 0.0826\n",
      "Epoch 33/300 - Train Loss: 0.1062, Val Loss: 0.0784\n",
      "Epoch 34/300 - Train Loss: 0.1081, Val Loss: 0.0790\n",
      "Epoch 35/300 - Train Loss: 0.1092, Val Loss: 0.0782\n",
      "Epoch 36/300 - Train Loss: 0.1067, Val Loss: 0.1032\n",
      "Epoch 37/300 - Train Loss: 0.1080, Val Loss: 0.0855\n",
      "Epoch 38/300 - Train Loss: 0.1064, Val Loss: 0.0788\n",
      "Epoch 39/300 - Train Loss: 0.1082, Val Loss: 0.0820\n",
      "Epoch 40/300 - Train Loss: 0.1071, Val Loss: 0.0959\n",
      "Epoch 41/300 - Train Loss: 0.1104, Val Loss: 0.0747\n",
      "Epoch 42/300 - Train Loss: 0.1067, Val Loss: 0.0747\n",
      "Epoch 43/300 - Train Loss: 0.1073, Val Loss: 0.0782\n",
      "Epoch 44/300 - Train Loss: 0.1098, Val Loss: 0.0765\n",
      "Epoch 45/300 - Train Loss: 0.1075, Val Loss: 0.0801\n",
      "Epoch 46/300 - Train Loss: 0.1055, Val Loss: 0.0823\n",
      "Epoch 47/300 - Train Loss: 0.1087, Val Loss: 0.0953\n",
      "Epoch 48/300 - Train Loss: 0.1078, Val Loss: 0.0897\n",
      "Epoch 49/300 - Train Loss: 0.1067, Val Loss: 0.0909\n",
      "Epoch 50/300 - Train Loss: 0.1072, Val Loss: 0.0743\n",
      "Epoch 51/300 - Train Loss: 0.1095, Val Loss: 0.0878\n",
      "Epoch 52/300 - Train Loss: 0.1058, Val Loss: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:52:36,579] Trial 172 finished with value: 0.9685936919434233 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.19039540814560968, 'learning_rate': 0.00038704238704172163, 'batch_size': 32, 'weight_decay': 0.006833757146698847}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300 - Train Loss: 0.1072, Val Loss: 0.0904\n",
      "Early stopping at epoch 53\n",
      "Macro F1 Score: 0.9686, Macro Precision: 0.9749, Macro Recall: 0.9627\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 174\n",
      "Training with F1=16, F2=32, D=4, dropout=0.27147665421848743, LR=9.151574976891246e-05, BS=32, WD=0.005886390522341577\n",
      "Epoch 1/300 - Train Loss: 0.2977, Val Loss: 0.1164\n",
      "Epoch 2/300 - Train Loss: 0.1247, Val Loss: 0.0871\n",
      "Epoch 3/300 - Train Loss: 0.1079, Val Loss: 0.0816\n",
      "Epoch 4/300 - Train Loss: 0.1025, Val Loss: 0.0794\n",
      "Epoch 5/300 - Train Loss: 0.1029, Val Loss: 0.0775\n",
      "Epoch 6/300 - Train Loss: 0.0980, Val Loss: 0.0748\n",
      "Epoch 7/300 - Train Loss: 0.1003, Val Loss: 0.0730\n",
      "Epoch 8/300 - Train Loss: 0.0961, Val Loss: 0.0758\n",
      "Epoch 9/300 - Train Loss: 0.0992, Val Loss: 0.0708\n",
      "Epoch 10/300 - Train Loss: 0.0986, Val Loss: 0.0730\n",
      "Epoch 11/300 - Train Loss: 0.0977, Val Loss: 0.0767\n",
      "Epoch 12/300 - Train Loss: 0.0983, Val Loss: 0.0743\n",
      "Epoch 13/300 - Train Loss: 0.0947, Val Loss: 0.0763\n",
      "Epoch 14/300 - Train Loss: 0.0975, Val Loss: 0.0742\n",
      "Epoch 15/300 - Train Loss: 0.0957, Val Loss: 0.0791\n",
      "Epoch 16/300 - Train Loss: 0.0970, Val Loss: 0.0731\n",
      "Epoch 17/300 - Train Loss: 0.0974, Val Loss: 0.0716\n",
      "Epoch 18/300 - Train Loss: 0.0954, Val Loss: 0.0741\n",
      "Epoch 19/300 - Train Loss: 0.0995, Val Loss: 0.0712\n",
      "Epoch 20/300 - Train Loss: 0.0960, Val Loss: 0.0871\n",
      "Epoch 21/300 - Train Loss: 0.1011, Val Loss: 0.0744\n",
      "Epoch 22/300 - Train Loss: 0.0999, Val Loss: 0.0744\n",
      "Epoch 23/300 - Train Loss: 0.1006, Val Loss: 0.0736\n",
      "Epoch 24/300 - Train Loss: 0.0992, Val Loss: 0.0735\n",
      "Epoch 25/300 - Train Loss: 0.0985, Val Loss: 0.0709\n",
      "Epoch 26/300 - Train Loss: 0.0977, Val Loss: 0.0726\n",
      "Epoch 27/300 - Train Loss: 0.0980, Val Loss: 0.0720\n",
      "Epoch 28/300 - Train Loss: 0.0980, Val Loss: 0.0745\n",
      "Epoch 29/300 - Train Loss: 0.0979, Val Loss: 0.0737\n",
      "Epoch 30/300 - Train Loss: 0.0993, Val Loss: 0.0729\n",
      "Epoch 31/300 - Train Loss: 0.0983, Val Loss: 0.0725\n",
      "Epoch 32/300 - Train Loss: 0.0997, Val Loss: 0.0718\n",
      "Epoch 33/300 - Train Loss: 0.0999, Val Loss: 0.0811\n",
      "Epoch 34/300 - Train Loss: 0.0994, Val Loss: 0.0730\n",
      "Epoch 35/300 - Train Loss: 0.1009, Val Loss: 0.0761\n",
      "Epoch 36/300 - Train Loss: 0.0996, Val Loss: 0.0723\n",
      "Epoch 37/300 - Train Loss: 0.1010, Val Loss: 0.0716\n",
      "Epoch 38/300 - Train Loss: 0.0985, Val Loss: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:54:14,475] Trial 173 finished with value: 0.9676980546403708 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.27147665421848743, 'learning_rate': 9.151574976891246e-05, 'batch_size': 32, 'weight_decay': 0.005886390522341577}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300 - Train Loss: 0.0996, Val Loss: 0.0808\n",
      "Early stopping at epoch 39\n",
      "Macro F1 Score: 0.9677, Macro Precision: 0.9643, Macro Recall: 0.9714\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 175\n",
      "Training with F1=16, F2=32, D=4, dropout=0.2974743975209906, LR=0.00012120611486915158, BS=32, WD=0.008488438440585389\n",
      "Epoch 1/300 - Train Loss: 0.2585, Val Loss: 0.1027\n",
      "Epoch 2/300 - Train Loss: 0.1204, Val Loss: 0.0876\n",
      "Epoch 3/300 - Train Loss: 0.1083, Val Loss: 0.0818\n",
      "Epoch 4/300 - Train Loss: 0.1074, Val Loss: 0.0852\n",
      "Epoch 5/300 - Train Loss: 0.1038, Val Loss: 0.0788\n",
      "Epoch 6/300 - Train Loss: 0.1050, Val Loss: 0.0749\n",
      "Epoch 7/300 - Train Loss: 0.1056, Val Loss: 0.0813\n",
      "Epoch 8/300 - Train Loss: 0.1019, Val Loss: 0.0748\n",
      "Epoch 9/300 - Train Loss: 0.1040, Val Loss: 0.0761\n",
      "Epoch 10/300 - Train Loss: 0.1033, Val Loss: 0.0748\n",
      "Epoch 11/300 - Train Loss: 0.1040, Val Loss: 0.0728\n",
      "Epoch 12/300 - Train Loss: 0.1057, Val Loss: 0.0758\n",
      "Epoch 13/300 - Train Loss: 0.1038, Val Loss: 0.0757\n",
      "Epoch 14/300 - Train Loss: 0.1045, Val Loss: 0.0732\n",
      "Epoch 15/300 - Train Loss: 0.1047, Val Loss: 0.0753\n",
      "Epoch 16/300 - Train Loss: 0.1052, Val Loss: 0.0774\n",
      "Epoch 17/300 - Train Loss: 0.1061, Val Loss: 0.0728\n",
      "Epoch 18/300 - Train Loss: 0.1083, Val Loss: 0.0791\n",
      "Epoch 19/300 - Train Loss: 0.1076, Val Loss: 0.0796\n",
      "Epoch 20/300 - Train Loss: 0.1074, Val Loss: 0.0825\n",
      "Epoch 21/300 - Train Loss: 0.1079, Val Loss: 0.0760\n",
      "Epoch 22/300 - Train Loss: 0.1053, Val Loss: 0.0824\n",
      "Epoch 23/300 - Train Loss: 0.1095, Val Loss: 0.0770\n",
      "Epoch 24/300 - Train Loss: 0.1066, Val Loss: 0.0758\n",
      "Epoch 25/300 - Train Loss: 0.1102, Val Loss: 0.1147\n",
      "Epoch 26/300 - Train Loss: 0.1089, Val Loss: 0.0761\n",
      "Epoch 27/300 - Train Loss: 0.1090, Val Loss: 0.0726\n",
      "Epoch 28/300 - Train Loss: 0.1093, Val Loss: 0.0814\n",
      "Epoch 29/300 - Train Loss: 0.1059, Val Loss: 0.0794\n",
      "Epoch 30/300 - Train Loss: 0.1112, Val Loss: 0.0730\n",
      "Epoch 31/300 - Train Loss: 0.1068, Val Loss: 0.0734\n",
      "Epoch 32/300 - Train Loss: 0.1095, Val Loss: 0.0774\n",
      "Epoch 33/300 - Train Loss: 0.1115, Val Loss: 0.0810\n",
      "Epoch 34/300 - Train Loss: 0.1092, Val Loss: 0.0818\n",
      "Epoch 35/300 - Train Loss: 0.1090, Val Loss: 0.0764\n",
      "Epoch 36/300 - Train Loss: 0.1101, Val Loss: 0.0768\n",
      "Epoch 37/300 - Train Loss: 0.1082, Val Loss: 0.0827\n",
      "Epoch 38/300 - Train Loss: 0.1097, Val Loss: 0.0766\n",
      "Epoch 39/300 - Train Loss: 0.1113, Val Loss: 0.1115\n",
      "Epoch 40/300 - Train Loss: 0.1085, Val Loss: 0.0773\n",
      "Epoch 41/300 - Train Loss: 0.1113, Val Loss: 0.0801\n",
      "Epoch 42/300 - Train Loss: 0.1103, Val Loss: 0.0824\n",
      "Epoch 43/300 - Train Loss: 0.1105, Val Loss: 0.0765\n",
      "Epoch 44/300 - Train Loss: 0.1103, Val Loss: 0.0757\n",
      "Epoch 45/300 - Train Loss: 0.1106, Val Loss: 0.0737\n",
      "Epoch 46/300 - Train Loss: 0.1097, Val Loss: 0.0768\n",
      "Epoch 47/300 - Train Loss: 0.1089, Val Loss: 0.0770\n",
      "Epoch 48/300 - Train Loss: 0.1086, Val Loss: 0.0743\n",
      "Epoch 49/300 - Train Loss: 0.1079, Val Loss: 0.0787\n",
      "Epoch 50/300 - Train Loss: 0.1075, Val Loss: 0.0878\n",
      "Epoch 51/300 - Train Loss: 0.1108, Val Loss: 0.0748\n",
      "Epoch 52/300 - Train Loss: 0.1109, Val Loss: 0.0802\n",
      "Epoch 53/300 - Train Loss: 0.1092, Val Loss: 0.0818\n",
      "Epoch 54/300 - Train Loss: 0.1108, Val Loss: 0.0791\n",
      "Epoch 55/300 - Train Loss: 0.1125, Val Loss: 0.0854\n",
      "Epoch 56/300 - Train Loss: 0.1094, Val Loss: 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 09:56:37,623] Trial 174 finished with value: 0.9661291463493852 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.2974743975209906, 'learning_rate': 0.00012120611486915158, 'batch_size': 32, 'weight_decay': 0.008488438440585389}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/300 - Train Loss: 0.1079, Val Loss: 0.0831\n",
      "Early stopping at epoch 57\n",
      "Macro F1 Score: 0.9661, Macro Precision: 0.9631, Macro Recall: 0.9696\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 176\n",
      "Training with F1=16, F2=32, D=4, dropout=0.25878979190595025, LR=3.989162184872791e-05, BS=32, WD=0.00739793094299869\n",
      "Epoch 1/300 - Train Loss: 0.4475, Val Loss: 0.2225\n",
      "Epoch 2/300 - Train Loss: 0.1989, Val Loss: 0.1431\n",
      "Epoch 3/300 - Train Loss: 0.1395, Val Loss: 0.1028\n",
      "Epoch 4/300 - Train Loss: 0.1193, Val Loss: 0.0854\n",
      "Epoch 5/300 - Train Loss: 0.1091, Val Loss: 0.0919\n",
      "Epoch 6/300 - Train Loss: 0.1073, Val Loss: 0.0812\n",
      "Epoch 7/300 - Train Loss: 0.1058, Val Loss: 0.0869\n",
      "Epoch 8/300 - Train Loss: 0.1039, Val Loss: 0.0870\n",
      "Epoch 9/300 - Train Loss: 0.1001, Val Loss: 0.0824\n",
      "Epoch 10/300 - Train Loss: 0.1024, Val Loss: 0.0816\n",
      "Epoch 11/300 - Train Loss: 0.1015, Val Loss: 0.0831\n",
      "Epoch 12/300 - Train Loss: 0.1015, Val Loss: 0.0806\n",
      "Epoch 13/300 - Train Loss: 0.0971, Val Loss: 0.0743\n",
      "Epoch 14/300 - Train Loss: 0.0977, Val Loss: 0.0772\n",
      "Epoch 15/300 - Train Loss: 0.0971, Val Loss: 0.0725\n",
      "Epoch 16/300 - Train Loss: 0.0987, Val Loss: 0.0749\n",
      "Epoch 17/300 - Train Loss: 0.0993, Val Loss: 0.0823\n",
      "Epoch 18/300 - Train Loss: 0.0980, Val Loss: 0.0739\n",
      "Epoch 19/300 - Train Loss: 0.0975, Val Loss: 0.0758\n",
      "Epoch 20/300 - Train Loss: 0.0967, Val Loss: 0.0722\n",
      "Epoch 21/300 - Train Loss: 0.1001, Val Loss: 0.0704\n",
      "Epoch 22/300 - Train Loss: 0.0977, Val Loss: 0.0725\n",
      "Epoch 23/300 - Train Loss: 0.0955, Val Loss: 0.0725\n",
      "Epoch 24/300 - Train Loss: 0.0969, Val Loss: 0.0718\n",
      "Epoch 25/300 - Train Loss: 0.0963, Val Loss: 0.0729\n",
      "Epoch 26/300 - Train Loss: 0.0965, Val Loss: 0.0750\n",
      "Epoch 27/300 - Train Loss: 0.0990, Val Loss: 0.0751\n",
      "Epoch 28/300 - Train Loss: 0.0954, Val Loss: 0.0707\n",
      "Epoch 29/300 - Train Loss: 0.0969, Val Loss: 0.0760\n",
      "Epoch 30/300 - Train Loss: 0.0959, Val Loss: 0.0706\n",
      "Epoch 31/300 - Train Loss: 0.0987, Val Loss: 0.0742\n",
      "Epoch 32/300 - Train Loss: 0.0973, Val Loss: 0.0697\n",
      "Epoch 33/300 - Train Loss: 0.1002, Val Loss: 0.0736\n",
      "Epoch 34/300 - Train Loss: 0.0964, Val Loss: 0.0736\n",
      "Epoch 35/300 - Train Loss: 0.0980, Val Loss: 0.0735\n",
      "Epoch 36/300 - Train Loss: 0.0975, Val Loss: 0.0714\n",
      "Epoch 37/300 - Train Loss: 0.0977, Val Loss: 0.0801\n",
      "Epoch 38/300 - Train Loss: 0.0991, Val Loss: 0.0727\n",
      "Epoch 39/300 - Train Loss: 0.0977, Val Loss: 0.0756\n",
      "Epoch 40/300 - Train Loss: 0.0992, Val Loss: 0.0726\n",
      "Epoch 41/300 - Train Loss: 0.0990, Val Loss: 0.0713\n",
      "Epoch 42/300 - Train Loss: 0.0974, Val Loss: 0.0710\n",
      "Epoch 43/300 - Train Loss: 0.0987, Val Loss: 0.0722\n",
      "Epoch 44/300 - Train Loss: 0.0980, Val Loss: 0.0723\n",
      "Epoch 45/300 - Train Loss: 0.1002, Val Loss: 0.0701\n",
      "Epoch 46/300 - Train Loss: 0.0990, Val Loss: 0.0817\n",
      "Epoch 47/300 - Train Loss: 0.1007, Val Loss: 0.0852\n",
      "Epoch 48/300 - Train Loss: 0.0992, Val Loss: 0.0751\n",
      "Epoch 49/300 - Train Loss: 0.0986, Val Loss: 0.0742\n",
      "Epoch 50/300 - Train Loss: 0.1014, Val Loss: 0.0733\n",
      "Epoch 51/300 - Train Loss: 0.1006, Val Loss: 0.0706\n",
      "Epoch 52/300 - Train Loss: 0.0988, Val Loss: 0.0721\n",
      "Epoch 53/300 - Train Loss: 0.1009, Val Loss: 0.0693\n",
      "Epoch 54/300 - Train Loss: 0.1000, Val Loss: 0.0733\n",
      "Epoch 55/300 - Train Loss: 0.0995, Val Loss: 0.0713\n",
      "Epoch 56/300 - Train Loss: 0.0994, Val Loss: 0.0770\n",
      "Epoch 57/300 - Train Loss: 0.0990, Val Loss: 0.0706\n",
      "Epoch 58/300 - Train Loss: 0.1023, Val Loss: 0.0722\n",
      "Epoch 59/300 - Train Loss: 0.0978, Val Loss: 0.0809\n",
      "Epoch 60/300 - Train Loss: 0.0985, Val Loss: 0.0719\n",
      "Epoch 61/300 - Train Loss: 0.1002, Val Loss: 0.0705\n",
      "Epoch 62/300 - Train Loss: 0.1001, Val Loss: 0.0744\n",
      "Epoch 63/300 - Train Loss: 0.0986, Val Loss: 0.0829\n",
      "Epoch 64/300 - Train Loss: 0.1012, Val Loss: 0.0755\n",
      "Epoch 65/300 - Train Loss: 0.1014, Val Loss: 0.0745\n",
      "Epoch 66/300 - Train Loss: 0.1018, Val Loss: 0.0688\n",
      "Epoch 67/300 - Train Loss: 0.1010, Val Loss: 0.0771\n",
      "Epoch 68/300 - Train Loss: 0.1005, Val Loss: 0.0722\n",
      "Epoch 69/300 - Train Loss: 0.1023, Val Loss: 0.0709\n",
      "Epoch 70/300 - Train Loss: 0.1000, Val Loss: 0.0696\n",
      "Epoch 71/300 - Train Loss: 0.1000, Val Loss: 0.0748\n",
      "Epoch 72/300 - Train Loss: 0.1029, Val Loss: 0.0745\n",
      "Epoch 73/300 - Train Loss: 0.1019, Val Loss: 0.0730\n",
      "Epoch 74/300 - Train Loss: 0.0999, Val Loss: 0.0720\n",
      "Epoch 75/300 - Train Loss: 0.1006, Val Loss: 0.0695\n",
      "Epoch 76/300 - Train Loss: 0.1018, Val Loss: 0.0770\n",
      "Epoch 77/300 - Train Loss: 0.1027, Val Loss: 0.0687\n",
      "Epoch 78/300 - Train Loss: 0.1010, Val Loss: 0.0743\n",
      "Epoch 79/300 - Train Loss: 0.1008, Val Loss: 0.0715\n",
      "Epoch 80/300 - Train Loss: 0.1021, Val Loss: 0.0736\n",
      "Epoch 81/300 - Train Loss: 0.1011, Val Loss: 0.0726\n",
      "Epoch 82/300 - Train Loss: 0.1018, Val Loss: 0.0713\n",
      "Epoch 83/300 - Train Loss: 0.1018, Val Loss: 0.0773\n",
      "Epoch 84/300 - Train Loss: 0.1009, Val Loss: 0.0709\n",
      "Epoch 85/300 - Train Loss: 0.1044, Val Loss: 0.0728\n",
      "Epoch 86/300 - Train Loss: 0.1036, Val Loss: 0.0736\n",
      "Epoch 87/300 - Train Loss: 0.1034, Val Loss: 0.0739\n",
      "Epoch 88/300 - Train Loss: 0.1022, Val Loss: 0.0718\n",
      "Epoch 89/300 - Train Loss: 0.1028, Val Loss: 0.0720\n",
      "Epoch 90/300 - Train Loss: 0.1048, Val Loss: 0.0701\n",
      "Epoch 91/300 - Train Loss: 0.1038, Val Loss: 0.0765\n",
      "Epoch 92/300 - Train Loss: 0.1037, Val Loss: 0.0736\n",
      "Epoch 93/300 - Train Loss: 0.1040, Val Loss: 0.0730\n",
      "Epoch 94/300 - Train Loss: 0.1017, Val Loss: 0.0704\n",
      "Epoch 95/300 - Train Loss: 0.1023, Val Loss: 0.0755\n",
      "Epoch 96/300 - Train Loss: 0.1020, Val Loss: 0.0720\n",
      "Epoch 97/300 - Train Loss: 0.1035, Val Loss: 0.0730\n",
      "Epoch 98/300 - Train Loss: 0.1030, Val Loss: 0.0713\n",
      "Epoch 99/300 - Train Loss: 0.1039, Val Loss: 0.0723\n",
      "Epoch 100/300 - Train Loss: 0.1009, Val Loss: 0.0768\n",
      "Epoch 101/300 - Train Loss: 0.1041, Val Loss: 0.0733\n",
      "Epoch 102/300 - Train Loss: 0.1023, Val Loss: 0.0713\n",
      "Epoch 103/300 - Train Loss: 0.1027, Val Loss: 0.0717\n",
      "Epoch 104/300 - Train Loss: 0.1040, Val Loss: 0.0716\n",
      "Epoch 105/300 - Train Loss: 0.1012, Val Loss: 0.0708\n",
      "Epoch 106/300 - Train Loss: 0.1033, Val Loss: 0.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:01:06,606] Trial 175 finished with value: 0.9705015109276397 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.25878979190595025, 'learning_rate': 3.989162184872791e-05, 'batch_size': 32, 'weight_decay': 0.00739793094299869}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/300 - Train Loss: 0.1017, Val Loss: 0.0890\n",
      "Early stopping at epoch 107\n",
      "Macro F1 Score: 0.9705, Macro Precision: 0.9766, Macro Recall: 0.9648\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 177\n",
      "Training with F1=16, F2=32, D=4, dropout=0.35321281414557576, LR=0.00010466581881648207, BS=128, WD=0.004985036590131446\n",
      "Epoch 1/300 - Train Loss: 0.4641, Val Loss: 0.2460\n",
      "Epoch 2/300 - Train Loss: 0.2091, Val Loss: 0.1801\n",
      "Epoch 3/300 - Train Loss: 0.1674, Val Loss: 0.1426\n",
      "Epoch 4/300 - Train Loss: 0.1307, Val Loss: 0.1140\n",
      "Epoch 5/300 - Train Loss: 0.1111, Val Loss: 0.1043\n",
      "Epoch 6/300 - Train Loss: 0.1029, Val Loss: 0.0936\n",
      "Epoch 7/300 - Train Loss: 0.0990, Val Loss: 0.0876\n",
      "Epoch 8/300 - Train Loss: 0.0967, Val Loss: 0.0853\n",
      "Epoch 9/300 - Train Loss: 0.0933, Val Loss: 0.0942\n",
      "Epoch 10/300 - Train Loss: 0.0915, Val Loss: 0.0842\n",
      "Epoch 11/300 - Train Loss: 0.0901, Val Loss: 0.0836\n",
      "Epoch 12/300 - Train Loss: 0.0877, Val Loss: 0.0877\n",
      "Epoch 13/300 - Train Loss: 0.0885, Val Loss: 0.0815\n",
      "Epoch 14/300 - Train Loss: 0.0878, Val Loss: 0.0777\n",
      "Epoch 15/300 - Train Loss: 0.0865, Val Loss: 0.0783\n",
      "Epoch 16/300 - Train Loss: 0.0841, Val Loss: 0.0804\n",
      "Epoch 17/300 - Train Loss: 0.0866, Val Loss: 0.0806\n",
      "Epoch 18/300 - Train Loss: 0.0867, Val Loss: 0.0773\n",
      "Epoch 19/300 - Train Loss: 0.0862, Val Loss: 0.0780\n",
      "Epoch 20/300 - Train Loss: 0.0869, Val Loss: 0.0791\n",
      "Epoch 21/300 - Train Loss: 0.0859, Val Loss: 0.0767\n",
      "Epoch 22/300 - Train Loss: 0.0823, Val Loss: 0.0776\n",
      "Epoch 23/300 - Train Loss: 0.0853, Val Loss: 0.0788\n",
      "Epoch 24/300 - Train Loss: 0.0845, Val Loss: 0.0772\n",
      "Epoch 25/300 - Train Loss: 0.0851, Val Loss: 0.0773\n",
      "Epoch 26/300 - Train Loss: 0.0849, Val Loss: 0.0783\n",
      "Epoch 27/300 - Train Loss: 0.0835, Val Loss: 0.0768\n",
      "Epoch 28/300 - Train Loss: 0.0862, Val Loss: 0.0768\n",
      "Epoch 29/300 - Train Loss: 0.0852, Val Loss: 0.0757\n",
      "Epoch 30/300 - Train Loss: 0.0847, Val Loss: 0.0793\n",
      "Epoch 31/300 - Train Loss: 0.0853, Val Loss: 0.0761\n",
      "Epoch 32/300 - Train Loss: 0.0831, Val Loss: 0.0732\n",
      "Epoch 33/300 - Train Loss: 0.0865, Val Loss: 0.0776\n",
      "Epoch 34/300 - Train Loss: 0.0864, Val Loss: 0.0775\n",
      "Epoch 35/300 - Train Loss: 0.0854, Val Loss: 0.0748\n",
      "Epoch 36/300 - Train Loss: 0.0857, Val Loss: 0.0743\n",
      "Epoch 37/300 - Train Loss: 0.0855, Val Loss: 0.0763\n",
      "Epoch 38/300 - Train Loss: 0.0859, Val Loss: 0.0759\n",
      "Epoch 39/300 - Train Loss: 0.0867, Val Loss: 0.0756\n",
      "Epoch 40/300 - Train Loss: 0.0876, Val Loss: 0.0776\n",
      "Epoch 41/300 - Train Loss: 0.0875, Val Loss: 0.0754\n",
      "Epoch 42/300 - Train Loss: 0.0864, Val Loss: 0.0779\n",
      "Epoch 43/300 - Train Loss: 0.0869, Val Loss: 0.0750\n",
      "Epoch 44/300 - Train Loss: 0.0869, Val Loss: 0.0761\n",
      "Epoch 45/300 - Train Loss: 0.0874, Val Loss: 0.0761\n",
      "Epoch 46/300 - Train Loss: 0.0867, Val Loss: 0.0768\n",
      "Epoch 47/300 - Train Loss: 0.0881, Val Loss: 0.0763\n",
      "Epoch 48/300 - Train Loss: 0.0878, Val Loss: 0.0775\n",
      "Epoch 49/300 - Train Loss: 0.0886, Val Loss: 0.0740\n",
      "Epoch 50/300 - Train Loss: 0.0886, Val Loss: 0.0790\n",
      "Epoch 51/300 - Train Loss: 0.0888, Val Loss: 0.0791\n",
      "Epoch 52/300 - Train Loss: 0.0874, Val Loss: 0.0751\n",
      "Epoch 53/300 - Train Loss: 0.0893, Val Loss: 0.0759\n",
      "Epoch 54/300 - Train Loss: 0.0863, Val Loss: 0.0756\n",
      "Epoch 55/300 - Train Loss: 0.0893, Val Loss: 0.0794\n",
      "Epoch 56/300 - Train Loss: 0.0882, Val Loss: 0.0754\n",
      "Epoch 57/300 - Train Loss: 0.0894, Val Loss: 0.0796\n",
      "Epoch 58/300 - Train Loss: 0.0903, Val Loss: 0.0739\n",
      "Epoch 59/300 - Train Loss: 0.0885, Val Loss: 0.0742\n",
      "Epoch 60/300 - Train Loss: 0.0889, Val Loss: 0.0785\n",
      "Epoch 61/300 - Train Loss: 0.0888, Val Loss: 0.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:03:07,693] Trial 176 finished with value: 0.9635055735520851 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.35321281414557576, 'learning_rate': 0.00010466581881648207, 'batch_size': 128, 'weight_decay': 0.004985036590131446}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/300 - Train Loss: 0.0913, Val Loss: 0.0784\n",
      "Early stopping at epoch 62\n",
      "Macro F1 Score: 0.9635, Macro Precision: 0.9581, Macro Recall: 0.9694\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 178\n",
      "Training with F1=16, F2=32, D=4, dropout=0.31206304216962977, LR=0.0001857172342409373, BS=32, WD=0.006016137974325712\n",
      "Epoch 1/300 - Train Loss: 0.2294, Val Loss: 0.0980\n",
      "Epoch 2/300 - Train Loss: 0.1134, Val Loss: 0.0837\n",
      "Epoch 3/300 - Train Loss: 0.1067, Val Loss: 0.0760\n",
      "Epoch 4/300 - Train Loss: 0.1011, Val Loss: 0.0744\n",
      "Epoch 5/300 - Train Loss: 0.1019, Val Loss: 0.0795\n",
      "Epoch 6/300 - Train Loss: 0.1018, Val Loss: 0.0747\n",
      "Epoch 7/300 - Train Loss: 0.0997, Val Loss: 0.0770\n",
      "Epoch 8/300 - Train Loss: 0.1025, Val Loss: 0.0755\n",
      "Epoch 9/300 - Train Loss: 0.1036, Val Loss: 0.0742\n",
      "Epoch 10/300 - Train Loss: 0.1059, Val Loss: 0.0789\n",
      "Epoch 11/300 - Train Loss: 0.1044, Val Loss: 0.0745\n",
      "Epoch 12/300 - Train Loss: 0.1041, Val Loss: 0.0745\n",
      "Epoch 13/300 - Train Loss: 0.1040, Val Loss: 0.0774\n",
      "Epoch 14/300 - Train Loss: 0.1055, Val Loss: 0.0769\n",
      "Epoch 15/300 - Train Loss: 0.1050, Val Loss: 0.0791\n",
      "Epoch 16/300 - Train Loss: 0.1042, Val Loss: 0.0756\n",
      "Epoch 17/300 - Train Loss: 0.1049, Val Loss: 0.0745\n",
      "Epoch 18/300 - Train Loss: 0.1064, Val Loss: 0.0965\n",
      "Epoch 19/300 - Train Loss: 0.1059, Val Loss: 0.0759\n",
      "Epoch 20/300 - Train Loss: 0.1086, Val Loss: 0.0753\n",
      "Epoch 21/300 - Train Loss: 0.1057, Val Loss: 0.0886\n",
      "Epoch 22/300 - Train Loss: 0.1084, Val Loss: 0.0810\n",
      "Epoch 23/300 - Train Loss: 0.1085, Val Loss: 0.0811\n",
      "Epoch 24/300 - Train Loss: 0.1101, Val Loss: 0.0753\n",
      "Epoch 25/300 - Train Loss: 0.1051, Val Loss: 0.1159\n",
      "Epoch 26/300 - Train Loss: 0.1069, Val Loss: 0.0772\n",
      "Epoch 27/300 - Train Loss: 0.1083, Val Loss: 0.0758\n",
      "Epoch 28/300 - Train Loss: 0.1062, Val Loss: 0.0782\n",
      "Epoch 29/300 - Train Loss: 0.1074, Val Loss: 0.0805\n",
      "Epoch 30/300 - Train Loss: 0.1078, Val Loss: 0.0747\n",
      "Epoch 31/300 - Train Loss: 0.1084, Val Loss: 0.0869\n",
      "Epoch 32/300 - Train Loss: 0.1067, Val Loss: 0.0774\n",
      "Epoch 33/300 - Train Loss: 0.1077, Val Loss: 0.0873\n",
      "Epoch 34/300 - Train Loss: 0.1106, Val Loss: 0.0759\n",
      "Epoch 35/300 - Train Loss: 0.1079, Val Loss: 0.0735\n",
      "Epoch 36/300 - Train Loss: 0.1096, Val Loss: 0.0785\n",
      "Epoch 37/300 - Train Loss: 0.1098, Val Loss: 0.0775\n",
      "Epoch 38/300 - Train Loss: 0.1075, Val Loss: 0.0749\n",
      "Epoch 39/300 - Train Loss: 0.1094, Val Loss: 0.0725\n",
      "Epoch 40/300 - Train Loss: 0.1083, Val Loss: 0.0914\n",
      "Epoch 41/300 - Train Loss: 0.1077, Val Loss: 0.0945\n",
      "Epoch 42/300 - Train Loss: 0.1101, Val Loss: 0.0848\n",
      "Epoch 43/300 - Train Loss: 0.1069, Val Loss: 0.0732\n",
      "Epoch 44/300 - Train Loss: 0.1042, Val Loss: 0.0833\n",
      "Epoch 45/300 - Train Loss: 0.1054, Val Loss: 0.0982\n",
      "Epoch 46/300 - Train Loss: 0.1085, Val Loss: 0.0741\n",
      "Epoch 47/300 - Train Loss: 0.1088, Val Loss: 0.0725\n",
      "Epoch 48/300 - Train Loss: 0.1109, Val Loss: 0.0775\n",
      "Epoch 49/300 - Train Loss: 0.1089, Val Loss: 0.0778\n",
      "Epoch 50/300 - Train Loss: 0.1088, Val Loss: 0.0747\n",
      "Epoch 51/300 - Train Loss: 0.1072, Val Loss: 0.0772\n",
      "Epoch 52/300 - Train Loss: 0.1096, Val Loss: 0.0741\n",
      "Epoch 53/300 - Train Loss: 0.1111, Val Loss: 0.0763\n",
      "Epoch 54/300 - Train Loss: 0.1097, Val Loss: 0.0722\n",
      "Epoch 55/300 - Train Loss: 0.1078, Val Loss: 0.0729\n",
      "Epoch 56/300 - Train Loss: 0.1078, Val Loss: 0.0759\n",
      "Epoch 57/300 - Train Loss: 0.1067, Val Loss: 0.0752\n",
      "Epoch 58/300 - Train Loss: 0.1070, Val Loss: 0.0771\n",
      "Epoch 59/300 - Train Loss: 0.1065, Val Loss: 0.0862\n",
      "Epoch 60/300 - Train Loss: 0.1094, Val Loss: 0.0803\n",
      "Epoch 61/300 - Train Loss: 0.1103, Val Loss: 0.0798\n",
      "Epoch 62/300 - Train Loss: 0.1105, Val Loss: 0.0745\n",
      "Epoch 63/300 - Train Loss: 0.1077, Val Loss: 0.0847\n",
      "Epoch 64/300 - Train Loss: 0.1073, Val Loss: 0.0789\n",
      "Epoch 65/300 - Train Loss: 0.1079, Val Loss: 0.0840\n",
      "Epoch 66/300 - Train Loss: 0.1079, Val Loss: 0.0858\n",
      "Epoch 67/300 - Train Loss: 0.1071, Val Loss: 0.0855\n",
      "Epoch 68/300 - Train Loss: 0.1063, Val Loss: 0.0957\n",
      "Epoch 69/300 - Train Loss: 0.1070, Val Loss: 0.0886\n",
      "Epoch 70/300 - Train Loss: 0.1110, Val Loss: 0.0788\n",
      "Epoch 71/300 - Train Loss: 0.1046, Val Loss: 0.0803\n",
      "Epoch 72/300 - Train Loss: 0.1053, Val Loss: 0.0781\n",
      "Epoch 73/300 - Train Loss: 0.1076, Val Loss: 0.0742\n",
      "Epoch 74/300 - Train Loss: 0.1065, Val Loss: 0.0739\n",
      "Epoch 75/300 - Train Loss: 0.1091, Val Loss: 0.0736\n",
      "Epoch 76/300 - Train Loss: 0.1079, Val Loss: 0.0799\n",
      "Epoch 77/300 - Train Loss: 0.1096, Val Loss: 0.0752\n",
      "Epoch 78/300 - Train Loss: 0.1094, Val Loss: 0.0869\n",
      "Epoch 79/300 - Train Loss: 0.1076, Val Loss: 0.0739\n",
      "Epoch 80/300 - Train Loss: 0.1082, Val Loss: 0.0743\n",
      "Epoch 81/300 - Train Loss: 0.1072, Val Loss: 0.0802\n",
      "Epoch 82/300 - Train Loss: 0.1065, Val Loss: 0.0874\n",
      "Epoch 83/300 - Train Loss: 0.1082, Val Loss: 0.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:06:38,290] Trial 177 finished with value: 0.9648005553354833 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.31206304216962977, 'learning_rate': 0.0001857172342409373, 'batch_size': 32, 'weight_decay': 0.006016137974325712}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300 - Train Loss: 0.1072, Val Loss: 0.0847\n",
      "Early stopping at epoch 84\n",
      "Macro F1 Score: 0.9648, Macro Precision: 0.9662, Macro Recall: 0.9637\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 179\n",
      "Training with F1=16, F2=32, D=4, dropout=0.44268833863997065, LR=5.764766801302875e-05, BS=32, WD=0.004224872465925137\n",
      "Epoch 1/300 - Train Loss: 0.4244, Val Loss: 0.1874\n",
      "Epoch 2/300 - Train Loss: 0.1742, Val Loss: 0.1311\n",
      "Epoch 3/300 - Train Loss: 0.1339, Val Loss: 0.1118\n",
      "Epoch 4/300 - Train Loss: 0.1213, Val Loss: 0.0941\n",
      "Epoch 5/300 - Train Loss: 0.1122, Val Loss: 0.0963\n",
      "Epoch 6/300 - Train Loss: 0.1090, Val Loss: 0.0869\n",
      "Epoch 7/300 - Train Loss: 0.1076, Val Loss: 0.0858\n",
      "Epoch 8/300 - Train Loss: 0.1056, Val Loss: 0.0813\n",
      "Epoch 9/300 - Train Loss: 0.1038, Val Loss: 0.0824\n",
      "Epoch 10/300 - Train Loss: 0.1017, Val Loss: 0.0794\n",
      "Epoch 11/300 - Train Loss: 0.1031, Val Loss: 0.0805\n",
      "Epoch 12/300 - Train Loss: 0.0999, Val Loss: 0.0756\n",
      "Epoch 13/300 - Train Loss: 0.1002, Val Loss: 0.0750\n",
      "Epoch 14/300 - Train Loss: 0.1005, Val Loss: 0.0754\n",
      "Epoch 15/300 - Train Loss: 0.0977, Val Loss: 0.0753\n",
      "Epoch 16/300 - Train Loss: 0.0995, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.0985, Val Loss: 0.0773\n",
      "Epoch 18/300 - Train Loss: 0.1001, Val Loss: 0.0786\n",
      "Epoch 19/300 - Train Loss: 0.0999, Val Loss: 0.0800\n",
      "Epoch 20/300 - Train Loss: 0.0997, Val Loss: 0.0772\n",
      "Epoch 21/300 - Train Loss: 0.0976, Val Loss: 0.0746\n",
      "Epoch 22/300 - Train Loss: 0.0993, Val Loss: 0.0720\n",
      "Epoch 23/300 - Train Loss: 0.0997, Val Loss: 0.0737\n",
      "Epoch 24/300 - Train Loss: 0.0978, Val Loss: 0.0759\n",
      "Epoch 25/300 - Train Loss: 0.0977, Val Loss: 0.0732\n",
      "Epoch 26/300 - Train Loss: 0.1002, Val Loss: 0.0771\n",
      "Epoch 27/300 - Train Loss: 0.1017, Val Loss: 0.0761\n",
      "Epoch 28/300 - Train Loss: 0.0988, Val Loss: 0.0735\n",
      "Epoch 29/300 - Train Loss: 0.0991, Val Loss: 0.0739\n",
      "Epoch 30/300 - Train Loss: 0.0997, Val Loss: 0.0715\n",
      "Epoch 31/300 - Train Loss: 0.1004, Val Loss: 0.0714\n",
      "Epoch 32/300 - Train Loss: 0.1006, Val Loss: 0.0792\n",
      "Epoch 33/300 - Train Loss: 0.0982, Val Loss: 0.0759\n",
      "Epoch 34/300 - Train Loss: 0.1007, Val Loss: 0.0751\n",
      "Epoch 35/300 - Train Loss: 0.1013, Val Loss: 0.0731\n",
      "Epoch 36/300 - Train Loss: 0.1027, Val Loss: 0.0709\n",
      "Epoch 37/300 - Train Loss: 0.0987, Val Loss: 0.0711\n",
      "Epoch 38/300 - Train Loss: 0.0983, Val Loss: 0.0741\n",
      "Epoch 39/300 - Train Loss: 0.1006, Val Loss: 0.0762\n",
      "Epoch 40/300 - Train Loss: 0.1000, Val Loss: 0.0736\n",
      "Epoch 41/300 - Train Loss: 0.1000, Val Loss: 0.0723\n",
      "Epoch 42/300 - Train Loss: 0.1002, Val Loss: 0.0713\n",
      "Epoch 43/300 - Train Loss: 0.0990, Val Loss: 0.0736\n",
      "Epoch 44/300 - Train Loss: 0.0981, Val Loss: 0.0735\n",
      "Epoch 45/300 - Train Loss: 0.1033, Val Loss: 0.0725\n",
      "Epoch 46/300 - Train Loss: 0.1028, Val Loss: 0.0728\n",
      "Epoch 47/300 - Train Loss: 0.1017, Val Loss: 0.0750\n",
      "Epoch 48/300 - Train Loss: 0.1004, Val Loss: 0.0727\n",
      "Epoch 49/300 - Train Loss: 0.1028, Val Loss: 0.0726\n",
      "Epoch 50/300 - Train Loss: 0.1019, Val Loss: 0.0755\n",
      "Epoch 51/300 - Train Loss: 0.1017, Val Loss: 0.0770\n",
      "Epoch 52/300 - Train Loss: 0.0990, Val Loss: 0.0735\n",
      "Epoch 53/300 - Train Loss: 0.1006, Val Loss: 0.0747\n",
      "Epoch 54/300 - Train Loss: 0.1017, Val Loss: 0.0762\n",
      "Epoch 55/300 - Train Loss: 0.1031, Val Loss: 0.0718\n",
      "Epoch 56/300 - Train Loss: 0.1036, Val Loss: 0.0726\n",
      "Epoch 57/300 - Train Loss: 0.1035, Val Loss: 0.0842\n",
      "Epoch 58/300 - Train Loss: 0.1034, Val Loss: 0.0718\n",
      "Epoch 59/300 - Train Loss: 0.1022, Val Loss: 0.0744\n",
      "Epoch 60/300 - Train Loss: 0.1035, Val Loss: 0.0759\n",
      "Epoch 61/300 - Train Loss: 0.1036, Val Loss: 0.0746\n",
      "Epoch 62/300 - Train Loss: 0.1031, Val Loss: 0.0716\n",
      "Epoch 63/300 - Train Loss: 0.1039, Val Loss: 0.0737\n",
      "Epoch 64/300 - Train Loss: 0.1021, Val Loss: 0.0737\n",
      "Epoch 65/300 - Train Loss: 0.1042, Val Loss: 0.0739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:09:23,554] Trial 178 finished with value: 0.962511693568036 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.44268833863997065, 'learning_rate': 5.764766801302875e-05, 'batch_size': 32, 'weight_decay': 0.004224872465925137}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/300 - Train Loss: 0.1022, Val Loss: 0.0722\n",
      "Early stopping at epoch 66\n",
      "Macro F1 Score: 0.9625, Macro Precision: 0.9573, Macro Recall: 0.9683\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 180\n",
      "Training with F1=8, F2=32, D=4, dropout=0.22431790976928206, LR=0.0009862210110376276, BS=32, WD=0.00349743740148773\n",
      "Epoch 1/300 - Train Loss: 0.1488, Val Loss: 0.1011\n",
      "Epoch 2/300 - Train Loss: 0.1105, Val Loss: 0.0812\n",
      "Epoch 3/300 - Train Loss: 0.1117, Val Loss: 0.1052\n",
      "Epoch 4/300 - Train Loss: 0.1096, Val Loss: 0.0783\n",
      "Epoch 5/300 - Train Loss: 0.1136, Val Loss: 0.0937\n",
      "Epoch 6/300 - Train Loss: 0.1092, Val Loss: 0.0779\n",
      "Epoch 7/300 - Train Loss: 0.1115, Val Loss: 0.0978\n",
      "Epoch 8/300 - Train Loss: 0.1121, Val Loss: 0.1090\n",
      "Epoch 9/300 - Train Loss: 0.1090, Val Loss: 0.0868\n",
      "Epoch 10/300 - Train Loss: 0.1110, Val Loss: 0.0839\n",
      "Epoch 11/300 - Train Loss: 0.1127, Val Loss: 0.0853\n",
      "Epoch 12/300 - Train Loss: 0.1140, Val Loss: 0.0771\n",
      "Epoch 13/300 - Train Loss: 0.1109, Val Loss: 0.0793\n",
      "Epoch 14/300 - Train Loss: 0.1119, Val Loss: 0.1105\n",
      "Epoch 15/300 - Train Loss: 0.1089, Val Loss: 0.0799\n",
      "Epoch 16/300 - Train Loss: 0.1086, Val Loss: 0.0785\n",
      "Epoch 17/300 - Train Loss: 0.1124, Val Loss: 0.0780\n",
      "Epoch 18/300 - Train Loss: 0.1112, Val Loss: 0.0762\n",
      "Epoch 19/300 - Train Loss: 0.1084, Val Loss: 0.1056\n",
      "Epoch 20/300 - Train Loss: 0.1086, Val Loss: 0.0816\n",
      "Epoch 21/300 - Train Loss: 0.1074, Val Loss: 0.0870\n",
      "Epoch 22/300 - Train Loss: 0.1102, Val Loss: 0.0836\n",
      "Epoch 23/300 - Train Loss: 0.1105, Val Loss: 0.0896\n",
      "Epoch 24/300 - Train Loss: 0.1102, Val Loss: 0.0834\n",
      "Epoch 25/300 - Train Loss: 0.1087, Val Loss: 0.0878\n",
      "Epoch 26/300 - Train Loss: 0.1072, Val Loss: 0.0764\n",
      "Epoch 27/300 - Train Loss: 0.1085, Val Loss: 0.0954\n",
      "Epoch 28/300 - Train Loss: 0.1086, Val Loss: 0.0786\n",
      "Epoch 29/300 - Train Loss: 0.1066, Val Loss: 0.0794\n",
      "Epoch 30/300 - Train Loss: 0.1062, Val Loss: 0.0754\n",
      "Epoch 31/300 - Train Loss: 0.1075, Val Loss: 0.0812\n",
      "Epoch 32/300 - Train Loss: 0.1083, Val Loss: 0.0869\n",
      "Epoch 33/300 - Train Loss: 0.1075, Val Loss: 0.0758\n",
      "Epoch 34/300 - Train Loss: 0.1082, Val Loss: 0.0779\n",
      "Epoch 35/300 - Train Loss: 0.1088, Val Loss: 0.0844\n",
      "Epoch 36/300 - Train Loss: 0.1090, Val Loss: 0.0788\n",
      "Epoch 37/300 - Train Loss: 0.1067, Val Loss: 0.1176\n",
      "Epoch 38/300 - Train Loss: 0.1070, Val Loss: 0.0899\n",
      "Epoch 39/300 - Train Loss: 0.1067, Val Loss: 0.0852\n",
      "Epoch 40/300 - Train Loss: 0.1090, Val Loss: 0.0798\n",
      "Epoch 41/300 - Train Loss: 0.1039, Val Loss: 0.0762\n",
      "Epoch 42/300 - Train Loss: 0.1060, Val Loss: 0.1094\n",
      "Epoch 43/300 - Train Loss: 0.1070, Val Loss: 0.0991\n",
      "Epoch 44/300 - Train Loss: 0.1079, Val Loss: 0.0788\n",
      "Epoch 45/300 - Train Loss: 0.1062, Val Loss: 0.0802\n",
      "Epoch 46/300 - Train Loss: 0.1050, Val Loss: 0.0792\n",
      "Epoch 47/300 - Train Loss: 0.1064, Val Loss: 0.0987\n",
      "Epoch 48/300 - Train Loss: 0.1066, Val Loss: 0.0937\n",
      "Epoch 49/300 - Train Loss: 0.1082, Val Loss: 0.0913\n",
      "Epoch 50/300 - Train Loss: 0.1057, Val Loss: 0.0806\n",
      "Epoch 51/300 - Train Loss: 0.1073, Val Loss: 0.0800\n",
      "Epoch 52/300 - Train Loss: 0.1073, Val Loss: 0.0787\n",
      "Epoch 53/300 - Train Loss: 0.1081, Val Loss: 0.0814\n",
      "Epoch 54/300 - Train Loss: 0.1064, Val Loss: 0.0818\n",
      "Epoch 55/300 - Train Loss: 0.1063, Val Loss: 0.0855\n",
      "Epoch 56/300 - Train Loss: 0.1086, Val Loss: 0.0874\n",
      "Epoch 57/300 - Train Loss: 0.1082, Val Loss: 0.0801\n",
      "Epoch 58/300 - Train Loss: 0.1041, Val Loss: 0.0872\n",
      "Epoch 59/300 - Train Loss: 0.1061, Val Loss: 0.0879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:11:26,575] Trial 179 finished with value: 0.9592621641378175 and parameters: {'F1': 8, 'F2': 32, 'D': 4, 'dropout': 0.22431790976928206, 'learning_rate': 0.0009862210110376276, 'batch_size': 32, 'weight_decay': 0.00349743740148773}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300 - Train Loss: 0.1078, Val Loss: 0.0795\n",
      "Early stopping at epoch 60\n",
      "Macro F1 Score: 0.9593, Macro Precision: 0.9735, Macro Recall: 0.9464\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       789\n",
      "           1       0.96      0.89      0.92        61\n",
      "           2       0.99      0.96      0.97       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.95      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 181\n",
      "Training with F1=16, F2=8, D=2, dropout=0.23520350218019595, LR=8.188685971188204e-05, BS=64, WD=0.008022211095291205\n",
      "Epoch 1/300 - Train Loss: 0.5798, Val Loss: 0.2749\n",
      "Epoch 2/300 - Train Loss: 0.2252, Val Loss: 0.1675\n",
      "Epoch 3/300 - Train Loss: 0.1603, Val Loss: 0.1282\n",
      "Epoch 4/300 - Train Loss: 0.1298, Val Loss: 0.1111\n",
      "Epoch 5/300 - Train Loss: 0.1185, Val Loss: 0.0990\n",
      "Epoch 6/300 - Train Loss: 0.1119, Val Loss: 0.0954\n",
      "Epoch 7/300 - Train Loss: 0.1099, Val Loss: 0.0880\n",
      "Epoch 8/300 - Train Loss: 0.1021, Val Loss: 0.0869\n",
      "Epoch 9/300 - Train Loss: 0.1009, Val Loss: 0.0830\n",
      "Epoch 10/300 - Train Loss: 0.1005, Val Loss: 0.0824\n",
      "Epoch 11/300 - Train Loss: 0.0991, Val Loss: 0.0806\n",
      "Epoch 12/300 - Train Loss: 0.0993, Val Loss: 0.0789\n",
      "Epoch 13/300 - Train Loss: 0.0984, Val Loss: 0.0820\n",
      "Epoch 14/300 - Train Loss: 0.0968, Val Loss: 0.0817\n",
      "Epoch 15/300 - Train Loss: 0.0976, Val Loss: 0.0808\n",
      "Epoch 16/300 - Train Loss: 0.0958, Val Loss: 0.0783\n",
      "Epoch 17/300 - Train Loss: 0.0952, Val Loss: 0.0775\n",
      "Epoch 18/300 - Train Loss: 0.0954, Val Loss: 0.0758\n",
      "Epoch 19/300 - Train Loss: 0.0943, Val Loss: 0.0765\n",
      "Epoch 20/300 - Train Loss: 0.0957, Val Loss: 0.0783\n",
      "Epoch 21/300 - Train Loss: 0.0948, Val Loss: 0.0771\n",
      "Epoch 22/300 - Train Loss: 0.0930, Val Loss: 0.0773\n",
      "Epoch 23/300 - Train Loss: 0.0936, Val Loss: 0.0777\n",
      "Epoch 24/300 - Train Loss: 0.0929, Val Loss: 0.0762\n",
      "Epoch 25/300 - Train Loss: 0.0941, Val Loss: 0.0777\n",
      "Epoch 26/300 - Train Loss: 0.0927, Val Loss: 0.0784\n",
      "Epoch 27/300 - Train Loss: 0.0929, Val Loss: 0.0772\n",
      "Epoch 28/300 - Train Loss: 0.0937, Val Loss: 0.0751\n",
      "Epoch 29/300 - Train Loss: 0.0932, Val Loss: 0.0735\n",
      "Epoch 30/300 - Train Loss: 0.0951, Val Loss: 0.0752\n",
      "Epoch 31/300 - Train Loss: 0.0937, Val Loss: 0.0752\n",
      "Epoch 32/300 - Train Loss: 0.0954, Val Loss: 0.0751\n",
      "Epoch 33/300 - Train Loss: 0.0941, Val Loss: 0.0764\n",
      "Epoch 34/300 - Train Loss: 0.0933, Val Loss: 0.0761\n",
      "Epoch 35/300 - Train Loss: 0.0943, Val Loss: 0.0759\n",
      "Epoch 36/300 - Train Loss: 0.0941, Val Loss: 0.0752\n",
      "Epoch 37/300 - Train Loss: 0.0941, Val Loss: 0.0741\n",
      "Epoch 38/300 - Train Loss: 0.0952, Val Loss: 0.0749\n",
      "Epoch 39/300 - Train Loss: 0.0951, Val Loss: 0.0744\n",
      "Epoch 40/300 - Train Loss: 0.0953, Val Loss: 0.0732\n",
      "Epoch 41/300 - Train Loss: 0.0947, Val Loss: 0.0723\n",
      "Epoch 42/300 - Train Loss: 0.0940, Val Loss: 0.0744\n",
      "Epoch 43/300 - Train Loss: 0.0943, Val Loss: 0.0741\n",
      "Epoch 44/300 - Train Loss: 0.0952, Val Loss: 0.0766\n",
      "Epoch 45/300 - Train Loss: 0.0939, Val Loss: 0.0755\n",
      "Epoch 46/300 - Train Loss: 0.0951, Val Loss: 0.0783\n",
      "Epoch 47/300 - Train Loss: 0.0949, Val Loss: 0.0740\n",
      "Epoch 48/300 - Train Loss: 0.0947, Val Loss: 0.0752\n",
      "Epoch 49/300 - Train Loss: 0.0941, Val Loss: 0.0753\n",
      "Epoch 50/300 - Train Loss: 0.0964, Val Loss: 0.0762\n",
      "Epoch 51/300 - Train Loss: 0.0956, Val Loss: 0.0744\n",
      "Epoch 52/300 - Train Loss: 0.0957, Val Loss: 0.0758\n",
      "Epoch 53/300 - Train Loss: 0.0951, Val Loss: 0.0822\n",
      "Epoch 54/300 - Train Loss: 0.0967, Val Loss: 0.0750\n",
      "Epoch 55/300 - Train Loss: 0.0955, Val Loss: 0.0743\n",
      "Epoch 56/300 - Train Loss: 0.0973, Val Loss: 0.0757\n",
      "Epoch 57/300 - Train Loss: 0.0936, Val Loss: 0.0767\n",
      "Epoch 58/300 - Train Loss: 0.0962, Val Loss: 0.0754\n",
      "Epoch 59/300 - Train Loss: 0.0956, Val Loss: 0.0742\n",
      "Epoch 60/300 - Train Loss: 0.0956, Val Loss: 0.0741\n",
      "Epoch 61/300 - Train Loss: 0.0948, Val Loss: 0.0744\n",
      "Epoch 62/300 - Train Loss: 0.0968, Val Loss: 0.0761\n",
      "Epoch 63/300 - Train Loss: 0.0978, Val Loss: 0.0753\n",
      "Epoch 64/300 - Train Loss: 0.0985, Val Loss: 0.0739\n",
      "Epoch 65/300 - Train Loss: 0.0956, Val Loss: 0.0766\n",
      "Epoch 66/300 - Train Loss: 0.0976, Val Loss: 0.0756\n",
      "Epoch 67/300 - Train Loss: 0.0940, Val Loss: 0.0753\n",
      "Epoch 68/300 - Train Loss: 0.0962, Val Loss: 0.0748\n",
      "Epoch 69/300 - Train Loss: 0.0968, Val Loss: 0.0746\n",
      "Epoch 70/300 - Train Loss: 0.0955, Val Loss: 0.0737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:13:08,431] Trial 180 finished with value: 0.9662131307211547 and parameters: {'F1': 16, 'F2': 8, 'D': 2, 'dropout': 0.23520350218019595, 'learning_rate': 8.188685971188204e-05, 'batch_size': 64, 'weight_decay': 0.008022211095291205}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300 - Train Loss: 0.0955, Val Loss: 0.0757\n",
      "Early stopping at epoch 71\n",
      "Macro F1 Score: 0.9662, Macro Precision: 0.9629, Macro Recall: 0.9698\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 182\n",
      "Training with F1=16, F2=32, D=8, dropout=0.24810113905999653, LR=0.00010966515820117808, BS=32, WD=0.008749733672703677\n",
      "Epoch 1/300 - Train Loss: 0.2776, Val Loss: 0.1127\n",
      "Epoch 2/300 - Train Loss: 0.1194, Val Loss: 0.0840\n",
      "Epoch 3/300 - Train Loss: 0.1055, Val Loss: 0.0757\n",
      "Epoch 4/300 - Train Loss: 0.1035, Val Loss: 0.0780\n",
      "Epoch 5/300 - Train Loss: 0.1011, Val Loss: 0.0839\n",
      "Epoch 6/300 - Train Loss: 0.0996, Val Loss: 0.0792\n",
      "Epoch 7/300 - Train Loss: 0.0976, Val Loss: 0.0734\n",
      "Epoch 8/300 - Train Loss: 0.0994, Val Loss: 0.0738\n",
      "Epoch 9/300 - Train Loss: 0.0981, Val Loss: 0.0713\n",
      "Epoch 10/300 - Train Loss: 0.0977, Val Loss: 0.0749\n",
      "Epoch 11/300 - Train Loss: 0.0991, Val Loss: 0.0775\n",
      "Epoch 12/300 - Train Loss: 0.1006, Val Loss: 0.0744\n",
      "Epoch 13/300 - Train Loss: 0.1003, Val Loss: 0.0714\n",
      "Epoch 14/300 - Train Loss: 0.1010, Val Loss: 0.0712\n",
      "Epoch 15/300 - Train Loss: 0.0999, Val Loss: 0.0777\n",
      "Epoch 16/300 - Train Loss: 0.1015, Val Loss: 0.0773\n",
      "Epoch 17/300 - Train Loss: 0.1030, Val Loss: 0.0747\n",
      "Epoch 18/300 - Train Loss: 0.1024, Val Loss: 0.0738\n",
      "Epoch 19/300 - Train Loss: 0.1003, Val Loss: 0.0839\n",
      "Epoch 20/300 - Train Loss: 0.1041, Val Loss: 0.0741\n",
      "Epoch 21/300 - Train Loss: 0.1027, Val Loss: 0.0781\n",
      "Epoch 22/300 - Train Loss: 0.1037, Val Loss: 0.0711\n",
      "Epoch 23/300 - Train Loss: 0.1022, Val Loss: 0.0860\n",
      "Epoch 24/300 - Train Loss: 0.1046, Val Loss: 0.0780\n",
      "Epoch 25/300 - Train Loss: 0.1055, Val Loss: 0.0883\n",
      "Epoch 26/300 - Train Loss: 0.1043, Val Loss: 0.0761\n",
      "Epoch 27/300 - Train Loss: 0.1079, Val Loss: 0.0719\n",
      "Epoch 28/300 - Train Loss: 0.1070, Val Loss: 0.0721\n",
      "Epoch 29/300 - Train Loss: 0.1032, Val Loss: 0.0735\n",
      "Epoch 30/300 - Train Loss: 0.1054, Val Loss: 0.0843\n",
      "Epoch 31/300 - Train Loss: 0.1046, Val Loss: 0.0744\n",
      "Epoch 32/300 - Train Loss: 0.1065, Val Loss: 0.0795\n",
      "Epoch 33/300 - Train Loss: 0.1057, Val Loss: 0.0721\n",
      "Epoch 34/300 - Train Loss: 0.1081, Val Loss: 0.0829\n",
      "Epoch 35/300 - Train Loss: 0.1052, Val Loss: 0.0780\n",
      "Epoch 36/300 - Train Loss: 0.1066, Val Loss: 0.0739\n",
      "Epoch 37/300 - Train Loss: 0.1055, Val Loss: 0.0793\n",
      "Epoch 38/300 - Train Loss: 0.1056, Val Loss: 0.0798\n",
      "Epoch 39/300 - Train Loss: 0.1057, Val Loss: 0.0972\n",
      "Epoch 40/300 - Train Loss: 0.1069, Val Loss: 0.0919\n",
      "Epoch 41/300 - Train Loss: 0.1054, Val Loss: 0.0721\n",
      "Epoch 42/300 - Train Loss: 0.1072, Val Loss: 0.0788\n",
      "Epoch 43/300 - Train Loss: 0.1046, Val Loss: 0.0911\n",
      "Epoch 44/300 - Train Loss: 0.1072, Val Loss: 0.0958\n",
      "Epoch 45/300 - Train Loss: 0.1066, Val Loss: 0.0722\n",
      "Epoch 46/300 - Train Loss: 0.1060, Val Loss: 0.0804\n",
      "Epoch 47/300 - Train Loss: 0.1077, Val Loss: 0.0892\n",
      "Epoch 48/300 - Train Loss: 0.1090, Val Loss: 0.0774\n",
      "Epoch 49/300 - Train Loss: 0.1070, Val Loss: 0.0895\n",
      "Epoch 50/300 - Train Loss: 0.1056, Val Loss: 0.0840\n",
      "Epoch 51/300 - Train Loss: 0.1084, Val Loss: 0.0870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:16:28,678] Trial 181 finished with value: 0.9677906659249415 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.24810113905999653, 'learning_rate': 0.00010966515820117808, 'batch_size': 32, 'weight_decay': 0.008749733672703677}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300 - Train Loss: 0.1081, Val Loss: 0.0910\n",
      "Early stopping at epoch 52\n",
      "Macro F1 Score: 0.9678, Macro Precision: 0.9759, Macro Recall: 0.9602\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.92      0.94        61\n",
      "           2       0.98      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 183\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2552229355213035, LR=0.00013613571220527981, BS=32, WD=0.009690385978264725\n",
      "Epoch 1/300 - Train Loss: 0.2314, Val Loss: 0.1162\n",
      "Epoch 2/300 - Train Loss: 0.1121, Val Loss: 0.0818\n",
      "Epoch 3/300 - Train Loss: 0.1028, Val Loss: 0.0812\n",
      "Epoch 4/300 - Train Loss: 0.0989, Val Loss: 0.0774\n",
      "Epoch 5/300 - Train Loss: 0.0980, Val Loss: 0.0842\n",
      "Epoch 6/300 - Train Loss: 0.0977, Val Loss: 0.0822\n",
      "Epoch 7/300 - Train Loss: 0.1001, Val Loss: 0.0768\n",
      "Epoch 8/300 - Train Loss: 0.1015, Val Loss: 0.0743\n",
      "Epoch 9/300 - Train Loss: 0.1012, Val Loss: 0.0759\n",
      "Epoch 10/300 - Train Loss: 0.1011, Val Loss: 0.0753\n",
      "Epoch 11/300 - Train Loss: 0.1021, Val Loss: 0.0806\n",
      "Epoch 12/300 - Train Loss: 0.1051, Val Loss: 0.0879\n",
      "Epoch 13/300 - Train Loss: 0.1059, Val Loss: 0.0789\n",
      "Epoch 14/300 - Train Loss: 0.1039, Val Loss: 0.0780\n",
      "Epoch 15/300 - Train Loss: 0.1051, Val Loss: 0.0870\n",
      "Epoch 16/300 - Train Loss: 0.1052, Val Loss: 0.0801\n",
      "Epoch 17/300 - Train Loss: 0.1054, Val Loss: 0.0770\n",
      "Epoch 18/300 - Train Loss: 0.1082, Val Loss: 0.0809\n",
      "Epoch 19/300 - Train Loss: 0.1080, Val Loss: 0.1410\n",
      "Epoch 20/300 - Train Loss: 0.1072, Val Loss: 0.0795\n",
      "Epoch 21/300 - Train Loss: 0.1090, Val Loss: 0.0888\n",
      "Epoch 22/300 - Train Loss: 0.1081, Val Loss: 0.0786\n",
      "Epoch 23/300 - Train Loss: 0.1076, Val Loss: 0.0867\n",
      "Epoch 24/300 - Train Loss: 0.1092, Val Loss: 0.0786\n",
      "Epoch 25/300 - Train Loss: 0.1124, Val Loss: 0.0975\n",
      "Epoch 26/300 - Train Loss: 0.1104, Val Loss: 0.1184\n",
      "Epoch 27/300 - Train Loss: 0.1104, Val Loss: 0.0814\n",
      "Epoch 28/300 - Train Loss: 0.1061, Val Loss: 0.0777\n",
      "Epoch 29/300 - Train Loss: 0.1097, Val Loss: 0.0889\n",
      "Epoch 30/300 - Train Loss: 0.1085, Val Loss: 0.0798\n",
      "Epoch 31/300 - Train Loss: 0.1122, Val Loss: 0.0906\n",
      "Epoch 32/300 - Train Loss: 0.1098, Val Loss: 0.0907\n",
      "Epoch 33/300 - Train Loss: 0.1117, Val Loss: 0.0871\n",
      "Epoch 34/300 - Train Loss: 0.1137, Val Loss: 0.1058\n",
      "Epoch 35/300 - Train Loss: 0.1133, Val Loss: 0.0786\n",
      "Epoch 36/300 - Train Loss: 0.1135, Val Loss: 0.0864\n",
      "Epoch 37/300 - Train Loss: 0.1113, Val Loss: 0.0871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:18:54,877] Trial 182 finished with value: 0.9555468656854229 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2552229355213035, 'learning_rate': 0.00013613571220527981, 'batch_size': 32, 'weight_decay': 0.009690385978264725}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300 - Train Loss: 0.1109, Val Loss: 0.0780\n",
      "Early stopping at epoch 38\n",
      "Macro F1 Score: 0.9555, Macro Precision: 0.9442, Macro Recall: 0.9683\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.87      0.95      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.94      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 184\n",
      "Training with F1=16, F2=32, D=8, dropout=0.20756029948192525, LR=9.496853582639667e-05, BS=32, WD=0.006885691397850287\n",
      "Epoch 1/300 - Train Loss: 0.2604, Val Loss: 0.1209\n",
      "Epoch 2/300 - Train Loss: 0.1202, Val Loss: 0.0969\n",
      "Epoch 3/300 - Train Loss: 0.1061, Val Loss: 0.0816\n",
      "Epoch 4/300 - Train Loss: 0.0995, Val Loss: 0.0815\n",
      "Epoch 5/300 - Train Loss: 0.0988, Val Loss: 0.0774\n",
      "Epoch 6/300 - Train Loss: 0.0981, Val Loss: 0.0798\n",
      "Epoch 7/300 - Train Loss: 0.0967, Val Loss: 0.0785\n",
      "Epoch 8/300 - Train Loss: 0.0936, Val Loss: 0.0728\n",
      "Epoch 9/300 - Train Loss: 0.0939, Val Loss: 0.0740\n",
      "Epoch 10/300 - Train Loss: 0.0974, Val Loss: 0.0752\n",
      "Epoch 11/300 - Train Loss: 0.0933, Val Loss: 0.0731\n",
      "Epoch 12/300 - Train Loss: 0.0947, Val Loss: 0.0770\n",
      "Epoch 13/300 - Train Loss: 0.0927, Val Loss: 0.0706\n",
      "Epoch 14/300 - Train Loss: 0.0925, Val Loss: 0.0828\n",
      "Epoch 15/300 - Train Loss: 0.0945, Val Loss: 0.0743\n",
      "Epoch 16/300 - Train Loss: 0.0976, Val Loss: 0.0787\n",
      "Epoch 17/300 - Train Loss: 0.0949, Val Loss: 0.0752\n",
      "Epoch 18/300 - Train Loss: 0.0961, Val Loss: 0.0718\n",
      "Epoch 19/300 - Train Loss: 0.0937, Val Loss: 0.0705\n",
      "Epoch 20/300 - Train Loss: 0.0934, Val Loss: 0.0701\n",
      "Epoch 21/300 - Train Loss: 0.0973, Val Loss: 0.0683\n",
      "Epoch 22/300 - Train Loss: 0.0956, Val Loss: 0.0724\n",
      "Epoch 23/300 - Train Loss: 0.0963, Val Loss: 0.0754\n",
      "Epoch 24/300 - Train Loss: 0.0963, Val Loss: 0.0710\n",
      "Epoch 25/300 - Train Loss: 0.0961, Val Loss: 0.0691\n",
      "Epoch 26/300 - Train Loss: 0.0958, Val Loss: 0.0839\n",
      "Epoch 27/300 - Train Loss: 0.0973, Val Loss: 0.0839\n",
      "Epoch 28/300 - Train Loss: 0.0980, Val Loss: 0.0752\n",
      "Epoch 29/300 - Train Loss: 0.0991, Val Loss: 0.0710\n",
      "Epoch 30/300 - Train Loss: 0.0990, Val Loss: 0.0709\n",
      "Epoch 31/300 - Train Loss: 0.0995, Val Loss: 0.0953\n",
      "Epoch 32/300 - Train Loss: 0.0977, Val Loss: 0.0751\n",
      "Epoch 33/300 - Train Loss: 0.0992, Val Loss: 0.0774\n",
      "Epoch 34/300 - Train Loss: 0.1003, Val Loss: 0.0867\n",
      "Epoch 35/300 - Train Loss: 0.0989, Val Loss: 0.0767\n",
      "Epoch 36/300 - Train Loss: 0.1001, Val Loss: 0.0847\n",
      "Epoch 37/300 - Train Loss: 0.0971, Val Loss: 0.0754\n",
      "Epoch 38/300 - Train Loss: 0.0980, Val Loss: 0.0784\n",
      "Epoch 39/300 - Train Loss: 0.0993, Val Loss: 0.0868\n",
      "Epoch 40/300 - Train Loss: 0.0986, Val Loss: 0.0785\n",
      "Epoch 41/300 - Train Loss: 0.1003, Val Loss: 0.0721\n",
      "Epoch 42/300 - Train Loss: 0.1026, Val Loss: 0.0742\n",
      "Epoch 43/300 - Train Loss: 0.0973, Val Loss: 0.0777\n",
      "Epoch 44/300 - Train Loss: 0.1010, Val Loss: 0.0860\n",
      "Epoch 45/300 - Train Loss: 0.0991, Val Loss: 0.0713\n",
      "Epoch 46/300 - Train Loss: 0.1002, Val Loss: 0.0773\n",
      "Epoch 47/300 - Train Loss: 0.1006, Val Loss: 0.0868\n",
      "Epoch 48/300 - Train Loss: 0.1017, Val Loss: 0.0742\n",
      "Epoch 49/300 - Train Loss: 0.0997, Val Loss: 0.0702\n",
      "Epoch 50/300 - Train Loss: 0.1021, Val Loss: 0.0796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:22:11,005] Trial 183 finished with value: 0.9652858117750244 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.20756029948192525, 'learning_rate': 9.496853582639667e-05, 'batch_size': 32, 'weight_decay': 0.006885691397850287}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300 - Train Loss: 0.0983, Val Loss: 0.0854\n",
      "Early stopping at epoch 51\n",
      "Macro F1 Score: 0.9653, Macro Precision: 0.9669, Macro Recall: 0.9639\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 185\n",
      "Training with F1=32, F2=32, D=8, dropout=0.27330716375478936, LR=0.00012087466106697978, BS=32, WD=6.27056148481422e-05\n",
      "Epoch 1/300 - Train Loss: 0.2141, Val Loss: 0.0855\n",
      "Epoch 2/300 - Train Loss: 0.1037, Val Loss: 0.0729\n",
      "Epoch 3/300 - Train Loss: 0.0945, Val Loss: 0.0762\n",
      "Epoch 4/300 - Train Loss: 0.0905, Val Loss: 0.0748\n",
      "Epoch 5/300 - Train Loss: 0.0872, Val Loss: 0.0820\n",
      "Epoch 6/300 - Train Loss: 0.0857, Val Loss: 0.0688\n",
      "Epoch 7/300 - Train Loss: 0.0834, Val Loss: 0.0706\n",
      "Epoch 8/300 - Train Loss: 0.0828, Val Loss: 0.0703\n",
      "Epoch 9/300 - Train Loss: 0.0802, Val Loss: 0.0675\n",
      "Epoch 10/300 - Train Loss: 0.0820, Val Loss: 0.0741\n",
      "Epoch 11/300 - Train Loss: 0.0784, Val Loss: 0.0662\n",
      "Epoch 12/300 - Train Loss: 0.0768, Val Loss: 0.0660\n",
      "Epoch 13/300 - Train Loss: 0.0758, Val Loss: 0.0729\n",
      "Epoch 14/300 - Train Loss: 0.0748, Val Loss: 0.0680\n",
      "Epoch 15/300 - Train Loss: 0.0754, Val Loss: 0.0640\n",
      "Epoch 16/300 - Train Loss: 0.0740, Val Loss: 0.0677\n",
      "Epoch 17/300 - Train Loss: 0.0735, Val Loss: 0.0763\n",
      "Epoch 18/300 - Train Loss: 0.0711, Val Loss: 0.0650\n",
      "Epoch 19/300 - Train Loss: 0.0697, Val Loss: 0.0685\n",
      "Epoch 20/300 - Train Loss: 0.0719, Val Loss: 0.0703\n",
      "Epoch 21/300 - Train Loss: 0.0706, Val Loss: 0.0712\n",
      "Epoch 22/300 - Train Loss: 0.0674, Val Loss: 0.0733\n",
      "Epoch 23/300 - Train Loss: 0.0694, Val Loss: 0.0702\n",
      "Epoch 24/300 - Train Loss: 0.0684, Val Loss: 0.0743\n",
      "Epoch 25/300 - Train Loss: 0.0697, Val Loss: 0.0714\n",
      "Epoch 26/300 - Train Loss: 0.0663, Val Loss: 0.0678\n",
      "Epoch 27/300 - Train Loss: 0.0655, Val Loss: 0.0708\n",
      "Epoch 28/300 - Train Loss: 0.0656, Val Loss: 0.0698\n",
      "Epoch 29/300 - Train Loss: 0.0659, Val Loss: 0.0681\n",
      "Epoch 30/300 - Train Loss: 0.0634, Val Loss: 0.0726\n",
      "Epoch 31/300 - Train Loss: 0.0655, Val Loss: 0.0725\n",
      "Epoch 32/300 - Train Loss: 0.0653, Val Loss: 0.0728\n",
      "Epoch 33/300 - Train Loss: 0.0643, Val Loss: 0.0731\n",
      "Epoch 34/300 - Train Loss: 0.0631, Val Loss: 0.0702\n",
      "Epoch 35/300 - Train Loss: 0.0604, Val Loss: 0.0702\n",
      "Epoch 36/300 - Train Loss: 0.0632, Val Loss: 0.0758\n",
      "Epoch 37/300 - Train Loss: 0.0633, Val Loss: 0.0681\n",
      "Epoch 38/300 - Train Loss: 0.0603, Val Loss: 0.0743\n",
      "Epoch 39/300 - Train Loss: 0.0607, Val Loss: 0.0690\n",
      "Epoch 40/300 - Train Loss: 0.0591, Val Loss: 0.0698\n",
      "Epoch 41/300 - Train Loss: 0.0586, Val Loss: 0.0795\n",
      "Epoch 42/300 - Train Loss: 0.0594, Val Loss: 0.0727\n",
      "Epoch 43/300 - Train Loss: 0.0583, Val Loss: 0.0683\n",
      "Epoch 44/300 - Train Loss: 0.0588, Val Loss: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:27:03,498] Trial 184 finished with value: 0.973130631461199 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.27330716375478936, 'learning_rate': 0.00012087466106697978, 'batch_size': 32, 'weight_decay': 6.27056148481422e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/300 - Train Loss: 0.0559, Val Loss: 0.0683\n",
      "Early stopping at epoch 45\n",
      "Macro F1 Score: 0.9731, Macro Precision: 0.9786, Macro Recall: 0.9679\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 186\n",
      "Training with F1=32, F2=32, D=8, dropout=0.2826158063608675, LR=6.604164372674511e-05, BS=32, WD=0.002792226789807044\n",
      "Epoch 1/300 - Train Loss: 0.2872, Val Loss: 0.1407\n",
      "Epoch 2/300 - Train Loss: 0.1247, Val Loss: 0.0907\n",
      "Epoch 3/300 - Train Loss: 0.1073, Val Loss: 0.0827\n",
      "Epoch 4/300 - Train Loss: 0.1007, Val Loss: 0.0831\n",
      "Epoch 5/300 - Train Loss: 0.0975, Val Loss: 0.0841\n",
      "Epoch 6/300 - Train Loss: 0.0956, Val Loss: 0.0773\n",
      "Epoch 7/300 - Train Loss: 0.0920, Val Loss: 0.0751\n",
      "Epoch 8/300 - Train Loss: 0.0899, Val Loss: 0.0786\n",
      "Epoch 9/300 - Train Loss: 0.0897, Val Loss: 0.0782\n",
      "Epoch 10/300 - Train Loss: 0.0892, Val Loss: 0.0714\n",
      "Epoch 11/300 - Train Loss: 0.0870, Val Loss: 0.0737\n",
      "Epoch 12/300 - Train Loss: 0.0876, Val Loss: 0.0694\n",
      "Epoch 13/300 - Train Loss: 0.0894, Val Loss: 0.0706\n",
      "Epoch 14/300 - Train Loss: 0.0860, Val Loss: 0.0703\n",
      "Epoch 15/300 - Train Loss: 0.0851, Val Loss: 0.0756\n",
      "Epoch 16/300 - Train Loss: 0.0868, Val Loss: 0.0690\n",
      "Epoch 17/300 - Train Loss: 0.0864, Val Loss: 0.0678\n",
      "Epoch 18/300 - Train Loss: 0.0856, Val Loss: 0.0728\n",
      "Epoch 19/300 - Train Loss: 0.0868, Val Loss: 0.0741\n",
      "Epoch 20/300 - Train Loss: 0.0848, Val Loss: 0.0685\n",
      "Epoch 21/300 - Train Loss: 0.0847, Val Loss: 0.0692\n",
      "Epoch 22/300 - Train Loss: 0.0862, Val Loss: 0.0665\n",
      "Epoch 23/300 - Train Loss: 0.0857, Val Loss: 0.0702\n",
      "Epoch 24/300 - Train Loss: 0.0848, Val Loss: 0.0683\n",
      "Epoch 25/300 - Train Loss: 0.0861, Val Loss: 0.0728\n",
      "Epoch 26/300 - Train Loss: 0.0857, Val Loss: 0.0715\n",
      "Epoch 27/300 - Train Loss: 0.0845, Val Loss: 0.0656\n",
      "Epoch 28/300 - Train Loss: 0.0858, Val Loss: 0.0715\n",
      "Epoch 29/300 - Train Loss: 0.0887, Val Loss: 0.0645\n",
      "Epoch 30/300 - Train Loss: 0.0868, Val Loss: 0.0693\n",
      "Epoch 31/300 - Train Loss: 0.0862, Val Loss: 0.0696\n",
      "Epoch 32/300 - Train Loss: 0.0851, Val Loss: 0.0708\n",
      "Epoch 33/300 - Train Loss: 0.0836, Val Loss: 0.0702\n",
      "Epoch 34/300 - Train Loss: 0.0847, Val Loss: 0.0706\n",
      "Epoch 35/300 - Train Loss: 0.0852, Val Loss: 0.0677\n",
      "Epoch 36/300 - Train Loss: 0.0871, Val Loss: 0.0803\n",
      "Epoch 37/300 - Train Loss: 0.0874, Val Loss: 0.0658\n",
      "Epoch 38/300 - Train Loss: 0.0861, Val Loss: 0.0715\n",
      "Epoch 39/300 - Train Loss: 0.0854, Val Loss: 0.0731\n",
      "Epoch 40/300 - Train Loss: 0.0869, Val Loss: 0.0702\n",
      "Epoch 41/300 - Train Loss: 0.0860, Val Loss: 0.0682\n",
      "Epoch 42/300 - Train Loss: 0.0867, Val Loss: 0.0702\n",
      "Epoch 43/300 - Train Loss: 0.0851, Val Loss: 0.0695\n",
      "Epoch 44/300 - Train Loss: 0.0854, Val Loss: 0.0723\n",
      "Epoch 45/300 - Train Loss: 0.0870, Val Loss: 0.0749\n",
      "Epoch 46/300 - Train Loss: 0.0877, Val Loss: 0.0697\n",
      "Epoch 47/300 - Train Loss: 0.0860, Val Loss: 0.0739\n",
      "Epoch 48/300 - Train Loss: 0.0860, Val Loss: 0.0731\n",
      "Epoch 49/300 - Train Loss: 0.0855, Val Loss: 0.0728\n",
      "Epoch 50/300 - Train Loss: 0.0851, Val Loss: 0.0674\n",
      "Epoch 51/300 - Train Loss: 0.0873, Val Loss: 0.0691\n",
      "Epoch 52/300 - Train Loss: 0.0875, Val Loss: 0.0694\n",
      "Epoch 53/300 - Train Loss: 0.0877, Val Loss: 0.0717\n",
      "Epoch 54/300 - Train Loss: 0.0900, Val Loss: 0.0683\n",
      "Epoch 55/300 - Train Loss: 0.0877, Val Loss: 0.0800\n",
      "Epoch 56/300 - Train Loss: 0.0886, Val Loss: 0.0686\n",
      "Epoch 57/300 - Train Loss: 0.0882, Val Loss: 0.0681\n",
      "Epoch 58/300 - Train Loss: 0.0902, Val Loss: 0.0736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:33:26,882] Trial 185 finished with value: 0.9648698447136711 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.2826158063608675, 'learning_rate': 6.604164372674511e-05, 'batch_size': 32, 'weight_decay': 0.002792226789807044}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300 - Train Loss: 0.0891, Val Loss: 0.0796\n",
      "Early stopping at epoch 59\n",
      "Macro F1 Score: 0.9649, Macro Precision: 0.9552, Macro Recall: 0.9756\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 187\n",
      "Training with F1=32, F2=32, D=8, dropout=0.2724595953203614, LR=0.0001192197635462907, BS=32, WD=5.998327641261585e-05\n",
      "Epoch 1/300 - Train Loss: 0.2248, Val Loss: 0.0924\n",
      "Epoch 2/300 - Train Loss: 0.1104, Val Loss: 0.0848\n",
      "Epoch 3/300 - Train Loss: 0.1003, Val Loss: 0.0831\n",
      "Epoch 4/300 - Train Loss: 0.0967, Val Loss: 0.0771\n",
      "Epoch 5/300 - Train Loss: 0.0915, Val Loss: 0.0763\n",
      "Epoch 6/300 - Train Loss: 0.0892, Val Loss: 0.0732\n",
      "Epoch 7/300 - Train Loss: 0.0848, Val Loss: 0.0728\n",
      "Epoch 8/300 - Train Loss: 0.0850, Val Loss: 0.0730\n",
      "Epoch 9/300 - Train Loss: 0.0849, Val Loss: 0.0702\n",
      "Epoch 10/300 - Train Loss: 0.0806, Val Loss: 0.0672\n",
      "Epoch 11/300 - Train Loss: 0.0802, Val Loss: 0.0685\n",
      "Epoch 12/300 - Train Loss: 0.0785, Val Loss: 0.0663\n",
      "Epoch 13/300 - Train Loss: 0.0769, Val Loss: 0.0670\n",
      "Epoch 14/300 - Train Loss: 0.0750, Val Loss: 0.0751\n",
      "Epoch 15/300 - Train Loss: 0.0744, Val Loss: 0.0683\n",
      "Epoch 16/300 - Train Loss: 0.0748, Val Loss: 0.0709\n",
      "Epoch 17/300 - Train Loss: 0.0758, Val Loss: 0.0684\n",
      "Epoch 18/300 - Train Loss: 0.0741, Val Loss: 0.0805\n",
      "Epoch 19/300 - Train Loss: 0.0718, Val Loss: 0.0692\n",
      "Epoch 20/300 - Train Loss: 0.0716, Val Loss: 0.0695\n",
      "Epoch 21/300 - Train Loss: 0.0718, Val Loss: 0.0659\n",
      "Epoch 22/300 - Train Loss: 0.0692, Val Loss: 0.0666\n",
      "Epoch 23/300 - Train Loss: 0.0679, Val Loss: 0.0667\n",
      "Epoch 24/300 - Train Loss: 0.0685, Val Loss: 0.0681\n",
      "Epoch 25/300 - Train Loss: 0.0667, Val Loss: 0.0694\n",
      "Epoch 26/300 - Train Loss: 0.0681, Val Loss: 0.0718\n",
      "Epoch 27/300 - Train Loss: 0.0679, Val Loss: 0.0680\n",
      "Epoch 28/300 - Train Loss: 0.0675, Val Loss: 0.0702\n",
      "Epoch 29/300 - Train Loss: 0.0649, Val Loss: 0.0644\n",
      "Epoch 30/300 - Train Loss: 0.0650, Val Loss: 0.0691\n",
      "Epoch 31/300 - Train Loss: 0.0641, Val Loss: 0.0782\n",
      "Epoch 32/300 - Train Loss: 0.0638, Val Loss: 0.0644\n",
      "Epoch 33/300 - Train Loss: 0.0615, Val Loss: 0.0634\n",
      "Epoch 34/300 - Train Loss: 0.0638, Val Loss: 0.0650\n",
      "Epoch 35/300 - Train Loss: 0.0617, Val Loss: 0.0664\n",
      "Epoch 36/300 - Train Loss: 0.0620, Val Loss: 0.0657\n",
      "Epoch 37/300 - Train Loss: 0.0624, Val Loss: 0.0690\n",
      "Epoch 38/300 - Train Loss: 0.0632, Val Loss: 0.0781\n",
      "Epoch 39/300 - Train Loss: 0.0614, Val Loss: 0.0652\n",
      "Epoch 40/300 - Train Loss: 0.0601, Val Loss: 0.0648\n",
      "Epoch 41/300 - Train Loss: 0.0589, Val Loss: 0.0673\n",
      "Epoch 42/300 - Train Loss: 0.0595, Val Loss: 0.0653\n",
      "Epoch 43/300 - Train Loss: 0.0582, Val Loss: 0.0661\n",
      "Epoch 44/300 - Train Loss: 0.0574, Val Loss: 0.0658\n",
      "Epoch 45/300 - Train Loss: 0.0604, Val Loss: 0.0668\n",
      "Epoch 46/300 - Train Loss: 0.0565, Val Loss: 0.0680\n",
      "Epoch 47/300 - Train Loss: 0.0573, Val Loss: 0.0694\n",
      "Epoch 48/300 - Train Loss: 0.0576, Val Loss: 0.0707\n",
      "Epoch 49/300 - Train Loss: 0.0552, Val Loss: 0.0720\n",
      "Epoch 50/300 - Train Loss: 0.0544, Val Loss: 0.0726\n",
      "Epoch 51/300 - Train Loss: 0.0549, Val Loss: 0.0630\n",
      "Epoch 52/300 - Train Loss: 0.0550, Val Loss: 0.0732\n",
      "Epoch 53/300 - Train Loss: 0.0529, Val Loss: 0.0706\n",
      "Epoch 54/300 - Train Loss: 0.0548, Val Loss: 0.0656\n",
      "Epoch 55/300 - Train Loss: 0.0550, Val Loss: 0.0736\n",
      "Epoch 56/300 - Train Loss: 0.0552, Val Loss: 0.0696\n",
      "Epoch 57/300 - Train Loss: 0.0538, Val Loss: 0.0752\n",
      "Epoch 58/300 - Train Loss: 0.0529, Val Loss: 0.0847\n",
      "Epoch 59/300 - Train Loss: 0.0508, Val Loss: 0.0726\n",
      "Epoch 60/300 - Train Loss: 0.0515, Val Loss: 0.0719\n",
      "Epoch 61/300 - Train Loss: 0.0501, Val Loss: 0.0686\n",
      "Epoch 62/300 - Train Loss: 0.0537, Val Loss: 0.0712\n",
      "Epoch 63/300 - Train Loss: 0.0527, Val Loss: 0.0702\n",
      "Epoch 64/300 - Train Loss: 0.0513, Val Loss: 0.0652\n",
      "Epoch 65/300 - Train Loss: 0.0514, Val Loss: 0.0695\n",
      "Epoch 66/300 - Train Loss: 0.0489, Val Loss: 0.0673\n",
      "Epoch 67/300 - Train Loss: 0.0513, Val Loss: 0.0659\n",
      "Epoch 68/300 - Train Loss: 0.0520, Val Loss: 0.0671\n",
      "Epoch 69/300 - Train Loss: 0.0477, Val Loss: 0.0668\n",
      "Epoch 70/300 - Train Loss: 0.0491, Val Loss: 0.0746\n",
      "Epoch 71/300 - Train Loss: 0.0516, Val Loss: 0.0690\n",
      "Epoch 72/300 - Train Loss: 0.0495, Val Loss: 0.0701\n",
      "Epoch 73/300 - Train Loss: 0.0476, Val Loss: 0.0671\n",
      "Epoch 74/300 - Train Loss: 0.0476, Val Loss: 0.0660\n",
      "Epoch 75/300 - Train Loss: 0.0480, Val Loss: 0.0662\n",
      "Epoch 76/300 - Train Loss: 0.0474, Val Loss: 0.0657\n",
      "Epoch 77/300 - Train Loss: 0.0483, Val Loss: 0.0637\n",
      "Epoch 78/300 - Train Loss: 0.0487, Val Loss: 0.0735\n",
      "Epoch 79/300 - Train Loss: 0.0454, Val Loss: 0.0742\n",
      "Epoch 80/300 - Train Loss: 0.0465, Val Loss: 0.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:42:13,329] Trial 186 finished with value: 0.9711894946523163 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.2724595953203614, 'learning_rate': 0.0001192197635462907, 'batch_size': 32, 'weight_decay': 5.998327641261585e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300 - Train Loss: 0.0459, Val Loss: 0.0698\n",
      "Early stopping at epoch 81\n",
      "Macro F1 Score: 0.9712, Macro Precision: 0.9610, Macro Recall: 0.9824\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 188\n",
      "Training with F1=32, F2=32, D=8, dropout=0.29552735271570935, LR=0.00014195336209159637, BS=32, WD=3.0912680921417454e-05\n",
      "Epoch 1/300 - Train Loss: 0.1996, Val Loss: 0.0838\n",
      "Epoch 2/300 - Train Loss: 0.1011, Val Loss: 0.0868\n",
      "Epoch 3/300 - Train Loss: 0.0978, Val Loss: 0.0749\n",
      "Epoch 4/300 - Train Loss: 0.0908, Val Loss: 0.0788\n",
      "Epoch 5/300 - Train Loss: 0.0864, Val Loss: 0.0684\n",
      "Epoch 6/300 - Train Loss: 0.0845, Val Loss: 0.0754\n",
      "Epoch 7/300 - Train Loss: 0.0832, Val Loss: 0.0719\n",
      "Epoch 8/300 - Train Loss: 0.0826, Val Loss: 0.0721\n",
      "Epoch 9/300 - Train Loss: 0.0823, Val Loss: 0.0686\n",
      "Epoch 10/300 - Train Loss: 0.0804, Val Loss: 0.0695\n",
      "Epoch 11/300 - Train Loss: 0.0788, Val Loss: 0.0721\n",
      "Epoch 12/300 - Train Loss: 0.0785, Val Loss: 0.0792\n",
      "Epoch 13/300 - Train Loss: 0.0769, Val Loss: 0.0762\n",
      "Epoch 14/300 - Train Loss: 0.0780, Val Loss: 0.0671\n",
      "Epoch 15/300 - Train Loss: 0.0767, Val Loss: 0.0678\n",
      "Epoch 16/300 - Train Loss: 0.0754, Val Loss: 0.0764\n",
      "Epoch 17/300 - Train Loss: 0.0728, Val Loss: 0.0694\n",
      "Epoch 18/300 - Train Loss: 0.0731, Val Loss: 0.0697\n",
      "Epoch 19/300 - Train Loss: 0.0720, Val Loss: 0.0718\n",
      "Epoch 20/300 - Train Loss: 0.0722, Val Loss: 0.0705\n",
      "Epoch 21/300 - Train Loss: 0.0736, Val Loss: 0.0718\n",
      "Epoch 22/300 - Train Loss: 0.0725, Val Loss: 0.0746\n",
      "Epoch 23/300 - Train Loss: 0.0700, Val Loss: 0.0688\n",
      "Epoch 24/300 - Train Loss: 0.0695, Val Loss: 0.0721\n",
      "Epoch 25/300 - Train Loss: 0.0676, Val Loss: 0.0729\n",
      "Epoch 26/300 - Train Loss: 0.0682, Val Loss: 0.0653\n",
      "Epoch 27/300 - Train Loss: 0.0675, Val Loss: 0.0757\n",
      "Epoch 28/300 - Train Loss: 0.0669, Val Loss: 0.0705\n",
      "Epoch 29/300 - Train Loss: 0.0678, Val Loss: 0.0708\n",
      "Epoch 30/300 - Train Loss: 0.0667, Val Loss: 0.0672\n",
      "Epoch 31/300 - Train Loss: 0.0667, Val Loss: 0.0839\n",
      "Epoch 32/300 - Train Loss: 0.0683, Val Loss: 0.0717\n",
      "Epoch 33/300 - Train Loss: 0.0672, Val Loss: 0.0731\n",
      "Epoch 34/300 - Train Loss: 0.0639, Val Loss: 0.0712\n",
      "Epoch 35/300 - Train Loss: 0.0655, Val Loss: 0.0730\n",
      "Epoch 36/300 - Train Loss: 0.0628, Val Loss: 0.0704\n",
      "Epoch 37/300 - Train Loss: 0.0627, Val Loss: 0.0738\n",
      "Epoch 38/300 - Train Loss: 0.0644, Val Loss: 0.0701\n",
      "Epoch 39/300 - Train Loss: 0.0620, Val Loss: 0.0734\n",
      "Epoch 40/300 - Train Loss: 0.0622, Val Loss: 0.0726\n",
      "Epoch 41/300 - Train Loss: 0.0597, Val Loss: 0.0797\n",
      "Epoch 42/300 - Train Loss: 0.0617, Val Loss: 0.0785\n",
      "Epoch 43/300 - Train Loss: 0.0648, Val Loss: 0.0706\n",
      "Epoch 44/300 - Train Loss: 0.0615, Val Loss: 0.0716\n",
      "Epoch 45/300 - Train Loss: 0.0609, Val Loss: 0.0784\n",
      "Epoch 46/300 - Train Loss: 0.0632, Val Loss: 0.0709\n",
      "Epoch 47/300 - Train Loss: 0.0600, Val Loss: 0.0670\n",
      "Epoch 48/300 - Train Loss: 0.0592, Val Loss: 0.0828\n",
      "Epoch 49/300 - Train Loss: 0.0586, Val Loss: 0.0762\n",
      "Epoch 50/300 - Train Loss: 0.0581, Val Loss: 0.0756\n",
      "Epoch 51/300 - Train Loss: 0.0561, Val Loss: 0.0728\n",
      "Epoch 52/300 - Train Loss: 0.0575, Val Loss: 0.0764\n",
      "Epoch 53/300 - Train Loss: 0.0554, Val Loss: 0.0720\n",
      "Epoch 54/300 - Train Loss: 0.0589, Val Loss: 0.0680\n",
      "Epoch 55/300 - Train Loss: 0.0546, Val Loss: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:48:17,179] Trial 187 finished with value: 0.9682018979065137 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.29552735271570935, 'learning_rate': 0.00014195336209159637, 'batch_size': 32, 'weight_decay': 3.0912680921417454e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300 - Train Loss: 0.0564, Val Loss: 0.0845\n",
      "Early stopping at epoch 56\n",
      "Macro F1 Score: 0.9682, Macro Precision: 0.9644, Macro Recall: 0.9722\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 189\n",
      "Training with F1=32, F2=32, D=4, dropout=0.2710032332670451, LR=7.668662166055964e-05, BS=256, WD=2.46429839155309e-05\n",
      "Epoch 1/300 - Train Loss: 0.5887, Val Loss: 0.3170\n",
      "Epoch 2/300 - Train Loss: 0.2522, Val Loss: 0.2219\n",
      "Epoch 3/300 - Train Loss: 0.1944, Val Loss: 0.1726\n",
      "Epoch 4/300 - Train Loss: 0.1626, Val Loss: 0.1500\n",
      "Epoch 5/300 - Train Loss: 0.1394, Val Loss: 0.1224\n",
      "Epoch 6/300 - Train Loss: 0.1229, Val Loss: 0.1143\n",
      "Epoch 7/300 - Train Loss: 0.1152, Val Loss: 0.0993\n",
      "Epoch 8/300 - Train Loss: 0.1035, Val Loss: 0.0934\n",
      "Epoch 9/300 - Train Loss: 0.0988, Val Loss: 0.0847\n",
      "Epoch 10/300 - Train Loss: 0.0957, Val Loss: 0.0844\n",
      "Epoch 11/300 - Train Loss: 0.0949, Val Loss: 0.0824\n",
      "Epoch 12/300 - Train Loss: 0.0904, Val Loss: 0.0785\n",
      "Epoch 13/300 - Train Loss: 0.0882, Val Loss: 0.0815\n",
      "Epoch 14/300 - Train Loss: 0.0852, Val Loss: 0.0808\n",
      "Epoch 15/300 - Train Loss: 0.0838, Val Loss: 0.0850\n",
      "Epoch 16/300 - Train Loss: 0.0841, Val Loss: 0.0813\n",
      "Epoch 17/300 - Train Loss: 0.0818, Val Loss: 0.0766\n",
      "Epoch 18/300 - Train Loss: 0.0813, Val Loss: 0.0763\n",
      "Epoch 19/300 - Train Loss: 0.0810, Val Loss: 0.0743\n",
      "Epoch 20/300 - Train Loss: 0.0785, Val Loss: 0.0771\n",
      "Epoch 21/300 - Train Loss: 0.0770, Val Loss: 0.0755\n",
      "Epoch 22/300 - Train Loss: 0.0780, Val Loss: 0.0759\n",
      "Epoch 23/300 - Train Loss: 0.0761, Val Loss: 0.0801\n",
      "Epoch 24/300 - Train Loss: 0.0765, Val Loss: 0.0716\n",
      "Epoch 25/300 - Train Loss: 0.0754, Val Loss: 0.0705\n",
      "Epoch 26/300 - Train Loss: 0.0744, Val Loss: 0.0732\n",
      "Epoch 27/300 - Train Loss: 0.0738, Val Loss: 0.0760\n",
      "Epoch 28/300 - Train Loss: 0.0734, Val Loss: 0.0742\n",
      "Epoch 29/300 - Train Loss: 0.0730, Val Loss: 0.0714\n",
      "Epoch 30/300 - Train Loss: 0.0736, Val Loss: 0.0690\n",
      "Epoch 31/300 - Train Loss: 0.0719, Val Loss: 0.0720\n",
      "Epoch 32/300 - Train Loss: 0.0722, Val Loss: 0.0736\n",
      "Epoch 33/300 - Train Loss: 0.0714, Val Loss: 0.0712\n",
      "Epoch 34/300 - Train Loss: 0.0703, Val Loss: 0.0716\n",
      "Epoch 35/300 - Train Loss: 0.0705, Val Loss: 0.0714\n",
      "Epoch 36/300 - Train Loss: 0.0705, Val Loss: 0.0717\n",
      "Epoch 37/300 - Train Loss: 0.0695, Val Loss: 0.0712\n",
      "Epoch 38/300 - Train Loss: 0.0699, Val Loss: 0.0717\n",
      "Epoch 39/300 - Train Loss: 0.0685, Val Loss: 0.0698\n",
      "Epoch 40/300 - Train Loss: 0.0696, Val Loss: 0.0714\n",
      "Epoch 41/300 - Train Loss: 0.0696, Val Loss: 0.0733\n",
      "Epoch 42/300 - Train Loss: 0.0682, Val Loss: 0.0703\n",
      "Epoch 43/300 - Train Loss: 0.0685, Val Loss: 0.0698\n",
      "Epoch 44/300 - Train Loss: 0.0679, Val Loss: 0.0733\n",
      "Epoch 45/300 - Train Loss: 0.0674, Val Loss: 0.0768\n",
      "Epoch 46/300 - Train Loss: 0.0670, Val Loss: 0.0702\n",
      "Epoch 47/300 - Train Loss: 0.0654, Val Loss: 0.0690\n",
      "Epoch 48/300 - Train Loss: 0.0655, Val Loss: 0.0680\n",
      "Epoch 49/300 - Train Loss: 0.0675, Val Loss: 0.0690\n",
      "Epoch 50/300 - Train Loss: 0.0657, Val Loss: 0.0677\n",
      "Epoch 51/300 - Train Loss: 0.0637, Val Loss: 0.0713\n",
      "Epoch 52/300 - Train Loss: 0.0644, Val Loss: 0.0678\n",
      "Epoch 53/300 - Train Loss: 0.0656, Val Loss: 0.0698\n",
      "Epoch 54/300 - Train Loss: 0.0653, Val Loss: 0.0742\n",
      "Epoch 55/300 - Train Loss: 0.0641, Val Loss: 0.0693\n",
      "Epoch 56/300 - Train Loss: 0.0639, Val Loss: 0.0722\n",
      "Epoch 57/300 - Train Loss: 0.0645, Val Loss: 0.0766\n",
      "Epoch 58/300 - Train Loss: 0.0643, Val Loss: 0.0677\n",
      "Epoch 59/300 - Train Loss: 0.0635, Val Loss: 0.0704\n",
      "Epoch 60/300 - Train Loss: 0.0652, Val Loss: 0.0722\n",
      "Epoch 61/300 - Train Loss: 0.0633, Val Loss: 0.0686\n",
      "Epoch 62/300 - Train Loss: 0.0636, Val Loss: 0.0687\n",
      "Epoch 63/300 - Train Loss: 0.0629, Val Loss: 0.0708\n",
      "Epoch 64/300 - Train Loss: 0.0627, Val Loss: 0.0674\n",
      "Epoch 65/300 - Train Loss: 0.0612, Val Loss: 0.0681\n",
      "Epoch 66/300 - Train Loss: 0.0631, Val Loss: 0.0690\n",
      "Epoch 67/300 - Train Loss: 0.0635, Val Loss: 0.0699\n",
      "Epoch 68/300 - Train Loss: 0.0611, Val Loss: 0.0689\n",
      "Epoch 69/300 - Train Loss: 0.0615, Val Loss: 0.0697\n",
      "Epoch 70/300 - Train Loss: 0.0622, Val Loss: 0.0797\n",
      "Epoch 71/300 - Train Loss: 0.0614, Val Loss: 0.0744\n",
      "Epoch 72/300 - Train Loss: 0.0606, Val Loss: 0.0706\n",
      "Epoch 73/300 - Train Loss: 0.0604, Val Loss: 0.0707\n",
      "Epoch 74/300 - Train Loss: 0.0611, Val Loss: 0.0660\n",
      "Epoch 75/300 - Train Loss: 0.0607, Val Loss: 0.0715\n",
      "Epoch 76/300 - Train Loss: 0.0598, Val Loss: 0.0676\n",
      "Epoch 77/300 - Train Loss: 0.0601, Val Loss: 0.0684\n",
      "Epoch 78/300 - Train Loss: 0.0580, Val Loss: 0.0707\n",
      "Epoch 79/300 - Train Loss: 0.0607, Val Loss: 0.0697\n",
      "Epoch 80/300 - Train Loss: 0.0590, Val Loss: 0.0686\n",
      "Epoch 81/300 - Train Loss: 0.0600, Val Loss: 0.0685\n",
      "Epoch 82/300 - Train Loss: 0.0578, Val Loss: 0.0693\n",
      "Epoch 83/300 - Train Loss: 0.0588, Val Loss: 0.0689\n",
      "Epoch 84/300 - Train Loss: 0.0595, Val Loss: 0.0670\n",
      "Epoch 85/300 - Train Loss: 0.0571, Val Loss: 0.0692\n",
      "Epoch 86/300 - Train Loss: 0.0573, Val Loss: 0.0685\n",
      "Epoch 87/300 - Train Loss: 0.0590, Val Loss: 0.0716\n",
      "Epoch 88/300 - Train Loss: 0.0572, Val Loss: 0.0713\n",
      "Epoch 89/300 - Train Loss: 0.0566, Val Loss: 0.0688\n",
      "Epoch 90/300 - Train Loss: 0.0579, Val Loss: 0.0712\n",
      "Epoch 91/300 - Train Loss: 0.0574, Val Loss: 0.0717\n",
      "Epoch 92/300 - Train Loss: 0.0565, Val Loss: 0.0681\n",
      "Epoch 93/300 - Train Loss: 0.0567, Val Loss: 0.0666\n",
      "Epoch 94/300 - Train Loss: 0.0560, Val Loss: 0.0685\n",
      "Epoch 95/300 - Train Loss: 0.0558, Val Loss: 0.0712\n",
      "Epoch 96/300 - Train Loss: 0.0561, Val Loss: 0.0698\n",
      "Epoch 97/300 - Train Loss: 0.0574, Val Loss: 0.0683\n",
      "Epoch 98/300 - Train Loss: 0.0561, Val Loss: 0.0715\n",
      "Epoch 99/300 - Train Loss: 0.0577, Val Loss: 0.0670\n",
      "Epoch 100/300 - Train Loss: 0.0545, Val Loss: 0.0681\n",
      "Epoch 101/300 - Train Loss: 0.0553, Val Loss: 0.0699\n",
      "Epoch 102/300 - Train Loss: 0.0558, Val Loss: 0.0693\n",
      "Epoch 103/300 - Train Loss: 0.0541, Val Loss: 0.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:54:05,748] Trial 188 finished with value: 0.9637311556264966 and parameters: {'F1': 32, 'F2': 32, 'D': 4, 'dropout': 0.2710032332670451, 'learning_rate': 7.668662166055964e-05, 'batch_size': 256, 'weight_decay': 2.46429839155309e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/300 - Train Loss: 0.0560, Val Loss: 0.0696\n",
      "Early stopping at epoch 104\n",
      "Macro F1 Score: 0.9637, Macro Precision: 0.9517, Macro Recall: 0.9771\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 190\n",
      "Training with F1=32, F2=32, D=8, dropout=0.37049214539916303, LR=0.0003243292243519421, BS=32, WD=3.713207611439938e-05\n",
      "Epoch 1/300 - Train Loss: 0.1651, Val Loss: 0.0870\n",
      "Epoch 2/300 - Train Loss: 0.0970, Val Loss: 0.0793\n",
      "Epoch 3/300 - Train Loss: 0.0940, Val Loss: 0.0791\n",
      "Epoch 4/300 - Train Loss: 0.0925, Val Loss: 0.0712\n",
      "Epoch 5/300 - Train Loss: 0.0863, Val Loss: 0.0728\n",
      "Epoch 6/300 - Train Loss: 0.0844, Val Loss: 0.0679\n",
      "Epoch 7/300 - Train Loss: 0.0826, Val Loss: 0.0710\n",
      "Epoch 8/300 - Train Loss: 0.0805, Val Loss: 0.0702\n",
      "Epoch 9/300 - Train Loss: 0.0816, Val Loss: 0.0725\n",
      "Epoch 10/300 - Train Loss: 0.0782, Val Loss: 0.0701\n",
      "Epoch 11/300 - Train Loss: 0.0808, Val Loss: 0.0740\n",
      "Epoch 12/300 - Train Loss: 0.0770, Val Loss: 0.0723\n",
      "Epoch 13/300 - Train Loss: 0.0771, Val Loss: 0.0726\n",
      "Epoch 14/300 - Train Loss: 0.0752, Val Loss: 0.0643\n",
      "Epoch 15/300 - Train Loss: 0.0746, Val Loss: 0.0719\n",
      "Epoch 16/300 - Train Loss: 0.0738, Val Loss: 0.0674\n",
      "Epoch 17/300 - Train Loss: 0.0731, Val Loss: 0.0710\n",
      "Epoch 18/300 - Train Loss: 0.0742, Val Loss: 0.0718\n",
      "Epoch 19/300 - Train Loss: 0.0709, Val Loss: 0.0751\n",
      "Epoch 20/300 - Train Loss: 0.0703, Val Loss: 0.0697\n",
      "Epoch 21/300 - Train Loss: 0.0711, Val Loss: 0.0689\n",
      "Epoch 22/300 - Train Loss: 0.0698, Val Loss: 0.0697\n",
      "Epoch 23/300 - Train Loss: 0.0682, Val Loss: 0.0753\n",
      "Epoch 24/300 - Train Loss: 0.0705, Val Loss: 0.0683\n",
      "Epoch 25/300 - Train Loss: 0.0689, Val Loss: 0.0681\n",
      "Epoch 26/300 - Train Loss: 0.0672, Val Loss: 0.0686\n",
      "Epoch 27/300 - Train Loss: 0.0668, Val Loss: 0.0679\n",
      "Epoch 28/300 - Train Loss: 0.0667, Val Loss: 0.0748\n",
      "Epoch 29/300 - Train Loss: 0.0657, Val Loss: 0.0664\n",
      "Epoch 30/300 - Train Loss: 0.0668, Val Loss: 0.0682\n",
      "Epoch 31/300 - Train Loss: 0.0660, Val Loss: 0.0659\n",
      "Epoch 32/300 - Train Loss: 0.0662, Val Loss: 0.0727\n",
      "Epoch 33/300 - Train Loss: 0.0626, Val Loss: 0.0853\n",
      "Epoch 34/300 - Train Loss: 0.0631, Val Loss: 0.0720\n",
      "Epoch 35/300 - Train Loss: 0.0640, Val Loss: 0.0687\n",
      "Epoch 36/300 - Train Loss: 0.0610, Val Loss: 0.0741\n",
      "Epoch 37/300 - Train Loss: 0.0607, Val Loss: 0.0706\n",
      "Epoch 38/300 - Train Loss: 0.0610, Val Loss: 0.0715\n",
      "Epoch 39/300 - Train Loss: 0.0637, Val Loss: 0.0692\n",
      "Epoch 40/300 - Train Loss: 0.0612, Val Loss: 0.0767\n",
      "Epoch 41/300 - Train Loss: 0.0604, Val Loss: 0.0742\n",
      "Epoch 42/300 - Train Loss: 0.0636, Val Loss: 0.0787\n",
      "Epoch 43/300 - Train Loss: 0.0594, Val Loss: 0.0706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 10:58:51,879] Trial 189 finished with value: 0.9646928229791744 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.37049214539916303, 'learning_rate': 0.0003243292243519421, 'batch_size': 32, 'weight_decay': 3.713207611439938e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300 - Train Loss: 0.0612, Val Loss: 0.0725\n",
      "Early stopping at epoch 44\n",
      "Macro F1 Score: 0.9647, Macro Precision: 0.9630, Macro Recall: 0.9665\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.93      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 191\n",
      "Training with F1=8, F2=32, D=8, dropout=0.30602875999461737, LR=0.00016400492431400358, BS=32, WD=0.0001081817476404326\n",
      "Epoch 1/300 - Train Loss: 0.2358, Val Loss: 0.1052\n",
      "Epoch 2/300 - Train Loss: 0.1154, Val Loss: 0.0795\n",
      "Epoch 3/300 - Train Loss: 0.1050, Val Loss: 0.0920\n",
      "Epoch 4/300 - Train Loss: 0.1010, Val Loss: 0.0864\n",
      "Epoch 5/300 - Train Loss: 0.0964, Val Loss: 0.0725\n",
      "Epoch 6/300 - Train Loss: 0.0946, Val Loss: 0.0826\n",
      "Epoch 7/300 - Train Loss: 0.0910, Val Loss: 0.0798\n",
      "Epoch 8/300 - Train Loss: 0.0888, Val Loss: 0.0800\n",
      "Epoch 9/300 - Train Loss: 0.0865, Val Loss: 0.0724\n",
      "Epoch 10/300 - Train Loss: 0.0851, Val Loss: 0.0796\n",
      "Epoch 11/300 - Train Loss: 0.0881, Val Loss: 0.0769\n",
      "Epoch 12/300 - Train Loss: 0.0842, Val Loss: 0.0717\n",
      "Epoch 13/300 - Train Loss: 0.0845, Val Loss: 0.0732\n",
      "Epoch 14/300 - Train Loss: 0.0827, Val Loss: 0.0690\n",
      "Epoch 15/300 - Train Loss: 0.0817, Val Loss: 0.0749\n",
      "Epoch 16/300 - Train Loss: 0.0802, Val Loss: 0.0722\n",
      "Epoch 17/300 - Train Loss: 0.0820, Val Loss: 0.0737\n",
      "Epoch 18/300 - Train Loss: 0.0815, Val Loss: 0.0812\n",
      "Epoch 19/300 - Train Loss: 0.0804, Val Loss: 0.0767\n",
      "Epoch 20/300 - Train Loss: 0.0795, Val Loss: 0.0703\n",
      "Epoch 21/300 - Train Loss: 0.0785, Val Loss: 0.0707\n",
      "Epoch 22/300 - Train Loss: 0.0789, Val Loss: 0.0687\n",
      "Epoch 23/300 - Train Loss: 0.0751, Val Loss: 0.0703\n",
      "Epoch 24/300 - Train Loss: 0.0762, Val Loss: 0.0707\n",
      "Epoch 25/300 - Train Loss: 0.0757, Val Loss: 0.0768\n",
      "Epoch 26/300 - Train Loss: 0.0753, Val Loss: 0.0736\n",
      "Epoch 27/300 - Train Loss: 0.0731, Val Loss: 0.0755\n",
      "Epoch 28/300 - Train Loss: 0.0734, Val Loss: 0.0721\n",
      "Epoch 29/300 - Train Loss: 0.0742, Val Loss: 0.0683\n",
      "Epoch 30/300 - Train Loss: 0.0734, Val Loss: 0.0715\n",
      "Epoch 31/300 - Train Loss: 0.0763, Val Loss: 0.0718\n",
      "Epoch 32/300 - Train Loss: 0.0722, Val Loss: 0.0722\n",
      "Epoch 33/300 - Train Loss: 0.0712, Val Loss: 0.0712\n",
      "Epoch 34/300 - Train Loss: 0.0732, Val Loss: 0.0720\n",
      "Epoch 35/300 - Train Loss: 0.0734, Val Loss: 0.0691\n",
      "Epoch 36/300 - Train Loss: 0.0725, Val Loss: 0.0721\n",
      "Epoch 37/300 - Train Loss: 0.0713, Val Loss: 0.0749\n",
      "Epoch 38/300 - Train Loss: 0.0701, Val Loss: 0.0738\n",
      "Epoch 39/300 - Train Loss: 0.0711, Val Loss: 0.0712\n",
      "Epoch 40/300 - Train Loss: 0.0704, Val Loss: 0.0735\n",
      "Epoch 41/300 - Train Loss: 0.0717, Val Loss: 0.0697\n",
      "Epoch 42/300 - Train Loss: 0.0704, Val Loss: 0.0706\n",
      "Epoch 43/300 - Train Loss: 0.0679, Val Loss: 0.0721\n",
      "Epoch 44/300 - Train Loss: 0.0695, Val Loss: 0.0692\n",
      "Epoch 45/300 - Train Loss: 0.0713, Val Loss: 0.0720\n",
      "Epoch 46/300 - Train Loss: 0.0681, Val Loss: 0.0736\n",
      "Epoch 47/300 - Train Loss: 0.0688, Val Loss: 0.0711\n",
      "Epoch 48/300 - Train Loss: 0.0668, Val Loss: 0.0730\n",
      "Epoch 49/300 - Train Loss: 0.0678, Val Loss: 0.0724\n",
      "Epoch 50/300 - Train Loss: 0.0687, Val Loss: 0.0701\n",
      "Epoch 51/300 - Train Loss: 0.0668, Val Loss: 0.0669\n",
      "Epoch 52/300 - Train Loss: 0.0667, Val Loss: 0.0724\n",
      "Epoch 53/300 - Train Loss: 0.0661, Val Loss: 0.0734\n",
      "Epoch 54/300 - Train Loss: 0.0644, Val Loss: 0.0682\n",
      "Epoch 55/300 - Train Loss: 0.0656, Val Loss: 0.0701\n",
      "Epoch 56/300 - Train Loss: 0.0673, Val Loss: 0.0702\n",
      "Epoch 57/300 - Train Loss: 0.0652, Val Loss: 0.0701\n",
      "Epoch 58/300 - Train Loss: 0.0638, Val Loss: 0.0735\n",
      "Epoch 59/300 - Train Loss: 0.0645, Val Loss: 0.0714\n",
      "Epoch 60/300 - Train Loss: 0.0658, Val Loss: 0.0747\n",
      "Epoch 61/300 - Train Loss: 0.0673, Val Loss: 0.0750\n",
      "Epoch 62/300 - Train Loss: 0.0640, Val Loss: 0.0659\n",
      "Epoch 63/300 - Train Loss: 0.0627, Val Loss: 0.0695\n",
      "Epoch 64/300 - Train Loss: 0.0652, Val Loss: 0.0700\n",
      "Epoch 65/300 - Train Loss: 0.0647, Val Loss: 0.0704\n",
      "Epoch 66/300 - Train Loss: 0.0642, Val Loss: 0.0714\n",
      "Epoch 67/300 - Train Loss: 0.0639, Val Loss: 0.0709\n",
      "Epoch 68/300 - Train Loss: 0.0639, Val Loss: 0.0694\n",
      "Epoch 69/300 - Train Loss: 0.0627, Val Loss: 0.0726\n",
      "Epoch 70/300 - Train Loss: 0.0632, Val Loss: 0.0728\n",
      "Epoch 71/300 - Train Loss: 0.0652, Val Loss: 0.0703\n",
      "Epoch 72/300 - Train Loss: 0.0640, Val Loss: 0.0693\n",
      "Epoch 73/300 - Train Loss: 0.0620, Val Loss: 0.0759\n",
      "Epoch 74/300 - Train Loss: 0.0627, Val Loss: 0.0721\n",
      "Epoch 75/300 - Train Loss: 0.0634, Val Loss: 0.0700\n",
      "Epoch 76/300 - Train Loss: 0.0632, Val Loss: 0.0651\n",
      "Epoch 77/300 - Train Loss: 0.0607, Val Loss: 0.0690\n",
      "Epoch 78/300 - Train Loss: 0.0637, Val Loss: 0.0658\n",
      "Epoch 79/300 - Train Loss: 0.0634, Val Loss: 0.0682\n",
      "Epoch 80/300 - Train Loss: 0.0634, Val Loss: 0.0748\n",
      "Epoch 81/300 - Train Loss: 0.0612, Val Loss: 0.0699\n",
      "Epoch 82/300 - Train Loss: 0.0597, Val Loss: 0.0701\n",
      "Epoch 83/300 - Train Loss: 0.0598, Val Loss: 0.0734\n",
      "Epoch 84/300 - Train Loss: 0.0632, Val Loss: 0.0694\n",
      "Epoch 85/300 - Train Loss: 0.0629, Val Loss: 0.0727\n",
      "Epoch 86/300 - Train Loss: 0.0606, Val Loss: 0.0722\n",
      "Epoch 87/300 - Train Loss: 0.0613, Val Loss: 0.0780\n",
      "Epoch 88/300 - Train Loss: 0.0607, Val Loss: 0.0813\n",
      "Epoch 89/300 - Train Loss: 0.0597, Val Loss: 0.0794\n",
      "Epoch 90/300 - Train Loss: 0.0604, Val Loss: 0.0708\n",
      "Epoch 91/300 - Train Loss: 0.0601, Val Loss: 0.0689\n",
      "Epoch 92/300 - Train Loss: 0.0627, Val Loss: 0.0691\n",
      "Epoch 93/300 - Train Loss: 0.0589, Val Loss: 0.0739\n",
      "Epoch 94/300 - Train Loss: 0.0596, Val Loss: 0.0707\n",
      "Epoch 95/300 - Train Loss: 0.0600, Val Loss: 0.0680\n",
      "Epoch 96/300 - Train Loss: 0.0601, Val Loss: 0.0739\n",
      "Epoch 97/300 - Train Loss: 0.0591, Val Loss: 0.0696\n",
      "Epoch 98/300 - Train Loss: 0.0580, Val Loss: 0.0736\n",
      "Epoch 99/300 - Train Loss: 0.0581, Val Loss: 0.0734\n",
      "Epoch 100/300 - Train Loss: 0.0589, Val Loss: 0.0756\n",
      "Epoch 101/300 - Train Loss: 0.0590, Val Loss: 0.0732\n",
      "Epoch 102/300 - Train Loss: 0.0588, Val Loss: 0.0714\n",
      "Epoch 103/300 - Train Loss: 0.0572, Val Loss: 0.0720\n",
      "Epoch 104/300 - Train Loss: 0.0587, Val Loss: 0.0708\n",
      "Epoch 105/300 - Train Loss: 0.0576, Val Loss: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:03:06,095] Trial 190 finished with value: 0.9716710804448727 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.30602875999461737, 'learning_rate': 0.00016400492431400358, 'batch_size': 32, 'weight_decay': 0.0001081817476404326}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/300 - Train Loss: 0.0583, Val Loss: 0.0703\n",
      "Early stopping at epoch 106\n",
      "Macro F1 Score: 0.9717, Macro Precision: 0.9680, Macro Recall: 0.9756\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 192\n",
      "Training with F1=8, F2=32, D=8, dropout=0.3087286548863716, LR=0.00014465426991339434, BS=32, WD=0.00010931290764368952\n",
      "Epoch 1/300 - Train Loss: 0.2771, Val Loss: 0.1545\n",
      "Epoch 2/300 - Train Loss: 0.1328, Val Loss: 0.0808\n",
      "Epoch 3/300 - Train Loss: 0.1041, Val Loss: 0.0841\n",
      "Epoch 4/300 - Train Loss: 0.1032, Val Loss: 0.0789\n",
      "Epoch 5/300 - Train Loss: 0.0974, Val Loss: 0.0743\n",
      "Epoch 6/300 - Train Loss: 0.0967, Val Loss: 0.0861\n",
      "Epoch 7/300 - Train Loss: 0.0961, Val Loss: 0.0743\n",
      "Epoch 8/300 - Train Loss: 0.0892, Val Loss: 0.0803\n",
      "Epoch 9/300 - Train Loss: 0.0906, Val Loss: 0.0776\n",
      "Epoch 10/300 - Train Loss: 0.0903, Val Loss: 0.0806\n",
      "Epoch 11/300 - Train Loss: 0.0886, Val Loss: 0.0811\n",
      "Epoch 12/300 - Train Loss: 0.0855, Val Loss: 0.0701\n",
      "Epoch 13/300 - Train Loss: 0.0866, Val Loss: 0.0700\n",
      "Epoch 14/300 - Train Loss: 0.0862, Val Loss: 0.0756\n",
      "Epoch 15/300 - Train Loss: 0.0840, Val Loss: 0.0755\n",
      "Epoch 16/300 - Train Loss: 0.0856, Val Loss: 0.0722\n",
      "Epoch 17/300 - Train Loss: 0.0823, Val Loss: 0.0700\n",
      "Epoch 18/300 - Train Loss: 0.0834, Val Loss: 0.0719\n",
      "Epoch 19/300 - Train Loss: 0.0793, Val Loss: 0.0778\n",
      "Epoch 20/300 - Train Loss: 0.0817, Val Loss: 0.0727\n",
      "Epoch 21/300 - Train Loss: 0.0791, Val Loss: 0.0771\n",
      "Epoch 22/300 - Train Loss: 0.0793, Val Loss: 0.0748\n",
      "Epoch 23/300 - Train Loss: 0.0790, Val Loss: 0.0771\n",
      "Epoch 24/300 - Train Loss: 0.0783, Val Loss: 0.0720\n",
      "Epoch 25/300 - Train Loss: 0.0760, Val Loss: 0.0685\n",
      "Epoch 26/300 - Train Loss: 0.0763, Val Loss: 0.0718\n",
      "Epoch 27/300 - Train Loss: 0.0780, Val Loss: 0.0714\n",
      "Epoch 28/300 - Train Loss: 0.0769, Val Loss: 0.0710\n",
      "Epoch 29/300 - Train Loss: 0.0757, Val Loss: 0.0712\n",
      "Epoch 30/300 - Train Loss: 0.0750, Val Loss: 0.0717\n",
      "Epoch 31/300 - Train Loss: 0.0751, Val Loss: 0.0660\n",
      "Epoch 32/300 - Train Loss: 0.0766, Val Loss: 0.0693\n",
      "Epoch 33/300 - Train Loss: 0.0737, Val Loss: 0.0747\n",
      "Epoch 34/300 - Train Loss: 0.0754, Val Loss: 0.0700\n",
      "Epoch 35/300 - Train Loss: 0.0719, Val Loss: 0.0714\n",
      "Epoch 36/300 - Train Loss: 0.0752, Val Loss: 0.0679\n",
      "Epoch 37/300 - Train Loss: 0.0734, Val Loss: 0.0680\n",
      "Epoch 38/300 - Train Loss: 0.0704, Val Loss: 0.0697\n",
      "Epoch 39/300 - Train Loss: 0.0724, Val Loss: 0.0684\n",
      "Epoch 40/300 - Train Loss: 0.0693, Val Loss: 0.0708\n",
      "Epoch 41/300 - Train Loss: 0.0724, Val Loss: 0.0699\n",
      "Epoch 42/300 - Train Loss: 0.0718, Val Loss: 0.0708\n",
      "Epoch 43/300 - Train Loss: 0.0687, Val Loss: 0.0731\n",
      "Epoch 44/300 - Train Loss: 0.0698, Val Loss: 0.0735\n",
      "Epoch 45/300 - Train Loss: 0.0689, Val Loss: 0.0674\n",
      "Epoch 46/300 - Train Loss: 0.0707, Val Loss: 0.0690\n",
      "Epoch 47/300 - Train Loss: 0.0694, Val Loss: 0.0677\n",
      "Epoch 48/300 - Train Loss: 0.0682, Val Loss: 0.0714\n",
      "Epoch 49/300 - Train Loss: 0.0722, Val Loss: 0.0657\n",
      "Epoch 50/300 - Train Loss: 0.0663, Val Loss: 0.0653\n",
      "Epoch 51/300 - Train Loss: 0.0691, Val Loss: 0.0672\n",
      "Epoch 52/300 - Train Loss: 0.0696, Val Loss: 0.0711\n",
      "Epoch 53/300 - Train Loss: 0.0662, Val Loss: 0.0698\n",
      "Epoch 54/300 - Train Loss: 0.0658, Val Loss: 0.0686\n",
      "Epoch 55/300 - Train Loss: 0.0675, Val Loss: 0.0734\n",
      "Epoch 56/300 - Train Loss: 0.0657, Val Loss: 0.0666\n",
      "Epoch 57/300 - Train Loss: 0.0680, Val Loss: 0.0713\n",
      "Epoch 58/300 - Train Loss: 0.0698, Val Loss: 0.0681\n",
      "Epoch 59/300 - Train Loss: 0.0670, Val Loss: 0.0668\n",
      "Epoch 60/300 - Train Loss: 0.0661, Val Loss: 0.0701\n",
      "Epoch 61/300 - Train Loss: 0.0665, Val Loss: 0.0696\n",
      "Epoch 62/300 - Train Loss: 0.0675, Val Loss: 0.0694\n",
      "Epoch 63/300 - Train Loss: 0.0675, Val Loss: 0.0681\n",
      "Epoch 64/300 - Train Loss: 0.0673, Val Loss: 0.0722\n",
      "Epoch 65/300 - Train Loss: 0.0626, Val Loss: 0.0737\n",
      "Epoch 66/300 - Train Loss: 0.0630, Val Loss: 0.0710\n",
      "Epoch 67/300 - Train Loss: 0.0635, Val Loss: 0.0690\n",
      "Epoch 68/300 - Train Loss: 0.0658, Val Loss: 0.0756\n",
      "Epoch 69/300 - Train Loss: 0.0650, Val Loss: 0.0784\n",
      "Epoch 70/300 - Train Loss: 0.0627, Val Loss: 0.0681\n",
      "Epoch 71/300 - Train Loss: 0.0641, Val Loss: 0.0743\n",
      "Epoch 72/300 - Train Loss: 0.0653, Val Loss: 0.0707\n",
      "Epoch 73/300 - Train Loss: 0.0659, Val Loss: 0.0722\n",
      "Epoch 74/300 - Train Loss: 0.0638, Val Loss: 0.0742\n",
      "Epoch 75/300 - Train Loss: 0.0627, Val Loss: 0.0742\n",
      "Epoch 76/300 - Train Loss: 0.0631, Val Loss: 0.0689\n",
      "Epoch 77/300 - Train Loss: 0.0619, Val Loss: 0.0688\n",
      "Epoch 78/300 - Train Loss: 0.0631, Val Loss: 0.0690\n",
      "Epoch 79/300 - Train Loss: 0.0634, Val Loss: 0.0671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:06:18,028] Trial 191 finished with value: 0.9649491254760144 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.3087286548863716, 'learning_rate': 0.00014465426991339434, 'batch_size': 32, 'weight_decay': 0.00010931290764368952}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300 - Train Loss: 0.0604, Val Loss: 0.0669\n",
      "Early stopping at epoch 80\n",
      "Macro F1 Score: 0.9649, Macro Precision: 0.9551, Macro Recall: 0.9759\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 193\n",
      "Training with F1=8, F2=32, D=8, dropout=0.33622948671648417, LR=0.0001658735605887675, BS=32, WD=8.197512746952087e-05\n",
      "Epoch 1/300 - Train Loss: 0.2388, Val Loss: 0.1115\n",
      "Epoch 2/300 - Train Loss: 0.1146, Val Loss: 0.0891\n",
      "Epoch 3/300 - Train Loss: 0.1036, Val Loss: 0.0862\n",
      "Epoch 4/300 - Train Loss: 0.0989, Val Loss: 0.0773\n",
      "Epoch 5/300 - Train Loss: 0.0985, Val Loss: 0.0830\n",
      "Epoch 6/300 - Train Loss: 0.0945, Val Loss: 0.0759\n",
      "Epoch 7/300 - Train Loss: 0.0903, Val Loss: 0.0743\n",
      "Epoch 8/300 - Train Loss: 0.0916, Val Loss: 0.0788\n",
      "Epoch 9/300 - Train Loss: 0.0899, Val Loss: 0.0740\n",
      "Epoch 10/300 - Train Loss: 0.0880, Val Loss: 0.0708\n",
      "Epoch 11/300 - Train Loss: 0.0860, Val Loss: 0.0673\n",
      "Epoch 12/300 - Train Loss: 0.0864, Val Loss: 0.0704\n",
      "Epoch 13/300 - Train Loss: 0.0866, Val Loss: 0.0763\n",
      "Epoch 14/300 - Train Loss: 0.0858, Val Loss: 0.0758\n",
      "Epoch 15/300 - Train Loss: 0.0840, Val Loss: 0.0712\n",
      "Epoch 16/300 - Train Loss: 0.0828, Val Loss: 0.0764\n",
      "Epoch 17/300 - Train Loss: 0.0834, Val Loss: 0.0713\n",
      "Epoch 18/300 - Train Loss: 0.0836, Val Loss: 0.0688\n",
      "Epoch 19/300 - Train Loss: 0.0809, Val Loss: 0.0729\n",
      "Epoch 20/300 - Train Loss: 0.0816, Val Loss: 0.0699\n",
      "Epoch 21/300 - Train Loss: 0.0797, Val Loss: 0.0687\n",
      "Epoch 22/300 - Train Loss: 0.0802, Val Loss: 0.0700\n",
      "Epoch 23/300 - Train Loss: 0.0794, Val Loss: 0.0673\n",
      "Epoch 24/300 - Train Loss: 0.0799, Val Loss: 0.0726\n",
      "Epoch 25/300 - Train Loss: 0.0785, Val Loss: 0.0695\n",
      "Epoch 26/300 - Train Loss: 0.0777, Val Loss: 0.0698\n",
      "Epoch 27/300 - Train Loss: 0.0775, Val Loss: 0.0708\n",
      "Epoch 28/300 - Train Loss: 0.0759, Val Loss: 0.0710\n",
      "Epoch 29/300 - Train Loss: 0.0757, Val Loss: 0.0723\n",
      "Epoch 30/300 - Train Loss: 0.0752, Val Loss: 0.0708\n",
      "Epoch 31/300 - Train Loss: 0.0748, Val Loss: 0.0767\n",
      "Epoch 32/300 - Train Loss: 0.0742, Val Loss: 0.0725\n",
      "Epoch 33/300 - Train Loss: 0.0738, Val Loss: 0.0769\n",
      "Epoch 34/300 - Train Loss: 0.0730, Val Loss: 0.0698\n",
      "Epoch 35/300 - Train Loss: 0.0744, Val Loss: 0.0717\n",
      "Epoch 36/300 - Train Loss: 0.0732, Val Loss: 0.0708\n",
      "Epoch 37/300 - Train Loss: 0.0717, Val Loss: 0.0703\n",
      "Epoch 38/300 - Train Loss: 0.0734, Val Loss: 0.0737\n",
      "Epoch 39/300 - Train Loss: 0.0732, Val Loss: 0.0718\n",
      "Epoch 40/300 - Train Loss: 0.0711, Val Loss: 0.0691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:07:56,510] Trial 192 finished with value: 0.9690431288330642 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.33622948671648417, 'learning_rate': 0.0001658735605887675, 'batch_size': 32, 'weight_decay': 8.197512746952087e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/300 - Train Loss: 0.0736, Val Loss: 0.0710\n",
      "Early stopping at epoch 41\n",
      "Macro F1 Score: 0.9690, Macro Precision: 0.9677, Macro Recall: 0.9705\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 194\n",
      "Training with F1=8, F2=32, D=8, dropout=0.2884534239688411, LR=0.00013074973214292077, BS=32, WD=0.00021899901480450888\n",
      "Epoch 1/300 - Train Loss: 0.2823, Val Loss: 0.1243\n",
      "Epoch 2/300 - Train Loss: 0.1331, Val Loss: 0.0966\n",
      "Epoch 3/300 - Train Loss: 0.1108, Val Loss: 0.0826\n",
      "Epoch 4/300 - Train Loss: 0.1037, Val Loss: 0.0879\n",
      "Epoch 5/300 - Train Loss: 0.0999, Val Loss: 0.0788\n",
      "Epoch 6/300 - Train Loss: 0.0949, Val Loss: 0.0803\n",
      "Epoch 7/300 - Train Loss: 0.0918, Val Loss: 0.0758\n",
      "Epoch 8/300 - Train Loss: 0.0932, Val Loss: 0.0825\n",
      "Epoch 9/300 - Train Loss: 0.0906, Val Loss: 0.0754\n",
      "Epoch 10/300 - Train Loss: 0.0889, Val Loss: 0.0828\n",
      "Epoch 11/300 - Train Loss: 0.0912, Val Loss: 0.0723\n",
      "Epoch 12/300 - Train Loss: 0.0862, Val Loss: 0.0768\n",
      "Epoch 13/300 - Train Loss: 0.0839, Val Loss: 0.0724\n",
      "Epoch 14/300 - Train Loss: 0.0849, Val Loss: 0.0772\n",
      "Epoch 15/300 - Train Loss: 0.0831, Val Loss: 0.0717\n",
      "Epoch 16/300 - Train Loss: 0.0827, Val Loss: 0.0755\n",
      "Epoch 17/300 - Train Loss: 0.0806, Val Loss: 0.0731\n",
      "Epoch 18/300 - Train Loss: 0.0812, Val Loss: 0.0744\n",
      "Epoch 19/300 - Train Loss: 0.0808, Val Loss: 0.0739\n",
      "Epoch 20/300 - Train Loss: 0.0803, Val Loss: 0.0762\n",
      "Epoch 21/300 - Train Loss: 0.0798, Val Loss: 0.0719\n",
      "Epoch 22/300 - Train Loss: 0.0777, Val Loss: 0.0695\n",
      "Epoch 23/300 - Train Loss: 0.0798, Val Loss: 0.0742\n",
      "Epoch 24/300 - Train Loss: 0.0776, Val Loss: 0.0740\n",
      "Epoch 25/300 - Train Loss: 0.0791, Val Loss: 0.0746\n",
      "Epoch 26/300 - Train Loss: 0.0801, Val Loss: 0.0703\n",
      "Epoch 27/300 - Train Loss: 0.0765, Val Loss: 0.0695\n",
      "Epoch 28/300 - Train Loss: 0.0766, Val Loss: 0.0717\n",
      "Epoch 29/300 - Train Loss: 0.0770, Val Loss: 0.0697\n",
      "Epoch 30/300 - Train Loss: 0.0748, Val Loss: 0.0761\n",
      "Epoch 31/300 - Train Loss: 0.0754, Val Loss: 0.0695\n",
      "Epoch 32/300 - Train Loss: 0.0742, Val Loss: 0.0772\n",
      "Epoch 33/300 - Train Loss: 0.0763, Val Loss: 0.0762\n",
      "Epoch 34/300 - Train Loss: 0.0730, Val Loss: 0.0720\n",
      "Epoch 35/300 - Train Loss: 0.0744, Val Loss: 0.0771\n",
      "Epoch 36/300 - Train Loss: 0.0748, Val Loss: 0.0702\n",
      "Epoch 37/300 - Train Loss: 0.0703, Val Loss: 0.0675\n",
      "Epoch 38/300 - Train Loss: 0.0720, Val Loss: 0.0685\n",
      "Epoch 39/300 - Train Loss: 0.0731, Val Loss: 0.0664\n",
      "Epoch 40/300 - Train Loss: 0.0732, Val Loss: 0.0691\n",
      "Epoch 41/300 - Train Loss: 0.0726, Val Loss: 0.0684\n",
      "Epoch 42/300 - Train Loss: 0.0710, Val Loss: 0.0729\n",
      "Epoch 43/300 - Train Loss: 0.0723, Val Loss: 0.0741\n",
      "Epoch 44/300 - Train Loss: 0.0717, Val Loss: 0.0677\n",
      "Epoch 45/300 - Train Loss: 0.0725, Val Loss: 0.0731\n",
      "Epoch 46/300 - Train Loss: 0.0738, Val Loss: 0.0722\n",
      "Epoch 47/300 - Train Loss: 0.0724, Val Loss: 0.0761\n",
      "Epoch 48/300 - Train Loss: 0.0734, Val Loss: 0.0765\n",
      "Epoch 49/300 - Train Loss: 0.0706, Val Loss: 0.0753\n",
      "Epoch 50/300 - Train Loss: 0.0686, Val Loss: 0.0738\n",
      "Epoch 51/300 - Train Loss: 0.0686, Val Loss: 0.0762\n",
      "Epoch 52/300 - Train Loss: 0.0672, Val Loss: 0.0697\n",
      "Epoch 53/300 - Train Loss: 0.0692, Val Loss: 0.0692\n",
      "Epoch 54/300 - Train Loss: 0.0670, Val Loss: 0.0731\n",
      "Epoch 55/300 - Train Loss: 0.0685, Val Loss: 0.0739\n",
      "Epoch 56/300 - Train Loss: 0.0684, Val Loss: 0.0673\n",
      "Epoch 57/300 - Train Loss: 0.0691, Val Loss: 0.0735\n",
      "Epoch 58/300 - Train Loss: 0.0679, Val Loss: 0.0654\n",
      "Epoch 59/300 - Train Loss: 0.0678, Val Loss: 0.0701\n",
      "Epoch 60/300 - Train Loss: 0.0683, Val Loss: 0.0702\n",
      "Epoch 61/300 - Train Loss: 0.0656, Val Loss: 0.0723\n",
      "Epoch 62/300 - Train Loss: 0.0654, Val Loss: 0.0679\n",
      "Epoch 63/300 - Train Loss: 0.0679, Val Loss: 0.0721\n",
      "Epoch 64/300 - Train Loss: 0.0659, Val Loss: 0.0665\n",
      "Epoch 65/300 - Train Loss: 0.0647, Val Loss: 0.0690\n",
      "Epoch 66/300 - Train Loss: 0.0700, Val Loss: 0.0706\n",
      "Epoch 67/300 - Train Loss: 0.0650, Val Loss: 0.0705\n",
      "Epoch 68/300 - Train Loss: 0.0663, Val Loss: 0.0729\n",
      "Epoch 69/300 - Train Loss: 0.0655, Val Loss: 0.0726\n",
      "Epoch 70/300 - Train Loss: 0.0639, Val Loss: 0.0764\n",
      "Epoch 71/300 - Train Loss: 0.0667, Val Loss: 0.0702\n",
      "Epoch 72/300 - Train Loss: 0.0638, Val Loss: 0.0709\n",
      "Epoch 73/300 - Train Loss: 0.0635, Val Loss: 0.0736\n",
      "Epoch 74/300 - Train Loss: 0.0657, Val Loss: 0.0681\n",
      "Epoch 75/300 - Train Loss: 0.0638, Val Loss: 0.0682\n",
      "Epoch 76/300 - Train Loss: 0.0646, Val Loss: 0.0647\n",
      "Epoch 77/300 - Train Loss: 0.0631, Val Loss: 0.0712\n",
      "Epoch 78/300 - Train Loss: 0.0656, Val Loss: 0.0676\n",
      "Epoch 79/300 - Train Loss: 0.0621, Val Loss: 0.0719\n",
      "Epoch 80/300 - Train Loss: 0.0627, Val Loss: 0.0728\n",
      "Epoch 81/300 - Train Loss: 0.0626, Val Loss: 0.0719\n",
      "Epoch 82/300 - Train Loss: 0.0620, Val Loss: 0.0740\n",
      "Epoch 83/300 - Train Loss: 0.0657, Val Loss: 0.0678\n",
      "Epoch 84/300 - Train Loss: 0.0624, Val Loss: 0.0680\n",
      "Epoch 85/300 - Train Loss: 0.0642, Val Loss: 0.0715\n",
      "Epoch 86/300 - Train Loss: 0.0631, Val Loss: 0.0766\n",
      "Epoch 87/300 - Train Loss: 0.0615, Val Loss: 0.0664\n",
      "Epoch 88/300 - Train Loss: 0.0640, Val Loss: 0.0708\n",
      "Epoch 89/300 - Train Loss: 0.0657, Val Loss: 0.0705\n",
      "Epoch 90/300 - Train Loss: 0.0651, Val Loss: 0.0688\n",
      "Epoch 91/300 - Train Loss: 0.0645, Val Loss: 0.0746\n",
      "Epoch 92/300 - Train Loss: 0.0628, Val Loss: 0.0689\n",
      "Epoch 93/300 - Train Loss: 0.0596, Val Loss: 0.0724\n",
      "Epoch 94/300 - Train Loss: 0.0631, Val Loss: 0.0717\n",
      "Epoch 95/300 - Train Loss: 0.0663, Val Loss: 0.0702\n",
      "Epoch 96/300 - Train Loss: 0.0629, Val Loss: 0.0667\n",
      "Epoch 97/300 - Train Loss: 0.0641, Val Loss: 0.0701\n",
      "Epoch 98/300 - Train Loss: 0.0621, Val Loss: 0.0716\n",
      "Epoch 99/300 - Train Loss: 0.0587, Val Loss: 0.0690\n",
      "Epoch 100/300 - Train Loss: 0.0614, Val Loss: 0.0725\n",
      "Epoch 101/300 - Train Loss: 0.0609, Val Loss: 0.0710\n",
      "Epoch 102/300 - Train Loss: 0.0622, Val Loss: 0.0686\n",
      "Epoch 103/300 - Train Loss: 0.0620, Val Loss: 0.0716\n",
      "Epoch 104/300 - Train Loss: 0.0628, Val Loss: 0.0720\n",
      "Epoch 105/300 - Train Loss: 0.0610, Val Loss: 0.0675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:12:10,773] Trial 193 finished with value: 0.9649554857435264 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.2884534239688411, 'learning_rate': 0.00013074973214292077, 'batch_size': 32, 'weight_decay': 0.00021899901480450888}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/300 - Train Loss: 0.0611, Val Loss: 0.0698\n",
      "Early stopping at epoch 106\n",
      "Macro F1 Score: 0.9650, Macro Precision: 0.9548, Macro Recall: 0.9761\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 195\n",
      "Training with F1=8, F2=32, D=8, dropout=0.26239398508841655, LR=0.00016127053757130128, BS=32, WD=6.126333017469187e-05\n",
      "Epoch 1/300 - Train Loss: 0.2503, Val Loss: 0.1133\n",
      "Epoch 2/300 - Train Loss: 0.1173, Val Loss: 0.0843\n",
      "Epoch 3/300 - Train Loss: 0.1043, Val Loss: 0.0838\n",
      "Epoch 4/300 - Train Loss: 0.1009, Val Loss: 0.0859\n",
      "Epoch 5/300 - Train Loss: 0.0981, Val Loss: 0.0798\n",
      "Epoch 6/300 - Train Loss: 0.0942, Val Loss: 0.0799\n",
      "Epoch 7/300 - Train Loss: 0.0921, Val Loss: 0.0751\n",
      "Epoch 8/300 - Train Loss: 0.0922, Val Loss: 0.0732\n",
      "Epoch 9/300 - Train Loss: 0.0903, Val Loss: 0.0778\n",
      "Epoch 10/300 - Train Loss: 0.0915, Val Loss: 0.0694\n",
      "Epoch 11/300 - Train Loss: 0.0862, Val Loss: 0.0825\n",
      "Epoch 12/300 - Train Loss: 0.0875, Val Loss: 0.0740\n",
      "Epoch 13/300 - Train Loss: 0.0846, Val Loss: 0.0719\n",
      "Epoch 14/300 - Train Loss: 0.0833, Val Loss: 0.0696\n",
      "Epoch 15/300 - Train Loss: 0.0826, Val Loss: 0.0768\n",
      "Epoch 16/300 - Train Loss: 0.0819, Val Loss: 0.0683\n",
      "Epoch 17/300 - Train Loss: 0.0797, Val Loss: 0.0756\n",
      "Epoch 18/300 - Train Loss: 0.0817, Val Loss: 0.0756\n",
      "Epoch 19/300 - Train Loss: 0.0790, Val Loss: 0.0772\n",
      "Epoch 20/300 - Train Loss: 0.0792, Val Loss: 0.0735\n",
      "Epoch 21/300 - Train Loss: 0.0771, Val Loss: 0.0705\n",
      "Epoch 22/300 - Train Loss: 0.0765, Val Loss: 0.0746\n",
      "Epoch 23/300 - Train Loss: 0.0777, Val Loss: 0.0727\n",
      "Epoch 24/300 - Train Loss: 0.0756, Val Loss: 0.0724\n",
      "Epoch 25/300 - Train Loss: 0.0725, Val Loss: 0.0683\n",
      "Epoch 26/300 - Train Loss: 0.0758, Val Loss: 0.0684\n",
      "Epoch 27/300 - Train Loss: 0.0749, Val Loss: 0.0653\n",
      "Epoch 28/300 - Train Loss: 0.0724, Val Loss: 0.0670\n",
      "Epoch 29/300 - Train Loss: 0.0740, Val Loss: 0.0707\n",
      "Epoch 30/300 - Train Loss: 0.0724, Val Loss: 0.0695\n",
      "Epoch 31/300 - Train Loss: 0.0737, Val Loss: 0.0685\n",
      "Epoch 32/300 - Train Loss: 0.0714, Val Loss: 0.0690\n",
      "Epoch 33/300 - Train Loss: 0.0727, Val Loss: 0.0688\n",
      "Epoch 34/300 - Train Loss: 0.0704, Val Loss: 0.0683\n",
      "Epoch 35/300 - Train Loss: 0.0684, Val Loss: 0.0686\n",
      "Epoch 36/300 - Train Loss: 0.0687, Val Loss: 0.0647\n",
      "Epoch 37/300 - Train Loss: 0.0691, Val Loss: 0.0680\n",
      "Epoch 38/300 - Train Loss: 0.0692, Val Loss: 0.0730\n",
      "Epoch 39/300 - Train Loss: 0.0684, Val Loss: 0.0703\n",
      "Epoch 40/300 - Train Loss: 0.0692, Val Loss: 0.0717\n",
      "Epoch 41/300 - Train Loss: 0.0679, Val Loss: 0.0668\n",
      "Epoch 42/300 - Train Loss: 0.0667, Val Loss: 0.0698\n",
      "Epoch 43/300 - Train Loss: 0.0677, Val Loss: 0.0675\n",
      "Epoch 44/300 - Train Loss: 0.0660, Val Loss: 0.0702\n",
      "Epoch 45/300 - Train Loss: 0.0667, Val Loss: 0.0723\n",
      "Epoch 46/300 - Train Loss: 0.0672, Val Loss: 0.0692\n",
      "Epoch 47/300 - Train Loss: 0.0666, Val Loss: 0.0668\n",
      "Epoch 48/300 - Train Loss: 0.0654, Val Loss: 0.0698\n",
      "Epoch 49/300 - Train Loss: 0.0658, Val Loss: 0.0706\n",
      "Epoch 50/300 - Train Loss: 0.0659, Val Loss: 0.0687\n",
      "Epoch 51/300 - Train Loss: 0.0637, Val Loss: 0.0709\n",
      "Epoch 52/300 - Train Loss: 0.0635, Val Loss: 0.0653\n",
      "Epoch 53/300 - Train Loss: 0.0649, Val Loss: 0.0679\n",
      "Epoch 54/300 - Train Loss: 0.0652, Val Loss: 0.0699\n",
      "Epoch 55/300 - Train Loss: 0.0630, Val Loss: 0.0656\n",
      "Epoch 56/300 - Train Loss: 0.0661, Val Loss: 0.0798\n",
      "Epoch 57/300 - Train Loss: 0.0609, Val Loss: 0.0665\n",
      "Epoch 58/300 - Train Loss: 0.0632, Val Loss: 0.0682\n",
      "Epoch 59/300 - Train Loss: 0.0624, Val Loss: 0.0773\n",
      "Epoch 60/300 - Train Loss: 0.0614, Val Loss: 0.0641\n",
      "Epoch 61/300 - Train Loss: 0.0615, Val Loss: 0.0664\n",
      "Epoch 62/300 - Train Loss: 0.0594, Val Loss: 0.0730\n",
      "Epoch 63/300 - Train Loss: 0.0640, Val Loss: 0.0664\n",
      "Epoch 64/300 - Train Loss: 0.0605, Val Loss: 0.0639\n",
      "Epoch 65/300 - Train Loss: 0.0632, Val Loss: 0.0705\n",
      "Epoch 66/300 - Train Loss: 0.0626, Val Loss: 0.0667\n",
      "Epoch 67/300 - Train Loss: 0.0607, Val Loss: 0.0746\n",
      "Epoch 68/300 - Train Loss: 0.0590, Val Loss: 0.0719\n",
      "Epoch 69/300 - Train Loss: 0.0600, Val Loss: 0.0679\n",
      "Epoch 70/300 - Train Loss: 0.0598, Val Loss: 0.0763\n",
      "Epoch 71/300 - Train Loss: 0.0609, Val Loss: 0.0705\n",
      "Epoch 72/300 - Train Loss: 0.0576, Val Loss: 0.0725\n",
      "Epoch 73/300 - Train Loss: 0.0584, Val Loss: 0.0724\n",
      "Epoch 74/300 - Train Loss: 0.0611, Val Loss: 0.0687\n",
      "Epoch 75/300 - Train Loss: 0.0569, Val Loss: 0.0691\n",
      "Epoch 76/300 - Train Loss: 0.0574, Val Loss: 0.0678\n",
      "Epoch 77/300 - Train Loss: 0.0594, Val Loss: 0.0708\n",
      "Epoch 78/300 - Train Loss: 0.0572, Val Loss: 0.0731\n",
      "Epoch 79/300 - Train Loss: 0.0573, Val Loss: 0.0692\n",
      "Epoch 80/300 - Train Loss: 0.0589, Val Loss: 0.0685\n",
      "Epoch 81/300 - Train Loss: 0.0571, Val Loss: 0.0696\n",
      "Epoch 82/300 - Train Loss: 0.0548, Val Loss: 0.0711\n",
      "Epoch 83/300 - Train Loss: 0.0564, Val Loss: 0.0687\n",
      "Epoch 84/300 - Train Loss: 0.0583, Val Loss: 0.0674\n",
      "Epoch 85/300 - Train Loss: 0.0573, Val Loss: 0.0656\n",
      "Epoch 86/300 - Train Loss: 0.0577, Val Loss: 0.0749\n",
      "Epoch 87/300 - Train Loss: 0.0569, Val Loss: 0.0751\n",
      "Epoch 88/300 - Train Loss: 0.0567, Val Loss: 0.0686\n",
      "Epoch 89/300 - Train Loss: 0.0550, Val Loss: 0.0714\n",
      "Epoch 90/300 - Train Loss: 0.0528, Val Loss: 0.0689\n",
      "Epoch 91/300 - Train Loss: 0.0534, Val Loss: 0.0668\n",
      "Epoch 92/300 - Train Loss: 0.0580, Val Loss: 0.0776\n",
      "Epoch 93/300 - Train Loss: 0.0537, Val Loss: 0.0699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:15:56,368] Trial 194 finished with value: 0.9677102862817596 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.26239398508841655, 'learning_rate': 0.00016127053757130128, 'batch_size': 32, 'weight_decay': 6.126333017469187e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300 - Train Loss: 0.0540, Val Loss: 0.0700\n",
      "Early stopping at epoch 94\n",
      "Macro F1 Score: 0.9677, Macro Precision: 0.9638, Macro Recall: 0.9718\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 196\n",
      "Training with F1=8, F2=32, D=8, dropout=0.31999787635367394, LR=0.0001874452943510854, BS=32, WD=0.0001229819433030945\n",
      "Epoch 1/300 - Train Loss: 0.2539, Val Loss: 0.1012\n",
      "Epoch 2/300 - Train Loss: 0.1207, Val Loss: 0.0913\n",
      "Epoch 3/300 - Train Loss: 0.1077, Val Loss: 0.0815\n",
      "Epoch 4/300 - Train Loss: 0.0996, Val Loss: 0.0787\n",
      "Epoch 5/300 - Train Loss: 0.1017, Val Loss: 0.0742\n",
      "Epoch 6/300 - Train Loss: 0.0939, Val Loss: 0.0740\n",
      "Epoch 7/300 - Train Loss: 0.0890, Val Loss: 0.0809\n",
      "Epoch 8/300 - Train Loss: 0.0874, Val Loss: 0.0725\n",
      "Epoch 9/300 - Train Loss: 0.0900, Val Loss: 0.0740\n",
      "Epoch 10/300 - Train Loss: 0.0863, Val Loss: 0.0713\n",
      "Epoch 11/300 - Train Loss: 0.0870, Val Loss: 0.0715\n",
      "Epoch 12/300 - Train Loss: 0.0828, Val Loss: 0.0765\n",
      "Epoch 13/300 - Train Loss: 0.0831, Val Loss: 0.0757\n",
      "Epoch 14/300 - Train Loss: 0.0815, Val Loss: 0.0737\n",
      "Epoch 15/300 - Train Loss: 0.0830, Val Loss: 0.0755\n",
      "Epoch 16/300 - Train Loss: 0.0822, Val Loss: 0.0733\n",
      "Epoch 17/300 - Train Loss: 0.0791, Val Loss: 0.0727\n",
      "Epoch 18/300 - Train Loss: 0.0825, Val Loss: 0.0750\n",
      "Epoch 19/300 - Train Loss: 0.0802, Val Loss: 0.0762\n",
      "Epoch 20/300 - Train Loss: 0.0804, Val Loss: 0.0736\n",
      "Epoch 21/300 - Train Loss: 0.0791, Val Loss: 0.0751\n",
      "Epoch 22/300 - Train Loss: 0.0804, Val Loss: 0.0787\n",
      "Epoch 23/300 - Train Loss: 0.0769, Val Loss: 0.0724\n",
      "Epoch 24/300 - Train Loss: 0.0786, Val Loss: 0.0724\n",
      "Epoch 25/300 - Train Loss: 0.0765, Val Loss: 0.0705\n",
      "Epoch 26/300 - Train Loss: 0.0760, Val Loss: 0.0731\n",
      "Epoch 27/300 - Train Loss: 0.0759, Val Loss: 0.0742\n",
      "Epoch 28/300 - Train Loss: 0.0766, Val Loss: 0.0717\n",
      "Epoch 29/300 - Train Loss: 0.0731, Val Loss: 0.0738\n",
      "Epoch 30/300 - Train Loss: 0.0734, Val Loss: 0.0753\n",
      "Epoch 31/300 - Train Loss: 0.0732, Val Loss: 0.0760\n",
      "Epoch 32/300 - Train Loss: 0.0725, Val Loss: 0.0750\n",
      "Epoch 33/300 - Train Loss: 0.0716, Val Loss: 0.0744\n",
      "Epoch 34/300 - Train Loss: 0.0708, Val Loss: 0.0731\n",
      "Epoch 35/300 - Train Loss: 0.0725, Val Loss: 0.0755\n",
      "Epoch 36/300 - Train Loss: 0.0713, Val Loss: 0.0725\n",
      "Epoch 37/300 - Train Loss: 0.0716, Val Loss: 0.0692\n",
      "Epoch 38/300 - Train Loss: 0.0720, Val Loss: 0.0701\n",
      "Epoch 39/300 - Train Loss: 0.0698, Val Loss: 0.0814\n",
      "Epoch 40/300 - Train Loss: 0.0702, Val Loss: 0.0746\n",
      "Epoch 41/300 - Train Loss: 0.0681, Val Loss: 0.0717\n",
      "Epoch 42/300 - Train Loss: 0.0695, Val Loss: 0.0715\n",
      "Epoch 43/300 - Train Loss: 0.0709, Val Loss: 0.0739\n",
      "Epoch 44/300 - Train Loss: 0.0704, Val Loss: 0.0693\n",
      "Epoch 45/300 - Train Loss: 0.0702, Val Loss: 0.0801\n",
      "Epoch 46/300 - Train Loss: 0.0686, Val Loss: 0.0751\n",
      "Epoch 47/300 - Train Loss: 0.0677, Val Loss: 0.0728\n",
      "Epoch 48/300 - Train Loss: 0.0682, Val Loss: 0.0740\n",
      "Epoch 49/300 - Train Loss: 0.0657, Val Loss: 0.0728\n",
      "Epoch 50/300 - Train Loss: 0.0673, Val Loss: 0.0732\n",
      "Epoch 51/300 - Train Loss: 0.0657, Val Loss: 0.0792\n",
      "Epoch 52/300 - Train Loss: 0.0675, Val Loss: 0.0732\n",
      "Epoch 53/300 - Train Loss: 0.0688, Val Loss: 0.0816\n",
      "Epoch 54/300 - Train Loss: 0.0669, Val Loss: 0.0740\n",
      "Epoch 55/300 - Train Loss: 0.0665, Val Loss: 0.0707\n",
      "Epoch 56/300 - Train Loss: 0.0670, Val Loss: 0.0788\n",
      "Epoch 57/300 - Train Loss: 0.0679, Val Loss: 0.0756\n",
      "Epoch 58/300 - Train Loss: 0.0680, Val Loss: 0.0735\n",
      "Epoch 59/300 - Train Loss: 0.0705, Val Loss: 0.0763\n",
      "Epoch 60/300 - Train Loss: 0.0649, Val Loss: 0.0783\n",
      "Epoch 61/300 - Train Loss: 0.0646, Val Loss: 0.0801\n",
      "Epoch 62/300 - Train Loss: 0.0679, Val Loss: 0.0786\n",
      "Epoch 63/300 - Train Loss: 0.0680, Val Loss: 0.0791\n",
      "Epoch 64/300 - Train Loss: 0.0647, Val Loss: 0.0852\n",
      "Epoch 65/300 - Train Loss: 0.0662, Val Loss: 0.0829\n",
      "Epoch 66/300 - Train Loss: 0.0644, Val Loss: 0.0783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:18:37,243] Trial 195 finished with value: 0.9670535325867898 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.31999787635367394, 'learning_rate': 0.0001874452943510854, 'batch_size': 32, 'weight_decay': 0.0001229819433030945}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/300 - Train Loss: 0.0642, Val Loss: 0.0747\n",
      "Early stopping at epoch 67\n",
      "Macro F1 Score: 0.9671, Macro Precision: 0.9591, Macro Recall: 0.9757\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 197\n",
      "Training with F1=4, F2=32, D=8, dropout=0.21654882976136677, LR=8.746781446563459e-05, BS=32, WD=0.00015130468301576924\n",
      "Epoch 1/300 - Train Loss: 0.3637, Val Loss: 0.1567\n",
      "Epoch 2/300 - Train Loss: 0.1625, Val Loss: 0.1091\n",
      "Epoch 3/300 - Train Loss: 0.1299, Val Loss: 0.1029\n",
      "Epoch 4/300 - Train Loss: 0.1134, Val Loss: 0.0862\n",
      "Epoch 5/300 - Train Loss: 0.1063, Val Loss: 0.0782\n",
      "Epoch 6/300 - Train Loss: 0.1023, Val Loss: 0.0780\n",
      "Epoch 7/300 - Train Loss: 0.0995, Val Loss: 0.0906\n",
      "Epoch 8/300 - Train Loss: 0.0970, Val Loss: 0.0788\n",
      "Epoch 9/300 - Train Loss: 0.0952, Val Loss: 0.0792\n",
      "Epoch 10/300 - Train Loss: 0.0938, Val Loss: 0.0812\n",
      "Epoch 11/300 - Train Loss: 0.0958, Val Loss: 0.0794\n",
      "Epoch 12/300 - Train Loss: 0.0949, Val Loss: 0.0799\n",
      "Epoch 13/300 - Train Loss: 0.0919, Val Loss: 0.0782\n",
      "Epoch 14/300 - Train Loss: 0.0897, Val Loss: 0.0818\n",
      "Epoch 15/300 - Train Loss: 0.0873, Val Loss: 0.0793\n",
      "Epoch 16/300 - Train Loss: 0.0902, Val Loss: 0.0780\n",
      "Epoch 17/300 - Train Loss: 0.0893, Val Loss: 0.0733\n",
      "Epoch 18/300 - Train Loss: 0.0897, Val Loss: 0.0755\n",
      "Epoch 19/300 - Train Loss: 0.0866, Val Loss: 0.0770\n",
      "Epoch 20/300 - Train Loss: 0.0874, Val Loss: 0.0770\n",
      "Epoch 21/300 - Train Loss: 0.0859, Val Loss: 0.0770\n",
      "Epoch 22/300 - Train Loss: 0.0842, Val Loss: 0.0749\n",
      "Epoch 23/300 - Train Loss: 0.0853, Val Loss: 0.0744\n",
      "Epoch 24/300 - Train Loss: 0.0878, Val Loss: 0.0722\n",
      "Epoch 25/300 - Train Loss: 0.0853, Val Loss: 0.0754\n",
      "Epoch 26/300 - Train Loss: 0.0838, Val Loss: 0.0767\n",
      "Epoch 27/300 - Train Loss: 0.0833, Val Loss: 0.0756\n",
      "Epoch 28/300 - Train Loss: 0.0809, Val Loss: 0.0762\n",
      "Epoch 29/300 - Train Loss: 0.0830, Val Loss: 0.0707\n",
      "Epoch 30/300 - Train Loss: 0.0833, Val Loss: 0.0798\n",
      "Epoch 31/300 - Train Loss: 0.0832, Val Loss: 0.0782\n",
      "Epoch 32/300 - Train Loss: 0.0813, Val Loss: 0.0745\n",
      "Epoch 33/300 - Train Loss: 0.0820, Val Loss: 0.0720\n",
      "Epoch 34/300 - Train Loss: 0.0807, Val Loss: 0.0750\n",
      "Epoch 35/300 - Train Loss: 0.0792, Val Loss: 0.0786\n",
      "Epoch 36/300 - Train Loss: 0.0830, Val Loss: 0.0785\n",
      "Epoch 37/300 - Train Loss: 0.0798, Val Loss: 0.0794\n",
      "Epoch 38/300 - Train Loss: 0.0780, Val Loss: 0.0784\n",
      "Epoch 39/300 - Train Loss: 0.0785, Val Loss: 0.0785\n",
      "Epoch 40/300 - Train Loss: 0.0789, Val Loss: 0.0826\n",
      "Epoch 41/300 - Train Loss: 0.0783, Val Loss: 0.0730\n",
      "Epoch 42/300 - Train Loss: 0.0784, Val Loss: 0.0720\n",
      "Epoch 43/300 - Train Loss: 0.0793, Val Loss: 0.0736\n",
      "Epoch 44/300 - Train Loss: 0.0772, Val Loss: 0.0758\n",
      "Epoch 45/300 - Train Loss: 0.0789, Val Loss: 0.0725\n",
      "Epoch 46/300 - Train Loss: 0.0765, Val Loss: 0.0729\n",
      "Epoch 47/300 - Train Loss: 0.0781, Val Loss: 0.0774\n",
      "Epoch 48/300 - Train Loss: 0.0760, Val Loss: 0.0767\n",
      "Epoch 49/300 - Train Loss: 0.0764, Val Loss: 0.0731\n",
      "Epoch 50/300 - Train Loss: 0.0806, Val Loss: 0.0716\n",
      "Epoch 51/300 - Train Loss: 0.0764, Val Loss: 0.0721\n",
      "Epoch 52/300 - Train Loss: 0.0751, Val Loss: 0.0750\n",
      "Epoch 53/300 - Train Loss: 0.0752, Val Loss: 0.0739\n",
      "Epoch 54/300 - Train Loss: 0.0728, Val Loss: 0.0754\n",
      "Epoch 55/300 - Train Loss: 0.0740, Val Loss: 0.0767\n",
      "Epoch 56/300 - Train Loss: 0.0715, Val Loss: 0.0778\n",
      "Epoch 57/300 - Train Loss: 0.0742, Val Loss: 0.0755\n",
      "Epoch 58/300 - Train Loss: 0.0769, Val Loss: 0.0808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:20:42,521] Trial 196 finished with value: 0.960167697623974 and parameters: {'F1': 4, 'F2': 32, 'D': 8, 'dropout': 0.21654882976136677, 'learning_rate': 8.746781446563459e-05, 'batch_size': 32, 'weight_decay': 0.00015130468301576924}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300 - Train Loss: 0.0733, Val Loss: 0.0749\n",
      "Early stopping at epoch 59\n",
      "Macro F1 Score: 0.9602, Macro Precision: 0.9541, Macro Recall: 0.9666\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.89      0.93      0.91        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 198\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2978102840030361, LR=7.094352852523759e-05, BS=32, WD=4.981953521661191e-05\n",
      "Epoch 1/300 - Train Loss: 0.2962, Val Loss: 0.1273\n",
      "Epoch 2/300 - Train Loss: 0.1234, Val Loss: 0.0947\n",
      "Epoch 3/300 - Train Loss: 0.1049, Val Loss: 0.0792\n",
      "Epoch 4/300 - Train Loss: 0.0977, Val Loss: 0.0848\n",
      "Epoch 5/300 - Train Loss: 0.0963, Val Loss: 0.0842\n",
      "Epoch 6/300 - Train Loss: 0.0915, Val Loss: 0.0736\n",
      "Epoch 7/300 - Train Loss: 0.0899, Val Loss: 0.0705\n",
      "Epoch 8/300 - Train Loss: 0.0872, Val Loss: 0.0714\n",
      "Epoch 9/300 - Train Loss: 0.0862, Val Loss: 0.0826\n",
      "Epoch 10/300 - Train Loss: 0.0875, Val Loss: 0.0697\n",
      "Epoch 11/300 - Train Loss: 0.0831, Val Loss: 0.0684\n",
      "Epoch 12/300 - Train Loss: 0.0818, Val Loss: 0.0715\n",
      "Epoch 13/300 - Train Loss: 0.0817, Val Loss: 0.0713\n",
      "Epoch 14/300 - Train Loss: 0.0814, Val Loss: 0.0737\n",
      "Epoch 15/300 - Train Loss: 0.0818, Val Loss: 0.0688\n",
      "Epoch 16/300 - Train Loss: 0.0811, Val Loss: 0.0699\n",
      "Epoch 17/300 - Train Loss: 0.0791, Val Loss: 0.0736\n",
      "Epoch 18/300 - Train Loss: 0.0786, Val Loss: 0.0771\n",
      "Epoch 19/300 - Train Loss: 0.0792, Val Loss: 0.0698\n",
      "Epoch 20/300 - Train Loss: 0.0769, Val Loss: 0.0715\n",
      "Epoch 21/300 - Train Loss: 0.0776, Val Loss: 0.0730\n",
      "Epoch 22/300 - Train Loss: 0.0770, Val Loss: 0.0713\n",
      "Epoch 23/300 - Train Loss: 0.0779, Val Loss: 0.0751\n",
      "Epoch 24/300 - Train Loss: 0.0753, Val Loss: 0.0717\n",
      "Epoch 25/300 - Train Loss: 0.0752, Val Loss: 0.0705\n",
      "Epoch 26/300 - Train Loss: 0.0760, Val Loss: 0.0668\n",
      "Epoch 27/300 - Train Loss: 0.0734, Val Loss: 0.0656\n",
      "Epoch 28/300 - Train Loss: 0.0714, Val Loss: 0.0800\n",
      "Epoch 29/300 - Train Loss: 0.0717, Val Loss: 0.0656\n",
      "Epoch 30/300 - Train Loss: 0.0731, Val Loss: 0.0660\n",
      "Epoch 31/300 - Train Loss: 0.0723, Val Loss: 0.0666\n",
      "Epoch 32/300 - Train Loss: 0.0712, Val Loss: 0.0728\n",
      "Epoch 33/300 - Train Loss: 0.0696, Val Loss: 0.0684\n",
      "Epoch 34/300 - Train Loss: 0.0702, Val Loss: 0.0704\n",
      "Epoch 35/300 - Train Loss: 0.0689, Val Loss: 0.0723\n",
      "Epoch 36/300 - Train Loss: 0.0704, Val Loss: 0.0688\n",
      "Epoch 37/300 - Train Loss: 0.0704, Val Loss: 0.0650\n",
      "Epoch 38/300 - Train Loss: 0.0684, Val Loss: 0.0663\n",
      "Epoch 39/300 - Train Loss: 0.0699, Val Loss: 0.0711\n",
      "Epoch 40/300 - Train Loss: 0.0681, Val Loss: 0.0662\n",
      "Epoch 41/300 - Train Loss: 0.0676, Val Loss: 0.0664\n",
      "Epoch 42/300 - Train Loss: 0.0687, Val Loss: 0.0683\n",
      "Epoch 43/300 - Train Loss: 0.0665, Val Loss: 0.0664\n",
      "Epoch 44/300 - Train Loss: 0.0675, Val Loss: 0.0660\n",
      "Epoch 45/300 - Train Loss: 0.0663, Val Loss: 0.0668\n",
      "Epoch 46/300 - Train Loss: 0.0683, Val Loss: 0.0722\n",
      "Epoch 47/300 - Train Loss: 0.0642, Val Loss: 0.0680\n",
      "Epoch 48/300 - Train Loss: 0.0650, Val Loss: 0.0664\n",
      "Epoch 49/300 - Train Loss: 0.0648, Val Loss: 0.0662\n",
      "Epoch 50/300 - Train Loss: 0.0665, Val Loss: 0.0687\n",
      "Epoch 51/300 - Train Loss: 0.0642, Val Loss: 0.0677\n",
      "Epoch 52/300 - Train Loss: 0.0665, Val Loss: 0.0667\n",
      "Epoch 53/300 - Train Loss: 0.0633, Val Loss: 0.0668\n",
      "Epoch 54/300 - Train Loss: 0.0613, Val Loss: 0.0696\n",
      "Epoch 55/300 - Train Loss: 0.0625, Val Loss: 0.0679\n",
      "Epoch 56/300 - Train Loss: 0.0671, Val Loss: 0.0703\n",
      "Epoch 57/300 - Train Loss: 0.0636, Val Loss: 0.0696\n",
      "Epoch 58/300 - Train Loss: 0.0631, Val Loss: 0.0686\n",
      "Epoch 59/300 - Train Loss: 0.0628, Val Loss: 0.0699\n",
      "Epoch 60/300 - Train Loss: 0.0612, Val Loss: 0.0634\n",
      "Epoch 61/300 - Train Loss: 0.0630, Val Loss: 0.0678\n",
      "Epoch 62/300 - Train Loss: 0.0642, Val Loss: 0.0695\n",
      "Epoch 63/300 - Train Loss: 0.0605, Val Loss: 0.0666\n",
      "Epoch 64/300 - Train Loss: 0.0606, Val Loss: 0.0642\n",
      "Epoch 65/300 - Train Loss: 0.0622, Val Loss: 0.0653\n",
      "Epoch 66/300 - Train Loss: 0.0621, Val Loss: 0.0672\n",
      "Epoch 67/300 - Train Loss: 0.0598, Val Loss: 0.0667\n",
      "Epoch 68/300 - Train Loss: 0.0597, Val Loss: 0.0677\n",
      "Epoch 69/300 - Train Loss: 0.0632, Val Loss: 0.0648\n",
      "Epoch 70/300 - Train Loss: 0.0605, Val Loss: 0.0679\n",
      "Epoch 71/300 - Train Loss: 0.0601, Val Loss: 0.0736\n",
      "Epoch 72/300 - Train Loss: 0.0597, Val Loss: 0.0630\n",
      "Epoch 73/300 - Train Loss: 0.0576, Val Loss: 0.0656\n",
      "Epoch 74/300 - Train Loss: 0.0595, Val Loss: 0.0693\n",
      "Epoch 75/300 - Train Loss: 0.0581, Val Loss: 0.0707\n",
      "Epoch 76/300 - Train Loss: 0.0591, Val Loss: 0.0642\n",
      "Epoch 77/300 - Train Loss: 0.0604, Val Loss: 0.0695\n",
      "Epoch 78/300 - Train Loss: 0.0599, Val Loss: 0.0697\n",
      "Epoch 79/300 - Train Loss: 0.0580, Val Loss: 0.0707\n",
      "Epoch 80/300 - Train Loss: 0.0580, Val Loss: 0.0676\n",
      "Epoch 81/300 - Train Loss: 0.0565, Val Loss: 0.0727\n",
      "Epoch 82/300 - Train Loss: 0.0588, Val Loss: 0.0682\n",
      "Epoch 83/300 - Train Loss: 0.0571, Val Loss: 0.0666\n",
      "Epoch 84/300 - Train Loss: 0.0588, Val Loss: 0.0651\n",
      "Epoch 85/300 - Train Loss: 0.0569, Val Loss: 0.0689\n",
      "Epoch 86/300 - Train Loss: 0.0575, Val Loss: 0.0738\n",
      "Epoch 87/300 - Train Loss: 0.0596, Val Loss: 0.0663\n",
      "Epoch 88/300 - Train Loss: 0.0570, Val Loss: 0.0695\n",
      "Epoch 89/300 - Train Loss: 0.0560, Val Loss: 0.0651\n",
      "Epoch 90/300 - Train Loss: 0.0577, Val Loss: 0.0655\n",
      "Epoch 91/300 - Train Loss: 0.0548, Val Loss: 0.0706\n",
      "Epoch 92/300 - Train Loss: 0.0551, Val Loss: 0.0679\n",
      "Epoch 93/300 - Train Loss: 0.0582, Val Loss: 0.0686\n",
      "Epoch 94/300 - Train Loss: 0.0545, Val Loss: 0.0718\n",
      "Epoch 95/300 - Train Loss: 0.0559, Val Loss: 0.0644\n",
      "Epoch 96/300 - Train Loss: 0.0573, Val Loss: 0.0691\n",
      "Epoch 97/300 - Train Loss: 0.0595, Val Loss: 0.0666\n",
      "Epoch 98/300 - Train Loss: 0.0550, Val Loss: 0.0689\n",
      "Epoch 99/300 - Train Loss: 0.0548, Val Loss: 0.0678\n",
      "Epoch 100/300 - Train Loss: 0.0555, Val Loss: 0.0694\n",
      "Epoch 101/300 - Train Loss: 0.0541, Val Loss: 0.0660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:27:14,807] Trial 197 finished with value: 0.9744693591169878 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2978102840030361, 'learning_rate': 7.094352852523759e-05, 'batch_size': 32, 'weight_decay': 4.981953521661191e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102/300 - Train Loss: 0.0536, Val Loss: 0.0681\n",
      "Early stopping at epoch 102\n",
      "Macro F1 Score: 0.9745, Macro Precision: 0.9662, Macro Recall: 0.9834\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.98      0.95        61\n",
      "           2       0.99      0.98      0.99       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 199\n",
      "Training with F1=16, F2=32, D=4, dropout=0.3056828250545342, LR=0.00010378255443069706, BS=256, WD=8.92845992213382e-05\n",
      "Epoch 1/300 - Train Loss: 0.6485, Val Loss: 0.3304\n",
      "Epoch 2/300 - Train Loss: 0.2624, Val Loss: 0.2073\n",
      "Epoch 3/300 - Train Loss: 0.1935, Val Loss: 0.1603\n",
      "Epoch 4/300 - Train Loss: 0.1564, Val Loss: 0.1364\n",
      "Epoch 5/300 - Train Loss: 0.1364, Val Loss: 0.1167\n",
      "Epoch 6/300 - Train Loss: 0.1205, Val Loss: 0.1077\n",
      "Epoch 7/300 - Train Loss: 0.1105, Val Loss: 0.1036\n",
      "Epoch 8/300 - Train Loss: 0.1057, Val Loss: 0.0945\n",
      "Epoch 9/300 - Train Loss: 0.0996, Val Loss: 0.0924\n",
      "Epoch 10/300 - Train Loss: 0.0965, Val Loss: 0.0881\n",
      "Epoch 11/300 - Train Loss: 0.0940, Val Loss: 0.0863\n",
      "Epoch 12/300 - Train Loss: 0.0931, Val Loss: 0.0853\n",
      "Epoch 13/300 - Train Loss: 0.0915, Val Loss: 0.0796\n",
      "Epoch 14/300 - Train Loss: 0.0898, Val Loss: 0.0821\n",
      "Epoch 15/300 - Train Loss: 0.0893, Val Loss: 0.0876\n",
      "Epoch 16/300 - Train Loss: 0.0882, Val Loss: 0.0817\n",
      "Epoch 17/300 - Train Loss: 0.0884, Val Loss: 0.0772\n",
      "Epoch 18/300 - Train Loss: 0.0857, Val Loss: 0.0778\n",
      "Epoch 19/300 - Train Loss: 0.0866, Val Loss: 0.0822\n",
      "Epoch 20/300 - Train Loss: 0.0854, Val Loss: 0.0825\n",
      "Epoch 21/300 - Train Loss: 0.0843, Val Loss: 0.0768\n",
      "Epoch 22/300 - Train Loss: 0.0834, Val Loss: 0.0798\n",
      "Epoch 23/300 - Train Loss: 0.0819, Val Loss: 0.0759\n",
      "Epoch 24/300 - Train Loss: 0.0813, Val Loss: 0.0765\n",
      "Epoch 25/300 - Train Loss: 0.0810, Val Loss: 0.0746\n",
      "Epoch 26/300 - Train Loss: 0.0815, Val Loss: 0.0776\n",
      "Epoch 27/300 - Train Loss: 0.0804, Val Loss: 0.0804\n",
      "Epoch 28/300 - Train Loss: 0.0810, Val Loss: 0.0762\n",
      "Epoch 29/300 - Train Loss: 0.0790, Val Loss: 0.0768\n",
      "Epoch 30/300 - Train Loss: 0.0776, Val Loss: 0.0785\n",
      "Epoch 31/300 - Train Loss: 0.0785, Val Loss: 0.0773\n",
      "Epoch 32/300 - Train Loss: 0.0772, Val Loss: 0.0737\n",
      "Epoch 33/300 - Train Loss: 0.0774, Val Loss: 0.0805\n",
      "Epoch 34/300 - Train Loss: 0.0768, Val Loss: 0.0755\n",
      "Epoch 35/300 - Train Loss: 0.0766, Val Loss: 0.0755\n",
      "Epoch 36/300 - Train Loss: 0.0772, Val Loss: 0.0792\n",
      "Epoch 37/300 - Train Loss: 0.0768, Val Loss: 0.0752\n",
      "Epoch 38/300 - Train Loss: 0.0766, Val Loss: 0.0739\n",
      "Epoch 39/300 - Train Loss: 0.0765, Val Loss: 0.0739\n",
      "Epoch 40/300 - Train Loss: 0.0760, Val Loss: 0.0764\n",
      "Epoch 41/300 - Train Loss: 0.0767, Val Loss: 0.0734\n",
      "Epoch 42/300 - Train Loss: 0.0750, Val Loss: 0.0756\n",
      "Epoch 43/300 - Train Loss: 0.0733, Val Loss: 0.0739\n",
      "Epoch 44/300 - Train Loss: 0.0749, Val Loss: 0.0734\n",
      "Epoch 45/300 - Train Loss: 0.0735, Val Loss: 0.0727\n",
      "Epoch 46/300 - Train Loss: 0.0731, Val Loss: 0.0752\n",
      "Epoch 47/300 - Train Loss: 0.0744, Val Loss: 0.0720\n",
      "Epoch 48/300 - Train Loss: 0.0725, Val Loss: 0.0730\n",
      "Epoch 49/300 - Train Loss: 0.0727, Val Loss: 0.0734\n",
      "Epoch 50/300 - Train Loss: 0.0722, Val Loss: 0.0726\n",
      "Epoch 51/300 - Train Loss: 0.0713, Val Loss: 0.0729\n",
      "Epoch 52/300 - Train Loss: 0.0719, Val Loss: 0.0725\n",
      "Epoch 53/300 - Train Loss: 0.0711, Val Loss: 0.0729\n",
      "Epoch 54/300 - Train Loss: 0.0717, Val Loss: 0.0725\n",
      "Epoch 55/300 - Train Loss: 0.0717, Val Loss: 0.0725\n",
      "Epoch 56/300 - Train Loss: 0.0690, Val Loss: 0.0731\n",
      "Epoch 57/300 - Train Loss: 0.0721, Val Loss: 0.0728\n",
      "Epoch 58/300 - Train Loss: 0.0704, Val Loss: 0.0733\n",
      "Epoch 59/300 - Train Loss: 0.0714, Val Loss: 0.0726\n",
      "Epoch 60/300 - Train Loss: 0.0696, Val Loss: 0.0713\n",
      "Epoch 61/300 - Train Loss: 0.0717, Val Loss: 0.0742\n",
      "Epoch 62/300 - Train Loss: 0.0708, Val Loss: 0.0722\n",
      "Epoch 63/300 - Train Loss: 0.0705, Val Loss: 0.0734\n",
      "Epoch 64/300 - Train Loss: 0.0709, Val Loss: 0.0737\n",
      "Epoch 65/300 - Train Loss: 0.0678, Val Loss: 0.0721\n",
      "Epoch 66/300 - Train Loss: 0.0686, Val Loss: 0.0731\n",
      "Epoch 67/300 - Train Loss: 0.0685, Val Loss: 0.0713\n",
      "Epoch 68/300 - Train Loss: 0.0691, Val Loss: 0.0725\n",
      "Epoch 69/300 - Train Loss: 0.0680, Val Loss: 0.0713\n",
      "Epoch 70/300 - Train Loss: 0.0678, Val Loss: 0.0736\n",
      "Epoch 71/300 - Train Loss: 0.0680, Val Loss: 0.0734\n",
      "Epoch 72/300 - Train Loss: 0.0663, Val Loss: 0.0745\n",
      "Epoch 73/300 - Train Loss: 0.0672, Val Loss: 0.0734\n",
      "Epoch 74/300 - Train Loss: 0.0673, Val Loss: 0.0742\n",
      "Epoch 75/300 - Train Loss: 0.0675, Val Loss: 0.0737\n",
      "Epoch 76/300 - Train Loss: 0.0667, Val Loss: 0.0715\n",
      "Epoch 77/300 - Train Loss: 0.0665, Val Loss: 0.0728\n",
      "Epoch 78/300 - Train Loss: 0.0665, Val Loss: 0.0736\n",
      "Epoch 79/300 - Train Loss: 0.0653, Val Loss: 0.0720\n",
      "Epoch 80/300 - Train Loss: 0.0656, Val Loss: 0.0715\n",
      "Epoch 81/300 - Train Loss: 0.0670, Val Loss: 0.0706\n",
      "Epoch 82/300 - Train Loss: 0.0657, Val Loss: 0.0710\n",
      "Epoch 83/300 - Train Loss: 0.0671, Val Loss: 0.0745\n",
      "Epoch 84/300 - Train Loss: 0.0667, Val Loss: 0.0721\n",
      "Epoch 85/300 - Train Loss: 0.0662, Val Loss: 0.0712\n",
      "Epoch 86/300 - Train Loss: 0.0645, Val Loss: 0.0698\n",
      "Epoch 87/300 - Train Loss: 0.0654, Val Loss: 0.0718\n",
      "Epoch 88/300 - Train Loss: 0.0661, Val Loss: 0.0714\n",
      "Epoch 89/300 - Train Loss: 0.0664, Val Loss: 0.0720\n",
      "Epoch 90/300 - Train Loss: 0.0631, Val Loss: 0.0736\n",
      "Epoch 91/300 - Train Loss: 0.0651, Val Loss: 0.0727\n",
      "Epoch 92/300 - Train Loss: 0.0643, Val Loss: 0.0731\n",
      "Epoch 93/300 - Train Loss: 0.0646, Val Loss: 0.0734\n",
      "Epoch 94/300 - Train Loss: 0.0647, Val Loss: 0.0731\n",
      "Epoch 95/300 - Train Loss: 0.0641, Val Loss: 0.0726\n",
      "Epoch 96/300 - Train Loss: 0.0630, Val Loss: 0.0720\n",
      "Epoch 97/300 - Train Loss: 0.0632, Val Loss: 0.0712\n",
      "Epoch 98/300 - Train Loss: 0.0632, Val Loss: 0.0729\n",
      "Epoch 99/300 - Train Loss: 0.0637, Val Loss: 0.0689\n",
      "Epoch 100/300 - Train Loss: 0.0646, Val Loss: 0.0730\n",
      "Epoch 101/300 - Train Loss: 0.0635, Val Loss: 0.0718\n",
      "Epoch 102/300 - Train Loss: 0.0637, Val Loss: 0.0714\n",
      "Epoch 103/300 - Train Loss: 0.0626, Val Loss: 0.0688\n",
      "Epoch 104/300 - Train Loss: 0.0635, Val Loss: 0.0716\n",
      "Epoch 105/300 - Train Loss: 0.0614, Val Loss: 0.0680\n",
      "Epoch 106/300 - Train Loss: 0.0618, Val Loss: 0.0702\n",
      "Epoch 107/300 - Train Loss: 0.0631, Val Loss: 0.0705\n",
      "Epoch 108/300 - Train Loss: 0.0623, Val Loss: 0.0715\n",
      "Epoch 109/300 - Train Loss: 0.0623, Val Loss: 0.0716\n",
      "Epoch 110/300 - Train Loss: 0.0613, Val Loss: 0.0721\n",
      "Epoch 111/300 - Train Loss: 0.0618, Val Loss: 0.0723\n",
      "Epoch 112/300 - Train Loss: 0.0624, Val Loss: 0.0734\n",
      "Epoch 113/300 - Train Loss: 0.0622, Val Loss: 0.0721\n",
      "Epoch 114/300 - Train Loss: 0.0609, Val Loss: 0.0720\n",
      "Epoch 115/300 - Train Loss: 0.0607, Val Loss: 0.0689\n",
      "Epoch 116/300 - Train Loss: 0.0622, Val Loss: 0.0717\n",
      "Epoch 117/300 - Train Loss: 0.0629, Val Loss: 0.0699\n",
      "Epoch 118/300 - Train Loss: 0.0607, Val Loss: 0.0706\n",
      "Epoch 119/300 - Train Loss: 0.0598, Val Loss: 0.0693\n",
      "Epoch 120/300 - Train Loss: 0.0606, Val Loss: 0.0693\n",
      "Epoch 121/300 - Train Loss: 0.0595, Val Loss: 0.0721\n",
      "Epoch 122/300 - Train Loss: 0.0606, Val Loss: 0.0699\n",
      "Epoch 123/300 - Train Loss: 0.0609, Val Loss: 0.0700\n",
      "Epoch 124/300 - Train Loss: 0.0596, Val Loss: 0.0685\n",
      "Epoch 125/300 - Train Loss: 0.0592, Val Loss: 0.0701\n",
      "Epoch 126/300 - Train Loss: 0.0610, Val Loss: 0.0720\n",
      "Epoch 127/300 - Train Loss: 0.0597, Val Loss: 0.0727\n",
      "Epoch 128/300 - Train Loss: 0.0617, Val Loss: 0.0711\n",
      "Epoch 129/300 - Train Loss: 0.0588, Val Loss: 0.0721\n",
      "Epoch 130/300 - Train Loss: 0.0595, Val Loss: 0.0709\n",
      "Epoch 131/300 - Train Loss: 0.0599, Val Loss: 0.0703\n",
      "Epoch 132/300 - Train Loss: 0.0583, Val Loss: 0.0700\n",
      "Epoch 133/300 - Train Loss: 0.0592, Val Loss: 0.0708\n",
      "Epoch 134/300 - Train Loss: 0.0597, Val Loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:31:30,216] Trial 198 finished with value: 0.9645025514302571 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.3056828250545342, 'learning_rate': 0.00010378255443069706, 'batch_size': 256, 'weight_decay': 8.92845992213382e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/300 - Train Loss: 0.0591, Val Loss: 0.0701\n",
      "Early stopping at epoch 135\n",
      "Macro F1 Score: 0.9645, Macro Precision: 0.9588, Macro Recall: 0.9707\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 200\n",
      "Training with F1=8, F2=32, D=8, dropout=0.278268254291391, LR=0.00012365049312214622, BS=32, WD=3.694800619021913e-05\n",
      "Epoch 1/300 - Train Loss: 0.2710, Val Loss: 0.1420\n",
      "Epoch 2/300 - Train Loss: 0.1203, Val Loss: 0.0814\n",
      "Epoch 3/300 - Train Loss: 0.1033, Val Loss: 0.0774\n",
      "Epoch 4/300 - Train Loss: 0.0974, Val Loss: 0.0793\n",
      "Epoch 5/300 - Train Loss: 0.0947, Val Loss: 0.0899\n",
      "Epoch 6/300 - Train Loss: 0.0937, Val Loss: 0.0736\n",
      "Epoch 7/300 - Train Loss: 0.0931, Val Loss: 0.0842\n",
      "Epoch 8/300 - Train Loss: 0.0929, Val Loss: 0.0781\n",
      "Epoch 9/300 - Train Loss: 0.0877, Val Loss: 0.0748\n",
      "Epoch 10/300 - Train Loss: 0.0905, Val Loss: 0.0721\n",
      "Epoch 11/300 - Train Loss: 0.0892, Val Loss: 0.0750\n",
      "Epoch 12/300 - Train Loss: 0.0869, Val Loss: 0.0734\n",
      "Epoch 13/300 - Train Loss: 0.0882, Val Loss: 0.0746\n",
      "Epoch 14/300 - Train Loss: 0.0859, Val Loss: 0.0799\n",
      "Epoch 15/300 - Train Loss: 0.0848, Val Loss: 0.0694\n",
      "Epoch 16/300 - Train Loss: 0.0845, Val Loss: 0.0762\n",
      "Epoch 17/300 - Train Loss: 0.0828, Val Loss: 0.0930\n",
      "Epoch 18/300 - Train Loss: 0.0827, Val Loss: 0.0777\n",
      "Epoch 19/300 - Train Loss: 0.0816, Val Loss: 0.0745\n",
      "Epoch 20/300 - Train Loss: 0.0844, Val Loss: 0.0786\n",
      "Epoch 21/300 - Train Loss: 0.0768, Val Loss: 0.0748\n",
      "Epoch 22/300 - Train Loss: 0.0774, Val Loss: 0.0692\n",
      "Epoch 23/300 - Train Loss: 0.0797, Val Loss: 0.0722\n",
      "Epoch 24/300 - Train Loss: 0.0768, Val Loss: 0.0757\n",
      "Epoch 25/300 - Train Loss: 0.0760, Val Loss: 0.0761\n",
      "Epoch 26/300 - Train Loss: 0.0776, Val Loss: 0.0684\n",
      "Epoch 27/300 - Train Loss: 0.0774, Val Loss: 0.0717\n",
      "Epoch 28/300 - Train Loss: 0.0742, Val Loss: 0.0728\n",
      "Epoch 29/300 - Train Loss: 0.0772, Val Loss: 0.0713\n",
      "Epoch 30/300 - Train Loss: 0.0765, Val Loss: 0.0715\n",
      "Epoch 31/300 - Train Loss: 0.0758, Val Loss: 0.0780\n",
      "Epoch 32/300 - Train Loss: 0.0758, Val Loss: 0.0757\n",
      "Epoch 33/300 - Train Loss: 0.0735, Val Loss: 0.0741\n",
      "Epoch 34/300 - Train Loss: 0.0717, Val Loss: 0.0701\n",
      "Epoch 35/300 - Train Loss: 0.0724, Val Loss: 0.0714\n",
      "Epoch 36/300 - Train Loss: 0.0731, Val Loss: 0.0706\n",
      "Epoch 37/300 - Train Loss: 0.0718, Val Loss: 0.0686\n",
      "Epoch 38/300 - Train Loss: 0.0718, Val Loss: 0.0686\n",
      "Epoch 39/300 - Train Loss: 0.0691, Val Loss: 0.0696\n",
      "Epoch 40/300 - Train Loss: 0.0701, Val Loss: 0.0684\n",
      "Epoch 41/300 - Train Loss: 0.0690, Val Loss: 0.0728\n",
      "Epoch 42/300 - Train Loss: 0.0695, Val Loss: 0.0729\n",
      "Epoch 43/300 - Train Loss: 0.0681, Val Loss: 0.0697\n",
      "Epoch 44/300 - Train Loss: 0.0694, Val Loss: 0.0698\n",
      "Epoch 45/300 - Train Loss: 0.0696, Val Loss: 0.0720\n",
      "Epoch 46/300 - Train Loss: 0.0685, Val Loss: 0.0703\n",
      "Epoch 47/300 - Train Loss: 0.0658, Val Loss: 0.0673\n",
      "Epoch 48/300 - Train Loss: 0.0669, Val Loss: 0.0676\n",
      "Epoch 49/300 - Train Loss: 0.0670, Val Loss: 0.0754\n",
      "Epoch 50/300 - Train Loss: 0.0667, Val Loss: 0.0673\n",
      "Epoch 51/300 - Train Loss: 0.0658, Val Loss: 0.0679\n",
      "Epoch 52/300 - Train Loss: 0.0671, Val Loss: 0.0709\n",
      "Epoch 53/300 - Train Loss: 0.0690, Val Loss: 0.0689\n",
      "Epoch 54/300 - Train Loss: 0.0654, Val Loss: 0.0675\n",
      "Epoch 55/300 - Train Loss: 0.0661, Val Loss: 0.0685\n",
      "Epoch 56/300 - Train Loss: 0.0647, Val Loss: 0.0701\n",
      "Epoch 57/300 - Train Loss: 0.0648, Val Loss: 0.0670\n",
      "Epoch 58/300 - Train Loss: 0.0633, Val Loss: 0.0664\n",
      "Epoch 59/300 - Train Loss: 0.0643, Val Loss: 0.0704\n",
      "Epoch 60/300 - Train Loss: 0.0634, Val Loss: 0.0684\n",
      "Epoch 61/300 - Train Loss: 0.0640, Val Loss: 0.0685\n",
      "Epoch 62/300 - Train Loss: 0.0638, Val Loss: 0.0687\n",
      "Epoch 63/300 - Train Loss: 0.0628, Val Loss: 0.0677\n",
      "Epoch 64/300 - Train Loss: 0.0638, Val Loss: 0.0720\n",
      "Epoch 65/300 - Train Loss: 0.0614, Val Loss: 0.0710\n",
      "Epoch 66/300 - Train Loss: 0.0603, Val Loss: 0.0685\n",
      "Epoch 67/300 - Train Loss: 0.0630, Val Loss: 0.0698\n",
      "Epoch 68/300 - Train Loss: 0.0610, Val Loss: 0.0702\n",
      "Epoch 69/300 - Train Loss: 0.0594, Val Loss: 0.0664\n",
      "Epoch 70/300 - Train Loss: 0.0625, Val Loss: 0.0682\n",
      "Epoch 71/300 - Train Loss: 0.0597, Val Loss: 0.0692\n",
      "Epoch 72/300 - Train Loss: 0.0613, Val Loss: 0.0675\n",
      "Epoch 73/300 - Train Loss: 0.0605, Val Loss: 0.0717\n",
      "Epoch 74/300 - Train Loss: 0.0615, Val Loss: 0.0697\n",
      "Epoch 75/300 - Train Loss: 0.0621, Val Loss: 0.0684\n",
      "Epoch 76/300 - Train Loss: 0.0600, Val Loss: 0.0703\n",
      "Epoch 77/300 - Train Loss: 0.0598, Val Loss: 0.0681\n",
      "Epoch 78/300 - Train Loss: 0.0598, Val Loss: 0.0681\n",
      "Epoch 79/300 - Train Loss: 0.0589, Val Loss: 0.0701\n",
      "Epoch 80/300 - Train Loss: 0.0592, Val Loss: 0.0687\n",
      "Epoch 81/300 - Train Loss: 0.0600, Val Loss: 0.0666\n",
      "Epoch 82/300 - Train Loss: 0.0590, Val Loss: 0.0687\n",
      "Epoch 83/300 - Train Loss: 0.0574, Val Loss: 0.0701\n",
      "Epoch 84/300 - Train Loss: 0.0570, Val Loss: 0.0660\n",
      "Epoch 85/300 - Train Loss: 0.0581, Val Loss: 0.0693\n",
      "Epoch 86/300 - Train Loss: 0.0594, Val Loss: 0.0696\n",
      "Epoch 87/300 - Train Loss: 0.0584, Val Loss: 0.0661\n",
      "Epoch 88/300 - Train Loss: 0.0571, Val Loss: 0.0697\n",
      "Epoch 89/300 - Train Loss: 0.0579, Val Loss: 0.0676\n",
      "Epoch 90/300 - Train Loss: 0.0571, Val Loss: 0.0695\n",
      "Epoch 91/300 - Train Loss: 0.0540, Val Loss: 0.0697\n",
      "Epoch 92/300 - Train Loss: 0.0552, Val Loss: 0.0670\n",
      "Epoch 93/300 - Train Loss: 0.0589, Val Loss: 0.0719\n",
      "Epoch 94/300 - Train Loss: 0.0593, Val Loss: 0.0664\n",
      "Epoch 95/300 - Train Loss: 0.0547, Val Loss: 0.0697\n",
      "Epoch 96/300 - Train Loss: 0.0553, Val Loss: 0.0731\n",
      "Epoch 97/300 - Train Loss: 0.0554, Val Loss: 0.0704\n",
      "Epoch 98/300 - Train Loss: 0.0561, Val Loss: 0.0705\n",
      "Epoch 99/300 - Train Loss: 0.0553, Val Loss: 0.0681\n",
      "Epoch 100/300 - Train Loss: 0.0556, Val Loss: 0.0703\n",
      "Epoch 101/300 - Train Loss: 0.0556, Val Loss: 0.0684\n",
      "Epoch 102/300 - Train Loss: 0.0549, Val Loss: 0.0698\n",
      "Epoch 103/300 - Train Loss: 0.0582, Val Loss: 0.0673\n",
      "Epoch 104/300 - Train Loss: 0.0576, Val Loss: 0.0684\n",
      "Epoch 105/300 - Train Loss: 0.0541, Val Loss: 0.0704\n",
      "Epoch 106/300 - Train Loss: 0.0558, Val Loss: 0.0726\n",
      "Epoch 107/300 - Train Loss: 0.0567, Val Loss: 0.0737\n",
      "Epoch 108/300 - Train Loss: 0.0532, Val Loss: 0.0705\n",
      "Epoch 109/300 - Train Loss: 0.0543, Val Loss: 0.0664\n",
      "Epoch 110/300 - Train Loss: 0.0528, Val Loss: 0.0694\n",
      "Epoch 111/300 - Train Loss: 0.0526, Val Loss: 0.0694\n",
      "Epoch 112/300 - Train Loss: 0.0530, Val Loss: 0.0730\n",
      "Epoch 113/300 - Train Loss: 0.0527, Val Loss: 0.0704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:36:03,530] Trial 199 finished with value: 0.9645104933180465 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.278268254291391, 'learning_rate': 0.00012365049312214622, 'batch_size': 32, 'weight_decay': 3.694800619021913e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/300 - Train Loss: 0.0557, Val Loss: 0.0712\n",
      "Early stopping at epoch 114\n",
      "Macro F1 Score: 0.9645, Macro Precision: 0.9585, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 201\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2924932044647889, LR=7.407566043831943e-05, BS=64, WD=4.6872234501639327e-05\n",
      "Epoch 1/300 - Train Loss: 0.3767, Val Loss: 0.1893\n",
      "Epoch 2/300 - Train Loss: 0.1601, Val Loss: 0.1134\n",
      "Epoch 3/300 - Train Loss: 0.1180, Val Loss: 0.0975\n",
      "Epoch 4/300 - Train Loss: 0.1050, Val Loss: 0.0915\n",
      "Epoch 5/300 - Train Loss: 0.0965, Val Loss: 0.0891\n",
      "Epoch 6/300 - Train Loss: 0.0943, Val Loss: 0.0832\n",
      "Epoch 7/300 - Train Loss: 0.0943, Val Loss: 0.0766\n",
      "Epoch 8/300 - Train Loss: 0.0903, Val Loss: 0.0753\n",
      "Epoch 9/300 - Train Loss: 0.0865, Val Loss: 0.0771\n",
      "Epoch 10/300 - Train Loss: 0.0859, Val Loss: 0.0765\n",
      "Epoch 11/300 - Train Loss: 0.0843, Val Loss: 0.0826\n",
      "Epoch 12/300 - Train Loss: 0.0829, Val Loss: 0.0773\n",
      "Epoch 13/300 - Train Loss: 0.0824, Val Loss: 0.0740\n",
      "Epoch 14/300 - Train Loss: 0.0805, Val Loss: 0.0791\n",
      "Epoch 15/300 - Train Loss: 0.0777, Val Loss: 0.0724\n",
      "Epoch 16/300 - Train Loss: 0.0806, Val Loss: 0.0787\n",
      "Epoch 17/300 - Train Loss: 0.0792, Val Loss: 0.0718\n",
      "Epoch 18/300 - Train Loss: 0.0766, Val Loss: 0.0728\n",
      "Epoch 19/300 - Train Loss: 0.0758, Val Loss: 0.0745\n",
      "Epoch 20/300 - Train Loss: 0.0756, Val Loss: 0.0724\n",
      "Epoch 21/300 - Train Loss: 0.0749, Val Loss: 0.0754\n",
      "Epoch 22/300 - Train Loss: 0.0757, Val Loss: 0.0720\n",
      "Epoch 23/300 - Train Loss: 0.0737, Val Loss: 0.0807\n",
      "Epoch 24/300 - Train Loss: 0.0747, Val Loss: 0.0792\n",
      "Epoch 25/300 - Train Loss: 0.0732, Val Loss: 0.0804\n",
      "Epoch 26/300 - Train Loss: 0.0714, Val Loss: 0.0702\n",
      "Epoch 27/300 - Train Loss: 0.0713, Val Loss: 0.0771\n",
      "Epoch 28/300 - Train Loss: 0.0718, Val Loss: 0.0750\n",
      "Epoch 29/300 - Train Loss: 0.0718, Val Loss: 0.0740\n",
      "Epoch 30/300 - Train Loss: 0.0694, Val Loss: 0.0710\n",
      "Epoch 31/300 - Train Loss: 0.0702, Val Loss: 0.0734\n",
      "Epoch 32/300 - Train Loss: 0.0700, Val Loss: 0.0739\n",
      "Epoch 33/300 - Train Loss: 0.0690, Val Loss: 0.0748\n",
      "Epoch 34/300 - Train Loss: 0.0689, Val Loss: 0.0688\n",
      "Epoch 35/300 - Train Loss: 0.0697, Val Loss: 0.0734\n",
      "Epoch 36/300 - Train Loss: 0.0695, Val Loss: 0.0751\n",
      "Epoch 37/300 - Train Loss: 0.0691, Val Loss: 0.0703\n",
      "Epoch 38/300 - Train Loss: 0.0696, Val Loss: 0.0703\n",
      "Epoch 39/300 - Train Loss: 0.0670, Val Loss: 0.0694\n",
      "Epoch 40/300 - Train Loss: 0.0678, Val Loss: 0.0688\n",
      "Epoch 41/300 - Train Loss: 0.0669, Val Loss: 0.0701\n",
      "Epoch 42/300 - Train Loss: 0.0654, Val Loss: 0.0698\n",
      "Epoch 43/300 - Train Loss: 0.0665, Val Loss: 0.0737\n",
      "Epoch 44/300 - Train Loss: 0.0663, Val Loss: 0.0690\n",
      "Epoch 45/300 - Train Loss: 0.0640, Val Loss: 0.0727\n",
      "Epoch 46/300 - Train Loss: 0.0646, Val Loss: 0.0718\n",
      "Epoch 47/300 - Train Loss: 0.0634, Val Loss: 0.0680\n",
      "Epoch 48/300 - Train Loss: 0.0641, Val Loss: 0.0692\n",
      "Epoch 49/300 - Train Loss: 0.0658, Val Loss: 0.0716\n",
      "Epoch 50/300 - Train Loss: 0.0643, Val Loss: 0.0673\n",
      "Epoch 51/300 - Train Loss: 0.0638, Val Loss: 0.0725\n",
      "Epoch 52/300 - Train Loss: 0.0624, Val Loss: 0.0703\n",
      "Epoch 53/300 - Train Loss: 0.0634, Val Loss: 0.0710\n",
      "Epoch 54/300 - Train Loss: 0.0612, Val Loss: 0.0672\n",
      "Epoch 55/300 - Train Loss: 0.0620, Val Loss: 0.0673\n",
      "Epoch 56/300 - Train Loss: 0.0620, Val Loss: 0.0706\n",
      "Epoch 57/300 - Train Loss: 0.0629, Val Loss: 0.0673\n",
      "Epoch 58/300 - Train Loss: 0.0618, Val Loss: 0.0702\n",
      "Epoch 59/300 - Train Loss: 0.0615, Val Loss: 0.0666\n",
      "Epoch 60/300 - Train Loss: 0.0613, Val Loss: 0.0728\n",
      "Epoch 61/300 - Train Loss: 0.0605, Val Loss: 0.0714\n",
      "Epoch 62/300 - Train Loss: 0.0605, Val Loss: 0.0694\n",
      "Epoch 63/300 - Train Loss: 0.0607, Val Loss: 0.0721\n",
      "Epoch 64/300 - Train Loss: 0.0602, Val Loss: 0.0704\n",
      "Epoch 65/300 - Train Loss: 0.0588, Val Loss: 0.0712\n",
      "Epoch 66/300 - Train Loss: 0.0600, Val Loss: 0.0708\n",
      "Epoch 67/300 - Train Loss: 0.0600, Val Loss: 0.0688\n",
      "Epoch 68/300 - Train Loss: 0.0574, Val Loss: 0.0693\n",
      "Epoch 69/300 - Train Loss: 0.0586, Val Loss: 0.0722\n",
      "Epoch 70/300 - Train Loss: 0.0577, Val Loss: 0.0683\n",
      "Epoch 71/300 - Train Loss: 0.0599, Val Loss: 0.0697\n",
      "Epoch 72/300 - Train Loss: 0.0582, Val Loss: 0.0704\n",
      "Epoch 73/300 - Train Loss: 0.0584, Val Loss: 0.0683\n",
      "Epoch 74/300 - Train Loss: 0.0575, Val Loss: 0.0728\n",
      "Epoch 75/300 - Train Loss: 0.0580, Val Loss: 0.0719\n",
      "Epoch 76/300 - Train Loss: 0.0563, Val Loss: 0.0697\n",
      "Epoch 77/300 - Train Loss: 0.0575, Val Loss: 0.0662\n",
      "Epoch 78/300 - Train Loss: 0.0572, Val Loss: 0.0687\n",
      "Epoch 79/300 - Train Loss: 0.0554, Val Loss: 0.0728\n",
      "Epoch 80/300 - Train Loss: 0.0557, Val Loss: 0.0715\n",
      "Epoch 81/300 - Train Loss: 0.0566, Val Loss: 0.0660\n",
      "Epoch 82/300 - Train Loss: 0.0563, Val Loss: 0.0705\n",
      "Epoch 83/300 - Train Loss: 0.0553, Val Loss: 0.0686\n",
      "Epoch 84/300 - Train Loss: 0.0552, Val Loss: 0.0699\n",
      "Epoch 85/300 - Train Loss: 0.0546, Val Loss: 0.0689\n",
      "Epoch 86/300 - Train Loss: 0.0544, Val Loss: 0.0682\n",
      "Epoch 87/300 - Train Loss: 0.0539, Val Loss: 0.0731\n",
      "Epoch 88/300 - Train Loss: 0.0540, Val Loss: 0.0702\n",
      "Epoch 89/300 - Train Loss: 0.0541, Val Loss: 0.0685\n",
      "Epoch 90/300 - Train Loss: 0.0523, Val Loss: 0.0707\n",
      "Epoch 91/300 - Train Loss: 0.0550, Val Loss: 0.0698\n",
      "Epoch 92/300 - Train Loss: 0.0549, Val Loss: 0.0698\n",
      "Epoch 93/300 - Train Loss: 0.0526, Val Loss: 0.0708\n",
      "Epoch 94/300 - Train Loss: 0.0533, Val Loss: 0.0756\n",
      "Epoch 95/300 - Train Loss: 0.0515, Val Loss: 0.0700\n",
      "Epoch 96/300 - Train Loss: 0.0532, Val Loss: 0.0699\n",
      "Epoch 97/300 - Train Loss: 0.0520, Val Loss: 0.0691\n",
      "Epoch 98/300 - Train Loss: 0.0527, Val Loss: 0.0696\n",
      "Epoch 99/300 - Train Loss: 0.0516, Val Loss: 0.0685\n",
      "Epoch 100/300 - Train Loss: 0.0526, Val Loss: 0.0683\n",
      "Epoch 101/300 - Train Loss: 0.0512, Val Loss: 0.0681\n",
      "Epoch 102/300 - Train Loss: 0.0536, Val Loss: 0.0710\n",
      "Epoch 103/300 - Train Loss: 0.0511, Val Loss: 0.0734\n",
      "Epoch 104/300 - Train Loss: 0.0520, Val Loss: 0.0684\n",
      "Epoch 105/300 - Train Loss: 0.0502, Val Loss: 0.0693\n",
      "Epoch 106/300 - Train Loss: 0.0502, Val Loss: 0.0702\n",
      "Epoch 107/300 - Train Loss: 0.0508, Val Loss: 0.0699\n",
      "Epoch 108/300 - Train Loss: 0.0492, Val Loss: 0.0715\n",
      "Epoch 109/300 - Train Loss: 0.0507, Val Loss: 0.0705\n",
      "Epoch 110/300 - Train Loss: 0.0483, Val Loss: 0.0704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:42:08,555] Trial 200 finished with value: 0.9699245890424578 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2924932044647889, 'learning_rate': 7.407566043831943e-05, 'batch_size': 64, 'weight_decay': 4.6872234501639327e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300 - Train Loss: 0.0481, Val Loss: 0.0693\n",
      "Early stopping at epoch 111\n",
      "Macro F1 Score: 0.9699, Macro Precision: 0.9578, Macro Recall: 0.9834\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.98      0.99       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 202\n",
      "Training with F1=16, F2=32, D=8, dropout=0.3301945310445359, LR=6.701992573937633e-05, BS=32, WD=4.241912728267344e-05\n",
      "Epoch 1/300 - Train Loss: 0.3158, Val Loss: 0.1373\n",
      "Epoch 2/300 - Train Loss: 0.1364, Val Loss: 0.1036\n",
      "Epoch 3/300 - Train Loss: 0.1166, Val Loss: 0.0972\n",
      "Epoch 4/300 - Train Loss: 0.1094, Val Loss: 0.0896\n",
      "Epoch 5/300 - Train Loss: 0.1052, Val Loss: 0.0938\n",
      "Epoch 6/300 - Train Loss: 0.1013, Val Loss: 0.0809\n",
      "Epoch 7/300 - Train Loss: 0.0958, Val Loss: 0.0833\n",
      "Epoch 8/300 - Train Loss: 0.0944, Val Loss: 0.0819\n",
      "Epoch 9/300 - Train Loss: 0.0936, Val Loss: 0.0827\n",
      "Epoch 10/300 - Train Loss: 0.0918, Val Loss: 0.0814\n",
      "Epoch 11/300 - Train Loss: 0.0902, Val Loss: 0.0755\n",
      "Epoch 12/300 - Train Loss: 0.0864, Val Loss: 0.0796\n",
      "Epoch 13/300 - Train Loss: 0.0889, Val Loss: 0.0757\n",
      "Epoch 14/300 - Train Loss: 0.0887, Val Loss: 0.0762\n",
      "Epoch 15/300 - Train Loss: 0.0868, Val Loss: 0.0771\n",
      "Epoch 16/300 - Train Loss: 0.0871, Val Loss: 0.0756\n",
      "Epoch 17/300 - Train Loss: 0.0832, Val Loss: 0.0754\n",
      "Epoch 18/300 - Train Loss: 0.0829, Val Loss: 0.0714\n",
      "Epoch 19/300 - Train Loss: 0.0807, Val Loss: 0.0790\n",
      "Epoch 20/300 - Train Loss: 0.0813, Val Loss: 0.0774\n",
      "Epoch 21/300 - Train Loss: 0.0801, Val Loss: 0.0821\n",
      "Epoch 22/300 - Train Loss: 0.0824, Val Loss: 0.0784\n",
      "Epoch 23/300 - Train Loss: 0.0807, Val Loss: 0.0742\n",
      "Epoch 24/300 - Train Loss: 0.0800, Val Loss: 0.0769\n",
      "Epoch 25/300 - Train Loss: 0.0793, Val Loss: 0.0764\n",
      "Epoch 26/300 - Train Loss: 0.0791, Val Loss: 0.0738\n",
      "Epoch 27/300 - Train Loss: 0.0802, Val Loss: 0.0766\n",
      "Epoch 28/300 - Train Loss: 0.0790, Val Loss: 0.0846\n",
      "Epoch 29/300 - Train Loss: 0.0775, Val Loss: 0.0753\n",
      "Epoch 30/300 - Train Loss: 0.0762, Val Loss: 0.0816\n",
      "Epoch 31/300 - Train Loss: 0.0761, Val Loss: 0.0747\n",
      "Epoch 32/300 - Train Loss: 0.0763, Val Loss: 0.0686\n",
      "Epoch 33/300 - Train Loss: 0.0757, Val Loss: 0.0754\n",
      "Epoch 34/300 - Train Loss: 0.0750, Val Loss: 0.0743\n",
      "Epoch 35/300 - Train Loss: 0.0757, Val Loss: 0.0705\n",
      "Epoch 36/300 - Train Loss: 0.0758, Val Loss: 0.0721\n",
      "Epoch 37/300 - Train Loss: 0.0757, Val Loss: 0.0702\n",
      "Epoch 38/300 - Train Loss: 0.0741, Val Loss: 0.0709\n",
      "Epoch 39/300 - Train Loss: 0.0714, Val Loss: 0.0669\n",
      "Epoch 40/300 - Train Loss: 0.0753, Val Loss: 0.0692\n",
      "Epoch 41/300 - Train Loss: 0.0710, Val Loss: 0.0688\n",
      "Epoch 42/300 - Train Loss: 0.0714, Val Loss: 0.0696\n",
      "Epoch 43/300 - Train Loss: 0.0710, Val Loss: 0.0730\n",
      "Epoch 44/300 - Train Loss: 0.0730, Val Loss: 0.0692\n",
      "Epoch 45/300 - Train Loss: 0.0714, Val Loss: 0.0724\n",
      "Epoch 46/300 - Train Loss: 0.0705, Val Loss: 0.0735\n",
      "Epoch 47/300 - Train Loss: 0.0713, Val Loss: 0.0686\n",
      "Epoch 48/300 - Train Loss: 0.0716, Val Loss: 0.0689\n",
      "Epoch 49/300 - Train Loss: 0.0702, Val Loss: 0.0723\n",
      "Epoch 50/300 - Train Loss: 0.0702, Val Loss: 0.0685\n",
      "Epoch 51/300 - Train Loss: 0.0722, Val Loss: 0.0737\n",
      "Epoch 52/300 - Train Loss: 0.0690, Val Loss: 0.0694\n",
      "Epoch 53/300 - Train Loss: 0.0687, Val Loss: 0.0722\n",
      "Epoch 54/300 - Train Loss: 0.0692, Val Loss: 0.0692\n",
      "Epoch 55/300 - Train Loss: 0.0680, Val Loss: 0.0720\n",
      "Epoch 56/300 - Train Loss: 0.0681, Val Loss: 0.0674\n",
      "Epoch 57/300 - Train Loss: 0.0700, Val Loss: 0.0677\n",
      "Epoch 58/300 - Train Loss: 0.0675, Val Loss: 0.0670\n",
      "Epoch 59/300 - Train Loss: 0.0685, Val Loss: 0.0739\n",
      "Epoch 60/300 - Train Loss: 0.0680, Val Loss: 0.0728\n",
      "Epoch 61/300 - Train Loss: 0.0656, Val Loss: 0.0737\n",
      "Epoch 62/300 - Train Loss: 0.0676, Val Loss: 0.0704\n",
      "Epoch 63/300 - Train Loss: 0.0671, Val Loss: 0.0662\n",
      "Epoch 64/300 - Train Loss: 0.0659, Val Loss: 0.0698\n",
      "Epoch 65/300 - Train Loss: 0.0646, Val Loss: 0.0676\n",
      "Epoch 66/300 - Train Loss: 0.0653, Val Loss: 0.0708\n",
      "Epoch 67/300 - Train Loss: 0.0625, Val Loss: 0.0683\n",
      "Epoch 68/300 - Train Loss: 0.0684, Val Loss: 0.0693\n",
      "Epoch 69/300 - Train Loss: 0.0655, Val Loss: 0.0667\n",
      "Epoch 70/300 - Train Loss: 0.0651, Val Loss: 0.0706\n",
      "Epoch 71/300 - Train Loss: 0.0644, Val Loss: 0.0694\n",
      "Epoch 72/300 - Train Loss: 0.0653, Val Loss: 0.0717\n",
      "Epoch 73/300 - Train Loss: 0.0641, Val Loss: 0.0662\n",
      "Epoch 74/300 - Train Loss: 0.0608, Val Loss: 0.0706\n",
      "Epoch 75/300 - Train Loss: 0.0619, Val Loss: 0.0667\n",
      "Epoch 76/300 - Train Loss: 0.0622, Val Loss: 0.0730\n",
      "Epoch 77/300 - Train Loss: 0.0644, Val Loss: 0.0652\n",
      "Epoch 78/300 - Train Loss: 0.0653, Val Loss: 0.0669\n",
      "Epoch 79/300 - Train Loss: 0.0602, Val Loss: 0.0694\n",
      "Epoch 80/300 - Train Loss: 0.0605, Val Loss: 0.0713\n",
      "Epoch 81/300 - Train Loss: 0.0629, Val Loss: 0.0669\n",
      "Epoch 82/300 - Train Loss: 0.0622, Val Loss: 0.0707\n",
      "Epoch 83/300 - Train Loss: 0.0627, Val Loss: 0.0693\n",
      "Epoch 84/300 - Train Loss: 0.0630, Val Loss: 0.0667\n",
      "Epoch 85/300 - Train Loss: 0.0602, Val Loss: 0.0754\n",
      "Epoch 86/300 - Train Loss: 0.0637, Val Loss: 0.0697\n",
      "Epoch 87/300 - Train Loss: 0.0607, Val Loss: 0.0667\n",
      "Epoch 88/300 - Train Loss: 0.0606, Val Loss: 0.0712\n",
      "Epoch 89/300 - Train Loss: 0.0603, Val Loss: 0.0687\n",
      "Epoch 90/300 - Train Loss: 0.0598, Val Loss: 0.0677\n",
      "Epoch 91/300 - Train Loss: 0.0614, Val Loss: 0.0723\n",
      "Epoch 92/300 - Train Loss: 0.0641, Val Loss: 0.0746\n",
      "Epoch 93/300 - Train Loss: 0.0569, Val Loss: 0.0716\n",
      "Epoch 94/300 - Train Loss: 0.0590, Val Loss: 0.0692\n",
      "Epoch 95/300 - Train Loss: 0.0594, Val Loss: 0.0682\n",
      "Epoch 96/300 - Train Loss: 0.0595, Val Loss: 0.0698\n",
      "Epoch 97/300 - Train Loss: 0.0600, Val Loss: 0.0718\n",
      "Epoch 98/300 - Train Loss: 0.0587, Val Loss: 0.0668\n",
      "Epoch 99/300 - Train Loss: 0.0606, Val Loss: 0.0748\n",
      "Epoch 100/300 - Train Loss: 0.0573, Val Loss: 0.0701\n",
      "Epoch 101/300 - Train Loss: 0.0624, Val Loss: 0.0704\n",
      "Epoch 102/300 - Train Loss: 0.0561, Val Loss: 0.0697\n",
      "Epoch 103/300 - Train Loss: 0.0593, Val Loss: 0.0697\n",
      "Epoch 104/300 - Train Loss: 0.0584, Val Loss: 0.0683\n",
      "Epoch 105/300 - Train Loss: 0.0580, Val Loss: 0.0656\n",
      "Epoch 106/300 - Train Loss: 0.0595, Val Loss: 0.0690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:48:59,989] Trial 201 finished with value: 0.9646910898393325 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.3301945310445359, 'learning_rate': 6.701992573937633e-05, 'batch_size': 32, 'weight_decay': 4.241912728267344e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/300 - Train Loss: 0.0567, Val Loss: 0.0698\n",
      "Early stopping at epoch 107\n",
      "Macro F1 Score: 0.9647, Macro Precision: 0.9490, Macro Recall: 0.9827\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       789\n",
      "           1       0.87      0.98      0.92        61\n",
      "           2       0.99      0.98      0.99       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 203\n",
      "Training with F1=16, F2=32, D=8, dropout=0.3198542761664788, LR=8.177843321391005e-05, BS=32, WD=5.469191845975773e-05\n",
      "Epoch 1/300 - Train Loss: 0.2848, Val Loss: 0.1404\n",
      "Epoch 2/300 - Train Loss: 0.1306, Val Loss: 0.0915\n",
      "Epoch 3/300 - Train Loss: 0.1056, Val Loss: 0.0792\n",
      "Epoch 4/300 - Train Loss: 0.0993, Val Loss: 0.0896\n",
      "Epoch 5/300 - Train Loss: 0.0952, Val Loss: 0.0881\n",
      "Epoch 6/300 - Train Loss: 0.0960, Val Loss: 0.0794\n",
      "Epoch 7/300 - Train Loss: 0.0925, Val Loss: 0.0740\n",
      "Epoch 8/300 - Train Loss: 0.0899, Val Loss: 0.0724\n",
      "Epoch 9/300 - Train Loss: 0.0866, Val Loss: 0.0787\n",
      "Epoch 10/300 - Train Loss: 0.0850, Val Loss: 0.0781\n",
      "Epoch 11/300 - Train Loss: 0.0854, Val Loss: 0.0698\n",
      "Epoch 12/300 - Train Loss: 0.0860, Val Loss: 0.0724\n",
      "Epoch 13/300 - Train Loss: 0.0846, Val Loss: 0.0787\n",
      "Epoch 14/300 - Train Loss: 0.0845, Val Loss: 0.0764\n",
      "Epoch 15/300 - Train Loss: 0.0849, Val Loss: 0.0699\n",
      "Epoch 16/300 - Train Loss: 0.0810, Val Loss: 0.0771\n",
      "Epoch 17/300 - Train Loss: 0.0808, Val Loss: 0.0722\n",
      "Epoch 18/300 - Train Loss: 0.0820, Val Loss: 0.0744\n",
      "Epoch 19/300 - Train Loss: 0.0822, Val Loss: 0.0881\n",
      "Epoch 20/300 - Train Loss: 0.0786, Val Loss: 0.0686\n",
      "Epoch 21/300 - Train Loss: 0.0759, Val Loss: 0.0685\n",
      "Epoch 22/300 - Train Loss: 0.0774, Val Loss: 0.0718\n",
      "Epoch 23/300 - Train Loss: 0.0779, Val Loss: 0.0730\n",
      "Epoch 24/300 - Train Loss: 0.0758, Val Loss: 0.0679\n",
      "Epoch 25/300 - Train Loss: 0.0769, Val Loss: 0.0674\n",
      "Epoch 26/300 - Train Loss: 0.0755, Val Loss: 0.0778\n",
      "Epoch 27/300 - Train Loss: 0.0735, Val Loss: 0.0673\n",
      "Epoch 28/300 - Train Loss: 0.0739, Val Loss: 0.0702\n",
      "Epoch 29/300 - Train Loss: 0.0767, Val Loss: 0.0649\n",
      "Epoch 30/300 - Train Loss: 0.0762, Val Loss: 0.0672\n",
      "Epoch 31/300 - Train Loss: 0.0727, Val Loss: 0.0695\n",
      "Epoch 32/300 - Train Loss: 0.0729, Val Loss: 0.0680\n",
      "Epoch 33/300 - Train Loss: 0.0744, Val Loss: 0.0665\n",
      "Epoch 34/300 - Train Loss: 0.0731, Val Loss: 0.0657\n",
      "Epoch 35/300 - Train Loss: 0.0723, Val Loss: 0.0687\n",
      "Epoch 36/300 - Train Loss: 0.0714, Val Loss: 0.0682\n",
      "Epoch 37/300 - Train Loss: 0.0707, Val Loss: 0.0678\n",
      "Epoch 38/300 - Train Loss: 0.0704, Val Loss: 0.0646\n",
      "Epoch 39/300 - Train Loss: 0.0735, Val Loss: 0.0692\n",
      "Epoch 40/300 - Train Loss: 0.0693, Val Loss: 0.0701\n",
      "Epoch 41/300 - Train Loss: 0.0710, Val Loss: 0.0669\n",
      "Epoch 42/300 - Train Loss: 0.0722, Val Loss: 0.0695\n",
      "Epoch 43/300 - Train Loss: 0.0700, Val Loss: 0.0668\n",
      "Epoch 44/300 - Train Loss: 0.0691, Val Loss: 0.0665\n",
      "Epoch 45/300 - Train Loss: 0.0707, Val Loss: 0.0666\n",
      "Epoch 46/300 - Train Loss: 0.0678, Val Loss: 0.0665\n",
      "Epoch 47/300 - Train Loss: 0.0686, Val Loss: 0.0654\n",
      "Epoch 48/300 - Train Loss: 0.0666, Val Loss: 0.0680\n",
      "Epoch 49/300 - Train Loss: 0.0684, Val Loss: 0.0679\n",
      "Epoch 50/300 - Train Loss: 0.0659, Val Loss: 0.0665\n",
      "Epoch 51/300 - Train Loss: 0.0666, Val Loss: 0.0691\n",
      "Epoch 52/300 - Train Loss: 0.0661, Val Loss: 0.0707\n",
      "Epoch 53/300 - Train Loss: 0.0673, Val Loss: 0.0682\n",
      "Epoch 54/300 - Train Loss: 0.0659, Val Loss: 0.0732\n",
      "Epoch 55/300 - Train Loss: 0.0651, Val Loss: 0.0657\n",
      "Epoch 56/300 - Train Loss: 0.0646, Val Loss: 0.0643\n",
      "Epoch 57/300 - Train Loss: 0.0645, Val Loss: 0.0664\n",
      "Epoch 58/300 - Train Loss: 0.0659, Val Loss: 0.0687\n",
      "Epoch 59/300 - Train Loss: 0.0640, Val Loss: 0.0718\n",
      "Epoch 60/300 - Train Loss: 0.0642, Val Loss: 0.0678\n",
      "Epoch 61/300 - Train Loss: 0.0614, Val Loss: 0.0713\n",
      "Epoch 62/300 - Train Loss: 0.0644, Val Loss: 0.0671\n",
      "Epoch 63/300 - Train Loss: 0.0621, Val Loss: 0.0700\n",
      "Epoch 64/300 - Train Loss: 0.0633, Val Loss: 0.0671\n",
      "Epoch 65/300 - Train Loss: 0.0634, Val Loss: 0.0667\n",
      "Epoch 66/300 - Train Loss: 0.0610, Val Loss: 0.0687\n",
      "Epoch 67/300 - Train Loss: 0.0626, Val Loss: 0.0702\n",
      "Epoch 68/300 - Train Loss: 0.0632, Val Loss: 0.0760\n",
      "Epoch 69/300 - Train Loss: 0.0605, Val Loss: 0.0821\n",
      "Epoch 70/300 - Train Loss: 0.0610, Val Loss: 0.0681\n",
      "Epoch 71/300 - Train Loss: 0.0621, Val Loss: 0.0729\n",
      "Epoch 72/300 - Train Loss: 0.0614, Val Loss: 0.0669\n",
      "Epoch 73/300 - Train Loss: 0.0585, Val Loss: 0.0735\n",
      "Epoch 74/300 - Train Loss: 0.0609, Val Loss: 0.0706\n",
      "Epoch 75/300 - Train Loss: 0.0609, Val Loss: 0.0664\n",
      "Epoch 76/300 - Train Loss: 0.0603, Val Loss: 0.0679\n",
      "Epoch 77/300 - Train Loss: 0.0582, Val Loss: 0.0730\n",
      "Epoch 78/300 - Train Loss: 0.0613, Val Loss: 0.0679\n",
      "Epoch 79/300 - Train Loss: 0.0585, Val Loss: 0.0699\n",
      "Epoch 80/300 - Train Loss: 0.0582, Val Loss: 0.0691\n",
      "Epoch 81/300 - Train Loss: 0.0582, Val Loss: 0.0711\n",
      "Epoch 82/300 - Train Loss: 0.0610, Val Loss: 0.0752\n",
      "Epoch 83/300 - Train Loss: 0.0571, Val Loss: 0.0726\n",
      "Epoch 84/300 - Train Loss: 0.0564, Val Loss: 0.0738\n",
      "Epoch 85/300 - Train Loss: 0.0587, Val Loss: 0.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:54:31,156] Trial 202 finished with value: 0.9695285252176338 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.3198542761664788, 'learning_rate': 8.177843321391005e-05, 'batch_size': 32, 'weight_decay': 5.469191845975773e-05}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/300 - Train Loss: 0.0576, Val Loss: 0.0695\n",
      "Early stopping at epoch 86\n",
      "Macro F1 Score: 0.9695, Macro Precision: 0.9613, Macro Recall: 0.9784\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 204\n",
      "Training with F1=16, F2=32, D=8, dropout=0.24020585493050242, LR=6.258646998109714e-05, BS=32, WD=0.00016804167396478701\n",
      "Epoch 1/300 - Train Loss: 0.3036, Val Loss: 0.1628\n",
      "Epoch 2/300 - Train Loss: 0.1367, Val Loss: 0.0904\n",
      "Epoch 3/300 - Train Loss: 0.1104, Val Loss: 0.0840\n",
      "Epoch 4/300 - Train Loss: 0.1046, Val Loss: 0.0857\n",
      "Epoch 5/300 - Train Loss: 0.0976, Val Loss: 0.0808\n",
      "Epoch 6/300 - Train Loss: 0.0954, Val Loss: 0.0742\n",
      "Epoch 7/300 - Train Loss: 0.0931, Val Loss: 0.0716\n",
      "Epoch 8/300 - Train Loss: 0.0910, Val Loss: 0.0733\n",
      "Epoch 9/300 - Train Loss: 0.0913, Val Loss: 0.0853\n",
      "Epoch 10/300 - Train Loss: 0.0892, Val Loss: 0.0858\n",
      "Epoch 11/300 - Train Loss: 0.0872, Val Loss: 0.0787\n",
      "Epoch 12/300 - Train Loss: 0.0880, Val Loss: 0.0693\n",
      "Epoch 13/300 - Train Loss: 0.0867, Val Loss: 0.0765\n",
      "Epoch 14/300 - Train Loss: 0.0845, Val Loss: 0.0855\n",
      "Epoch 15/300 - Train Loss: 0.0848, Val Loss: 0.0702\n",
      "Epoch 16/300 - Train Loss: 0.0829, Val Loss: 0.0818\n",
      "Epoch 17/300 - Train Loss: 0.0819, Val Loss: 0.0724\n",
      "Epoch 18/300 - Train Loss: 0.0828, Val Loss: 0.0691\n",
      "Epoch 19/300 - Train Loss: 0.0836, Val Loss: 0.0705\n",
      "Epoch 20/300 - Train Loss: 0.0812, Val Loss: 0.0662\n",
      "Epoch 21/300 - Train Loss: 0.0800, Val Loss: 0.0754\n",
      "Epoch 22/300 - Train Loss: 0.0806, Val Loss: 0.0767\n",
      "Epoch 23/300 - Train Loss: 0.0799, Val Loss: 0.0773\n",
      "Epoch 24/300 - Train Loss: 0.0800, Val Loss: 0.0731\n",
      "Epoch 25/300 - Train Loss: 0.0790, Val Loss: 0.0757\n",
      "Epoch 26/300 - Train Loss: 0.0775, Val Loss: 0.0733\n",
      "Epoch 27/300 - Train Loss: 0.0797, Val Loss: 0.0711\n",
      "Epoch 28/300 - Train Loss: 0.0762, Val Loss: 0.0727\n",
      "Epoch 29/300 - Train Loss: 0.0762, Val Loss: 0.0675\n",
      "Epoch 30/300 - Train Loss: 0.0767, Val Loss: 0.0671\n",
      "Epoch 31/300 - Train Loss: 0.0751, Val Loss: 0.0678\n",
      "Epoch 32/300 - Train Loss: 0.0770, Val Loss: 0.0668\n",
      "Epoch 33/300 - Train Loss: 0.0754, Val Loss: 0.0768\n",
      "Epoch 34/300 - Train Loss: 0.0724, Val Loss: 0.0692\n",
      "Epoch 35/300 - Train Loss: 0.0731, Val Loss: 0.0701\n",
      "Epoch 36/300 - Train Loss: 0.0743, Val Loss: 0.0664\n",
      "Epoch 37/300 - Train Loss: 0.0718, Val Loss: 0.0710\n",
      "Epoch 38/300 - Train Loss: 0.0721, Val Loss: 0.0712\n",
      "Epoch 39/300 - Train Loss: 0.0725, Val Loss: 0.0685\n",
      "Epoch 40/300 - Train Loss: 0.0715, Val Loss: 0.0713\n",
      "Epoch 41/300 - Train Loss: 0.0702, Val Loss: 0.0663\n",
      "Epoch 42/300 - Train Loss: 0.0721, Val Loss: 0.0699\n",
      "Epoch 43/300 - Train Loss: 0.0702, Val Loss: 0.0741\n",
      "Epoch 44/300 - Train Loss: 0.0695, Val Loss: 0.0675\n",
      "Epoch 45/300 - Train Loss: 0.0687, Val Loss: 0.0649\n",
      "Epoch 46/300 - Train Loss: 0.0684, Val Loss: 0.0672\n",
      "Epoch 47/300 - Train Loss: 0.0691, Val Loss: 0.0727\n",
      "Epoch 48/300 - Train Loss: 0.0705, Val Loss: 0.0627\n",
      "Epoch 49/300 - Train Loss: 0.0679, Val Loss: 0.0690\n",
      "Epoch 50/300 - Train Loss: 0.0689, Val Loss: 0.0670\n",
      "Epoch 51/300 - Train Loss: 0.0681, Val Loss: 0.0664\n",
      "Epoch 52/300 - Train Loss: 0.0678, Val Loss: 0.0667\n",
      "Epoch 53/300 - Train Loss: 0.0695, Val Loss: 0.0641\n",
      "Epoch 54/300 - Train Loss: 0.0670, Val Loss: 0.0686\n",
      "Epoch 55/300 - Train Loss: 0.0681, Val Loss: 0.0716\n",
      "Epoch 56/300 - Train Loss: 0.0658, Val Loss: 0.0672\n",
      "Epoch 57/300 - Train Loss: 0.0656, Val Loss: 0.0669\n",
      "Epoch 58/300 - Train Loss: 0.0676, Val Loss: 0.0664\n",
      "Epoch 59/300 - Train Loss: 0.0658, Val Loss: 0.0681\n",
      "Epoch 60/300 - Train Loss: 0.0675, Val Loss: 0.0651\n",
      "Epoch 61/300 - Train Loss: 0.0680, Val Loss: 0.0698\n",
      "Epoch 62/300 - Train Loss: 0.0651, Val Loss: 0.0722\n",
      "Epoch 63/300 - Train Loss: 0.0644, Val Loss: 0.0667\n",
      "Epoch 64/300 - Train Loss: 0.0636, Val Loss: 0.0642\n",
      "Epoch 65/300 - Train Loss: 0.0648, Val Loss: 0.0637\n",
      "Epoch 66/300 - Train Loss: 0.0643, Val Loss: 0.0673\n",
      "Epoch 67/300 - Train Loss: 0.0632, Val Loss: 0.0629\n",
      "Epoch 68/300 - Train Loss: 0.0638, Val Loss: 0.0667\n",
      "Epoch 69/300 - Train Loss: 0.0647, Val Loss: 0.0647\n",
      "Epoch 70/300 - Train Loss: 0.0668, Val Loss: 0.0673\n",
      "Epoch 71/300 - Train Loss: 0.0650, Val Loss: 0.0647\n",
      "Epoch 72/300 - Train Loss: 0.0646, Val Loss: 0.0644\n",
      "Epoch 73/300 - Train Loss: 0.0621, Val Loss: 0.0641\n",
      "Epoch 74/300 - Train Loss: 0.0610, Val Loss: 0.0734\n",
      "Epoch 75/300 - Train Loss: 0.0631, Val Loss: 0.0762\n",
      "Epoch 76/300 - Train Loss: 0.0604, Val Loss: 0.0674\n",
      "Epoch 77/300 - Train Loss: 0.0627, Val Loss: 0.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 11:59:31,072] Trial 203 finished with value: 0.972666020483449 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.24020585493050242, 'learning_rate': 6.258646998109714e-05, 'batch_size': 32, 'weight_decay': 0.00016804167396478701}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300 - Train Loss: 0.0629, Val Loss: 0.0728\n",
      "Early stopping at epoch 78\n",
      "Macro F1 Score: 0.9727, Macro Precision: 0.9687, Macro Recall: 0.9768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 205\n",
      "Training with F1=16, F2=32, D=8, dropout=0.26175079963118053, LR=6.145413976263163e-05, BS=32, WD=0.00029875144980814433\n",
      "Epoch 1/300 - Train Loss: 0.3132, Val Loss: 0.1701\n",
      "Epoch 2/300 - Train Loss: 0.1681, Val Loss: 0.1227\n",
      "Epoch 3/300 - Train Loss: 0.1229, Val Loss: 0.1026\n",
      "Epoch 4/300 - Train Loss: 0.1070, Val Loss: 0.0943\n",
      "Epoch 5/300 - Train Loss: 0.0996, Val Loss: 0.0796\n",
      "Epoch 6/300 - Train Loss: 0.0962, Val Loss: 0.0813\n",
      "Epoch 7/300 - Train Loss: 0.0948, Val Loss: 0.0814\n",
      "Epoch 8/300 - Train Loss: 0.0934, Val Loss: 0.0781\n",
      "Epoch 9/300 - Train Loss: 0.0906, Val Loss: 0.0981\n",
      "Epoch 10/300 - Train Loss: 0.0906, Val Loss: 0.0800\n",
      "Epoch 11/300 - Train Loss: 0.0929, Val Loss: 0.0748\n",
      "Epoch 12/300 - Train Loss: 0.0870, Val Loss: 0.0783\n",
      "Epoch 13/300 - Train Loss: 0.0854, Val Loss: 0.0732\n",
      "Epoch 14/300 - Train Loss: 0.0847, Val Loss: 0.0719\n",
      "Epoch 15/300 - Train Loss: 0.0849, Val Loss: 0.0697\n",
      "Epoch 16/300 - Train Loss: 0.0822, Val Loss: 0.0741\n",
      "Epoch 17/300 - Train Loss: 0.0828, Val Loss: 0.0758\n",
      "Epoch 18/300 - Train Loss: 0.0813, Val Loss: 0.0760\n",
      "Epoch 19/300 - Train Loss: 0.0795, Val Loss: 0.0802\n",
      "Epoch 20/300 - Train Loss: 0.0801, Val Loss: 0.0763\n",
      "Epoch 21/300 - Train Loss: 0.0787, Val Loss: 0.0650\n",
      "Epoch 22/300 - Train Loss: 0.0793, Val Loss: 0.0674\n",
      "Epoch 23/300 - Train Loss: 0.0804, Val Loss: 0.0713\n",
      "Epoch 24/300 - Train Loss: 0.0773, Val Loss: 0.0753\n",
      "Epoch 25/300 - Train Loss: 0.0767, Val Loss: 0.0695\n",
      "Epoch 26/300 - Train Loss: 0.0781, Val Loss: 0.0845\n",
      "Epoch 27/300 - Train Loss: 0.0747, Val Loss: 0.0676\n",
      "Epoch 28/300 - Train Loss: 0.0754, Val Loss: 0.0693\n",
      "Epoch 29/300 - Train Loss: 0.0751, Val Loss: 0.0688\n",
      "Epoch 30/300 - Train Loss: 0.0760, Val Loss: 0.0694\n",
      "Epoch 31/300 - Train Loss: 0.0736, Val Loss: 0.0687\n",
      "Epoch 32/300 - Train Loss: 0.0739, Val Loss: 0.0667\n",
      "Epoch 33/300 - Train Loss: 0.0754, Val Loss: 0.0734\n",
      "Epoch 34/300 - Train Loss: 0.0741, Val Loss: 0.0679\n",
      "Epoch 35/300 - Train Loss: 0.0720, Val Loss: 0.0664\n",
      "Epoch 36/300 - Train Loss: 0.0715, Val Loss: 0.0666\n",
      "Epoch 37/300 - Train Loss: 0.0716, Val Loss: 0.0647\n",
      "Epoch 38/300 - Train Loss: 0.0733, Val Loss: 0.0719\n",
      "Epoch 39/300 - Train Loss: 0.0718, Val Loss: 0.0649\n",
      "Epoch 40/300 - Train Loss: 0.0718, Val Loss: 0.0684\n",
      "Epoch 41/300 - Train Loss: 0.0707, Val Loss: 0.0662\n",
      "Epoch 42/300 - Train Loss: 0.0707, Val Loss: 0.0646\n",
      "Epoch 43/300 - Train Loss: 0.0739, Val Loss: 0.0659\n",
      "Epoch 44/300 - Train Loss: 0.0699, Val Loss: 0.0658\n",
      "Epoch 45/300 - Train Loss: 0.0688, Val Loss: 0.0687\n",
      "Epoch 46/300 - Train Loss: 0.0700, Val Loss: 0.0670\n",
      "Epoch 47/300 - Train Loss: 0.0687, Val Loss: 0.0676\n",
      "Epoch 48/300 - Train Loss: 0.0685, Val Loss: 0.0675\n",
      "Epoch 49/300 - Train Loss: 0.0682, Val Loss: 0.0638\n",
      "Epoch 50/300 - Train Loss: 0.0684, Val Loss: 0.0692\n",
      "Epoch 51/300 - Train Loss: 0.0670, Val Loss: 0.0641\n",
      "Epoch 52/300 - Train Loss: 0.0686, Val Loss: 0.0705\n",
      "Epoch 53/300 - Train Loss: 0.0684, Val Loss: 0.0680\n",
      "Epoch 54/300 - Train Loss: 0.0680, Val Loss: 0.0665\n",
      "Epoch 55/300 - Train Loss: 0.0690, Val Loss: 0.0740\n",
      "Epoch 56/300 - Train Loss: 0.0680, Val Loss: 0.0684\n",
      "Epoch 57/300 - Train Loss: 0.0653, Val Loss: 0.0682\n",
      "Epoch 58/300 - Train Loss: 0.0683, Val Loss: 0.0702\n",
      "Epoch 59/300 - Train Loss: 0.0668, Val Loss: 0.0638\n",
      "Epoch 60/300 - Train Loss: 0.0661, Val Loss: 0.0650\n",
      "Epoch 61/300 - Train Loss: 0.0644, Val Loss: 0.0642\n",
      "Epoch 62/300 - Train Loss: 0.0656, Val Loss: 0.0637\n",
      "Epoch 63/300 - Train Loss: 0.0666, Val Loss: 0.0675\n",
      "Epoch 64/300 - Train Loss: 0.0678, Val Loss: 0.0616\n",
      "Epoch 65/300 - Train Loss: 0.0664, Val Loss: 0.0658\n",
      "Epoch 66/300 - Train Loss: 0.0660, Val Loss: 0.0664\n",
      "Epoch 67/300 - Train Loss: 0.0674, Val Loss: 0.0641\n",
      "Epoch 68/300 - Train Loss: 0.0627, Val Loss: 0.0652\n",
      "Epoch 69/300 - Train Loss: 0.0645, Val Loss: 0.0657\n",
      "Epoch 70/300 - Train Loss: 0.0629, Val Loss: 0.0647\n",
      "Epoch 71/300 - Train Loss: 0.0634, Val Loss: 0.0681\n",
      "Epoch 72/300 - Train Loss: 0.0617, Val Loss: 0.0631\n",
      "Epoch 73/300 - Train Loss: 0.0644, Val Loss: 0.0668\n",
      "Epoch 74/300 - Train Loss: 0.0628, Val Loss: 0.0656\n",
      "Epoch 75/300 - Train Loss: 0.0644, Val Loss: 0.0655\n",
      "Epoch 76/300 - Train Loss: 0.0616, Val Loss: 0.0645\n",
      "Epoch 77/300 - Train Loss: 0.0623, Val Loss: 0.0641\n",
      "Epoch 78/300 - Train Loss: 0.0621, Val Loss: 0.0668\n",
      "Epoch 79/300 - Train Loss: 0.0614, Val Loss: 0.0672\n",
      "Epoch 80/300 - Train Loss: 0.0614, Val Loss: 0.0673\n",
      "Epoch 81/300 - Train Loss: 0.0619, Val Loss: 0.0676\n",
      "Epoch 82/300 - Train Loss: 0.0631, Val Loss: 0.0783\n",
      "Epoch 83/300 - Train Loss: 0.0598, Val Loss: 0.0643\n",
      "Epoch 84/300 - Train Loss: 0.0615, Val Loss: 0.0661\n",
      "Epoch 85/300 - Train Loss: 0.0618, Val Loss: 0.0653\n",
      "Epoch 86/300 - Train Loss: 0.0623, Val Loss: 0.0706\n",
      "Epoch 87/300 - Train Loss: 0.0632, Val Loss: 0.0652\n",
      "Epoch 88/300 - Train Loss: 0.0616, Val Loss: 0.0661\n",
      "Epoch 89/300 - Train Loss: 0.0628, Val Loss: 0.0680\n",
      "Epoch 90/300 - Train Loss: 0.0600, Val Loss: 0.0637\n",
      "Epoch 91/300 - Train Loss: 0.0621, Val Loss: 0.0650\n",
      "Epoch 92/300 - Train Loss: 0.0595, Val Loss: 0.0666\n",
      "Epoch 93/300 - Train Loss: 0.0589, Val Loss: 0.0647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:05:32,637] Trial 204 finished with value: 0.9757321236524853 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.26175079963118053, 'learning_rate': 6.145413976263163e-05, 'batch_size': 32, 'weight_decay': 0.00029875144980814433}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300 - Train Loss: 0.0584, Val Loss: 0.0659\n",
      "Early stopping at epoch 94\n",
      "Macro F1 Score: 0.9757, Macro Precision: 0.9789, Macro Recall: 0.9726\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 206\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2500728928938433, LR=6.021889318714777e-05, BS=32, WD=0.00017950899016494926\n",
      "Epoch 1/300 - Train Loss: 0.3130, Val Loss: 0.1419\n",
      "Epoch 2/300 - Train Loss: 0.1395, Val Loss: 0.1220\n",
      "Epoch 3/300 - Train Loss: 0.1135, Val Loss: 0.0857\n",
      "Epoch 4/300 - Train Loss: 0.1061, Val Loss: 0.0853\n",
      "Epoch 5/300 - Train Loss: 0.0995, Val Loss: 0.0760\n",
      "Epoch 6/300 - Train Loss: 0.0961, Val Loss: 0.0751\n",
      "Epoch 7/300 - Train Loss: 0.0940, Val Loss: 0.0905\n",
      "Epoch 8/300 - Train Loss: 0.0946, Val Loss: 0.0793\n",
      "Epoch 9/300 - Train Loss: 0.0901, Val Loss: 0.0748\n",
      "Epoch 10/300 - Train Loss: 0.0883, Val Loss: 0.0726\n",
      "Epoch 11/300 - Train Loss: 0.0873, Val Loss: 0.0739\n",
      "Epoch 12/300 - Train Loss: 0.0875, Val Loss: 0.0738\n",
      "Epoch 13/300 - Train Loss: 0.0837, Val Loss: 0.0724\n",
      "Epoch 14/300 - Train Loss: 0.0846, Val Loss: 0.0758\n",
      "Epoch 15/300 - Train Loss: 0.0832, Val Loss: 0.0731\n",
      "Epoch 16/300 - Train Loss: 0.0821, Val Loss: 0.0705\n",
      "Epoch 17/300 - Train Loss: 0.0807, Val Loss: 0.0736\n",
      "Epoch 18/300 - Train Loss: 0.0774, Val Loss: 0.0738\n",
      "Epoch 19/300 - Train Loss: 0.0801, Val Loss: 0.0742\n",
      "Epoch 20/300 - Train Loss: 0.0811, Val Loss: 0.0666\n",
      "Epoch 21/300 - Train Loss: 0.0787, Val Loss: 0.0673\n",
      "Epoch 22/300 - Train Loss: 0.0780, Val Loss: 0.0723\n",
      "Epoch 23/300 - Train Loss: 0.0813, Val Loss: 0.0697\n",
      "Epoch 24/300 - Train Loss: 0.0770, Val Loss: 0.0755\n",
      "Epoch 25/300 - Train Loss: 0.0781, Val Loss: 0.0727\n",
      "Epoch 26/300 - Train Loss: 0.0756, Val Loss: 0.0692\n",
      "Epoch 27/300 - Train Loss: 0.0748, Val Loss: 0.0676\n",
      "Epoch 28/300 - Train Loss: 0.0760, Val Loss: 0.0725\n",
      "Epoch 29/300 - Train Loss: 0.0722, Val Loss: 0.0667\n",
      "Epoch 30/300 - Train Loss: 0.0731, Val Loss: 0.0654\n",
      "Epoch 31/300 - Train Loss: 0.0732, Val Loss: 0.0669\n",
      "Epoch 32/300 - Train Loss: 0.0746, Val Loss: 0.0787\n",
      "Epoch 33/300 - Train Loss: 0.0717, Val Loss: 0.0652\n",
      "Epoch 34/300 - Train Loss: 0.0725, Val Loss: 0.0666\n",
      "Epoch 35/300 - Train Loss: 0.0708, Val Loss: 0.0710\n",
      "Epoch 36/300 - Train Loss: 0.0705, Val Loss: 0.0690\n",
      "Epoch 37/300 - Train Loss: 0.0718, Val Loss: 0.0668\n",
      "Epoch 38/300 - Train Loss: 0.0709, Val Loss: 0.0662\n",
      "Epoch 39/300 - Train Loss: 0.0700, Val Loss: 0.0669\n",
      "Epoch 40/300 - Train Loss: 0.0699, Val Loss: 0.0680\n",
      "Epoch 41/300 - Train Loss: 0.0691, Val Loss: 0.0672\n",
      "Epoch 42/300 - Train Loss: 0.0701, Val Loss: 0.0751\n",
      "Epoch 43/300 - Train Loss: 0.0708, Val Loss: 0.0699\n",
      "Epoch 44/300 - Train Loss: 0.0665, Val Loss: 0.0661\n",
      "Epoch 45/300 - Train Loss: 0.0674, Val Loss: 0.0661\n",
      "Epoch 46/300 - Train Loss: 0.0665, Val Loss: 0.0648\n",
      "Epoch 47/300 - Train Loss: 0.0655, Val Loss: 0.0681\n",
      "Epoch 48/300 - Train Loss: 0.0670, Val Loss: 0.0672\n",
      "Epoch 49/300 - Train Loss: 0.0682, Val Loss: 0.0662\n",
      "Epoch 50/300 - Train Loss: 0.0681, Val Loss: 0.0676\n",
      "Epoch 51/300 - Train Loss: 0.0652, Val Loss: 0.0671\n",
      "Epoch 52/300 - Train Loss: 0.0659, Val Loss: 0.0706\n",
      "Epoch 53/300 - Train Loss: 0.0636, Val Loss: 0.0694\n",
      "Epoch 54/300 - Train Loss: 0.0681, Val Loss: 0.0648\n",
      "Epoch 55/300 - Train Loss: 0.0661, Val Loss: 0.0668\n",
      "Epoch 56/300 - Train Loss: 0.0648, Val Loss: 0.0650\n",
      "Epoch 57/300 - Train Loss: 0.0642, Val Loss: 0.0657\n",
      "Epoch 58/300 - Train Loss: 0.0647, Val Loss: 0.0703\n",
      "Epoch 59/300 - Train Loss: 0.0620, Val Loss: 0.0665\n",
      "Epoch 60/300 - Train Loss: 0.0631, Val Loss: 0.0714\n",
      "Epoch 61/300 - Train Loss: 0.0607, Val Loss: 0.0682\n",
      "Epoch 62/300 - Train Loss: 0.0633, Val Loss: 0.0680\n",
      "Epoch 63/300 - Train Loss: 0.0600, Val Loss: 0.0683\n",
      "Epoch 64/300 - Train Loss: 0.0619, Val Loss: 0.0675\n",
      "Epoch 65/300 - Train Loss: 0.0637, Val Loss: 0.0659\n",
      "Epoch 66/300 - Train Loss: 0.0604, Val Loss: 0.0669\n",
      "Epoch 67/300 - Train Loss: 0.0645, Val Loss: 0.0611\n",
      "Epoch 68/300 - Train Loss: 0.0608, Val Loss: 0.0630\n",
      "Epoch 69/300 - Train Loss: 0.0610, Val Loss: 0.0647\n",
      "Epoch 70/300 - Train Loss: 0.0600, Val Loss: 0.0676\n",
      "Epoch 71/300 - Train Loss: 0.0628, Val Loss: 0.0654\n",
      "Epoch 72/300 - Train Loss: 0.0598, Val Loss: 0.0690\n",
      "Epoch 73/300 - Train Loss: 0.0581, Val Loss: 0.0650\n",
      "Epoch 74/300 - Train Loss: 0.0591, Val Loss: 0.0665\n",
      "Epoch 75/300 - Train Loss: 0.0596, Val Loss: 0.0654\n",
      "Epoch 76/300 - Train Loss: 0.0623, Val Loss: 0.0690\n",
      "Epoch 77/300 - Train Loss: 0.0588, Val Loss: 0.0683\n",
      "Epoch 78/300 - Train Loss: 0.0594, Val Loss: 0.0689\n",
      "Epoch 79/300 - Train Loss: 0.0593, Val Loss: 0.0655\n",
      "Epoch 80/300 - Train Loss: 0.0611, Val Loss: 0.0629\n",
      "Epoch 81/300 - Train Loss: 0.0592, Val Loss: 0.0683\n",
      "Epoch 82/300 - Train Loss: 0.0613, Val Loss: 0.0685\n",
      "Epoch 83/300 - Train Loss: 0.0617, Val Loss: 0.0662\n",
      "Epoch 84/300 - Train Loss: 0.0562, Val Loss: 0.0681\n",
      "Epoch 85/300 - Train Loss: 0.0570, Val Loss: 0.0653\n",
      "Epoch 86/300 - Train Loss: 0.0577, Val Loss: 0.0651\n",
      "Epoch 87/300 - Train Loss: 0.0576, Val Loss: 0.0741\n",
      "Epoch 88/300 - Train Loss: 0.0574, Val Loss: 0.0676\n",
      "Epoch 89/300 - Train Loss: 0.0573, Val Loss: 0.0660\n",
      "Epoch 90/300 - Train Loss: 0.0553, Val Loss: 0.0709\n",
      "Epoch 91/300 - Train Loss: 0.0556, Val Loss: 0.0668\n",
      "Epoch 92/300 - Train Loss: 0.0559, Val Loss: 0.0642\n",
      "Epoch 93/300 - Train Loss: 0.0548, Val Loss: 0.0674\n",
      "Epoch 94/300 - Train Loss: 0.0577, Val Loss: 0.0724\n",
      "Epoch 95/300 - Train Loss: 0.0555, Val Loss: 0.0705\n",
      "Epoch 96/300 - Train Loss: 0.0547, Val Loss: 0.0649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:11:46,156] Trial 205 finished with value: 0.969546946223293 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2500728928938433, 'learning_rate': 6.021889318714777e-05, 'batch_size': 32, 'weight_decay': 0.00017950899016494926}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300 - Train Loss: 0.0577, Val Loss: 0.0645\n",
      "Early stopping at epoch 97\n",
      "Macro F1 Score: 0.9695, Macro Precision: 0.9678, Macro Recall: 0.9714\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 207\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2620487546381191, LR=5.0651809362776405e-05, BS=32, WD=0.00032015891715939234\n",
      "Epoch 1/300 - Train Loss: 0.3406, Val Loss: 0.1735\n",
      "Epoch 2/300 - Train Loss: 0.1739, Val Loss: 0.1402\n",
      "Epoch 3/300 - Train Loss: 0.1414, Val Loss: 0.1020\n",
      "Epoch 4/300 - Train Loss: 0.1170, Val Loss: 0.0957\n",
      "Epoch 5/300 - Train Loss: 0.1086, Val Loss: 0.0883\n",
      "Epoch 6/300 - Train Loss: 0.1031, Val Loss: 0.0809\n",
      "Epoch 7/300 - Train Loss: 0.0985, Val Loss: 0.0847\n",
      "Epoch 8/300 - Train Loss: 0.0950, Val Loss: 0.0800\n",
      "Epoch 9/300 - Train Loss: 0.0938, Val Loss: 0.0827\n",
      "Epoch 10/300 - Train Loss: 0.0928, Val Loss: 0.0771\n",
      "Epoch 11/300 - Train Loss: 0.0900, Val Loss: 0.0789\n",
      "Epoch 12/300 - Train Loss: 0.0881, Val Loss: 0.0724\n",
      "Epoch 13/300 - Train Loss: 0.0868, Val Loss: 0.0759\n",
      "Epoch 14/300 - Train Loss: 0.0885, Val Loss: 0.0803\n",
      "Epoch 15/300 - Train Loss: 0.0848, Val Loss: 0.0718\n",
      "Epoch 16/300 - Train Loss: 0.0841, Val Loss: 0.0672\n",
      "Epoch 17/300 - Train Loss: 0.0831, Val Loss: 0.0698\n",
      "Epoch 18/300 - Train Loss: 0.0809, Val Loss: 0.0741\n",
      "Epoch 19/300 - Train Loss: 0.0805, Val Loss: 0.0787\n",
      "Epoch 20/300 - Train Loss: 0.0789, Val Loss: 0.0705\n",
      "Epoch 21/300 - Train Loss: 0.0797, Val Loss: 0.0698\n",
      "Epoch 22/300 - Train Loss: 0.0804, Val Loss: 0.0720\n",
      "Epoch 23/300 - Train Loss: 0.0779, Val Loss: 0.0664\n",
      "Epoch 24/300 - Train Loss: 0.0790, Val Loss: 0.0696\n",
      "Epoch 25/300 - Train Loss: 0.0789, Val Loss: 0.0752\n",
      "Epoch 26/300 - Train Loss: 0.0795, Val Loss: 0.0676\n",
      "Epoch 27/300 - Train Loss: 0.0799, Val Loss: 0.0697\n",
      "Epoch 28/300 - Train Loss: 0.0766, Val Loss: 0.0695\n",
      "Epoch 29/300 - Train Loss: 0.0753, Val Loss: 0.0680\n",
      "Epoch 30/300 - Train Loss: 0.0775, Val Loss: 0.0701\n",
      "Epoch 31/300 - Train Loss: 0.0776, Val Loss: 0.0732\n",
      "Epoch 32/300 - Train Loss: 0.0750, Val Loss: 0.0741\n",
      "Epoch 33/300 - Train Loss: 0.0754, Val Loss: 0.0670\n",
      "Epoch 34/300 - Train Loss: 0.0765, Val Loss: 0.0660\n",
      "Epoch 35/300 - Train Loss: 0.0751, Val Loss: 0.0648\n",
      "Epoch 36/300 - Train Loss: 0.0750, Val Loss: 0.0708\n",
      "Epoch 37/300 - Train Loss: 0.0755, Val Loss: 0.0710\n",
      "Epoch 38/300 - Train Loss: 0.0715, Val Loss: 0.0702\n",
      "Epoch 39/300 - Train Loss: 0.0705, Val Loss: 0.0741\n",
      "Epoch 40/300 - Train Loss: 0.0729, Val Loss: 0.0692\n",
      "Epoch 41/300 - Train Loss: 0.0739, Val Loss: 0.0655\n",
      "Epoch 42/300 - Train Loss: 0.0744, Val Loss: 0.0741\n",
      "Epoch 43/300 - Train Loss: 0.0702, Val Loss: 0.0684\n",
      "Epoch 44/300 - Train Loss: 0.0716, Val Loss: 0.0784\n",
      "Epoch 45/300 - Train Loss: 0.0735, Val Loss: 0.0665\n",
      "Epoch 46/300 - Train Loss: 0.0686, Val Loss: 0.0664\n",
      "Epoch 47/300 - Train Loss: 0.0725, Val Loss: 0.0668\n",
      "Epoch 48/300 - Train Loss: 0.0706, Val Loss: 0.0709\n",
      "Epoch 49/300 - Train Loss: 0.0707, Val Loss: 0.0682\n",
      "Epoch 50/300 - Train Loss: 0.0702, Val Loss: 0.0702\n",
      "Epoch 51/300 - Train Loss: 0.0694, Val Loss: 0.0770\n",
      "Epoch 52/300 - Train Loss: 0.0701, Val Loss: 0.0690\n",
      "Epoch 53/300 - Train Loss: 0.0692, Val Loss: 0.0702\n",
      "Epoch 54/300 - Train Loss: 0.0685, Val Loss: 0.0672\n",
      "Epoch 55/300 - Train Loss: 0.0718, Val Loss: 0.0708\n",
      "Epoch 56/300 - Train Loss: 0.0697, Val Loss: 0.0671\n",
      "Epoch 57/300 - Train Loss: 0.0688, Val Loss: 0.0651\n",
      "Epoch 58/300 - Train Loss: 0.0685, Val Loss: 0.0707\n",
      "Epoch 59/300 - Train Loss: 0.0674, Val Loss: 0.0686\n",
      "Epoch 60/300 - Train Loss: 0.0670, Val Loss: 0.0679\n",
      "Epoch 61/300 - Train Loss: 0.0667, Val Loss: 0.0709\n",
      "Epoch 62/300 - Train Loss: 0.0676, Val Loss: 0.0668\n",
      "Epoch 63/300 - Train Loss: 0.0647, Val Loss: 0.0697\n",
      "Epoch 64/300 - Train Loss: 0.0640, Val Loss: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:15:56,179] Trial 206 finished with value: 0.965494052089816 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2620487546381191, 'learning_rate': 5.0651809362776405e-05, 'batch_size': 32, 'weight_decay': 0.00032015891715939234}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300 - Train Loss: 0.0666, Val Loss: 0.0670\n",
      "Early stopping at epoch 65\n",
      "Macro F1 Score: 0.9655, Macro Precision: 0.9596, Macro Recall: 0.9718\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 208\n",
      "Training with F1=16, F2=16, D=4, dropout=0.23659275283430847, LR=6.26354329132869e-05, BS=32, WD=0.0001414480367303363\n",
      "Epoch 1/300 - Train Loss: 0.4205, Val Loss: 0.1889\n",
      "Epoch 2/300 - Train Loss: 0.1608, Val Loss: 0.1057\n",
      "Epoch 3/300 - Train Loss: 0.1239, Val Loss: 0.0945\n",
      "Epoch 4/300 - Train Loss: 0.1131, Val Loss: 0.0880\n",
      "Epoch 5/300 - Train Loss: 0.1034, Val Loss: 0.0819\n",
      "Epoch 6/300 - Train Loss: 0.1015, Val Loss: 0.0820\n",
      "Epoch 7/300 - Train Loss: 0.0999, Val Loss: 0.0797\n",
      "Epoch 8/300 - Train Loss: 0.0954, Val Loss: 0.0787\n",
      "Epoch 9/300 - Train Loss: 0.0945, Val Loss: 0.0803\n",
      "Epoch 10/300 - Train Loss: 0.0939, Val Loss: 0.0784\n",
      "Epoch 11/300 - Train Loss: 0.0933, Val Loss: 0.0733\n",
      "Epoch 12/300 - Train Loss: 0.0906, Val Loss: 0.0727\n",
      "Epoch 13/300 - Train Loss: 0.0900, Val Loss: 0.0759\n",
      "Epoch 14/300 - Train Loss: 0.0909, Val Loss: 0.0783\n",
      "Epoch 15/300 - Train Loss: 0.0883, Val Loss: 0.0732\n",
      "Epoch 16/300 - Train Loss: 0.0882, Val Loss: 0.0789\n",
      "Epoch 17/300 - Train Loss: 0.0860, Val Loss: 0.0736\n",
      "Epoch 18/300 - Train Loss: 0.0861, Val Loss: 0.0765\n",
      "Epoch 19/300 - Train Loss: 0.0862, Val Loss: 0.0711\n",
      "Epoch 20/300 - Train Loss: 0.0844, Val Loss: 0.0759\n",
      "Epoch 21/300 - Train Loss: 0.0844, Val Loss: 0.0760\n",
      "Epoch 22/300 - Train Loss: 0.0858, Val Loss: 0.0802\n",
      "Epoch 23/300 - Train Loss: 0.0843, Val Loss: 0.0669\n",
      "Epoch 24/300 - Train Loss: 0.0852, Val Loss: 0.0721\n",
      "Epoch 25/300 - Train Loss: 0.0857, Val Loss: 0.0719\n",
      "Epoch 26/300 - Train Loss: 0.0831, Val Loss: 0.0699\n",
      "Epoch 27/300 - Train Loss: 0.0821, Val Loss: 0.0714\n",
      "Epoch 28/300 - Train Loss: 0.0808, Val Loss: 0.0719\n",
      "Epoch 29/300 - Train Loss: 0.0833, Val Loss: 0.0706\n",
      "Epoch 30/300 - Train Loss: 0.0819, Val Loss: 0.0694\n",
      "Epoch 31/300 - Train Loss: 0.0800, Val Loss: 0.0676\n",
      "Epoch 32/300 - Train Loss: 0.0795, Val Loss: 0.0733\n",
      "Epoch 33/300 - Train Loss: 0.0798, Val Loss: 0.0668\n",
      "Epoch 34/300 - Train Loss: 0.0849, Val Loss: 0.0718\n",
      "Epoch 35/300 - Train Loss: 0.0797, Val Loss: 0.0705\n",
      "Epoch 36/300 - Train Loss: 0.0809, Val Loss: 0.0702\n",
      "Epoch 37/300 - Train Loss: 0.0804, Val Loss: 0.0705\n",
      "Epoch 38/300 - Train Loss: 0.0802, Val Loss: 0.0714\n",
      "Epoch 39/300 - Train Loss: 0.0815, Val Loss: 0.0702\n",
      "Epoch 40/300 - Train Loss: 0.0769, Val Loss: 0.0711\n",
      "Epoch 41/300 - Train Loss: 0.0792, Val Loss: 0.0674\n",
      "Epoch 42/300 - Train Loss: 0.0781, Val Loss: 0.0685\n",
      "Epoch 43/300 - Train Loss: 0.0799, Val Loss: 0.0680\n",
      "Epoch 44/300 - Train Loss: 0.0785, Val Loss: 0.0727\n",
      "Epoch 45/300 - Train Loss: 0.0775, Val Loss: 0.0699\n",
      "Epoch 46/300 - Train Loss: 0.0784, Val Loss: 0.0737\n",
      "Epoch 47/300 - Train Loss: 0.0787, Val Loss: 0.0714\n",
      "Epoch 48/300 - Train Loss: 0.0781, Val Loss: 0.0705\n",
      "Epoch 49/300 - Train Loss: 0.0768, Val Loss: 0.0713\n",
      "Epoch 50/300 - Train Loss: 0.0750, Val Loss: 0.0786\n",
      "Epoch 51/300 - Train Loss: 0.0782, Val Loss: 0.0824\n",
      "Epoch 52/300 - Train Loss: 0.0766, Val Loss: 0.0710\n",
      "Epoch 53/300 - Train Loss: 0.0779, Val Loss: 0.0686\n",
      "Epoch 54/300 - Train Loss: 0.0770, Val Loss: 0.0716\n",
      "Epoch 55/300 - Train Loss: 0.0744, Val Loss: 0.0679\n",
      "Epoch 56/300 - Train Loss: 0.0762, Val Loss: 0.0687\n",
      "Epoch 57/300 - Train Loss: 0.0775, Val Loss: 0.0723\n",
      "Epoch 58/300 - Train Loss: 0.0728, Val Loss: 0.0673\n",
      "Epoch 59/300 - Train Loss: 0.0742, Val Loss: 0.0700\n",
      "Epoch 60/300 - Train Loss: 0.0736, Val Loss: 0.0697\n",
      "Epoch 61/300 - Train Loss: 0.0740, Val Loss: 0.0745\n",
      "Epoch 62/300 - Train Loss: 0.0743, Val Loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:18:29,540] Trial 207 finished with value: 0.9623333836793434 and parameters: {'F1': 16, 'F2': 16, 'D': 4, 'dropout': 0.23659275283430847, 'learning_rate': 6.26354329132869e-05, 'batch_size': 32, 'weight_decay': 0.0001414480367303363}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300 - Train Loss: 0.0728, Val Loss: 0.0704\n",
      "Early stopping at epoch 63\n",
      "Macro F1 Score: 0.9623, Macro Precision: 0.9544, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 209\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2766122214736869, LR=5.3944371410453625e-05, BS=32, WD=0.00018448301186140548\n",
      "Epoch 1/300 - Train Loss: 0.3342, Val Loss: 0.1992\n",
      "Epoch 2/300 - Train Loss: 0.1789, Val Loss: 0.1201\n",
      "Epoch 3/300 - Train Loss: 0.1227, Val Loss: 0.0899\n",
      "Epoch 4/300 - Train Loss: 0.1057, Val Loss: 0.0864\n",
      "Epoch 5/300 - Train Loss: 0.1007, Val Loss: 0.0792\n",
      "Epoch 6/300 - Train Loss: 0.0980, Val Loss: 0.0838\n",
      "Epoch 7/300 - Train Loss: 0.0952, Val Loss: 0.0775\n",
      "Epoch 8/300 - Train Loss: 0.0960, Val Loss: 0.0705\n",
      "Epoch 9/300 - Train Loss: 0.0921, Val Loss: 0.0756\n",
      "Epoch 10/300 - Train Loss: 0.0890, Val Loss: 0.0718\n",
      "Epoch 11/300 - Train Loss: 0.0886, Val Loss: 0.0781\n",
      "Epoch 12/300 - Train Loss: 0.0908, Val Loss: 0.1013\n",
      "Epoch 13/300 - Train Loss: 0.0882, Val Loss: 0.0756\n",
      "Epoch 14/300 - Train Loss: 0.0858, Val Loss: 0.0786\n",
      "Epoch 15/300 - Train Loss: 0.0859, Val Loss: 0.0689\n",
      "Epoch 16/300 - Train Loss: 0.0840, Val Loss: 0.0748\n",
      "Epoch 17/300 - Train Loss: 0.0824, Val Loss: 0.0750\n",
      "Epoch 18/300 - Train Loss: 0.0847, Val Loss: 0.0722\n",
      "Epoch 19/300 - Train Loss: 0.0833, Val Loss: 0.0741\n",
      "Epoch 20/300 - Train Loss: 0.0833, Val Loss: 0.0752\n",
      "Epoch 21/300 - Train Loss: 0.0798, Val Loss: 0.0719\n",
      "Epoch 22/300 - Train Loss: 0.0808, Val Loss: 0.0698\n",
      "Epoch 23/300 - Train Loss: 0.0804, Val Loss: 0.0735\n",
      "Epoch 24/300 - Train Loss: 0.0799, Val Loss: 0.0735\n",
      "Epoch 25/300 - Train Loss: 0.0804, Val Loss: 0.0693\n",
      "Epoch 26/300 - Train Loss: 0.0793, Val Loss: 0.0690\n",
      "Epoch 27/300 - Train Loss: 0.0784, Val Loss: 0.0701\n",
      "Epoch 28/300 - Train Loss: 0.0794, Val Loss: 0.0688\n",
      "Epoch 29/300 - Train Loss: 0.0780, Val Loss: 0.0678\n",
      "Epoch 30/300 - Train Loss: 0.0763, Val Loss: 0.0683\n",
      "Epoch 31/300 - Train Loss: 0.0789, Val Loss: 0.0744\n",
      "Epoch 32/300 - Train Loss: 0.0763, Val Loss: 0.0683\n",
      "Epoch 33/300 - Train Loss: 0.0777, Val Loss: 0.0715\n",
      "Epoch 34/300 - Train Loss: 0.0760, Val Loss: 0.0718\n",
      "Epoch 35/300 - Train Loss: 0.0761, Val Loss: 0.0721\n",
      "Epoch 36/300 - Train Loss: 0.0747, Val Loss: 0.0712\n",
      "Epoch 37/300 - Train Loss: 0.0770, Val Loss: 0.0729\n",
      "Epoch 38/300 - Train Loss: 0.0753, Val Loss: 0.0676\n",
      "Epoch 39/300 - Train Loss: 0.0760, Val Loss: 0.0656\n",
      "Epoch 40/300 - Train Loss: 0.0740, Val Loss: 0.0680\n",
      "Epoch 41/300 - Train Loss: 0.0726, Val Loss: 0.0669\n",
      "Epoch 42/300 - Train Loss: 0.0746, Val Loss: 0.0692\n",
      "Epoch 43/300 - Train Loss: 0.0722, Val Loss: 0.0668\n",
      "Epoch 44/300 - Train Loss: 0.0711, Val Loss: 0.0704\n",
      "Epoch 45/300 - Train Loss: 0.0711, Val Loss: 0.0732\n",
      "Epoch 46/300 - Train Loss: 0.0719, Val Loss: 0.0735\n",
      "Epoch 47/300 - Train Loss: 0.0731, Val Loss: 0.0693\n",
      "Epoch 48/300 - Train Loss: 0.0703, Val Loss: 0.0635\n",
      "Epoch 49/300 - Train Loss: 0.0717, Val Loss: 0.0684\n",
      "Epoch 50/300 - Train Loss: 0.0738, Val Loss: 0.0719\n",
      "Epoch 51/300 - Train Loss: 0.0706, Val Loss: 0.0755\n",
      "Epoch 52/300 - Train Loss: 0.0701, Val Loss: 0.0678\n",
      "Epoch 53/300 - Train Loss: 0.0669, Val Loss: 0.0640\n",
      "Epoch 54/300 - Train Loss: 0.0695, Val Loss: 0.0660\n",
      "Epoch 55/300 - Train Loss: 0.0701, Val Loss: 0.0650\n",
      "Epoch 56/300 - Train Loss: 0.0684, Val Loss: 0.0685\n",
      "Epoch 57/300 - Train Loss: 0.0702, Val Loss: 0.0654\n",
      "Epoch 58/300 - Train Loss: 0.0689, Val Loss: 0.0791\n",
      "Epoch 59/300 - Train Loss: 0.0671, Val Loss: 0.0672\n",
      "Epoch 60/300 - Train Loss: 0.0661, Val Loss: 0.0674\n",
      "Epoch 61/300 - Train Loss: 0.0681, Val Loss: 0.0681\n",
      "Epoch 62/300 - Train Loss: 0.0673, Val Loss: 0.0681\n",
      "Epoch 63/300 - Train Loss: 0.0641, Val Loss: 0.0706\n",
      "Epoch 64/300 - Train Loss: 0.0665, Val Loss: 0.0646\n",
      "Epoch 65/300 - Train Loss: 0.0684, Val Loss: 0.0648\n",
      "Epoch 66/300 - Train Loss: 0.0665, Val Loss: 0.0666\n",
      "Epoch 67/300 - Train Loss: 0.0681, Val Loss: 0.0704\n",
      "Epoch 68/300 - Train Loss: 0.0653, Val Loss: 0.0704\n",
      "Epoch 69/300 - Train Loss: 0.0665, Val Loss: 0.0662\n",
      "Epoch 70/300 - Train Loss: 0.0660, Val Loss: 0.0668\n",
      "Epoch 71/300 - Train Loss: 0.0676, Val Loss: 0.0639\n",
      "Epoch 72/300 - Train Loss: 0.0663, Val Loss: 0.0665\n",
      "Epoch 73/300 - Train Loss: 0.0659, Val Loss: 0.0670\n",
      "Epoch 74/300 - Train Loss: 0.0666, Val Loss: 0.0643\n",
      "Epoch 75/300 - Train Loss: 0.0632, Val Loss: 0.0704\n",
      "Epoch 76/300 - Train Loss: 0.0644, Val Loss: 0.0672\n",
      "Epoch 77/300 - Train Loss: 0.0640, Val Loss: 0.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:23:29,486] Trial 208 finished with value: 0.9689365032151303 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2766122214736869, 'learning_rate': 5.3944371410453625e-05, 'batch_size': 32, 'weight_decay': 0.00018448301186140548}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300 - Train Loss: 0.0642, Val Loss: 0.0648\n",
      "Early stopping at epoch 78\n",
      "Macro F1 Score: 0.9689, Macro Precision: 0.9568, Macro Recall: 0.9824\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 210\n",
      "Training with F1=8, F2=32, D=8, dropout=0.2526726350768936, LR=7.33607198062128e-05, BS=32, WD=0.0002466353905917411\n",
      "Epoch 1/300 - Train Loss: 0.3346, Val Loss: 0.1742\n",
      "Epoch 2/300 - Train Loss: 0.1681, Val Loss: 0.1160\n",
      "Epoch 3/300 - Train Loss: 0.1280, Val Loss: 0.0968\n",
      "Epoch 4/300 - Train Loss: 0.1117, Val Loss: 0.0913\n",
      "Epoch 5/300 - Train Loss: 0.1072, Val Loss: 0.0884\n",
      "Epoch 6/300 - Train Loss: 0.1004, Val Loss: 0.0776\n",
      "Epoch 7/300 - Train Loss: 0.1003, Val Loss: 0.0792\n",
      "Epoch 8/300 - Train Loss: 0.0957, Val Loss: 0.0808\n",
      "Epoch 9/300 - Train Loss: 0.0938, Val Loss: 0.0775\n",
      "Epoch 10/300 - Train Loss: 0.0922, Val Loss: 0.0749\n",
      "Epoch 11/300 - Train Loss: 0.0911, Val Loss: 0.0733\n",
      "Epoch 12/300 - Train Loss: 0.0901, Val Loss: 0.0747\n",
      "Epoch 13/300 - Train Loss: 0.0875, Val Loss: 0.0733\n",
      "Epoch 14/300 - Train Loss: 0.0881, Val Loss: 0.0707\n",
      "Epoch 15/300 - Train Loss: 0.0858, Val Loss: 0.0782\n",
      "Epoch 16/300 - Train Loss: 0.0864, Val Loss: 0.0746\n",
      "Epoch 17/300 - Train Loss: 0.0846, Val Loss: 0.0778\n",
      "Epoch 18/300 - Train Loss: 0.0852, Val Loss: 0.0730\n",
      "Epoch 19/300 - Train Loss: 0.0846, Val Loss: 0.0726\n",
      "Epoch 20/300 - Train Loss: 0.0847, Val Loss: 0.0753\n",
      "Epoch 21/300 - Train Loss: 0.0841, Val Loss: 0.0744\n",
      "Epoch 22/300 - Train Loss: 0.0812, Val Loss: 0.0750\n",
      "Epoch 23/300 - Train Loss: 0.0815, Val Loss: 0.0668\n",
      "Epoch 24/300 - Train Loss: 0.0798, Val Loss: 0.0672\n",
      "Epoch 25/300 - Train Loss: 0.0801, Val Loss: 0.0713\n",
      "Epoch 26/300 - Train Loss: 0.0819, Val Loss: 0.0686\n",
      "Epoch 27/300 - Train Loss: 0.0798, Val Loss: 0.0710\n",
      "Epoch 28/300 - Train Loss: 0.0793, Val Loss: 0.0720\n",
      "Epoch 29/300 - Train Loss: 0.0788, Val Loss: 0.0713\n",
      "Epoch 30/300 - Train Loss: 0.0771, Val Loss: 0.0692\n",
      "Epoch 31/300 - Train Loss: 0.0773, Val Loss: 0.0730\n",
      "Epoch 32/300 - Train Loss: 0.0775, Val Loss: 0.0693\n",
      "Epoch 33/300 - Train Loss: 0.0767, Val Loss: 0.0700\n",
      "Epoch 34/300 - Train Loss: 0.0803, Val Loss: 0.0681\n",
      "Epoch 35/300 - Train Loss: 0.0744, Val Loss: 0.0710\n",
      "Epoch 36/300 - Train Loss: 0.0768, Val Loss: 0.0768\n",
      "Epoch 37/300 - Train Loss: 0.0746, Val Loss: 0.0737\n",
      "Epoch 38/300 - Train Loss: 0.0775, Val Loss: 0.0681\n",
      "Epoch 39/300 - Train Loss: 0.0751, Val Loss: 0.0709\n",
      "Epoch 40/300 - Train Loss: 0.0766, Val Loss: 0.0696\n",
      "Epoch 41/300 - Train Loss: 0.0748, Val Loss: 0.0705\n",
      "Epoch 42/300 - Train Loss: 0.0753, Val Loss: 0.0708\n",
      "Epoch 43/300 - Train Loss: 0.0726, Val Loss: 0.0713\n",
      "Epoch 44/300 - Train Loss: 0.0761, Val Loss: 0.0724\n",
      "Epoch 45/300 - Train Loss: 0.0727, Val Loss: 0.0671\n",
      "Epoch 46/300 - Train Loss: 0.0726, Val Loss: 0.0685\n",
      "Epoch 47/300 - Train Loss: 0.0717, Val Loss: 0.0716\n",
      "Epoch 48/300 - Train Loss: 0.0744, Val Loss: 0.0678\n",
      "Epoch 49/300 - Train Loss: 0.0734, Val Loss: 0.0706\n",
      "Epoch 50/300 - Train Loss: 0.0708, Val Loss: 0.0703\n",
      "Epoch 51/300 - Train Loss: 0.0707, Val Loss: 0.0682\n",
      "Epoch 52/300 - Train Loss: 0.0729, Val Loss: 0.0706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:25:36,948] Trial 209 finished with value: 0.9650386408841904 and parameters: {'F1': 8, 'F2': 32, 'D': 8, 'dropout': 0.2526726350768936, 'learning_rate': 7.33607198062128e-05, 'batch_size': 32, 'weight_decay': 0.0002466353905917411}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300 - Train Loss: 0.0736, Val Loss: 0.0685\n",
      "Early stopping at epoch 53\n",
      "Macro F1 Score: 0.9650, Macro Precision: 0.9634, Macro Recall: 0.9668\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 211\n",
      "Training with F1=16, F2=32, D=4, dropout=0.30260749155851385, LR=0.00023496120723625572, BS=256, WD=0.00039765473949030563\n",
      "Epoch 1/300 - Train Loss: 0.4144, Val Loss: 0.2099\n",
      "Epoch 2/300 - Train Loss: 0.1802, Val Loss: 0.1521\n",
      "Epoch 3/300 - Train Loss: 0.1408, Val Loss: 0.1246\n",
      "Epoch 4/300 - Train Loss: 0.1119, Val Loss: 0.0888\n",
      "Epoch 5/300 - Train Loss: 0.0988, Val Loss: 0.0854\n",
      "Epoch 6/300 - Train Loss: 0.0940, Val Loss: 0.0774\n",
      "Epoch 7/300 - Train Loss: 0.0892, Val Loss: 0.0832\n",
      "Epoch 8/300 - Train Loss: 0.0872, Val Loss: 0.0840\n",
      "Epoch 9/300 - Train Loss: 0.0852, Val Loss: 0.0729\n",
      "Epoch 10/300 - Train Loss: 0.0832, Val Loss: 0.0805\n",
      "Epoch 11/300 - Train Loss: 0.0811, Val Loss: 0.0788\n",
      "Epoch 12/300 - Train Loss: 0.0823, Val Loss: 0.0713\n",
      "Epoch 13/300 - Train Loss: 0.0798, Val Loss: 0.0753\n",
      "Epoch 14/300 - Train Loss: 0.0777, Val Loss: 0.0736\n",
      "Epoch 15/300 - Train Loss: 0.0791, Val Loss: 0.0728\n",
      "Epoch 16/300 - Train Loss: 0.0759, Val Loss: 0.0751\n",
      "Epoch 17/300 - Train Loss: 0.0771, Val Loss: 0.0709\n",
      "Epoch 18/300 - Train Loss: 0.0766, Val Loss: 0.0744\n",
      "Epoch 19/300 - Train Loss: 0.0785, Val Loss: 0.0697\n",
      "Epoch 20/300 - Train Loss: 0.0740, Val Loss: 0.0695\n",
      "Epoch 21/300 - Train Loss: 0.0747, Val Loss: 0.0716\n",
      "Epoch 22/300 - Train Loss: 0.0750, Val Loss: 0.0701\n",
      "Epoch 23/300 - Train Loss: 0.0739, Val Loss: 0.0708\n",
      "Epoch 24/300 - Train Loss: 0.0752, Val Loss: 0.0699\n",
      "Epoch 25/300 - Train Loss: 0.0748, Val Loss: 0.0714\n",
      "Epoch 26/300 - Train Loss: 0.0723, Val Loss: 0.0690\n",
      "Epoch 27/300 - Train Loss: 0.0735, Val Loss: 0.0718\n",
      "Epoch 28/300 - Train Loss: 0.0718, Val Loss: 0.0695\n",
      "Epoch 29/300 - Train Loss: 0.0717, Val Loss: 0.0714\n",
      "Epoch 30/300 - Train Loss: 0.0714, Val Loss: 0.0707\n",
      "Epoch 31/300 - Train Loss: 0.0711, Val Loss: 0.0714\n",
      "Epoch 32/300 - Train Loss: 0.0712, Val Loss: 0.0693\n",
      "Epoch 33/300 - Train Loss: 0.0705, Val Loss: 0.0698\n",
      "Epoch 34/300 - Train Loss: 0.0700, Val Loss: 0.0704\n",
      "Epoch 35/300 - Train Loss: 0.0699, Val Loss: 0.0704\n",
      "Epoch 36/300 - Train Loss: 0.0688, Val Loss: 0.0681\n",
      "Epoch 37/300 - Train Loss: 0.0703, Val Loss: 0.0693\n",
      "Epoch 38/300 - Train Loss: 0.0699, Val Loss: 0.0698\n",
      "Epoch 39/300 - Train Loss: 0.0686, Val Loss: 0.0684\n",
      "Epoch 40/300 - Train Loss: 0.0694, Val Loss: 0.0711\n",
      "Epoch 41/300 - Train Loss: 0.0690, Val Loss: 0.0657\n",
      "Epoch 42/300 - Train Loss: 0.0687, Val Loss: 0.0700\n",
      "Epoch 43/300 - Train Loss: 0.0676, Val Loss: 0.0715\n",
      "Epoch 44/300 - Train Loss: 0.0689, Val Loss: 0.0700\n",
      "Epoch 45/300 - Train Loss: 0.0675, Val Loss: 0.0691\n",
      "Epoch 46/300 - Train Loss: 0.0669, Val Loss: 0.0674\n",
      "Epoch 47/300 - Train Loss: 0.0659, Val Loss: 0.0668\n",
      "Epoch 48/300 - Train Loss: 0.0671, Val Loss: 0.0716\n",
      "Epoch 49/300 - Train Loss: 0.0643, Val Loss: 0.0693\n",
      "Epoch 50/300 - Train Loss: 0.0667, Val Loss: 0.0710\n",
      "Epoch 51/300 - Train Loss: 0.0667, Val Loss: 0.0692\n",
      "Epoch 52/300 - Train Loss: 0.0666, Val Loss: 0.0683\n",
      "Epoch 53/300 - Train Loss: 0.0665, Val Loss: 0.0682\n",
      "Epoch 54/300 - Train Loss: 0.0650, Val Loss: 0.0696\n",
      "Epoch 55/300 - Train Loss: 0.0659, Val Loss: 0.0679\n",
      "Epoch 56/300 - Train Loss: 0.0671, Val Loss: 0.0680\n",
      "Epoch 57/300 - Train Loss: 0.0648, Val Loss: 0.0671\n",
      "Epoch 58/300 - Train Loss: 0.0637, Val Loss: 0.0675\n",
      "Epoch 59/300 - Train Loss: 0.0648, Val Loss: 0.0696\n",
      "Epoch 60/300 - Train Loss: 0.0637, Val Loss: 0.0697\n",
      "Epoch 61/300 - Train Loss: 0.0631, Val Loss: 0.0683\n",
      "Epoch 62/300 - Train Loss: 0.0637, Val Loss: 0.0711\n",
      "Epoch 63/300 - Train Loss: 0.0627, Val Loss: 0.0685\n",
      "Epoch 64/300 - Train Loss: 0.0630, Val Loss: 0.0715\n",
      "Epoch 65/300 - Train Loss: 0.0631, Val Loss: 0.0702\n",
      "Epoch 66/300 - Train Loss: 0.0644, Val Loss: 0.0696\n",
      "Epoch 67/300 - Train Loss: 0.0636, Val Loss: 0.0685\n",
      "Epoch 68/300 - Train Loss: 0.0637, Val Loss: 0.0707\n",
      "Epoch 69/300 - Train Loss: 0.0628, Val Loss: 0.0668\n",
      "Epoch 70/300 - Train Loss: 0.0629, Val Loss: 0.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:27:51,226] Trial 210 finished with value: 0.9682018979065137 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.30260749155851385, 'learning_rate': 0.00023496120723625572, 'batch_size': 256, 'weight_decay': 0.00039765473949030563}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300 - Train Loss: 0.0638, Val Loss: 0.0702\n",
      "Early stopping at epoch 71\n",
      "Macro F1 Score: 0.9682, Macro Precision: 0.9644, Macro Recall: 0.9722\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 212\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2376378545494338, LR=5.722315622148384e-05, BS=32, WD=0.0002882414911962092\n",
      "Epoch 1/300 - Train Loss: 0.3295, Val Loss: 0.1688\n",
      "Epoch 2/300 - Train Loss: 0.1528, Val Loss: 0.1144\n",
      "Epoch 3/300 - Train Loss: 0.1171, Val Loss: 0.0883\n",
      "Epoch 4/300 - Train Loss: 0.1037, Val Loss: 0.0807\n",
      "Epoch 5/300 - Train Loss: 0.0975, Val Loss: 0.0857\n",
      "Epoch 6/300 - Train Loss: 0.0958, Val Loss: 0.0770\n",
      "Epoch 7/300 - Train Loss: 0.0944, Val Loss: 0.0801\n",
      "Epoch 8/300 - Train Loss: 0.0922, Val Loss: 0.0815\n",
      "Epoch 9/300 - Train Loss: 0.0888, Val Loss: 0.0820\n",
      "Epoch 10/300 - Train Loss: 0.0856, Val Loss: 0.0771\n",
      "Epoch 11/300 - Train Loss: 0.0863, Val Loss: 0.0753\n",
      "Epoch 12/300 - Train Loss: 0.0828, Val Loss: 0.0695\n",
      "Epoch 13/300 - Train Loss: 0.0856, Val Loss: 0.0689\n",
      "Epoch 14/300 - Train Loss: 0.0838, Val Loss: 0.0712\n",
      "Epoch 15/300 - Train Loss: 0.0832, Val Loss: 0.0815\n",
      "Epoch 16/300 - Train Loss: 0.0824, Val Loss: 0.0722\n",
      "Epoch 17/300 - Train Loss: 0.0809, Val Loss: 0.0693\n",
      "Epoch 18/300 - Train Loss: 0.0824, Val Loss: 0.0736\n",
      "Epoch 19/300 - Train Loss: 0.0796, Val Loss: 0.0694\n",
      "Epoch 20/300 - Train Loss: 0.0794, Val Loss: 0.0701\n",
      "Epoch 21/300 - Train Loss: 0.0779, Val Loss: 0.0779\n",
      "Epoch 22/300 - Train Loss: 0.0765, Val Loss: 0.0729\n",
      "Epoch 23/300 - Train Loss: 0.0772, Val Loss: 0.0652\n",
      "Epoch 24/300 - Train Loss: 0.0765, Val Loss: 0.0818\n",
      "Epoch 25/300 - Train Loss: 0.0756, Val Loss: 0.0676\n",
      "Epoch 26/300 - Train Loss: 0.0763, Val Loss: 0.0665\n",
      "Epoch 27/300 - Train Loss: 0.0748, Val Loss: 0.0727\n",
      "Epoch 28/300 - Train Loss: 0.0758, Val Loss: 0.0680\n",
      "Epoch 29/300 - Train Loss: 0.0752, Val Loss: 0.0652\n",
      "Epoch 30/300 - Train Loss: 0.0766, Val Loss: 0.0708\n",
      "Epoch 31/300 - Train Loss: 0.0757, Val Loss: 0.0707\n",
      "Epoch 32/300 - Train Loss: 0.0740, Val Loss: 0.0667\n",
      "Epoch 33/300 - Train Loss: 0.0727, Val Loss: 0.0732\n",
      "Epoch 34/300 - Train Loss: 0.0711, Val Loss: 0.0777\n",
      "Epoch 35/300 - Train Loss: 0.0729, Val Loss: 0.0647\n",
      "Epoch 36/300 - Train Loss: 0.0722, Val Loss: 0.0649\n",
      "Epoch 37/300 - Train Loss: 0.0691, Val Loss: 0.0702\n",
      "Epoch 38/300 - Train Loss: 0.0721, Val Loss: 0.0654\n",
      "Epoch 39/300 - Train Loss: 0.0701, Val Loss: 0.0659\n",
      "Epoch 40/300 - Train Loss: 0.0679, Val Loss: 0.0741\n",
      "Epoch 41/300 - Train Loss: 0.0695, Val Loss: 0.0654\n",
      "Epoch 42/300 - Train Loss: 0.0705, Val Loss: 0.0666\n",
      "Epoch 43/300 - Train Loss: 0.0689, Val Loss: 0.0681\n",
      "Epoch 44/300 - Train Loss: 0.0676, Val Loss: 0.0660\n",
      "Epoch 45/300 - Train Loss: 0.0679, Val Loss: 0.0670\n",
      "Epoch 46/300 - Train Loss: 0.0662, Val Loss: 0.0660\n",
      "Epoch 47/300 - Train Loss: 0.0675, Val Loss: 0.0672\n",
      "Epoch 48/300 - Train Loss: 0.0687, Val Loss: 0.0654\n",
      "Epoch 49/300 - Train Loss: 0.0670, Val Loss: 0.0684\n",
      "Epoch 50/300 - Train Loss: 0.0669, Val Loss: 0.0648\n",
      "Epoch 51/300 - Train Loss: 0.0663, Val Loss: 0.0674\n",
      "Epoch 52/300 - Train Loss: 0.0662, Val Loss: 0.0689\n",
      "Epoch 53/300 - Train Loss: 0.0642, Val Loss: 0.0674\n",
      "Epoch 54/300 - Train Loss: 0.0663, Val Loss: 0.0663\n",
      "Epoch 55/300 - Train Loss: 0.0650, Val Loss: 0.0662\n",
      "Epoch 56/300 - Train Loss: 0.0638, Val Loss: 0.0662\n",
      "Epoch 57/300 - Train Loss: 0.0642, Val Loss: 0.0668\n",
      "Epoch 58/300 - Train Loss: 0.0625, Val Loss: 0.0649\n",
      "Epoch 59/300 - Train Loss: 0.0635, Val Loss: 0.0669\n",
      "Epoch 60/300 - Train Loss: 0.0623, Val Loss: 0.0656\n",
      "Epoch 61/300 - Train Loss: 0.0648, Val Loss: 0.0662\n",
      "Epoch 62/300 - Train Loss: 0.0636, Val Loss: 0.0676\n",
      "Epoch 63/300 - Train Loss: 0.0636, Val Loss: 0.0651\n",
      "Epoch 64/300 - Train Loss: 0.0652, Val Loss: 0.0685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:32:01,206] Trial 211 finished with value: 0.9695349040099591 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2376378545494338, 'learning_rate': 5.722315622148384e-05, 'batch_size': 32, 'weight_decay': 0.0002882414911962092}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300 - Train Loss: 0.0618, Val Loss: 0.0659\n",
      "Early stopping at epoch 65\n",
      "Macro F1 Score: 0.9695, Macro Precision: 0.9683, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 213\n",
      "Training with F1=16, F2=32, D=8, dropout=0.22459165632318787, LR=4.510643816065694e-05, BS=32, WD=0.0002546412813219155\n",
      "Epoch 1/300 - Train Loss: 0.3481, Val Loss: 0.1654\n",
      "Epoch 2/300 - Train Loss: 0.1611, Val Loss: 0.1245\n",
      "Epoch 3/300 - Train Loss: 0.1275, Val Loss: 0.0998\n",
      "Epoch 4/300 - Train Loss: 0.1131, Val Loss: 0.0921\n",
      "Epoch 5/300 - Train Loss: 0.1083, Val Loss: 0.0900\n",
      "Epoch 6/300 - Train Loss: 0.1034, Val Loss: 0.0884\n",
      "Epoch 7/300 - Train Loss: 0.1017, Val Loss: 0.0792\n",
      "Epoch 8/300 - Train Loss: 0.0993, Val Loss: 0.0778\n",
      "Epoch 9/300 - Train Loss: 0.0968, Val Loss: 0.0824\n",
      "Epoch 10/300 - Train Loss: 0.0909, Val Loss: 0.0788\n",
      "Epoch 11/300 - Train Loss: 0.0956, Val Loss: 0.0774\n",
      "Epoch 12/300 - Train Loss: 0.0906, Val Loss: 0.0743\n",
      "Epoch 13/300 - Train Loss: 0.0889, Val Loss: 0.0764\n",
      "Epoch 14/300 - Train Loss: 0.0903, Val Loss: 0.0773\n",
      "Epoch 15/300 - Train Loss: 0.0880, Val Loss: 0.0733\n",
      "Epoch 16/300 - Train Loss: 0.0852, Val Loss: 0.0760\n",
      "Epoch 17/300 - Train Loss: 0.0870, Val Loss: 0.0774\n",
      "Epoch 18/300 - Train Loss: 0.0858, Val Loss: 0.0725\n",
      "Epoch 19/300 - Train Loss: 0.0880, Val Loss: 0.0744\n",
      "Epoch 20/300 - Train Loss: 0.0856, Val Loss: 0.0731\n",
      "Epoch 21/300 - Train Loss: 0.0818, Val Loss: 0.0710\n",
      "Epoch 22/300 - Train Loss: 0.0835, Val Loss: 0.0732\n",
      "Epoch 23/300 - Train Loss: 0.0830, Val Loss: 0.0725\n",
      "Epoch 24/300 - Train Loss: 0.0794, Val Loss: 0.0742\n",
      "Epoch 25/300 - Train Loss: 0.0804, Val Loss: 0.0748\n",
      "Epoch 26/300 - Train Loss: 0.0806, Val Loss: 0.0743\n",
      "Epoch 27/300 - Train Loss: 0.0789, Val Loss: 0.0733\n",
      "Epoch 28/300 - Train Loss: 0.0785, Val Loss: 0.0692\n",
      "Epoch 29/300 - Train Loss: 0.0808, Val Loss: 0.0744\n",
      "Epoch 30/300 - Train Loss: 0.0775, Val Loss: 0.0678\n",
      "Epoch 31/300 - Train Loss: 0.0768, Val Loss: 0.0737\n",
      "Epoch 32/300 - Train Loss: 0.0758, Val Loss: 0.0711\n",
      "Epoch 33/300 - Train Loss: 0.0757, Val Loss: 0.0701\n",
      "Epoch 34/300 - Train Loss: 0.0743, Val Loss: 0.0711\n",
      "Epoch 35/300 - Train Loss: 0.0764, Val Loss: 0.0703\n",
      "Epoch 36/300 - Train Loss: 0.0757, Val Loss: 0.0694\n",
      "Epoch 37/300 - Train Loss: 0.0746, Val Loss: 0.0693\n",
      "Epoch 38/300 - Train Loss: 0.0747, Val Loss: 0.0687\n",
      "Epoch 39/300 - Train Loss: 0.0733, Val Loss: 0.0694\n",
      "Epoch 40/300 - Train Loss: 0.0720, Val Loss: 0.0662\n",
      "Epoch 41/300 - Train Loss: 0.0735, Val Loss: 0.0683\n",
      "Epoch 42/300 - Train Loss: 0.0742, Val Loss: 0.0693\n",
      "Epoch 43/300 - Train Loss: 0.0723, Val Loss: 0.0704\n",
      "Epoch 44/300 - Train Loss: 0.0731, Val Loss: 0.0699\n",
      "Epoch 45/300 - Train Loss: 0.0729, Val Loss: 0.0677\n",
      "Epoch 46/300 - Train Loss: 0.0728, Val Loss: 0.0672\n",
      "Epoch 47/300 - Train Loss: 0.0728, Val Loss: 0.0680\n",
      "Epoch 48/300 - Train Loss: 0.0704, Val Loss: 0.0705\n",
      "Epoch 49/300 - Train Loss: 0.0736, Val Loss: 0.0681\n",
      "Epoch 50/300 - Train Loss: 0.0694, Val Loss: 0.0687\n",
      "Epoch 51/300 - Train Loss: 0.0709, Val Loss: 0.0686\n",
      "Epoch 52/300 - Train Loss: 0.0698, Val Loss: 0.0679\n",
      "Epoch 53/300 - Train Loss: 0.0687, Val Loss: 0.0654\n",
      "Epoch 54/300 - Train Loss: 0.0699, Val Loss: 0.0693\n",
      "Epoch 55/300 - Train Loss: 0.0718, Val Loss: 0.0685\n",
      "Epoch 56/300 - Train Loss: 0.0697, Val Loss: 0.0694\n",
      "Epoch 57/300 - Train Loss: 0.0682, Val Loss: 0.0692\n",
      "Epoch 58/300 - Train Loss: 0.0688, Val Loss: 0.0686\n",
      "Epoch 59/300 - Train Loss: 0.0673, Val Loss: 0.0667\n",
      "Epoch 60/300 - Train Loss: 0.0697, Val Loss: 0.0685\n",
      "Epoch 61/300 - Train Loss: 0.0658, Val Loss: 0.0656\n",
      "Epoch 62/300 - Train Loss: 0.0655, Val Loss: 0.0658\n",
      "Epoch 63/300 - Train Loss: 0.0651, Val Loss: 0.0698\n",
      "Epoch 64/300 - Train Loss: 0.0682, Val Loss: 0.0662\n",
      "Epoch 65/300 - Train Loss: 0.0671, Val Loss: 0.0677\n",
      "Epoch 66/300 - Train Loss: 0.0687, Val Loss: 0.0677\n",
      "Epoch 67/300 - Train Loss: 0.0655, Val Loss: 0.0720\n",
      "Epoch 68/300 - Train Loss: 0.0673, Val Loss: 0.0768\n",
      "Epoch 69/300 - Train Loss: 0.0666, Val Loss: 0.0672\n",
      "Epoch 70/300 - Train Loss: 0.0651, Val Loss: 0.0664\n",
      "Epoch 71/300 - Train Loss: 0.0660, Val Loss: 0.0664\n",
      "Epoch 72/300 - Train Loss: 0.0648, Val Loss: 0.0681\n",
      "Epoch 73/300 - Train Loss: 0.0657, Val Loss: 0.0657\n",
      "Epoch 74/300 - Train Loss: 0.0630, Val Loss: 0.0699\n",
      "Epoch 75/300 - Train Loss: 0.0628, Val Loss: 0.0729\n",
      "Epoch 76/300 - Train Loss: 0.0648, Val Loss: 0.0708\n",
      "Epoch 77/300 - Train Loss: 0.0660, Val Loss: 0.0633\n",
      "Epoch 78/300 - Train Loss: 0.0620, Val Loss: 0.0686\n",
      "Epoch 79/300 - Train Loss: 0.0640, Val Loss: 0.0672\n",
      "Epoch 80/300 - Train Loss: 0.0630, Val Loss: 0.0676\n",
      "Epoch 81/300 - Train Loss: 0.0661, Val Loss: 0.0666\n",
      "Epoch 82/300 - Train Loss: 0.0624, Val Loss: 0.0660\n",
      "Epoch 83/300 - Train Loss: 0.0634, Val Loss: 0.0664\n",
      "Epoch 84/300 - Train Loss: 0.0618, Val Loss: 0.0662\n",
      "Epoch 85/300 - Train Loss: 0.0611, Val Loss: 0.0706\n",
      "Epoch 86/300 - Train Loss: 0.0646, Val Loss: 0.0700\n",
      "Epoch 87/300 - Train Loss: 0.0611, Val Loss: 0.0694\n",
      "Epoch 88/300 - Train Loss: 0.0605, Val Loss: 0.0667\n",
      "Epoch 89/300 - Train Loss: 0.0608, Val Loss: 0.0679\n",
      "Epoch 90/300 - Train Loss: 0.0606, Val Loss: 0.0675\n",
      "Epoch 91/300 - Train Loss: 0.0628, Val Loss: 0.0658\n",
      "Epoch 92/300 - Train Loss: 0.0641, Val Loss: 0.0745\n",
      "Epoch 93/300 - Train Loss: 0.0646, Val Loss: 0.0689\n",
      "Epoch 94/300 - Train Loss: 0.0616, Val Loss: 0.0665\n",
      "Epoch 95/300 - Train Loss: 0.0596, Val Loss: 0.0649\n",
      "Epoch 96/300 - Train Loss: 0.0596, Val Loss: 0.0736\n",
      "Epoch 97/300 - Train Loss: 0.0624, Val Loss: 0.0653\n",
      "Epoch 98/300 - Train Loss: 0.0596, Val Loss: 0.0643\n",
      "Epoch 99/300 - Train Loss: 0.0595, Val Loss: 0.0748\n",
      "Epoch 100/300 - Train Loss: 0.0616, Val Loss: 0.0683\n",
      "Epoch 101/300 - Train Loss: 0.0601, Val Loss: 0.0688\n",
      "Epoch 102/300 - Train Loss: 0.0581, Val Loss: 0.0662\n",
      "Epoch 103/300 - Train Loss: 0.0566, Val Loss: 0.0659\n",
      "Epoch 104/300 - Train Loss: 0.0585, Val Loss: 0.0681\n",
      "Epoch 105/300 - Train Loss: 0.0605, Val Loss: 0.0663\n",
      "Epoch 106/300 - Train Loss: 0.0571, Val Loss: 0.0711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:38:53,022] Trial 212 finished with value: 0.9713149106401352 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.22459165632318787, 'learning_rate': 4.510643816065694e-05, 'batch_size': 32, 'weight_decay': 0.0002546412813219155}. Best is trial 162 with value: 0.9761195541557947.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/300 - Train Loss: 0.0586, Val Loss: 0.0664\n",
      "Early stopping at epoch 107\n",
      "Macro F1 Score: 0.9713, Macro Precision: 0.9575, Macro Recall: 0.9871\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.90      1.00      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.99      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 214\n",
      "Training with F1=16, F2=32, D=8, dropout=0.19590961809488963, LR=9.320667876809647e-05, BS=32, WD=0.0001111853969514484\n",
      "Epoch 1/300 - Train Loss: 0.2692, Val Loss: 0.1449\n",
      "Epoch 2/300 - Train Loss: 0.1178, Val Loss: 0.0828\n",
      "Epoch 3/300 - Train Loss: 0.0996, Val Loss: 0.0788\n",
      "Epoch 4/300 - Train Loss: 0.0947, Val Loss: 0.0970\n",
      "Epoch 5/300 - Train Loss: 0.0917, Val Loss: 0.0807\n",
      "Epoch 6/300 - Train Loss: 0.0879, Val Loss: 0.0809\n",
      "Epoch 7/300 - Train Loss: 0.0858, Val Loss: 0.0797\n",
      "Epoch 8/300 - Train Loss: 0.0851, Val Loss: 0.0682\n",
      "Epoch 9/300 - Train Loss: 0.0830, Val Loss: 0.0703\n",
      "Epoch 10/300 - Train Loss: 0.0814, Val Loss: 0.0741\n",
      "Epoch 11/300 - Train Loss: 0.0812, Val Loss: 0.0789\n",
      "Epoch 12/300 - Train Loss: 0.0799, Val Loss: 0.0709\n",
      "Epoch 13/300 - Train Loss: 0.0788, Val Loss: 0.0718\n",
      "Epoch 14/300 - Train Loss: 0.0779, Val Loss: 0.0719\n",
      "Epoch 15/300 - Train Loss: 0.0786, Val Loss: 0.0697\n",
      "Epoch 16/300 - Train Loss: 0.0759, Val Loss: 0.0708\n",
      "Epoch 17/300 - Train Loss: 0.0756, Val Loss: 0.0671\n",
      "Epoch 18/300 - Train Loss: 0.0730, Val Loss: 0.0704\n",
      "Epoch 19/300 - Train Loss: 0.0753, Val Loss: 0.0676\n",
      "Epoch 20/300 - Train Loss: 0.0725, Val Loss: 0.0710\n",
      "Epoch 21/300 - Train Loss: 0.0716, Val Loss: 0.0653\n",
      "Epoch 22/300 - Train Loss: 0.0725, Val Loss: 0.0672\n",
      "Epoch 23/300 - Train Loss: 0.0726, Val Loss: 0.0662\n",
      "Epoch 24/300 - Train Loss: 0.0711, Val Loss: 0.0712\n",
      "Epoch 25/300 - Train Loss: 0.0709, Val Loss: 0.0780\n",
      "Epoch 26/300 - Train Loss: 0.0698, Val Loss: 0.0711\n",
      "Epoch 27/300 - Train Loss: 0.0688, Val Loss: 0.0688\n",
      "Epoch 28/300 - Train Loss: 0.0680, Val Loss: 0.0653\n",
      "Epoch 29/300 - Train Loss: 0.0697, Val Loss: 0.0689\n",
      "Epoch 30/300 - Train Loss: 0.0685, Val Loss: 0.0763\n",
      "Epoch 31/300 - Train Loss: 0.0660, Val Loss: 0.0691\n",
      "Epoch 32/300 - Train Loss: 0.0665, Val Loss: 0.0719\n",
      "Epoch 33/300 - Train Loss: 0.0636, Val Loss: 0.0645\n",
      "Epoch 34/300 - Train Loss: 0.0645, Val Loss: 0.0631\n",
      "Epoch 35/300 - Train Loss: 0.0637, Val Loss: 0.0740\n",
      "Epoch 36/300 - Train Loss: 0.0661, Val Loss: 0.0681\n",
      "Epoch 37/300 - Train Loss: 0.0646, Val Loss: 0.0705\n",
      "Epoch 38/300 - Train Loss: 0.0638, Val Loss: 0.0671\n",
      "Epoch 39/300 - Train Loss: 0.0642, Val Loss: 0.0691\n",
      "Epoch 40/300 - Train Loss: 0.0649, Val Loss: 0.0647\n",
      "Epoch 41/300 - Train Loss: 0.0624, Val Loss: 0.0652\n",
      "Epoch 42/300 - Train Loss: 0.0627, Val Loss: 0.0685\n",
      "Epoch 43/300 - Train Loss: 0.0617, Val Loss: 0.0675\n",
      "Epoch 44/300 - Train Loss: 0.0601, Val Loss: 0.0702\n",
      "Epoch 45/300 - Train Loss: 0.0616, Val Loss: 0.0644\n",
      "Epoch 46/300 - Train Loss: 0.0607, Val Loss: 0.0671\n",
      "Epoch 47/300 - Train Loss: 0.0619, Val Loss: 0.0664\n",
      "Epoch 48/300 - Train Loss: 0.0609, Val Loss: 0.0663\n",
      "Epoch 49/300 - Train Loss: 0.0630, Val Loss: 0.0683\n",
      "Epoch 50/300 - Train Loss: 0.0579, Val Loss: 0.0657\n",
      "Epoch 51/300 - Train Loss: 0.0582, Val Loss: 0.0691\n",
      "Epoch 52/300 - Train Loss: 0.0597, Val Loss: 0.0688\n",
      "Epoch 53/300 - Train Loss: 0.0586, Val Loss: 0.0658\n",
      "Epoch 54/300 - Train Loss: 0.0605, Val Loss: 0.0663\n",
      "Epoch 55/300 - Train Loss: 0.0560, Val Loss: 0.0670\n",
      "Epoch 56/300 - Train Loss: 0.0581, Val Loss: 0.0659\n",
      "Epoch 57/300 - Train Loss: 0.0564, Val Loss: 0.0691\n",
      "Epoch 58/300 - Train Loss: 0.0563, Val Loss: 0.0691\n",
      "Epoch 59/300 - Train Loss: 0.0566, Val Loss: 0.0658\n",
      "Epoch 60/300 - Train Loss: 0.0570, Val Loss: 0.0651\n",
      "Epoch 61/300 - Train Loss: 0.0530, Val Loss: 0.0673\n",
      "Epoch 62/300 - Train Loss: 0.0546, Val Loss: 0.0671\n",
      "Epoch 63/300 - Train Loss: 0.0576, Val Loss: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:42:59,483] Trial 213 finished with value: 0.979026323031575 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.19590961809488963, 'learning_rate': 9.320667876809647e-05, 'batch_size': 32, 'weight_decay': 0.0001111853969514484}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300 - Train Loss: 0.0550, Val Loss: 0.0679\n",
      "Early stopping at epoch 64\n",
      "Macro F1 Score: 0.9790, Macro Precision: 0.9752, Macro Recall: 0.9831\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.98      0.97        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 215\n",
      "Training with F1=16, F2=32, D=8, dropout=0.17696959183287955, LR=9.643302002485942e-05, BS=32, WD=6.532468468264105e-05\n",
      "Epoch 1/300 - Train Loss: 0.2652, Val Loss: 0.1158\n",
      "Epoch 2/300 - Train Loss: 0.1142, Val Loss: 0.0872\n",
      "Epoch 3/300 - Train Loss: 0.1012, Val Loss: 0.0794\n",
      "Epoch 4/300 - Train Loss: 0.0957, Val Loss: 0.0904\n",
      "Epoch 5/300 - Train Loss: 0.0922, Val Loss: 0.0863\n",
      "Epoch 6/300 - Train Loss: 0.0900, Val Loss: 0.0837\n",
      "Epoch 7/300 - Train Loss: 0.0861, Val Loss: 0.0777\n",
      "Epoch 8/300 - Train Loss: 0.0853, Val Loss: 0.0701\n",
      "Epoch 9/300 - Train Loss: 0.0848, Val Loss: 0.0743\n",
      "Epoch 10/300 - Train Loss: 0.0835, Val Loss: 0.0733\n",
      "Epoch 11/300 - Train Loss: 0.0832, Val Loss: 0.0699\n",
      "Epoch 12/300 - Train Loss: 0.0796, Val Loss: 0.0705\n",
      "Epoch 13/300 - Train Loss: 0.0783, Val Loss: 0.0756\n",
      "Epoch 14/300 - Train Loss: 0.0797, Val Loss: 0.0714\n",
      "Epoch 15/300 - Train Loss: 0.0784, Val Loss: 0.0693\n",
      "Epoch 16/300 - Train Loss: 0.0764, Val Loss: 0.0825\n",
      "Epoch 17/300 - Train Loss: 0.0767, Val Loss: 0.0677\n",
      "Epoch 18/300 - Train Loss: 0.0762, Val Loss: 0.0682\n",
      "Epoch 19/300 - Train Loss: 0.0730, Val Loss: 0.0695\n",
      "Epoch 20/300 - Train Loss: 0.0730, Val Loss: 0.0713\n",
      "Epoch 21/300 - Train Loss: 0.0731, Val Loss: 0.0807\n",
      "Epoch 22/300 - Train Loss: 0.0736, Val Loss: 0.0667\n",
      "Epoch 23/300 - Train Loss: 0.0713, Val Loss: 0.0675\n",
      "Epoch 24/300 - Train Loss: 0.0711, Val Loss: 0.0706\n",
      "Epoch 25/300 - Train Loss: 0.0708, Val Loss: 0.0712\n",
      "Epoch 26/300 - Train Loss: 0.0723, Val Loss: 0.0689\n",
      "Epoch 27/300 - Train Loss: 0.0690, Val Loss: 0.0676\n",
      "Epoch 28/300 - Train Loss: 0.0696, Val Loss: 0.0657\n",
      "Epoch 29/300 - Train Loss: 0.0677, Val Loss: 0.0668\n",
      "Epoch 30/300 - Train Loss: 0.0661, Val Loss: 0.0713\n",
      "Epoch 31/300 - Train Loss: 0.0656, Val Loss: 0.0674\n",
      "Epoch 32/300 - Train Loss: 0.0646, Val Loss: 0.0689\n",
      "Epoch 33/300 - Train Loss: 0.0669, Val Loss: 0.0670\n",
      "Epoch 34/300 - Train Loss: 0.0650, Val Loss: 0.0648\n",
      "Epoch 35/300 - Train Loss: 0.0656, Val Loss: 0.0714\n",
      "Epoch 36/300 - Train Loss: 0.0652, Val Loss: 0.0686\n",
      "Epoch 37/300 - Train Loss: 0.0635, Val Loss: 0.0676\n",
      "Epoch 38/300 - Train Loss: 0.0616, Val Loss: 0.0712\n",
      "Epoch 39/300 - Train Loss: 0.0626, Val Loss: 0.0693\n",
      "Epoch 40/300 - Train Loss: 0.0609, Val Loss: 0.0664\n",
      "Epoch 41/300 - Train Loss: 0.0610, Val Loss: 0.0661\n",
      "Epoch 42/300 - Train Loss: 0.0594, Val Loss: 0.0693\n",
      "Epoch 43/300 - Train Loss: 0.0597, Val Loss: 0.0652\n",
      "Epoch 44/300 - Train Loss: 0.0594, Val Loss: 0.0694\n",
      "Epoch 45/300 - Train Loss: 0.0576, Val Loss: 0.0865\n",
      "Epoch 46/300 - Train Loss: 0.0601, Val Loss: 0.0662\n",
      "Epoch 47/300 - Train Loss: 0.0575, Val Loss: 0.0649\n",
      "Epoch 48/300 - Train Loss: 0.0583, Val Loss: 0.0809\n",
      "Epoch 49/300 - Train Loss: 0.0567, Val Loss: 0.0689\n",
      "Epoch 50/300 - Train Loss: 0.0575, Val Loss: 0.0701\n",
      "Epoch 51/300 - Train Loss: 0.0600, Val Loss: 0.0722\n",
      "Epoch 52/300 - Train Loss: 0.0578, Val Loss: 0.0666\n",
      "Epoch 53/300 - Train Loss: 0.0556, Val Loss: 0.0691\n",
      "Epoch 54/300 - Train Loss: 0.0551, Val Loss: 0.0678\n",
      "Epoch 55/300 - Train Loss: 0.0558, Val Loss: 0.0689\n",
      "Epoch 56/300 - Train Loss: 0.0545, Val Loss: 0.0709\n",
      "Epoch 57/300 - Train Loss: 0.0554, Val Loss: 0.0649\n",
      "Epoch 58/300 - Train Loss: 0.0548, Val Loss: 0.0662\n",
      "Epoch 59/300 - Train Loss: 0.0549, Val Loss: 0.0702\n",
      "Epoch 60/300 - Train Loss: 0.0528, Val Loss: 0.0673\n",
      "Epoch 61/300 - Train Loss: 0.0525, Val Loss: 0.0711\n",
      "Epoch 62/300 - Train Loss: 0.0522, Val Loss: 0.0721\n",
      "Epoch 63/300 - Train Loss: 0.0533, Val Loss: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:47:05,654] Trial 214 finished with value: 0.9671972968981754 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.17696959183287955, 'learning_rate': 9.643302002485942e-05, 'batch_size': 32, 'weight_decay': 6.532468468264105e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300 - Train Loss: 0.0509, Val Loss: 0.0706\n",
      "Early stopping at epoch 64\n",
      "Macro F1 Score: 0.9672, Macro Precision: 0.9594, Macro Recall: 0.9757\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 216\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1934407372684297, LR=8.500962455064873e-05, BS=32, WD=0.00011153509524887242\n",
      "Epoch 1/300 - Train Loss: 0.2640, Val Loss: 0.1369\n",
      "Epoch 2/300 - Train Loss: 0.1159, Val Loss: 0.0838\n",
      "Epoch 3/300 - Train Loss: 0.1021, Val Loss: 0.0825\n",
      "Epoch 4/300 - Train Loss: 0.0960, Val Loss: 0.0725\n",
      "Epoch 5/300 - Train Loss: 0.0941, Val Loss: 0.0746\n",
      "Epoch 6/300 - Train Loss: 0.0920, Val Loss: 0.0790\n",
      "Epoch 7/300 - Train Loss: 0.0900, Val Loss: 0.0813\n",
      "Epoch 8/300 - Train Loss: 0.0881, Val Loss: 0.0754\n",
      "Epoch 9/300 - Train Loss: 0.0874, Val Loss: 0.0798\n",
      "Epoch 10/300 - Train Loss: 0.0844, Val Loss: 0.0777\n",
      "Epoch 11/300 - Train Loss: 0.0889, Val Loss: 0.0682\n",
      "Epoch 12/300 - Train Loss: 0.0835, Val Loss: 0.0769\n",
      "Epoch 13/300 - Train Loss: 0.0813, Val Loss: 0.0744\n",
      "Epoch 14/300 - Train Loss: 0.0816, Val Loss: 0.0742\n",
      "Epoch 15/300 - Train Loss: 0.0797, Val Loss: 0.0744\n",
      "Epoch 16/300 - Train Loss: 0.0783, Val Loss: 0.0713\n",
      "Epoch 17/300 - Train Loss: 0.0787, Val Loss: 0.0689\n",
      "Epoch 18/300 - Train Loss: 0.0776, Val Loss: 0.0703\n",
      "Epoch 19/300 - Train Loss: 0.0766, Val Loss: 0.0641\n",
      "Epoch 20/300 - Train Loss: 0.0734, Val Loss: 0.0720\n",
      "Epoch 21/300 - Train Loss: 0.0730, Val Loss: 0.0731\n",
      "Epoch 22/300 - Train Loss: 0.0746, Val Loss: 0.0740\n",
      "Epoch 23/300 - Train Loss: 0.0728, Val Loss: 0.0696\n",
      "Epoch 24/300 - Train Loss: 0.0701, Val Loss: 0.0641\n",
      "Epoch 25/300 - Train Loss: 0.0722, Val Loss: 0.0674\n",
      "Epoch 26/300 - Train Loss: 0.0713, Val Loss: 0.0683\n",
      "Epoch 27/300 - Train Loss: 0.0699, Val Loss: 0.0678\n",
      "Epoch 28/300 - Train Loss: 0.0704, Val Loss: 0.0680\n",
      "Epoch 29/300 - Train Loss: 0.0697, Val Loss: 0.0652\n",
      "Epoch 30/300 - Train Loss: 0.0703, Val Loss: 0.0677\n",
      "Epoch 31/300 - Train Loss: 0.0680, Val Loss: 0.0675\n",
      "Epoch 32/300 - Train Loss: 0.0679, Val Loss: 0.0764\n",
      "Epoch 33/300 - Train Loss: 0.0686, Val Loss: 0.0674\n",
      "Epoch 34/300 - Train Loss: 0.0660, Val Loss: 0.0649\n",
      "Epoch 35/300 - Train Loss: 0.0662, Val Loss: 0.0655\n",
      "Epoch 36/300 - Train Loss: 0.0657, Val Loss: 0.0754\n",
      "Epoch 37/300 - Train Loss: 0.0660, Val Loss: 0.0686\n",
      "Epoch 38/300 - Train Loss: 0.0671, Val Loss: 0.0716\n",
      "Epoch 39/300 - Train Loss: 0.0656, Val Loss: 0.0652\n",
      "Epoch 40/300 - Train Loss: 0.0639, Val Loss: 0.0629\n",
      "Epoch 41/300 - Train Loss: 0.0648, Val Loss: 0.0666\n",
      "Epoch 42/300 - Train Loss: 0.0642, Val Loss: 0.0713\n",
      "Epoch 43/300 - Train Loss: 0.0609, Val Loss: 0.0692\n",
      "Epoch 44/300 - Train Loss: 0.0666, Val Loss: 0.0685\n",
      "Epoch 45/300 - Train Loss: 0.0616, Val Loss: 0.0642\n",
      "Epoch 46/300 - Train Loss: 0.0635, Val Loss: 0.0692\n",
      "Epoch 47/300 - Train Loss: 0.0634, Val Loss: 0.0632\n",
      "Epoch 48/300 - Train Loss: 0.0590, Val Loss: 0.0672\n",
      "Epoch 49/300 - Train Loss: 0.0582, Val Loss: 0.0682\n",
      "Epoch 50/300 - Train Loss: 0.0600, Val Loss: 0.0684\n",
      "Epoch 51/300 - Train Loss: 0.0596, Val Loss: 0.0639\n",
      "Epoch 52/300 - Train Loss: 0.0589, Val Loss: 0.0655\n",
      "Epoch 53/300 - Train Loss: 0.0586, Val Loss: 0.0705\n",
      "Epoch 54/300 - Train Loss: 0.0582, Val Loss: 0.0637\n",
      "Epoch 55/300 - Train Loss: 0.0579, Val Loss: 0.0677\n",
      "Epoch 56/300 - Train Loss: 0.0578, Val Loss: 0.0649\n",
      "Epoch 57/300 - Train Loss: 0.0574, Val Loss: 0.0673\n",
      "Epoch 58/300 - Train Loss: 0.0568, Val Loss: 0.0664\n",
      "Epoch 59/300 - Train Loss: 0.0569, Val Loss: 0.0688\n",
      "Epoch 60/300 - Train Loss: 0.0566, Val Loss: 0.0644\n",
      "Epoch 61/300 - Train Loss: 0.0566, Val Loss: 0.0673\n",
      "Epoch 62/300 - Train Loss: 0.0556, Val Loss: 0.0653\n",
      "Epoch 63/300 - Train Loss: 0.0546, Val Loss: 0.0699\n",
      "Epoch 64/300 - Train Loss: 0.0548, Val Loss: 0.0660\n",
      "Epoch 65/300 - Train Loss: 0.0546, Val Loss: 0.0675\n",
      "Epoch 66/300 - Train Loss: 0.0528, Val Loss: 0.0638\n",
      "Epoch 67/300 - Train Loss: 0.0554, Val Loss: 0.0701\n",
      "Epoch 68/300 - Train Loss: 0.0537, Val Loss: 0.0699\n",
      "Epoch 69/300 - Train Loss: 0.0551, Val Loss: 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:51:34,859] Trial 215 finished with value: 0.9724084441065574 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1934407372684297, 'learning_rate': 8.500962455064873e-05, 'batch_size': 32, 'weight_decay': 0.00011153509524887242}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/300 - Train Loss: 0.0519, Val Loss: 0.0687\n",
      "Early stopping at epoch 70\n",
      "Macro F1 Score: 0.9724, Macro Precision: 0.9647, Macro Recall: 0.9809\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.98      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 217\n",
      "Training with F1=16, F2=32, D=8, dropout=0.19481967045544976, LR=8.929222061868264e-05, BS=128, WD=0.00010865688138557323\n",
      "Epoch 1/300 - Train Loss: 0.4424, Val Loss: 0.1986\n",
      "Epoch 2/300 - Train Loss: 0.1622, Val Loss: 0.1289\n",
      "Epoch 3/300 - Train Loss: 0.1165, Val Loss: 0.1032\n",
      "Epoch 4/300 - Train Loss: 0.0991, Val Loss: 0.0973\n",
      "Epoch 5/300 - Train Loss: 0.0936, Val Loss: 0.0886\n",
      "Epoch 6/300 - Train Loss: 0.0899, Val Loss: 0.0930\n",
      "Epoch 7/300 - Train Loss: 0.0857, Val Loss: 0.0857\n",
      "Epoch 8/300 - Train Loss: 0.0859, Val Loss: 0.0788\n",
      "Epoch 9/300 - Train Loss: 0.0811, Val Loss: 0.0875\n",
      "Epoch 10/300 - Train Loss: 0.0786, Val Loss: 0.0812\n",
      "Epoch 11/300 - Train Loss: 0.0794, Val Loss: 0.0790\n",
      "Epoch 12/300 - Train Loss: 0.0782, Val Loss: 0.0808\n",
      "Epoch 13/300 - Train Loss: 0.0787, Val Loss: 0.0776\n",
      "Epoch 14/300 - Train Loss: 0.0766, Val Loss: 0.0795\n",
      "Epoch 15/300 - Train Loss: 0.0751, Val Loss: 0.0785\n",
      "Epoch 16/300 - Train Loss: 0.0750, Val Loss: 0.0845\n",
      "Epoch 17/300 - Train Loss: 0.0741, Val Loss: 0.0837\n",
      "Epoch 18/300 - Train Loss: 0.0732, Val Loss: 0.0796\n",
      "Epoch 19/300 - Train Loss: 0.0736, Val Loss: 0.0757\n",
      "Epoch 20/300 - Train Loss: 0.0707, Val Loss: 0.0785\n",
      "Epoch 21/300 - Train Loss: 0.0710, Val Loss: 0.0801\n",
      "Epoch 22/300 - Train Loss: 0.0711, Val Loss: 0.0889\n",
      "Epoch 23/300 - Train Loss: 0.0710, Val Loss: 0.0762\n",
      "Epoch 24/300 - Train Loss: 0.0704, Val Loss: 0.0763\n",
      "Epoch 25/300 - Train Loss: 0.0691, Val Loss: 0.0798\n",
      "Epoch 26/300 - Train Loss: 0.0699, Val Loss: 0.0763\n",
      "Epoch 27/300 - Train Loss: 0.0694, Val Loss: 0.0736\n",
      "Epoch 28/300 - Train Loss: 0.0690, Val Loss: 0.0756\n",
      "Epoch 29/300 - Train Loss: 0.0679, Val Loss: 0.0819\n",
      "Epoch 30/300 - Train Loss: 0.0677, Val Loss: 0.0730\n",
      "Epoch 31/300 - Train Loss: 0.0664, Val Loss: 0.0781\n",
      "Epoch 32/300 - Train Loss: 0.0677, Val Loss: 0.0764\n",
      "Epoch 33/300 - Train Loss: 0.0652, Val Loss: 0.0819\n",
      "Epoch 34/300 - Train Loss: 0.0652, Val Loss: 0.0778\n",
      "Epoch 35/300 - Train Loss: 0.0652, Val Loss: 0.0767\n",
      "Epoch 36/300 - Train Loss: 0.0633, Val Loss: 0.0732\n",
      "Epoch 37/300 - Train Loss: 0.0646, Val Loss: 0.0742\n",
      "Epoch 38/300 - Train Loss: 0.0646, Val Loss: 0.0739\n",
      "Epoch 39/300 - Train Loss: 0.0631, Val Loss: 0.0732\n",
      "Epoch 40/300 - Train Loss: 0.0627, Val Loss: 0.0728\n",
      "Epoch 41/300 - Train Loss: 0.0622, Val Loss: 0.0793\n",
      "Epoch 42/300 - Train Loss: 0.0611, Val Loss: 0.0736\n",
      "Epoch 43/300 - Train Loss: 0.0619, Val Loss: 0.0719\n",
      "Epoch 44/300 - Train Loss: 0.0630, Val Loss: 0.0773\n",
      "Epoch 45/300 - Train Loss: 0.0607, Val Loss: 0.0757\n",
      "Epoch 46/300 - Train Loss: 0.0603, Val Loss: 0.0738\n",
      "Epoch 47/300 - Train Loss: 0.0617, Val Loss: 0.0721\n",
      "Epoch 48/300 - Train Loss: 0.0596, Val Loss: 0.0769\n",
      "Epoch 49/300 - Train Loss: 0.0599, Val Loss: 0.0719\n",
      "Epoch 50/300 - Train Loss: 0.0598, Val Loss: 0.0742\n",
      "Epoch 51/300 - Train Loss: 0.0593, Val Loss: 0.0757\n",
      "Epoch 52/300 - Train Loss: 0.0566, Val Loss: 0.0727\n",
      "Epoch 53/300 - Train Loss: 0.0583, Val Loss: 0.0761\n",
      "Epoch 54/300 - Train Loss: 0.0590, Val Loss: 0.0828\n",
      "Epoch 55/300 - Train Loss: 0.0570, Val Loss: 0.0722\n",
      "Epoch 56/300 - Train Loss: 0.0566, Val Loss: 0.0736\n",
      "Epoch 57/300 - Train Loss: 0.0564, Val Loss: 0.0725\n",
      "Epoch 58/300 - Train Loss: 0.0566, Val Loss: 0.0721\n",
      "Epoch 59/300 - Train Loss: 0.0575, Val Loss: 0.0692\n",
      "Epoch 60/300 - Train Loss: 0.0565, Val Loss: 0.0714\n",
      "Epoch 61/300 - Train Loss: 0.0549, Val Loss: 0.0831\n",
      "Epoch 62/300 - Train Loss: 0.0555, Val Loss: 0.0704\n",
      "Epoch 63/300 - Train Loss: 0.0558, Val Loss: 0.0748\n",
      "Epoch 64/300 - Train Loss: 0.0541, Val Loss: 0.0770\n",
      "Epoch 65/300 - Train Loss: 0.0539, Val Loss: 0.0754\n",
      "Epoch 66/300 - Train Loss: 0.0560, Val Loss: 0.0716\n",
      "Epoch 67/300 - Train Loss: 0.0540, Val Loss: 0.0737\n",
      "Epoch 68/300 - Train Loss: 0.0551, Val Loss: 0.0703\n",
      "Epoch 69/300 - Train Loss: 0.0539, Val Loss: 0.0708\n",
      "Epoch 70/300 - Train Loss: 0.0515, Val Loss: 0.0724\n",
      "Epoch 71/300 - Train Loss: 0.0529, Val Loss: 0.0717\n",
      "Epoch 72/300 - Train Loss: 0.0531, Val Loss: 0.0787\n",
      "Epoch 73/300 - Train Loss: 0.0546, Val Loss: 0.0721\n",
      "Epoch 74/300 - Train Loss: 0.0523, Val Loss: 0.0726\n",
      "Epoch 75/300 - Train Loss: 0.0518, Val Loss: 0.0737\n",
      "Epoch 76/300 - Train Loss: 0.0537, Val Loss: 0.0718\n",
      "Epoch 77/300 - Train Loss: 0.0520, Val Loss: 0.0705\n",
      "Epoch 78/300 - Train Loss: 0.0516, Val Loss: 0.0743\n",
      "Epoch 79/300 - Train Loss: 0.0520, Val Loss: 0.0753\n",
      "Epoch 80/300 - Train Loss: 0.0507, Val Loss: 0.0740\n",
      "Epoch 81/300 - Train Loss: 0.0509, Val Loss: 0.0759\n",
      "Epoch 82/300 - Train Loss: 0.0508, Val Loss: 0.0711\n",
      "Epoch 83/300 - Train Loss: 0.0500, Val Loss: 0.0724\n",
      "Epoch 84/300 - Train Loss: 0.0496, Val Loss: 0.0749\n",
      "Epoch 85/300 - Train Loss: 0.0497, Val Loss: 0.0741\n",
      "Epoch 86/300 - Train Loss: 0.0495, Val Loss: 0.0743\n",
      "Epoch 87/300 - Train Loss: 0.0499, Val Loss: 0.0779\n",
      "Epoch 88/300 - Train Loss: 0.0495, Val Loss: 0.0708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 12:56:08,016] Trial 216 finished with value: 0.9638764063336542 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.19481967045544976, 'learning_rate': 8.929222061868264e-05, 'batch_size': 128, 'weight_decay': 0.00010865688138557323}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/300 - Train Loss: 0.0499, Val Loss: 0.0771\n",
      "Early stopping at epoch 89\n",
      "Macro F1 Score: 0.9639, Macro Precision: 0.9544, Macro Recall: 0.9744\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 218\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1627313114090028, LR=8.058490629987056e-05, BS=32, WD=0.00013632035078871407\n",
      "Epoch 1/300 - Train Loss: 0.2667, Val Loss: 0.1007\n",
      "Epoch 2/300 - Train Loss: 0.1121, Val Loss: 0.0882\n",
      "Epoch 3/300 - Train Loss: 0.0986, Val Loss: 0.0766\n",
      "Epoch 4/300 - Train Loss: 0.0924, Val Loss: 0.0799\n",
      "Epoch 5/300 - Train Loss: 0.0905, Val Loss: 0.0773\n",
      "Epoch 6/300 - Train Loss: 0.0863, Val Loss: 0.0726\n",
      "Epoch 7/300 - Train Loss: 0.0863, Val Loss: 0.0789\n",
      "Epoch 8/300 - Train Loss: 0.0822, Val Loss: 0.0667\n",
      "Epoch 9/300 - Train Loss: 0.0812, Val Loss: 0.0701\n",
      "Epoch 10/300 - Train Loss: 0.0803, Val Loss: 0.0744\n",
      "Epoch 11/300 - Train Loss: 0.0786, Val Loss: 0.0778\n",
      "Epoch 12/300 - Train Loss: 0.0802, Val Loss: 0.0707\n",
      "Epoch 13/300 - Train Loss: 0.0771, Val Loss: 0.0713\n",
      "Epoch 14/300 - Train Loss: 0.0759, Val Loss: 0.0702\n",
      "Epoch 15/300 - Train Loss: 0.0755, Val Loss: 0.0733\n",
      "Epoch 16/300 - Train Loss: 0.0760, Val Loss: 0.0683\n",
      "Epoch 17/300 - Train Loss: 0.0736, Val Loss: 0.0694\n",
      "Epoch 18/300 - Train Loss: 0.0719, Val Loss: 0.0717\n",
      "Epoch 19/300 - Train Loss: 0.0703, Val Loss: 0.0663\n",
      "Epoch 20/300 - Train Loss: 0.0729, Val Loss: 0.0684\n",
      "Epoch 21/300 - Train Loss: 0.0705, Val Loss: 0.0785\n",
      "Epoch 22/300 - Train Loss: 0.0710, Val Loss: 0.0676\n",
      "Epoch 23/300 - Train Loss: 0.0710, Val Loss: 0.0660\n",
      "Epoch 24/300 - Train Loss: 0.0706, Val Loss: 0.0687\n",
      "Epoch 25/300 - Train Loss: 0.0694, Val Loss: 0.0707\n",
      "Epoch 26/300 - Train Loss: 0.0676, Val Loss: 0.0683\n",
      "Epoch 27/300 - Train Loss: 0.0659, Val Loss: 0.0743\n",
      "Epoch 28/300 - Train Loss: 0.0671, Val Loss: 0.0666\n",
      "Epoch 29/300 - Train Loss: 0.0670, Val Loss: 0.0664\n",
      "Epoch 30/300 - Train Loss: 0.0685, Val Loss: 0.0683\n",
      "Epoch 31/300 - Train Loss: 0.0629, Val Loss: 0.0689\n",
      "Epoch 32/300 - Train Loss: 0.0652, Val Loss: 0.0782\n",
      "Epoch 33/300 - Train Loss: 0.0637, Val Loss: 0.0673\n",
      "Epoch 34/300 - Train Loss: 0.0622, Val Loss: 0.0688\n",
      "Epoch 35/300 - Train Loss: 0.0638, Val Loss: 0.0687\n",
      "Epoch 36/300 - Train Loss: 0.0641, Val Loss: 0.0692\n",
      "Epoch 37/300 - Train Loss: 0.0617, Val Loss: 0.0649\n",
      "Epoch 38/300 - Train Loss: 0.0623, Val Loss: 0.0663\n",
      "Epoch 39/300 - Train Loss: 0.0626, Val Loss: 0.0714\n",
      "Epoch 40/300 - Train Loss: 0.0581, Val Loss: 0.0725\n",
      "Epoch 41/300 - Train Loss: 0.0598, Val Loss: 0.0728\n",
      "Epoch 42/300 - Train Loss: 0.0619, Val Loss: 0.0673\n",
      "Epoch 43/300 - Train Loss: 0.0599, Val Loss: 0.0683\n",
      "Epoch 44/300 - Train Loss: 0.0599, Val Loss: 0.0746\n",
      "Epoch 45/300 - Train Loss: 0.0576, Val Loss: 0.0728\n",
      "Epoch 46/300 - Train Loss: 0.0608, Val Loss: 0.0671\n",
      "Epoch 47/300 - Train Loss: 0.0584, Val Loss: 0.0667\n",
      "Epoch 48/300 - Train Loss: 0.0565, Val Loss: 0.0685\n",
      "Epoch 49/300 - Train Loss: 0.0561, Val Loss: 0.0681\n",
      "Epoch 50/300 - Train Loss: 0.0554, Val Loss: 0.0779\n",
      "Epoch 51/300 - Train Loss: 0.0584, Val Loss: 0.0667\n",
      "Epoch 52/300 - Train Loss: 0.0565, Val Loss: 0.0657\n",
      "Epoch 53/300 - Train Loss: 0.0559, Val Loss: 0.0684\n",
      "Epoch 54/300 - Train Loss: 0.0560, Val Loss: 0.0701\n",
      "Epoch 55/300 - Train Loss: 0.0531, Val Loss: 0.0678\n",
      "Epoch 56/300 - Train Loss: 0.0542, Val Loss: 0.0731\n",
      "Epoch 57/300 - Train Loss: 0.0541, Val Loss: 0.0679\n",
      "Epoch 58/300 - Train Loss: 0.0541, Val Loss: 0.0720\n",
      "Epoch 59/300 - Train Loss: 0.0539, Val Loss: 0.0680\n",
      "Epoch 60/300 - Train Loss: 0.0541, Val Loss: 0.0721\n",
      "Epoch 61/300 - Train Loss: 0.0537, Val Loss: 0.0664\n",
      "Epoch 62/300 - Train Loss: 0.0535, Val Loss: 0.0696\n",
      "Epoch 63/300 - Train Loss: 0.0529, Val Loss: 0.0715\n",
      "Epoch 64/300 - Train Loss: 0.0537, Val Loss: 0.0718\n",
      "Epoch 65/300 - Train Loss: 0.0508, Val Loss: 0.0720\n",
      "Epoch 66/300 - Train Loss: 0.0551, Val Loss: 0.0686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:00:25,556] Trial 217 finished with value: 0.9671324408016266 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1627313114090028, 'learning_rate': 8.058490629987056e-05, 'batch_size': 32, 'weight_decay': 0.00013632035078871407}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/300 - Train Loss: 0.0509, Val Loss: 0.0723\n",
      "Early stopping at epoch 67\n",
      "Macro F1 Score: 0.9671, Macro Precision: 0.9589, Macro Recall: 0.9760\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 219\n",
      "Training with F1=16, F2=32, D=8, dropout=0.2016286918084934, LR=7.134178849009431e-05, BS=32, WD=0.00016192353124432144\n",
      "Epoch 1/300 - Train Loss: 0.2958, Val Loss: 0.1385\n",
      "Epoch 2/300 - Train Loss: 0.1253, Val Loss: 0.0984\n",
      "Epoch 3/300 - Train Loss: 0.1049, Val Loss: 0.0875\n",
      "Epoch 4/300 - Train Loss: 0.0963, Val Loss: 0.0965\n",
      "Epoch 5/300 - Train Loss: 0.0948, Val Loss: 0.0809\n",
      "Epoch 6/300 - Train Loss: 0.0905, Val Loss: 0.0777\n",
      "Epoch 7/300 - Train Loss: 0.0896, Val Loss: 0.0758\n",
      "Epoch 8/300 - Train Loss: 0.0862, Val Loss: 0.0752\n",
      "Epoch 9/300 - Train Loss: 0.0843, Val Loss: 0.0758\n",
      "Epoch 10/300 - Train Loss: 0.0826, Val Loss: 0.0763\n",
      "Epoch 11/300 - Train Loss: 0.0814, Val Loss: 0.0693\n",
      "Epoch 12/300 - Train Loss: 0.0805, Val Loss: 0.0849\n",
      "Epoch 13/300 - Train Loss: 0.0799, Val Loss: 0.0732\n",
      "Epoch 14/300 - Train Loss: 0.0792, Val Loss: 0.0739\n",
      "Epoch 15/300 - Train Loss: 0.0788, Val Loss: 0.0752\n",
      "Epoch 16/300 - Train Loss: 0.0778, Val Loss: 0.0682\n",
      "Epoch 17/300 - Train Loss: 0.0772, Val Loss: 0.0736\n",
      "Epoch 18/300 - Train Loss: 0.0770, Val Loss: 0.0707\n",
      "Epoch 19/300 - Train Loss: 0.0775, Val Loss: 0.0662\n",
      "Epoch 20/300 - Train Loss: 0.0745, Val Loss: 0.0726\n",
      "Epoch 21/300 - Train Loss: 0.0758, Val Loss: 0.0680\n",
      "Epoch 22/300 - Train Loss: 0.0748, Val Loss: 0.0689\n",
      "Epoch 23/300 - Train Loss: 0.0743, Val Loss: 0.0786\n",
      "Epoch 24/300 - Train Loss: 0.0736, Val Loss: 0.0660\n",
      "Epoch 25/300 - Train Loss: 0.0741, Val Loss: 0.0707\n",
      "Epoch 26/300 - Train Loss: 0.0724, Val Loss: 0.0679\n",
      "Epoch 27/300 - Train Loss: 0.0739, Val Loss: 0.0758\n",
      "Epoch 28/300 - Train Loss: 0.0730, Val Loss: 0.0743\n",
      "Epoch 29/300 - Train Loss: 0.0724, Val Loss: 0.0698\n",
      "Epoch 30/300 - Train Loss: 0.0708, Val Loss: 0.0707\n",
      "Epoch 31/300 - Train Loss: 0.0727, Val Loss: 0.0701\n",
      "Epoch 32/300 - Train Loss: 0.0706, Val Loss: 0.0665\n",
      "Epoch 33/300 - Train Loss: 0.0717, Val Loss: 0.0671\n",
      "Epoch 34/300 - Train Loss: 0.0673, Val Loss: 0.0710\n",
      "Epoch 35/300 - Train Loss: 0.0693, Val Loss: 0.0666\n",
      "Epoch 36/300 - Train Loss: 0.0693, Val Loss: 0.0663\n",
      "Epoch 37/300 - Train Loss: 0.0692, Val Loss: 0.0699\n",
      "Epoch 38/300 - Train Loss: 0.0667, Val Loss: 0.0691\n",
      "Epoch 39/300 - Train Loss: 0.0670, Val Loss: 0.0670\n",
      "Epoch 40/300 - Train Loss: 0.0673, Val Loss: 0.0715\n",
      "Epoch 41/300 - Train Loss: 0.0687, Val Loss: 0.0655\n",
      "Epoch 42/300 - Train Loss: 0.0656, Val Loss: 0.0666\n",
      "Epoch 43/300 - Train Loss: 0.0709, Val Loss: 0.0662\n",
      "Epoch 44/300 - Train Loss: 0.0668, Val Loss: 0.0661\n",
      "Epoch 45/300 - Train Loss: 0.0639, Val Loss: 0.0652\n",
      "Epoch 46/300 - Train Loss: 0.0679, Val Loss: 0.0683\n",
      "Epoch 47/300 - Train Loss: 0.0667, Val Loss: 0.0680\n",
      "Epoch 48/300 - Train Loss: 0.0643, Val Loss: 0.0686\n",
      "Epoch 49/300 - Train Loss: 0.0647, Val Loss: 0.0674\n",
      "Epoch 50/300 - Train Loss: 0.0654, Val Loss: 0.0665\n",
      "Epoch 51/300 - Train Loss: 0.0647, Val Loss: 0.0670\n",
      "Epoch 52/300 - Train Loss: 0.0640, Val Loss: 0.0759\n",
      "Epoch 53/300 - Train Loss: 0.0621, Val Loss: 0.0660\n",
      "Epoch 54/300 - Train Loss: 0.0624, Val Loss: 0.0647\n",
      "Epoch 55/300 - Train Loss: 0.0619, Val Loss: 0.0652\n",
      "Epoch 56/300 - Train Loss: 0.0606, Val Loss: 0.0685\n",
      "Epoch 57/300 - Train Loss: 0.0606, Val Loss: 0.0667\n",
      "Epoch 58/300 - Train Loss: 0.0604, Val Loss: 0.0696\n",
      "Epoch 59/300 - Train Loss: 0.0610, Val Loss: 0.0646\n",
      "Epoch 60/300 - Train Loss: 0.0600, Val Loss: 0.0681\n",
      "Epoch 61/300 - Train Loss: 0.0618, Val Loss: 0.0684\n",
      "Epoch 62/300 - Train Loss: 0.0624, Val Loss: 0.0653\n",
      "Epoch 63/300 - Train Loss: 0.0601, Val Loss: 0.0672\n",
      "Epoch 64/300 - Train Loss: 0.0598, Val Loss: 0.0668\n",
      "Epoch 65/300 - Train Loss: 0.0599, Val Loss: 0.0672\n",
      "Epoch 66/300 - Train Loss: 0.0592, Val Loss: 0.0649\n",
      "Epoch 67/300 - Train Loss: 0.0585, Val Loss: 0.0652\n",
      "Epoch 68/300 - Train Loss: 0.0586, Val Loss: 0.0680\n",
      "Epoch 69/300 - Train Loss: 0.0569, Val Loss: 0.0685\n",
      "Epoch 70/300 - Train Loss: 0.0583, Val Loss: 0.0656\n",
      "Epoch 71/300 - Train Loss: 0.0627, Val Loss: 0.0645\n",
      "Epoch 72/300 - Train Loss: 0.0571, Val Loss: 0.0682\n",
      "Epoch 73/300 - Train Loss: 0.0586, Val Loss: 0.0692\n",
      "Epoch 74/300 - Train Loss: 0.0566, Val Loss: 0.0669\n",
      "Epoch 75/300 - Train Loss: 0.0579, Val Loss: 0.0707\n",
      "Epoch 76/300 - Train Loss: 0.0586, Val Loss: 0.0707\n",
      "Epoch 77/300 - Train Loss: 0.0569, Val Loss: 0.0665\n",
      "Epoch 78/300 - Train Loss: 0.0561, Val Loss: 0.0673\n",
      "Epoch 79/300 - Train Loss: 0.0559, Val Loss: 0.0725\n",
      "Epoch 80/300 - Train Loss: 0.0551, Val Loss: 0.0680\n",
      "Epoch 81/300 - Train Loss: 0.0568, Val Loss: 0.0682\n",
      "Epoch 82/300 - Train Loss: 0.0547, Val Loss: 0.0668\n",
      "Epoch 83/300 - Train Loss: 0.0578, Val Loss: 0.0671\n",
      "Epoch 84/300 - Train Loss: 0.0559, Val Loss: 0.0717\n",
      "Epoch 85/300 - Train Loss: 0.0539, Val Loss: 0.0669\n",
      "Epoch 86/300 - Train Loss: 0.0551, Val Loss: 0.0666\n",
      "Epoch 87/300 - Train Loss: 0.0527, Val Loss: 0.0696\n",
      "Epoch 88/300 - Train Loss: 0.0537, Val Loss: 0.0686\n",
      "Epoch 89/300 - Train Loss: 0.0552, Val Loss: 0.0695\n",
      "Epoch 90/300 - Train Loss: 0.0543, Val Loss: 0.0683\n",
      "Epoch 91/300 - Train Loss: 0.0536, Val Loss: 0.0649\n",
      "Epoch 92/300 - Train Loss: 0.0521, Val Loss: 0.0660\n",
      "Epoch 93/300 - Train Loss: 0.0551, Val Loss: 0.0698\n",
      "Epoch 94/300 - Train Loss: 0.0544, Val Loss: 0.0706\n",
      "Epoch 95/300 - Train Loss: 0.0523, Val Loss: 0.0727\n",
      "Epoch 96/300 - Train Loss: 0.0534, Val Loss: 0.0683\n",
      "Epoch 97/300 - Train Loss: 0.0530, Val Loss: 0.0692\n",
      "Epoch 98/300 - Train Loss: 0.0532, Val Loss: 0.0698\n",
      "Epoch 99/300 - Train Loss: 0.0518, Val Loss: 0.0685\n",
      "Epoch 100/300 - Train Loss: 0.0523, Val Loss: 0.0666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:06:53,831] Trial 218 finished with value: 0.96630265473173 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.2016286918084934, 'learning_rate': 7.134178849009431e-05, 'batch_size': 32, 'weight_decay': 0.00016192353124432144}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/300 - Train Loss: 0.0536, Val Loss: 0.0705\n",
      "Early stopping at epoch 101\n",
      "Macro F1 Score: 0.9663, Macro Precision: 0.9624, Macro Recall: 0.9704\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 220\n",
      "Training with F1=16, F2=8, D=8, dropout=0.6394061139211302, LR=0.00011180672546914628, BS=32, WD=0.00011027230988160924\n",
      "Epoch 1/300 - Train Loss: 0.4529, Val Loss: 0.1679\n",
      "Epoch 2/300 - Train Loss: 0.1655, Val Loss: 0.0990\n",
      "Epoch 3/300 - Train Loss: 0.1324, Val Loss: 0.0967\n",
      "Epoch 4/300 - Train Loss: 0.1230, Val Loss: 0.0880\n",
      "Epoch 5/300 - Train Loss: 0.1173, Val Loss: 0.0809\n",
      "Epoch 6/300 - Train Loss: 0.1124, Val Loss: 0.0760\n",
      "Epoch 7/300 - Train Loss: 0.1107, Val Loss: 0.0804\n",
      "Epoch 8/300 - Train Loss: 0.1111, Val Loss: 0.0797\n",
      "Epoch 9/300 - Train Loss: 0.1085, Val Loss: 0.0783\n",
      "Epoch 10/300 - Train Loss: 0.1058, Val Loss: 0.0755\n",
      "Epoch 11/300 - Train Loss: 0.1102, Val Loss: 0.0717\n",
      "Epoch 12/300 - Train Loss: 0.1036, Val Loss: 0.0736\n",
      "Epoch 13/300 - Train Loss: 0.1035, Val Loss: 0.0741\n",
      "Epoch 14/300 - Train Loss: 0.1013, Val Loss: 0.0750\n",
      "Epoch 15/300 - Train Loss: 0.1025, Val Loss: 0.0723\n",
      "Epoch 16/300 - Train Loss: 0.1013, Val Loss: 0.0722\n",
      "Epoch 17/300 - Train Loss: 0.1023, Val Loss: 0.0731\n",
      "Epoch 18/300 - Train Loss: 0.1015, Val Loss: 0.0716\n",
      "Epoch 19/300 - Train Loss: 0.1002, Val Loss: 0.0737\n",
      "Epoch 20/300 - Train Loss: 0.1004, Val Loss: 0.0732\n",
      "Epoch 21/300 - Train Loss: 0.1004, Val Loss: 0.0719\n",
      "Epoch 22/300 - Train Loss: 0.0961, Val Loss: 0.0698\n",
      "Epoch 23/300 - Train Loss: 0.0988, Val Loss: 0.0686\n",
      "Epoch 24/300 - Train Loss: 0.0995, Val Loss: 0.0721\n",
      "Epoch 25/300 - Train Loss: 0.0988, Val Loss: 0.0698\n",
      "Epoch 26/300 - Train Loss: 0.0979, Val Loss: 0.0716\n",
      "Epoch 27/300 - Train Loss: 0.0986, Val Loss: 0.0673\n",
      "Epoch 28/300 - Train Loss: 0.0986, Val Loss: 0.0721\n",
      "Epoch 29/300 - Train Loss: 0.0972, Val Loss: 0.0713\n",
      "Epoch 30/300 - Train Loss: 0.0964, Val Loss: 0.0678\n",
      "Epoch 31/300 - Train Loss: 0.0944, Val Loss: 0.0777\n",
      "Epoch 32/300 - Train Loss: 0.1003, Val Loss: 0.0757\n",
      "Epoch 33/300 - Train Loss: 0.0992, Val Loss: 0.0721\n",
      "Epoch 34/300 - Train Loss: 0.0948, Val Loss: 0.0688\n",
      "Epoch 35/300 - Train Loss: 0.0955, Val Loss: 0.0678\n",
      "Epoch 36/300 - Train Loss: 0.0935, Val Loss: 0.0702\n",
      "Epoch 37/300 - Train Loss: 0.0946, Val Loss: 0.0704\n",
      "Epoch 38/300 - Train Loss: 0.0935, Val Loss: 0.0714\n",
      "Epoch 39/300 - Train Loss: 0.0938, Val Loss: 0.0667\n",
      "Epoch 40/300 - Train Loss: 0.0952, Val Loss: 0.0696\n",
      "Epoch 41/300 - Train Loss: 0.0942, Val Loss: 0.0697\n",
      "Epoch 42/300 - Train Loss: 0.0939, Val Loss: 0.0682\n",
      "Epoch 43/300 - Train Loss: 0.0962, Val Loss: 0.0678\n",
      "Epoch 44/300 - Train Loss: 0.0938, Val Loss: 0.0680\n",
      "Epoch 45/300 - Train Loss: 0.0957, Val Loss: 0.0696\n",
      "Epoch 46/300 - Train Loss: 0.0923, Val Loss: 0.0716\n",
      "Epoch 47/300 - Train Loss: 0.0940, Val Loss: 0.0686\n",
      "Epoch 48/300 - Train Loss: 0.0911, Val Loss: 0.0690\n",
      "Epoch 49/300 - Train Loss: 0.0928, Val Loss: 0.0767\n",
      "Epoch 50/300 - Train Loss: 0.0941, Val Loss: 0.0707\n",
      "Epoch 51/300 - Train Loss: 0.0925, Val Loss: 0.0682\n",
      "Epoch 52/300 - Train Loss: 0.0940, Val Loss: 0.0681\n",
      "Epoch 53/300 - Train Loss: 0.0929, Val Loss: 0.0710\n",
      "Epoch 54/300 - Train Loss: 0.0930, Val Loss: 0.0690\n",
      "Epoch 55/300 - Train Loss: 0.0926, Val Loss: 0.0699\n",
      "Epoch 56/300 - Train Loss: 0.0939, Val Loss: 0.0709\n",
      "Epoch 57/300 - Train Loss: 0.0924, Val Loss: 0.0671\n",
      "Epoch 58/300 - Train Loss: 0.0919, Val Loss: 0.0691\n",
      "Epoch 59/300 - Train Loss: 0.0919, Val Loss: 0.0723\n",
      "Epoch 60/300 - Train Loss: 0.0938, Val Loss: 0.0728\n",
      "Epoch 61/300 - Train Loss: 0.0930, Val Loss: 0.0672\n",
      "Epoch 62/300 - Train Loss: 0.0933, Val Loss: 0.0707\n",
      "Epoch 63/300 - Train Loss: 0.0906, Val Loss: 0.0696\n",
      "Epoch 64/300 - Train Loss: 0.0911, Val Loss: 0.0691\n",
      "Epoch 65/300 - Train Loss: 0.0912, Val Loss: 0.0678\n",
      "Epoch 66/300 - Train Loss: 0.0953, Val Loss: 0.0704\n",
      "Epoch 67/300 - Train Loss: 0.0890, Val Loss: 0.0658\n",
      "Epoch 68/300 - Train Loss: 0.0917, Val Loss: 0.0717\n",
      "Epoch 69/300 - Train Loss: 0.0904, Val Loss: 0.0680\n",
      "Epoch 70/300 - Train Loss: 0.0912, Val Loss: 0.0693\n",
      "Epoch 71/300 - Train Loss: 0.0915, Val Loss: 0.0771\n",
      "Epoch 72/300 - Train Loss: 0.0899, Val Loss: 0.0712\n",
      "Epoch 73/300 - Train Loss: 0.0920, Val Loss: 0.0673\n",
      "Epoch 74/300 - Train Loss: 0.0923, Val Loss: 0.0701\n",
      "Epoch 75/300 - Train Loss: 0.0891, Val Loss: 0.0701\n",
      "Epoch 76/300 - Train Loss: 0.0901, Val Loss: 0.0684\n",
      "Epoch 77/300 - Train Loss: 0.0928, Val Loss: 0.0770\n",
      "Epoch 78/300 - Train Loss: 0.0915, Val Loss: 0.0725\n",
      "Epoch 79/300 - Train Loss: 0.0910, Val Loss: 0.0702\n",
      "Epoch 80/300 - Train Loss: 0.0918, Val Loss: 0.0727\n",
      "Epoch 81/300 - Train Loss: 0.0930, Val Loss: 0.0682\n",
      "Epoch 82/300 - Train Loss: 0.0932, Val Loss: 0.0689\n",
      "Epoch 83/300 - Train Loss: 0.0933, Val Loss: 0.0679\n",
      "Epoch 84/300 - Train Loss: 0.0894, Val Loss: 0.0710\n",
      "Epoch 85/300 - Train Loss: 0.0899, Val Loss: 0.0671\n",
      "Epoch 86/300 - Train Loss: 0.0919, Val Loss: 0.0678\n",
      "Epoch 87/300 - Train Loss: 0.0897, Val Loss: 0.0714\n",
      "Epoch 88/300 - Train Loss: 0.0919, Val Loss: 0.0682\n",
      "Epoch 89/300 - Train Loss: 0.0911, Val Loss: 0.0695\n",
      "Epoch 90/300 - Train Loss: 0.0907, Val Loss: 0.0692\n",
      "Epoch 91/300 - Train Loss: 0.0889, Val Loss: 0.0678\n",
      "Epoch 92/300 - Train Loss: 0.0907, Val Loss: 0.0715\n",
      "Epoch 93/300 - Train Loss: 0.0894, Val Loss: 0.0760\n",
      "Epoch 94/300 - Train Loss: 0.0881, Val Loss: 0.0718\n",
      "Epoch 95/300 - Train Loss: 0.0909, Val Loss: 0.0699\n",
      "Epoch 96/300 - Train Loss: 0.0889, Val Loss: 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:12:24,747] Trial 219 finished with value: 0.9662987234561525 and parameters: {'F1': 16, 'F2': 8, 'D': 8, 'dropout': 0.6394061139211302, 'learning_rate': 0.00011180672546914628, 'batch_size': 32, 'weight_decay': 0.00011027230988160924}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300 - Train Loss: 0.0901, Val Loss: 0.0690\n",
      "Early stopping at epoch 97\n",
      "Macro F1 Score: 0.9663, Macro Precision: 0.9626, Macro Recall: 0.9702\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 221\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1858321747621717, LR=8.608339562969721e-05, BS=32, WD=0.00020720269753271887\n",
      "Epoch 1/300 - Train Loss: 0.2670, Val Loss: 0.1276\n",
      "Epoch 2/300 - Train Loss: 0.1202, Val Loss: 0.0806\n",
      "Epoch 3/300 - Train Loss: 0.0988, Val Loss: 0.0865\n",
      "Epoch 4/300 - Train Loss: 0.0942, Val Loss: 0.0782\n",
      "Epoch 5/300 - Train Loss: 0.0900, Val Loss: 0.0717\n",
      "Epoch 6/300 - Train Loss: 0.0868, Val Loss: 0.0711\n",
      "Epoch 7/300 - Train Loss: 0.0859, Val Loss: 0.0684\n",
      "Epoch 8/300 - Train Loss: 0.0839, Val Loss: 0.0734\n",
      "Epoch 9/300 - Train Loss: 0.0828, Val Loss: 0.0706\n",
      "Epoch 10/300 - Train Loss: 0.0837, Val Loss: 0.0710\n",
      "Epoch 11/300 - Train Loss: 0.0821, Val Loss: 0.0764\n",
      "Epoch 12/300 - Train Loss: 0.0821, Val Loss: 0.0665\n",
      "Epoch 13/300 - Train Loss: 0.0811, Val Loss: 0.0701\n",
      "Epoch 14/300 - Train Loss: 0.0811, Val Loss: 0.0692\n",
      "Epoch 15/300 - Train Loss: 0.0775, Val Loss: 0.0750\n",
      "Epoch 16/300 - Train Loss: 0.0781, Val Loss: 0.0667\n",
      "Epoch 17/300 - Train Loss: 0.0771, Val Loss: 0.0687\n",
      "Epoch 18/300 - Train Loss: 0.0788, Val Loss: 0.0742\n",
      "Epoch 19/300 - Train Loss: 0.0784, Val Loss: 0.0716\n",
      "Epoch 20/300 - Train Loss: 0.0764, Val Loss: 0.0766\n",
      "Epoch 21/300 - Train Loss: 0.0746, Val Loss: 0.0669\n",
      "Epoch 22/300 - Train Loss: 0.0751, Val Loss: 0.0723\n",
      "Epoch 23/300 - Train Loss: 0.0729, Val Loss: 0.0705\n",
      "Epoch 24/300 - Train Loss: 0.0729, Val Loss: 0.0669\n",
      "Epoch 25/300 - Train Loss: 0.0709, Val Loss: 0.0649\n",
      "Epoch 26/300 - Train Loss: 0.0714, Val Loss: 0.0694\n",
      "Epoch 27/300 - Train Loss: 0.0731, Val Loss: 0.0672\n",
      "Epoch 28/300 - Train Loss: 0.0706, Val Loss: 0.0700\n",
      "Epoch 29/300 - Train Loss: 0.0717, Val Loss: 0.0684\n",
      "Epoch 30/300 - Train Loss: 0.0699, Val Loss: 0.0684\n",
      "Epoch 31/300 - Train Loss: 0.0678, Val Loss: 0.0755\n",
      "Epoch 32/300 - Train Loss: 0.0689, Val Loss: 0.0677\n",
      "Epoch 33/300 - Train Loss: 0.0704, Val Loss: 0.0632\n",
      "Epoch 34/300 - Train Loss: 0.0699, Val Loss: 0.0659\n",
      "Epoch 35/300 - Train Loss: 0.0686, Val Loss: 0.0666\n",
      "Epoch 36/300 - Train Loss: 0.0680, Val Loss: 0.0702\n",
      "Epoch 37/300 - Train Loss: 0.0677, Val Loss: 0.0815\n",
      "Epoch 38/300 - Train Loss: 0.0671, Val Loss: 0.0710\n",
      "Epoch 39/300 - Train Loss: 0.0653, Val Loss: 0.0663\n",
      "Epoch 40/300 - Train Loss: 0.0678, Val Loss: 0.0680\n",
      "Epoch 41/300 - Train Loss: 0.0668, Val Loss: 0.0670\n",
      "Epoch 42/300 - Train Loss: 0.0653, Val Loss: 0.0664\n",
      "Epoch 43/300 - Train Loss: 0.0649, Val Loss: 0.0666\n",
      "Epoch 44/300 - Train Loss: 0.0633, Val Loss: 0.0660\n",
      "Epoch 45/300 - Train Loss: 0.0637, Val Loss: 0.0662\n",
      "Epoch 46/300 - Train Loss: 0.0635, Val Loss: 0.0710\n",
      "Epoch 47/300 - Train Loss: 0.0625, Val Loss: 0.0722\n",
      "Epoch 48/300 - Train Loss: 0.0641, Val Loss: 0.0711\n",
      "Epoch 49/300 - Train Loss: 0.0643, Val Loss: 0.0648\n",
      "Epoch 50/300 - Train Loss: 0.0621, Val Loss: 0.0667\n",
      "Epoch 51/300 - Train Loss: 0.0627, Val Loss: 0.0693\n",
      "Epoch 52/300 - Train Loss: 0.0641, Val Loss: 0.0637\n",
      "Epoch 53/300 - Train Loss: 0.0621, Val Loss: 0.0687\n",
      "Epoch 54/300 - Train Loss: 0.0609, Val Loss: 0.0687\n",
      "Epoch 55/300 - Train Loss: 0.0628, Val Loss: 0.0657\n",
      "Epoch 56/300 - Train Loss: 0.0616, Val Loss: 0.0660\n",
      "Epoch 57/300 - Train Loss: 0.0623, Val Loss: 0.0682\n",
      "Epoch 58/300 - Train Loss: 0.0594, Val Loss: 0.0681\n",
      "Epoch 59/300 - Train Loss: 0.0586, Val Loss: 0.0703\n",
      "Epoch 60/300 - Train Loss: 0.0619, Val Loss: 0.0698\n",
      "Epoch 61/300 - Train Loss: 0.0591, Val Loss: 0.0675\n",
      "Epoch 62/300 - Train Loss: 0.0585, Val Loss: 0.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:16:26,954] Trial 220 finished with value: 0.971028396670735 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1858321747621717, 'learning_rate': 8.608339562969721e-05, 'batch_size': 32, 'weight_decay': 0.00020720269753271887}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300 - Train Loss: 0.0581, Val Loss: 0.0690\n",
      "Early stopping at epoch 63\n",
      "Macro F1 Score: 0.9710, Macro Precision: 0.9692, Macro Recall: 0.9729\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 222\n",
      "Training with F1=16, F2=32, D=8, dropout=0.26875841070890905, LR=9.682378208962111e-05, BS=32, WD=0.0001301162693505586\n",
      "Epoch 1/300 - Train Loss: 0.2579, Val Loss: 0.1355\n",
      "Epoch 2/300 - Train Loss: 0.1204, Val Loss: 0.0929\n",
      "Epoch 3/300 - Train Loss: 0.1073, Val Loss: 0.0968\n",
      "Epoch 4/300 - Train Loss: 0.1015, Val Loss: 0.0915\n",
      "Epoch 5/300 - Train Loss: 0.0951, Val Loss: 0.0760\n",
      "Epoch 6/300 - Train Loss: 0.0926, Val Loss: 0.0909\n",
      "Epoch 7/300 - Train Loss: 0.0916, Val Loss: 0.0759\n",
      "Epoch 8/300 - Train Loss: 0.0894, Val Loss: 0.0740\n",
      "Epoch 9/300 - Train Loss: 0.0859, Val Loss: 0.0795\n",
      "Epoch 10/300 - Train Loss: 0.0855, Val Loss: 0.0809\n",
      "Epoch 11/300 - Train Loss: 0.0851, Val Loss: 0.0750\n",
      "Epoch 12/300 - Train Loss: 0.0819, Val Loss: 0.0693\n",
      "Epoch 13/300 - Train Loss: 0.0834, Val Loss: 0.0737\n",
      "Epoch 14/300 - Train Loss: 0.0804, Val Loss: 0.0805\n",
      "Epoch 15/300 - Train Loss: 0.0798, Val Loss: 0.0728\n",
      "Epoch 16/300 - Train Loss: 0.0818, Val Loss: 0.0716\n",
      "Epoch 17/300 - Train Loss: 0.0780, Val Loss: 0.0731\n",
      "Epoch 18/300 - Train Loss: 0.0807, Val Loss: 0.0705\n",
      "Epoch 19/300 - Train Loss: 0.0779, Val Loss: 0.0735\n",
      "Epoch 20/300 - Train Loss: 0.0788, Val Loss: 0.0795\n",
      "Epoch 21/300 - Train Loss: 0.0774, Val Loss: 0.0774\n",
      "Epoch 22/300 - Train Loss: 0.0775, Val Loss: 0.0758\n",
      "Epoch 23/300 - Train Loss: 0.0738, Val Loss: 0.0722\n",
      "Epoch 24/300 - Train Loss: 0.0761, Val Loss: 0.0721\n",
      "Epoch 25/300 - Train Loss: 0.0729, Val Loss: 0.0660\n",
      "Epoch 26/300 - Train Loss: 0.0740, Val Loss: 0.0682\n",
      "Epoch 27/300 - Train Loss: 0.0761, Val Loss: 0.0689\n",
      "Epoch 28/300 - Train Loss: 0.0723, Val Loss: 0.0742\n",
      "Epoch 29/300 - Train Loss: 0.0733, Val Loss: 0.0672\n",
      "Epoch 30/300 - Train Loss: 0.0721, Val Loss: 0.0647\n",
      "Epoch 31/300 - Train Loss: 0.0737, Val Loss: 0.0704\n",
      "Epoch 32/300 - Train Loss: 0.0717, Val Loss: 0.0676\n",
      "Epoch 33/300 - Train Loss: 0.0712, Val Loss: 0.0695\n",
      "Epoch 34/300 - Train Loss: 0.0723, Val Loss: 0.0724\n",
      "Epoch 35/300 - Train Loss: 0.0696, Val Loss: 0.0761\n",
      "Epoch 36/300 - Train Loss: 0.0714, Val Loss: 0.0673\n",
      "Epoch 37/300 - Train Loss: 0.0684, Val Loss: 0.0706\n",
      "Epoch 38/300 - Train Loss: 0.0694, Val Loss: 0.0656\n",
      "Epoch 39/300 - Train Loss: 0.0669, Val Loss: 0.0739\n",
      "Epoch 40/300 - Train Loss: 0.0680, Val Loss: 0.0670\n",
      "Epoch 41/300 - Train Loss: 0.0677, Val Loss: 0.0671\n",
      "Epoch 42/300 - Train Loss: 0.0660, Val Loss: 0.0655\n",
      "Epoch 43/300 - Train Loss: 0.0698, Val Loss: 0.0653\n",
      "Epoch 44/300 - Train Loss: 0.0655, Val Loss: 0.0659\n",
      "Epoch 45/300 - Train Loss: 0.0663, Val Loss: 0.0690\n",
      "Epoch 46/300 - Train Loss: 0.0667, Val Loss: 0.0710\n",
      "Epoch 47/300 - Train Loss: 0.0670, Val Loss: 0.0708\n",
      "Epoch 48/300 - Train Loss: 0.0663, Val Loss: 0.0721\n",
      "Epoch 49/300 - Train Loss: 0.0659, Val Loss: 0.0753\n",
      "Epoch 50/300 - Train Loss: 0.0643, Val Loss: 0.0684\n",
      "Epoch 51/300 - Train Loss: 0.0658, Val Loss: 0.0656\n",
      "Epoch 52/300 - Train Loss: 0.0661, Val Loss: 0.0679\n",
      "Epoch 53/300 - Train Loss: 0.0643, Val Loss: 0.0677\n",
      "Epoch 54/300 - Train Loss: 0.0624, Val Loss: 0.0703\n",
      "Epoch 55/300 - Train Loss: 0.0625, Val Loss: 0.0649\n",
      "Epoch 56/300 - Train Loss: 0.0613, Val Loss: 0.0798\n",
      "Epoch 57/300 - Train Loss: 0.0620, Val Loss: 0.0718\n",
      "Epoch 58/300 - Train Loss: 0.0628, Val Loss: 0.0669\n",
      "Epoch 59/300 - Train Loss: 0.0611, Val Loss: 0.0697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:20:18,317] Trial 221 finished with value: 0.9685411867601571 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.26875841070890905, 'learning_rate': 9.682378208962111e-05, 'batch_size': 32, 'weight_decay': 0.0001301162693505586}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300 - Train Loss: 0.0637, Val Loss: 0.0680\n",
      "Early stopping at epoch 60\n",
      "Macro F1 Score: 0.9685, Macro Precision: 0.9603, Macro Recall: 0.9774\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 223\n",
      "Training with F1=32, F2=32, D=8, dropout=0.28756988749260903, LR=7.728688153471754e-05, BS=32, WD=7.615816578940512e-05\n",
      "Epoch 1/300 - Train Loss: 0.2556, Val Loss: 0.1159\n",
      "Epoch 2/300 - Train Loss: 0.1204, Val Loss: 0.0852\n",
      "Epoch 3/300 - Train Loss: 0.1035, Val Loss: 0.0791\n",
      "Epoch 4/300 - Train Loss: 0.0977, Val Loss: 0.0759\n",
      "Epoch 5/300 - Train Loss: 0.0922, Val Loss: 0.0735\n",
      "Epoch 6/300 - Train Loss: 0.0885, Val Loss: 0.0810\n",
      "Epoch 7/300 - Train Loss: 0.0891, Val Loss: 0.0716\n",
      "Epoch 8/300 - Train Loss: 0.0846, Val Loss: 0.0773\n",
      "Epoch 9/300 - Train Loss: 0.0851, Val Loss: 0.0721\n",
      "Epoch 10/300 - Train Loss: 0.0839, Val Loss: 0.0687\n",
      "Epoch 11/300 - Train Loss: 0.0800, Val Loss: 0.0679\n",
      "Epoch 12/300 - Train Loss: 0.0812, Val Loss: 0.0670\n",
      "Epoch 13/300 - Train Loss: 0.0814, Val Loss: 0.0809\n",
      "Epoch 14/300 - Train Loss: 0.0793, Val Loss: 0.0749\n",
      "Epoch 15/300 - Train Loss: 0.0770, Val Loss: 0.0706\n",
      "Epoch 16/300 - Train Loss: 0.0773, Val Loss: 0.0679\n",
      "Epoch 17/300 - Train Loss: 0.0774, Val Loss: 0.0674\n",
      "Epoch 18/300 - Train Loss: 0.0755, Val Loss: 0.0720\n",
      "Epoch 19/300 - Train Loss: 0.0739, Val Loss: 0.0727\n",
      "Epoch 20/300 - Train Loss: 0.0724, Val Loss: 0.0707\n",
      "Epoch 21/300 - Train Loss: 0.0721, Val Loss: 0.0706\n",
      "Epoch 22/300 - Train Loss: 0.0725, Val Loss: 0.0724\n",
      "Epoch 23/300 - Train Loss: 0.0730, Val Loss: 0.0705\n",
      "Epoch 24/300 - Train Loss: 0.0729, Val Loss: 0.0686\n",
      "Epoch 25/300 - Train Loss: 0.0697, Val Loss: 0.0732\n",
      "Epoch 26/300 - Train Loss: 0.0707, Val Loss: 0.0680\n",
      "Epoch 27/300 - Train Loss: 0.0695, Val Loss: 0.0662\n",
      "Epoch 28/300 - Train Loss: 0.0700, Val Loss: 0.0783\n",
      "Epoch 29/300 - Train Loss: 0.0668, Val Loss: 0.0694\n",
      "Epoch 30/300 - Train Loss: 0.0687, Val Loss: 0.0716\n",
      "Epoch 31/300 - Train Loss: 0.0659, Val Loss: 0.0704\n",
      "Epoch 32/300 - Train Loss: 0.0667, Val Loss: 0.0686\n",
      "Epoch 33/300 - Train Loss: 0.0677, Val Loss: 0.0667\n",
      "Epoch 34/300 - Train Loss: 0.0684, Val Loss: 0.0678\n",
      "Epoch 35/300 - Train Loss: 0.0686, Val Loss: 0.0689\n",
      "Epoch 36/300 - Train Loss: 0.0652, Val Loss: 0.0661\n",
      "Epoch 37/300 - Train Loss: 0.0647, Val Loss: 0.0694\n",
      "Epoch 38/300 - Train Loss: 0.0659, Val Loss: 0.0686\n",
      "Epoch 39/300 - Train Loss: 0.0627, Val Loss: 0.0662\n",
      "Epoch 40/300 - Train Loss: 0.0640, Val Loss: 0.0653\n",
      "Epoch 41/300 - Train Loss: 0.0624, Val Loss: 0.0723\n",
      "Epoch 42/300 - Train Loss: 0.0641, Val Loss: 0.0691\n",
      "Epoch 43/300 - Train Loss: 0.0632, Val Loss: 0.0672\n",
      "Epoch 44/300 - Train Loss: 0.0618, Val Loss: 0.0653\n",
      "Epoch 45/300 - Train Loss: 0.0608, Val Loss: 0.0680\n",
      "Epoch 46/300 - Train Loss: 0.0627, Val Loss: 0.0671\n",
      "Epoch 47/300 - Train Loss: 0.0592, Val Loss: 0.0676\n",
      "Epoch 48/300 - Train Loss: 0.0618, Val Loss: 0.0714\n",
      "Epoch 49/300 - Train Loss: 0.0610, Val Loss: 0.0657\n",
      "Epoch 50/300 - Train Loss: 0.0622, Val Loss: 0.0702\n",
      "Epoch 51/300 - Train Loss: 0.0603, Val Loss: 0.0666\n",
      "Epoch 52/300 - Train Loss: 0.0589, Val Loss: 0.0706\n",
      "Epoch 53/300 - Train Loss: 0.0606, Val Loss: 0.0705\n",
      "Epoch 54/300 - Train Loss: 0.0608, Val Loss: 0.0687\n",
      "Epoch 55/300 - Train Loss: 0.0597, Val Loss: 0.0691\n",
      "Epoch 56/300 - Train Loss: 0.0592, Val Loss: 0.0802\n",
      "Epoch 57/300 - Train Loss: 0.0584, Val Loss: 0.0726\n",
      "Epoch 58/300 - Train Loss: 0.0601, Val Loss: 0.0670\n",
      "Epoch 59/300 - Train Loss: 0.0576, Val Loss: 0.0689\n",
      "Epoch 60/300 - Train Loss: 0.0599, Val Loss: 0.0649\n",
      "Epoch 61/300 - Train Loss: 0.0581, Val Loss: 0.0689\n",
      "Epoch 62/300 - Train Loss: 0.0567, Val Loss: 0.0708\n",
      "Epoch 63/300 - Train Loss: 0.0560, Val Loss: 0.0735\n",
      "Epoch 64/300 - Train Loss: 0.0561, Val Loss: 0.0702\n",
      "Epoch 65/300 - Train Loss: 0.0580, Val Loss: 0.0669\n",
      "Epoch 66/300 - Train Loss: 0.0549, Val Loss: 0.0686\n",
      "Epoch 67/300 - Train Loss: 0.0556, Val Loss: 0.0685\n",
      "Epoch 68/300 - Train Loss: 0.0557, Val Loss: 0.0729\n",
      "Epoch 69/300 - Train Loss: 0.0548, Val Loss: 0.0692\n",
      "Epoch 70/300 - Train Loss: 0.0536, Val Loss: 0.0706\n",
      "Epoch 71/300 - Train Loss: 0.0558, Val Loss: 0.0663\n",
      "Epoch 72/300 - Train Loss: 0.0535, Val Loss: 0.0749\n",
      "Epoch 73/300 - Train Loss: 0.0529, Val Loss: 0.0713\n",
      "Epoch 74/300 - Train Loss: 0.0547, Val Loss: 0.0746\n",
      "Epoch 75/300 - Train Loss: 0.0533, Val Loss: 0.0746\n",
      "Epoch 76/300 - Train Loss: 0.0519, Val Loss: 0.0687\n",
      "Epoch 77/300 - Train Loss: 0.0513, Val Loss: 0.0718\n",
      "Epoch 78/300 - Train Loss: 0.0520, Val Loss: 0.0700\n",
      "Epoch 79/300 - Train Loss: 0.0522, Val Loss: 0.0750\n",
      "Epoch 80/300 - Train Loss: 0.0534, Val Loss: 0.0667\n",
      "Epoch 81/300 - Train Loss: 0.0521, Val Loss: 0.0706\n",
      "Epoch 82/300 - Train Loss: 0.0502, Val Loss: 0.0681\n",
      "Epoch 83/300 - Train Loss: 0.0513, Val Loss: 0.0681\n",
      "Epoch 84/300 - Train Loss: 0.0526, Val Loss: 0.0694\n",
      "Epoch 85/300 - Train Loss: 0.0493, Val Loss: 0.0704\n",
      "Epoch 86/300 - Train Loss: 0.0506, Val Loss: 0.0710\n",
      "Epoch 87/300 - Train Loss: 0.0533, Val Loss: 0.0766\n",
      "Epoch 88/300 - Train Loss: 0.0504, Val Loss: 0.0697\n",
      "Epoch 89/300 - Train Loss: 0.0475, Val Loss: 0.0683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:30:04,581] Trial 222 finished with value: 0.9654701595935271 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.28756988749260903, 'learning_rate': 7.728688153471754e-05, 'batch_size': 32, 'weight_decay': 7.615816578940512e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/300 - Train Loss: 0.0498, Val Loss: 0.0714\n",
      "Early stopping at epoch 90\n",
      "Macro F1 Score: 0.9655, Macro Precision: 0.9659, Macro Recall: 0.9651\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.98      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 224\n",
      "Training with F1=16, F2=32, D=4, dropout=0.20862672312725622, LR=6.496036366024574e-05, BS=32, WD=0.00010225105601911783\n",
      "Epoch 1/300 - Train Loss: 0.3503, Val Loss: 0.1823\n",
      "Epoch 2/300 - Train Loss: 0.1688, Val Loss: 0.1335\n",
      "Epoch 3/300 - Train Loss: 0.1187, Val Loss: 0.0898\n",
      "Epoch 4/300 - Train Loss: 0.1065, Val Loss: 0.0888\n",
      "Epoch 5/300 - Train Loss: 0.0992, Val Loss: 0.0826\n",
      "Epoch 6/300 - Train Loss: 0.0944, Val Loss: 0.0758\n",
      "Epoch 7/300 - Train Loss: 0.0943, Val Loss: 0.0744\n",
      "Epoch 8/300 - Train Loss: 0.0908, Val Loss: 0.0779\n",
      "Epoch 9/300 - Train Loss: 0.0907, Val Loss: 0.0751\n",
      "Epoch 10/300 - Train Loss: 0.0889, Val Loss: 0.0722\n",
      "Epoch 11/300 - Train Loss: 0.0866, Val Loss: 0.0748\n",
      "Epoch 12/300 - Train Loss: 0.0834, Val Loss: 0.0832\n",
      "Epoch 13/300 - Train Loss: 0.0848, Val Loss: 0.0744\n",
      "Epoch 14/300 - Train Loss: 0.0823, Val Loss: 0.0755\n",
      "Epoch 15/300 - Train Loss: 0.0839, Val Loss: 0.0771\n",
      "Epoch 16/300 - Train Loss: 0.0819, Val Loss: 0.0739\n",
      "Epoch 17/300 - Train Loss: 0.0827, Val Loss: 0.0761\n",
      "Epoch 18/300 - Train Loss: 0.0822, Val Loss: 0.0747\n",
      "Epoch 19/300 - Train Loss: 0.0831, Val Loss: 0.0774\n",
      "Epoch 20/300 - Train Loss: 0.0786, Val Loss: 0.0752\n",
      "Epoch 21/300 - Train Loss: 0.0808, Val Loss: 0.0713\n",
      "Epoch 22/300 - Train Loss: 0.0805, Val Loss: 0.0719\n",
      "Epoch 23/300 - Train Loss: 0.0795, Val Loss: 0.0710\n",
      "Epoch 24/300 - Train Loss: 0.0802, Val Loss: 0.0732\n",
      "Epoch 25/300 - Train Loss: 0.0779, Val Loss: 0.0741\n",
      "Epoch 26/300 - Train Loss: 0.0764, Val Loss: 0.0719\n",
      "Epoch 27/300 - Train Loss: 0.0753, Val Loss: 0.0739\n",
      "Epoch 28/300 - Train Loss: 0.0760, Val Loss: 0.0726\n",
      "Epoch 29/300 - Train Loss: 0.0755, Val Loss: 0.0743\n",
      "Epoch 30/300 - Train Loss: 0.0769, Val Loss: 0.0743\n",
      "Epoch 31/300 - Train Loss: 0.0749, Val Loss: 0.0716\n",
      "Epoch 32/300 - Train Loss: 0.0748, Val Loss: 0.0709\n",
      "Epoch 33/300 - Train Loss: 0.0766, Val Loss: 0.0713\n",
      "Epoch 34/300 - Train Loss: 0.0726, Val Loss: 0.0702\n",
      "Epoch 35/300 - Train Loss: 0.0730, Val Loss: 0.0750\n",
      "Epoch 36/300 - Train Loss: 0.0728, Val Loss: 0.0737\n",
      "Epoch 37/300 - Train Loss: 0.0739, Val Loss: 0.0686\n",
      "Epoch 38/300 - Train Loss: 0.0734, Val Loss: 0.0690\n",
      "Epoch 39/300 - Train Loss: 0.0725, Val Loss: 0.0712\n",
      "Epoch 40/300 - Train Loss: 0.0743, Val Loss: 0.0726\n",
      "Epoch 41/300 - Train Loss: 0.0731, Val Loss: 0.0709\n",
      "Epoch 42/300 - Train Loss: 0.0714, Val Loss: 0.0722\n",
      "Epoch 43/300 - Train Loss: 0.0715, Val Loss: 0.0711\n",
      "Epoch 44/300 - Train Loss: 0.0710, Val Loss: 0.0697\n",
      "Epoch 45/300 - Train Loss: 0.0721, Val Loss: 0.0721\n",
      "Epoch 46/300 - Train Loss: 0.0713, Val Loss: 0.0893\n",
      "Epoch 47/300 - Train Loss: 0.0712, Val Loss: 0.0733\n",
      "Epoch 48/300 - Train Loss: 0.0693, Val Loss: 0.0731\n",
      "Epoch 49/300 - Train Loss: 0.0693, Val Loss: 0.0710\n",
      "Epoch 50/300 - Train Loss: 0.0681, Val Loss: 0.0740\n",
      "Epoch 51/300 - Train Loss: 0.0708, Val Loss: 0.0702\n",
      "Epoch 52/300 - Train Loss: 0.0685, Val Loss: 0.0723\n",
      "Epoch 53/300 - Train Loss: 0.0676, Val Loss: 0.0680\n",
      "Epoch 54/300 - Train Loss: 0.0709, Val Loss: 0.0723\n",
      "Epoch 55/300 - Train Loss: 0.0695, Val Loss: 0.0691\n",
      "Epoch 56/300 - Train Loss: 0.0675, Val Loss: 0.0715\n",
      "Epoch 57/300 - Train Loss: 0.0678, Val Loss: 0.0691\n",
      "Epoch 58/300 - Train Loss: 0.0671, Val Loss: 0.0667\n",
      "Epoch 59/300 - Train Loss: 0.0671, Val Loss: 0.0680\n",
      "Epoch 60/300 - Train Loss: 0.0702, Val Loss: 0.0713\n",
      "Epoch 61/300 - Train Loss: 0.0666, Val Loss: 0.0677\n",
      "Epoch 62/300 - Train Loss: 0.0657, Val Loss: 0.0689\n",
      "Epoch 63/300 - Train Loss: 0.0679, Val Loss: 0.0714\n",
      "Epoch 64/300 - Train Loss: 0.0633, Val Loss: 0.0693\n",
      "Epoch 65/300 - Train Loss: 0.0657, Val Loss: 0.0730\n",
      "Epoch 66/300 - Train Loss: 0.0647, Val Loss: 0.0724\n",
      "Epoch 67/300 - Train Loss: 0.0636, Val Loss: 0.0749\n",
      "Epoch 68/300 - Train Loss: 0.0648, Val Loss: 0.0690\n",
      "Epoch 69/300 - Train Loss: 0.0654, Val Loss: 0.0710\n",
      "Epoch 70/300 - Train Loss: 0.0653, Val Loss: 0.0782\n",
      "Epoch 71/300 - Train Loss: 0.0646, Val Loss: 0.0685\n",
      "Epoch 72/300 - Train Loss: 0.0643, Val Loss: 0.0732\n",
      "Epoch 73/300 - Train Loss: 0.0672, Val Loss: 0.0693\n",
      "Epoch 74/300 - Train Loss: 0.0662, Val Loss: 0.0696\n",
      "Epoch 75/300 - Train Loss: 0.0624, Val Loss: 0.0673\n",
      "Epoch 76/300 - Train Loss: 0.0662, Val Loss: 0.0722\n",
      "Epoch 77/300 - Train Loss: 0.0621, Val Loss: 0.0717\n",
      "Epoch 78/300 - Train Loss: 0.0621, Val Loss: 0.0689\n",
      "Epoch 79/300 - Train Loss: 0.0628, Val Loss: 0.0762\n",
      "Epoch 80/300 - Train Loss: 0.0614, Val Loss: 0.0699\n",
      "Epoch 81/300 - Train Loss: 0.0646, Val Loss: 0.0723\n",
      "Epoch 82/300 - Train Loss: 0.0618, Val Loss: 0.0720\n",
      "Epoch 83/300 - Train Loss: 0.0606, Val Loss: 0.0735\n",
      "Epoch 84/300 - Train Loss: 0.0607, Val Loss: 0.0682\n",
      "Epoch 85/300 - Train Loss: 0.0625, Val Loss: 0.0699\n",
      "Epoch 86/300 - Train Loss: 0.0590, Val Loss: 0.0701\n",
      "Epoch 87/300 - Train Loss: 0.0620, Val Loss: 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:33:45,724] Trial 223 finished with value: 0.9711080965747363 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.20862672312725622, 'learning_rate': 6.496036366024574e-05, 'batch_size': 32, 'weight_decay': 0.00010225105601911783}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/300 - Train Loss: 0.0615, Val Loss: 0.0689\n",
      "Early stopping at epoch 88\n",
      "Macro F1 Score: 0.9711, Macro Precision: 0.9613, Macro Recall: 0.9820\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 225\n",
      "Training with F1=16, F2=32, D=2, dropout=0.2662130732634095, LR=0.000149681778163496, BS=32, WD=0.0003481795001868335\n",
      "Epoch 1/300 - Train Loss: 0.2806, Val Loss: 0.1280\n",
      "Epoch 2/300 - Train Loss: 0.1282, Val Loss: 0.0884\n",
      "Epoch 3/300 - Train Loss: 0.1105, Val Loss: 0.0965\n",
      "Epoch 4/300 - Train Loss: 0.1055, Val Loss: 0.0875\n",
      "Epoch 5/300 - Train Loss: 0.1007, Val Loss: 0.0832\n",
      "Epoch 6/300 - Train Loss: 0.0983, Val Loss: 0.0786\n",
      "Epoch 7/300 - Train Loss: 0.0936, Val Loss: 0.0747\n",
      "Epoch 8/300 - Train Loss: 0.0943, Val Loss: 0.0759\n",
      "Epoch 9/300 - Train Loss: 0.0919, Val Loss: 0.0802\n",
      "Epoch 10/300 - Train Loss: 0.0918, Val Loss: 0.0750\n",
      "Epoch 11/300 - Train Loss: 0.0884, Val Loss: 0.0708\n",
      "Epoch 12/300 - Train Loss: 0.0858, Val Loss: 0.0786\n",
      "Epoch 13/300 - Train Loss: 0.0870, Val Loss: 0.0770\n",
      "Epoch 14/300 - Train Loss: 0.0870, Val Loss: 0.0729\n",
      "Epoch 15/300 - Train Loss: 0.0860, Val Loss: 0.0757\n",
      "Epoch 16/300 - Train Loss: 0.0870, Val Loss: 0.0787\n",
      "Epoch 17/300 - Train Loss: 0.0841, Val Loss: 0.0713\n",
      "Epoch 18/300 - Train Loss: 0.0833, Val Loss: 0.0699\n",
      "Epoch 19/300 - Train Loss: 0.0838, Val Loss: 0.0737\n",
      "Epoch 20/300 - Train Loss: 0.0840, Val Loss: 0.0724\n",
      "Epoch 21/300 - Train Loss: 0.0837, Val Loss: 0.0723\n",
      "Epoch 22/300 - Train Loss: 0.0834, Val Loss: 0.0737\n",
      "Epoch 23/300 - Train Loss: 0.0829, Val Loss: 0.0708\n",
      "Epoch 24/300 - Train Loss: 0.0834, Val Loss: 0.0717\n",
      "Epoch 25/300 - Train Loss: 0.0818, Val Loss: 0.0691\n",
      "Epoch 26/300 - Train Loss: 0.0803, Val Loss: 0.0689\n",
      "Epoch 27/300 - Train Loss: 0.0810, Val Loss: 0.0763\n",
      "Epoch 28/300 - Train Loss: 0.0813, Val Loss: 0.0756\n",
      "Epoch 29/300 - Train Loss: 0.0794, Val Loss: 0.0679\n",
      "Epoch 30/300 - Train Loss: 0.0828, Val Loss: 0.0764\n",
      "Epoch 31/300 - Train Loss: 0.0804, Val Loss: 0.0690\n",
      "Epoch 32/300 - Train Loss: 0.0779, Val Loss: 0.0681\n",
      "Epoch 33/300 - Train Loss: 0.0790, Val Loss: 0.0727\n",
      "Epoch 34/300 - Train Loss: 0.0755, Val Loss: 0.0719\n",
      "Epoch 35/300 - Train Loss: 0.0765, Val Loss: 0.0761\n",
      "Epoch 36/300 - Train Loss: 0.0800, Val Loss: 0.0745\n",
      "Epoch 37/300 - Train Loss: 0.0784, Val Loss: 0.0727\n",
      "Epoch 38/300 - Train Loss: 0.0775, Val Loss: 0.0705\n",
      "Epoch 39/300 - Train Loss: 0.0797, Val Loss: 0.0738\n",
      "Epoch 40/300 - Train Loss: 0.0772, Val Loss: 0.0714\n",
      "Epoch 41/300 - Train Loss: 0.0760, Val Loss: 0.0770\n",
      "Epoch 42/300 - Train Loss: 0.0750, Val Loss: 0.0726\n",
      "Epoch 43/300 - Train Loss: 0.0746, Val Loss: 0.0776\n",
      "Epoch 44/300 - Train Loss: 0.0748, Val Loss: 0.0705\n",
      "Epoch 45/300 - Train Loss: 0.0750, Val Loss: 0.0707\n",
      "Epoch 46/300 - Train Loss: 0.0740, Val Loss: 0.0725\n",
      "Epoch 47/300 - Train Loss: 0.0770, Val Loss: 0.0709\n",
      "Epoch 48/300 - Train Loss: 0.0745, Val Loss: 0.0710\n",
      "Epoch 49/300 - Train Loss: 0.0729, Val Loss: 0.0722\n",
      "Epoch 50/300 - Train Loss: 0.0754, Val Loss: 0.0707\n",
      "Epoch 51/300 - Train Loss: 0.0747, Val Loss: 0.0757\n",
      "Epoch 52/300 - Train Loss: 0.0748, Val Loss: 0.0716\n",
      "Epoch 53/300 - Train Loss: 0.0729, Val Loss: 0.0701\n",
      "Epoch 54/300 - Train Loss: 0.0736, Val Loss: 0.0697\n",
      "Epoch 55/300 - Train Loss: 0.0748, Val Loss: 0.0703\n",
      "Epoch 56/300 - Train Loss: 0.0707, Val Loss: 0.0750\n",
      "Epoch 57/300 - Train Loss: 0.0726, Val Loss: 0.0701\n",
      "Epoch 58/300 - Train Loss: 0.0733, Val Loss: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:35:52,719] Trial 224 finished with value: 0.9664784294024383 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.2662130732634095, 'learning_rate': 0.000149681778163496, 'batch_size': 32, 'weight_decay': 0.0003481795001868335}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300 - Train Loss: 0.0745, Val Loss: 0.0711\n",
      "Early stopping at epoch 59\n",
      "Macro F1 Score: 0.9665, Macro Precision: 0.9608, Macro Recall: 0.9726\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 226\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1733138749598674, LR=0.00010186208283486587, BS=32, WD=4.329081583032333e-05\n",
      "Epoch 1/300 - Train Loss: 0.2393, Val Loss: 0.1150\n",
      "Epoch 2/300 - Train Loss: 0.1097, Val Loss: 0.0800\n",
      "Epoch 3/300 - Train Loss: 0.1000, Val Loss: 0.0759\n",
      "Epoch 4/300 - Train Loss: 0.0935, Val Loss: 0.0779\n",
      "Epoch 5/300 - Train Loss: 0.0894, Val Loss: 0.0806\n",
      "Epoch 6/300 - Train Loss: 0.0864, Val Loss: 0.0705\n",
      "Epoch 7/300 - Train Loss: 0.0853, Val Loss: 0.0699\n",
      "Epoch 8/300 - Train Loss: 0.0850, Val Loss: 0.0827\n",
      "Epoch 9/300 - Train Loss: 0.0851, Val Loss: 0.0760\n",
      "Epoch 10/300 - Train Loss: 0.0799, Val Loss: 0.0693\n",
      "Epoch 11/300 - Train Loss: 0.0801, Val Loss: 0.0717\n",
      "Epoch 12/300 - Train Loss: 0.0791, Val Loss: 0.0672\n",
      "Epoch 13/300 - Train Loss: 0.0781, Val Loss: 0.0691\n",
      "Epoch 14/300 - Train Loss: 0.0781, Val Loss: 0.0683\n",
      "Epoch 15/300 - Train Loss: 0.0754, Val Loss: 0.0645\n",
      "Epoch 16/300 - Train Loss: 0.0748, Val Loss: 0.0719\n",
      "Epoch 17/300 - Train Loss: 0.0739, Val Loss: 0.0687\n",
      "Epoch 18/300 - Train Loss: 0.0737, Val Loss: 0.0703\n",
      "Epoch 19/300 - Train Loss: 0.0704, Val Loss: 0.0676\n",
      "Epoch 20/300 - Train Loss: 0.0709, Val Loss: 0.0708\n",
      "Epoch 21/300 - Train Loss: 0.0702, Val Loss: 0.0696\n",
      "Epoch 22/300 - Train Loss: 0.0705, Val Loss: 0.0673\n",
      "Epoch 23/300 - Train Loss: 0.0691, Val Loss: 0.0644\n",
      "Epoch 24/300 - Train Loss: 0.0722, Val Loss: 0.0823\n",
      "Epoch 25/300 - Train Loss: 0.0734, Val Loss: 0.0651\n",
      "Epoch 26/300 - Train Loss: 0.0703, Val Loss: 0.0692\n",
      "Epoch 27/300 - Train Loss: 0.0678, Val Loss: 0.0664\n",
      "Epoch 28/300 - Train Loss: 0.0660, Val Loss: 0.0644\n",
      "Epoch 29/300 - Train Loss: 0.0653, Val Loss: 0.0681\n",
      "Epoch 30/300 - Train Loss: 0.0674, Val Loss: 0.0718\n",
      "Epoch 31/300 - Train Loss: 0.0662, Val Loss: 0.0724\n",
      "Epoch 32/300 - Train Loss: 0.0654, Val Loss: 0.0674\n",
      "Epoch 33/300 - Train Loss: 0.0641, Val Loss: 0.0695\n",
      "Epoch 34/300 - Train Loss: 0.0648, Val Loss: 0.0640\n",
      "Epoch 35/300 - Train Loss: 0.0640, Val Loss: 0.0690\n",
      "Epoch 36/300 - Train Loss: 0.0630, Val Loss: 0.0635\n",
      "Epoch 37/300 - Train Loss: 0.0621, Val Loss: 0.0653\n",
      "Epoch 38/300 - Train Loss: 0.0615, Val Loss: 0.0648\n",
      "Epoch 39/300 - Train Loss: 0.0596, Val Loss: 0.0620\n",
      "Epoch 40/300 - Train Loss: 0.0610, Val Loss: 0.0616\n",
      "Epoch 41/300 - Train Loss: 0.0598, Val Loss: 0.0682\n",
      "Epoch 42/300 - Train Loss: 0.0591, Val Loss: 0.0677\n",
      "Epoch 43/300 - Train Loss: 0.0596, Val Loss: 0.0691\n",
      "Epoch 44/300 - Train Loss: 0.0583, Val Loss: 0.0699\n",
      "Epoch 45/300 - Train Loss: 0.0580, Val Loss: 0.0605\n",
      "Epoch 46/300 - Train Loss: 0.0585, Val Loss: 0.0627\n",
      "Epoch 47/300 - Train Loss: 0.0574, Val Loss: 0.0651\n",
      "Epoch 48/300 - Train Loss: 0.0561, Val Loss: 0.0643\n",
      "Epoch 49/300 - Train Loss: 0.0571, Val Loss: 0.0747\n",
      "Epoch 50/300 - Train Loss: 0.0584, Val Loss: 0.0796\n",
      "Epoch 51/300 - Train Loss: 0.0585, Val Loss: 0.0672\n",
      "Epoch 52/300 - Train Loss: 0.0546, Val Loss: 0.0642\n",
      "Epoch 53/300 - Train Loss: 0.0565, Val Loss: 0.0655\n",
      "Epoch 54/300 - Train Loss: 0.0558, Val Loss: 0.0667\n",
      "Epoch 55/300 - Train Loss: 0.0578, Val Loss: 0.0672\n",
      "Epoch 56/300 - Train Loss: 0.0536, Val Loss: 0.0659\n",
      "Epoch 57/300 - Train Loss: 0.0531, Val Loss: 0.0697\n",
      "Epoch 58/300 - Train Loss: 0.0546, Val Loss: 0.0656\n",
      "Epoch 59/300 - Train Loss: 0.0550, Val Loss: 0.0655\n",
      "Epoch 60/300 - Train Loss: 0.0516, Val Loss: 0.0655\n",
      "Epoch 61/300 - Train Loss: 0.0514, Val Loss: 0.0644\n",
      "Epoch 62/300 - Train Loss: 0.0501, Val Loss: 0.0692\n",
      "Epoch 63/300 - Train Loss: 0.0505, Val Loss: 0.0639\n",
      "Epoch 64/300 - Train Loss: 0.0505, Val Loss: 0.0694\n",
      "Epoch 65/300 - Train Loss: 0.0521, Val Loss: 0.0666\n",
      "Epoch 66/300 - Train Loss: 0.0495, Val Loss: 0.0669\n",
      "Epoch 67/300 - Train Loss: 0.0485, Val Loss: 0.0690\n",
      "Epoch 68/300 - Train Loss: 0.0495, Val Loss: 0.0702\n",
      "Epoch 69/300 - Train Loss: 0.0491, Val Loss: 0.0675\n",
      "Epoch 70/300 - Train Loss: 0.0501, Val Loss: 0.0680\n",
      "Epoch 71/300 - Train Loss: 0.0483, Val Loss: 0.0703\n",
      "Epoch 72/300 - Train Loss: 0.0483, Val Loss: 0.0670\n",
      "Epoch 73/300 - Train Loss: 0.0475, Val Loss: 0.0660\n",
      "Epoch 74/300 - Train Loss: 0.0498, Val Loss: 0.0648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:40:42,123] Trial 225 finished with value: 0.9733977597094897 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1733138749598674, 'learning_rate': 0.00010186208283486587, 'batch_size': 32, 'weight_decay': 4.329081583032333e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300 - Train Loss: 0.0467, Val Loss: 0.0658\n",
      "Early stopping at epoch 75\n",
      "Macro F1 Score: 0.9734, Macro Precision: 0.9739, Macro Recall: 0.9729\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 227\n",
      "Training with F1=16, F2=32, D=8, dropout=0.17119767621008897, LR=0.00010127792150029031, BS=32, WD=4.24029427192872e-05\n",
      "Epoch 1/300 - Train Loss: 0.2429, Val Loss: 0.1004\n",
      "Epoch 2/300 - Train Loss: 0.1095, Val Loss: 0.0883\n",
      "Epoch 3/300 - Train Loss: 0.0982, Val Loss: 0.0788\n",
      "Epoch 4/300 - Train Loss: 0.0958, Val Loss: 0.0704\n",
      "Epoch 5/300 - Train Loss: 0.0884, Val Loss: 0.0849\n",
      "Epoch 6/300 - Train Loss: 0.0878, Val Loss: 0.0834\n",
      "Epoch 7/300 - Train Loss: 0.0860, Val Loss: 0.0742\n",
      "Epoch 8/300 - Train Loss: 0.0835, Val Loss: 0.0723\n",
      "Epoch 9/300 - Train Loss: 0.0796, Val Loss: 0.0707\n",
      "Epoch 10/300 - Train Loss: 0.0802, Val Loss: 0.0760\n",
      "Epoch 11/300 - Train Loss: 0.0793, Val Loss: 0.0864\n",
      "Epoch 12/300 - Train Loss: 0.0767, Val Loss: 0.0793\n",
      "Epoch 13/300 - Train Loss: 0.0780, Val Loss: 0.0691\n",
      "Epoch 14/300 - Train Loss: 0.0727, Val Loss: 0.0747\n",
      "Epoch 15/300 - Train Loss: 0.0730, Val Loss: 0.0757\n",
      "Epoch 16/300 - Train Loss: 0.0717, Val Loss: 0.0757\n",
      "Epoch 17/300 - Train Loss: 0.0732, Val Loss: 0.0716\n",
      "Epoch 18/300 - Train Loss: 0.0697, Val Loss: 0.0745\n",
      "Epoch 19/300 - Train Loss: 0.0720, Val Loss: 0.0747\n",
      "Epoch 20/300 - Train Loss: 0.0711, Val Loss: 0.0770\n",
      "Epoch 21/300 - Train Loss: 0.0676, Val Loss: 0.0678\n",
      "Epoch 22/300 - Train Loss: 0.0715, Val Loss: 0.0730\n",
      "Epoch 23/300 - Train Loss: 0.0681, Val Loss: 0.0677\n",
      "Epoch 24/300 - Train Loss: 0.0661, Val Loss: 0.0972\n",
      "Epoch 25/300 - Train Loss: 0.0708, Val Loss: 0.0739\n",
      "Epoch 26/300 - Train Loss: 0.0677, Val Loss: 0.0764\n",
      "Epoch 27/300 - Train Loss: 0.0649, Val Loss: 0.0675\n",
      "Epoch 28/300 - Train Loss: 0.0668, Val Loss: 0.0680\n",
      "Epoch 29/300 - Train Loss: 0.0627, Val Loss: 0.0699\n",
      "Epoch 30/300 - Train Loss: 0.0643, Val Loss: 0.0726\n",
      "Epoch 31/300 - Train Loss: 0.0654, Val Loss: 0.0721\n",
      "Epoch 32/300 - Train Loss: 0.0641, Val Loss: 0.0762\n",
      "Epoch 33/300 - Train Loss: 0.0632, Val Loss: 0.0737\n",
      "Epoch 34/300 - Train Loss: 0.0619, Val Loss: 0.0706\n",
      "Epoch 35/300 - Train Loss: 0.0594, Val Loss: 0.0666\n",
      "Epoch 36/300 - Train Loss: 0.0604, Val Loss: 0.0760\n",
      "Epoch 37/300 - Train Loss: 0.0596, Val Loss: 0.0700\n",
      "Epoch 38/300 - Train Loss: 0.0603, Val Loss: 0.0682\n",
      "Epoch 39/300 - Train Loss: 0.0618, Val Loss: 0.0708\n",
      "Epoch 40/300 - Train Loss: 0.0584, Val Loss: 0.0756\n",
      "Epoch 41/300 - Train Loss: 0.0607, Val Loss: 0.0718\n",
      "Epoch 42/300 - Train Loss: 0.0599, Val Loss: 0.0666\n",
      "Epoch 43/300 - Train Loss: 0.0598, Val Loss: 0.0707\n",
      "Epoch 44/300 - Train Loss: 0.0591, Val Loss: 0.0688\n",
      "Epoch 45/300 - Train Loss: 0.0562, Val Loss: 0.0677\n",
      "Epoch 46/300 - Train Loss: 0.0567, Val Loss: 0.0724\n",
      "Epoch 47/300 - Train Loss: 0.0568, Val Loss: 0.0709\n",
      "Epoch 48/300 - Train Loss: 0.0545, Val Loss: 0.0689\n",
      "Epoch 49/300 - Train Loss: 0.0540, Val Loss: 0.0722\n",
      "Epoch 50/300 - Train Loss: 0.0552, Val Loss: 0.0714\n",
      "Epoch 51/300 - Train Loss: 0.0551, Val Loss: 0.0693\n",
      "Epoch 52/300 - Train Loss: 0.0538, Val Loss: 0.0694\n",
      "Epoch 53/300 - Train Loss: 0.0528, Val Loss: 0.0703\n",
      "Epoch 54/300 - Train Loss: 0.0549, Val Loss: 0.0710\n",
      "Epoch 55/300 - Train Loss: 0.0537, Val Loss: 0.0707\n",
      "Epoch 56/300 - Train Loss: 0.0516, Val Loss: 0.0735\n",
      "Epoch 57/300 - Train Loss: 0.0517, Val Loss: 0.0706\n",
      "Epoch 58/300 - Train Loss: 0.0524, Val Loss: 0.0741\n",
      "Epoch 59/300 - Train Loss: 0.0511, Val Loss: 0.0685\n",
      "Epoch 60/300 - Train Loss: 0.0508, Val Loss: 0.0700\n",
      "Epoch 61/300 - Train Loss: 0.0498, Val Loss: 0.0792\n",
      "Epoch 62/300 - Train Loss: 0.0504, Val Loss: 0.0701\n",
      "Epoch 63/300 - Train Loss: 0.0536, Val Loss: 0.0788\n",
      "Epoch 64/300 - Train Loss: 0.0521, Val Loss: 0.0761\n",
      "Epoch 65/300 - Train Loss: 0.0499, Val Loss: 0.0752\n",
      "Epoch 66/300 - Train Loss: 0.0498, Val Loss: 0.0710\n",
      "Epoch 67/300 - Train Loss: 0.0484, Val Loss: 0.0737\n",
      "Epoch 68/300 - Train Loss: 0.0487, Val Loss: 0.0715\n",
      "Epoch 69/300 - Train Loss: 0.0485, Val Loss: 0.0728\n",
      "Epoch 70/300 - Train Loss: 0.0488, Val Loss: 0.0705\n",
      "Epoch 71/300 - Train Loss: 0.0475, Val Loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:45:19,635] Trial 226 finished with value: 0.9708273854231493 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.17119767621008897, 'learning_rate': 0.00010127792150029031, 'batch_size': 32, 'weight_decay': 4.24029427192872e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300 - Train Loss: 0.0483, Val Loss: 0.0755\n",
      "Early stopping at epoch 72\n",
      "Macro F1 Score: 0.9708, Macro Precision: 0.9648, Macro Recall: 0.9773\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 228\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1856845297709775, LR=0.00011165617365945767, BS=64, WD=5.191129112810572e-05\n",
      "Epoch 1/300 - Train Loss: 0.2891, Val Loss: 0.1332\n",
      "Epoch 2/300 - Train Loss: 0.1136, Val Loss: 0.0975\n",
      "Epoch 3/300 - Train Loss: 0.0966, Val Loss: 0.0803\n",
      "Epoch 4/300 - Train Loss: 0.0913, Val Loss: 0.0861\n",
      "Epoch 5/300 - Train Loss: 0.0871, Val Loss: 0.0812\n",
      "Epoch 6/300 - Train Loss: 0.0841, Val Loss: 0.0845\n",
      "Epoch 7/300 - Train Loss: 0.0813, Val Loss: 0.0756\n",
      "Epoch 8/300 - Train Loss: 0.0792, Val Loss: 0.0745\n",
      "Epoch 9/300 - Train Loss: 0.0769, Val Loss: 0.0799\n",
      "Epoch 10/300 - Train Loss: 0.0767, Val Loss: 0.0711\n",
      "Epoch 11/300 - Train Loss: 0.0764, Val Loss: 0.0726\n",
      "Epoch 12/300 - Train Loss: 0.0744, Val Loss: 0.0714\n",
      "Epoch 13/300 - Train Loss: 0.0737, Val Loss: 0.0709\n",
      "Epoch 14/300 - Train Loss: 0.0723, Val Loss: 0.0687\n",
      "Epoch 15/300 - Train Loss: 0.0727, Val Loss: 0.0699\n",
      "Epoch 16/300 - Train Loss: 0.0720, Val Loss: 0.0708\n",
      "Epoch 17/300 - Train Loss: 0.0724, Val Loss: 0.0702\n",
      "Epoch 18/300 - Train Loss: 0.0706, Val Loss: 0.0685\n",
      "Epoch 19/300 - Train Loss: 0.0686, Val Loss: 0.0806\n",
      "Epoch 20/300 - Train Loss: 0.0683, Val Loss: 0.0910\n",
      "Epoch 21/300 - Train Loss: 0.0670, Val Loss: 0.0743\n",
      "Epoch 22/300 - Train Loss: 0.0659, Val Loss: 0.0679\n",
      "Epoch 23/300 - Train Loss: 0.0674, Val Loss: 0.0704\n",
      "Epoch 24/300 - Train Loss: 0.0657, Val Loss: 0.0666\n",
      "Epoch 25/300 - Train Loss: 0.0641, Val Loss: 0.0682\n",
      "Epoch 26/300 - Train Loss: 0.0653, Val Loss: 0.0676\n",
      "Epoch 27/300 - Train Loss: 0.0635, Val Loss: 0.0721\n",
      "Epoch 28/300 - Train Loss: 0.0635, Val Loss: 0.0700\n",
      "Epoch 29/300 - Train Loss: 0.0634, Val Loss: 0.0677\n",
      "Epoch 30/300 - Train Loss: 0.0627, Val Loss: 0.0689\n",
      "Epoch 31/300 - Train Loss: 0.0631, Val Loss: 0.0707\n",
      "Epoch 32/300 - Train Loss: 0.0596, Val Loss: 0.0706\n",
      "Epoch 33/300 - Train Loss: 0.0599, Val Loss: 0.0702\n",
      "Epoch 34/300 - Train Loss: 0.0602, Val Loss: 0.0704\n",
      "Epoch 35/300 - Train Loss: 0.0588, Val Loss: 0.0702\n",
      "Epoch 36/300 - Train Loss: 0.0590, Val Loss: 0.0719\n",
      "Epoch 37/300 - Train Loss: 0.0587, Val Loss: 0.0669\n",
      "Epoch 38/300 - Train Loss: 0.0580, Val Loss: 0.0686\n",
      "Epoch 39/300 - Train Loss: 0.0574, Val Loss: 0.0666\n",
      "Epoch 40/300 - Train Loss: 0.0583, Val Loss: 0.0697\n",
      "Epoch 41/300 - Train Loss: 0.0582, Val Loss: 0.0667\n",
      "Epoch 42/300 - Train Loss: 0.0548, Val Loss: 0.0673\n",
      "Epoch 43/300 - Train Loss: 0.0558, Val Loss: 0.0703\n",
      "Epoch 44/300 - Train Loss: 0.0547, Val Loss: 0.0721\n",
      "Epoch 45/300 - Train Loss: 0.0546, Val Loss: 0.0688\n",
      "Epoch 46/300 - Train Loss: 0.0534, Val Loss: 0.0693\n",
      "Epoch 47/300 - Train Loss: 0.0550, Val Loss: 0.0681\n",
      "Epoch 48/300 - Train Loss: 0.0540, Val Loss: 0.0711\n",
      "Epoch 49/300 - Train Loss: 0.0536, Val Loss: 0.0711\n",
      "Epoch 50/300 - Train Loss: 0.0530, Val Loss: 0.0726\n",
      "Epoch 51/300 - Train Loss: 0.0529, Val Loss: 0.0688\n",
      "Epoch 52/300 - Train Loss: 0.0522, Val Loss: 0.0711\n",
      "Epoch 53/300 - Train Loss: 0.0499, Val Loss: 0.0693\n",
      "Epoch 54/300 - Train Loss: 0.0511, Val Loss: 0.0699\n",
      "Epoch 55/300 - Train Loss: 0.0513, Val Loss: 0.0689\n",
      "Epoch 56/300 - Train Loss: 0.0515, Val Loss: 0.0731\n",
      "Epoch 57/300 - Train Loss: 0.0488, Val Loss: 0.0721\n",
      "Epoch 58/300 - Train Loss: 0.0489, Val Loss: 0.0720\n",
      "Epoch 59/300 - Train Loss: 0.0502, Val Loss: 0.0744\n",
      "Epoch 60/300 - Train Loss: 0.0488, Val Loss: 0.0712\n",
      "Epoch 61/300 - Train Loss: 0.0477, Val Loss: 0.0738\n",
      "Epoch 62/300 - Train Loss: 0.0477, Val Loss: 0.0722\n",
      "Epoch 63/300 - Train Loss: 0.0477, Val Loss: 0.0707\n",
      "Epoch 64/300 - Train Loss: 0.0472, Val Loss: 0.0683\n",
      "Epoch 65/300 - Train Loss: 0.0473, Val Loss: 0.0717\n",
      "Epoch 66/300 - Train Loss: 0.0446, Val Loss: 0.0711\n",
      "Epoch 67/300 - Train Loss: 0.0454, Val Loss: 0.0741\n",
      "Epoch 68/300 - Train Loss: 0.0467, Val Loss: 0.0696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:49:06,902] Trial 227 finished with value: 0.9672896763974274 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1856845297709775, 'learning_rate': 0.00011165617365945767, 'batch_size': 64, 'weight_decay': 5.191129112810572e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.0453, Val Loss: 0.0747\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9673, Macro Precision: 0.9634, Macro Recall: 0.9714\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 229\n",
      "Training with F1=16, F2=32, D=8, dropout=0.17164370834552914, LR=9.080449675683114e-05, BS=32, WD=3.0073415332483427e-05\n",
      "Epoch 1/300 - Train Loss: 0.2619, Val Loss: 0.1477\n",
      "Epoch 2/300 - Train Loss: 0.1294, Val Loss: 0.0832\n",
      "Epoch 3/300 - Train Loss: 0.1010, Val Loss: 0.0741\n",
      "Epoch 4/300 - Train Loss: 0.0962, Val Loss: 0.0744\n",
      "Epoch 5/300 - Train Loss: 0.0894, Val Loss: 0.0701\n",
      "Epoch 6/300 - Train Loss: 0.0866, Val Loss: 0.0726\n",
      "Epoch 7/300 - Train Loss: 0.0882, Val Loss: 0.0746\n",
      "Epoch 8/300 - Train Loss: 0.0851, Val Loss: 0.0806\n",
      "Epoch 9/300 - Train Loss: 0.0808, Val Loss: 0.0757\n",
      "Epoch 10/300 - Train Loss: 0.0800, Val Loss: 0.0809\n",
      "Epoch 11/300 - Train Loss: 0.0799, Val Loss: 0.0691\n",
      "Epoch 12/300 - Train Loss: 0.0801, Val Loss: 0.0699\n",
      "Epoch 13/300 - Train Loss: 0.0783, Val Loss: 0.0706\n",
      "Epoch 14/300 - Train Loss: 0.0783, Val Loss: 0.0723\n",
      "Epoch 15/300 - Train Loss: 0.0756, Val Loss: 0.0683\n",
      "Epoch 16/300 - Train Loss: 0.0756, Val Loss: 0.0686\n",
      "Epoch 17/300 - Train Loss: 0.0785, Val Loss: 0.0784\n",
      "Epoch 18/300 - Train Loss: 0.0742, Val Loss: 0.0781\n",
      "Epoch 19/300 - Train Loss: 0.0721, Val Loss: 0.0707\n",
      "Epoch 20/300 - Train Loss: 0.0721, Val Loss: 0.0659\n",
      "Epoch 21/300 - Train Loss: 0.0722, Val Loss: 0.0685\n",
      "Epoch 22/300 - Train Loss: 0.0714, Val Loss: 0.0683\n",
      "Epoch 23/300 - Train Loss: 0.0708, Val Loss: 0.0651\n",
      "Epoch 24/300 - Train Loss: 0.0694, Val Loss: 0.0654\n",
      "Epoch 25/300 - Train Loss: 0.0692, Val Loss: 0.0961\n",
      "Epoch 26/300 - Train Loss: 0.0695, Val Loss: 0.0736\n",
      "Epoch 27/300 - Train Loss: 0.0673, Val Loss: 0.0742\n",
      "Epoch 28/300 - Train Loss: 0.0673, Val Loss: 0.0685\n",
      "Epoch 29/300 - Train Loss: 0.0662, Val Loss: 0.0666\n",
      "Epoch 30/300 - Train Loss: 0.0663, Val Loss: 0.0694\n",
      "Epoch 31/300 - Train Loss: 0.0643, Val Loss: 0.0645\n",
      "Epoch 32/300 - Train Loss: 0.0650, Val Loss: 0.0658\n",
      "Epoch 33/300 - Train Loss: 0.0642, Val Loss: 0.0672\n",
      "Epoch 34/300 - Train Loss: 0.0638, Val Loss: 0.0723\n",
      "Epoch 35/300 - Train Loss: 0.0622, Val Loss: 0.0672\n",
      "Epoch 36/300 - Train Loss: 0.0624, Val Loss: 0.0663\n",
      "Epoch 37/300 - Train Loss: 0.0630, Val Loss: 0.0657\n",
      "Epoch 38/300 - Train Loss: 0.0620, Val Loss: 0.0623\n",
      "Epoch 39/300 - Train Loss: 0.0602, Val Loss: 0.0670\n",
      "Epoch 40/300 - Train Loss: 0.0602, Val Loss: 0.0649\n",
      "Epoch 41/300 - Train Loss: 0.0624, Val Loss: 0.0665\n",
      "Epoch 42/300 - Train Loss: 0.0594, Val Loss: 0.0635\n",
      "Epoch 43/300 - Train Loss: 0.0595, Val Loss: 0.0667\n",
      "Epoch 44/300 - Train Loss: 0.0561, Val Loss: 0.0637\n",
      "Epoch 45/300 - Train Loss: 0.0584, Val Loss: 0.0692\n",
      "Epoch 46/300 - Train Loss: 0.0571, Val Loss: 0.0700\n",
      "Epoch 47/300 - Train Loss: 0.0573, Val Loss: 0.0661\n",
      "Epoch 48/300 - Train Loss: 0.0560, Val Loss: 0.0645\n",
      "Epoch 49/300 - Train Loss: 0.0546, Val Loss: 0.0649\n",
      "Epoch 50/300 - Train Loss: 0.0593, Val Loss: 0.0650\n",
      "Epoch 51/300 - Train Loss: 0.0563, Val Loss: 0.0644\n",
      "Epoch 52/300 - Train Loss: 0.0541, Val Loss: 0.0675\n",
      "Epoch 53/300 - Train Loss: 0.0547, Val Loss: 0.0642\n",
      "Epoch 54/300 - Train Loss: 0.0552, Val Loss: 0.0677\n",
      "Epoch 55/300 - Train Loss: 0.0538, Val Loss: 0.0669\n",
      "Epoch 56/300 - Train Loss: 0.0547, Val Loss: 0.0674\n",
      "Epoch 57/300 - Train Loss: 0.0529, Val Loss: 0.0655\n",
      "Epoch 58/300 - Train Loss: 0.0529, Val Loss: 0.0675\n",
      "Epoch 59/300 - Train Loss: 0.0515, Val Loss: 0.0660\n",
      "Epoch 60/300 - Train Loss: 0.0540, Val Loss: 0.0702\n",
      "Epoch 61/300 - Train Loss: 0.0521, Val Loss: 0.0638\n",
      "Epoch 62/300 - Train Loss: 0.0510, Val Loss: 0.0664\n",
      "Epoch 63/300 - Train Loss: 0.0506, Val Loss: 0.0669\n",
      "Epoch 64/300 - Train Loss: 0.0494, Val Loss: 0.0682\n",
      "Epoch 65/300 - Train Loss: 0.0521, Val Loss: 0.0687\n",
      "Epoch 66/300 - Train Loss: 0.0502, Val Loss: 0.0675\n",
      "Epoch 67/300 - Train Loss: 0.0504, Val Loss: 0.0681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:53:29,109] Trial 228 finished with value: 0.9704629457236047 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.17164370834552914, 'learning_rate': 9.080449675683114e-05, 'batch_size': 32, 'weight_decay': 3.0073415332483427e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/300 - Train Loss: 0.0486, Val Loss: 0.0652\n",
      "Early stopping at epoch 68\n",
      "Macro F1 Score: 0.9705, Macro Precision: 0.9686, Macro Recall: 0.9724\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 230\n",
      "Training with F1=16, F2=32, D=8, dropout=0.13787192622422761, LR=8.499921317662713e-05, BS=32, WD=8.894017009179826e-05\n",
      "Epoch 1/300 - Train Loss: 0.2656, Val Loss: 0.1080\n",
      "Epoch 2/300 - Train Loss: 0.1138, Val Loss: 0.0772\n",
      "Epoch 3/300 - Train Loss: 0.1004, Val Loss: 0.0826\n",
      "Epoch 4/300 - Train Loss: 0.0952, Val Loss: 0.0736\n",
      "Epoch 5/300 - Train Loss: 0.0909, Val Loss: 0.0719\n",
      "Epoch 6/300 - Train Loss: 0.0908, Val Loss: 0.0737\n",
      "Epoch 7/300 - Train Loss: 0.0874, Val Loss: 0.0728\n",
      "Epoch 8/300 - Train Loss: 0.0841, Val Loss: 0.0774\n",
      "Epoch 9/300 - Train Loss: 0.0838, Val Loss: 0.0714\n",
      "Epoch 10/300 - Train Loss: 0.0812, Val Loss: 0.0693\n",
      "Epoch 11/300 - Train Loss: 0.0816, Val Loss: 0.0663\n",
      "Epoch 12/300 - Train Loss: 0.0814, Val Loss: 0.0671\n",
      "Epoch 13/300 - Train Loss: 0.0782, Val Loss: 0.0658\n",
      "Epoch 14/300 - Train Loss: 0.0780, Val Loss: 0.0623\n",
      "Epoch 15/300 - Train Loss: 0.0777, Val Loss: 0.0714\n",
      "Epoch 16/300 - Train Loss: 0.0780, Val Loss: 0.0675\n",
      "Epoch 17/300 - Train Loss: 0.0750, Val Loss: 0.0637\n",
      "Epoch 18/300 - Train Loss: 0.0724, Val Loss: 0.0749\n",
      "Epoch 19/300 - Train Loss: 0.0740, Val Loss: 0.0637\n",
      "Epoch 20/300 - Train Loss: 0.0723, Val Loss: 0.0638\n",
      "Epoch 21/300 - Train Loss: 0.0715, Val Loss: 0.0665\n",
      "Epoch 22/300 - Train Loss: 0.0731, Val Loss: 0.0651\n",
      "Epoch 23/300 - Train Loss: 0.0710, Val Loss: 0.0676\n",
      "Epoch 24/300 - Train Loss: 0.0703, Val Loss: 0.0671\n",
      "Epoch 25/300 - Train Loss: 0.0687, Val Loss: 0.0642\n",
      "Epoch 26/300 - Train Loss: 0.0690, Val Loss: 0.0609\n",
      "Epoch 27/300 - Train Loss: 0.0684, Val Loss: 0.0730\n",
      "Epoch 28/300 - Train Loss: 0.0667, Val Loss: 0.0648\n",
      "Epoch 29/300 - Train Loss: 0.0670, Val Loss: 0.0602\n",
      "Epoch 30/300 - Train Loss: 0.0671, Val Loss: 0.0661\n",
      "Epoch 31/300 - Train Loss: 0.0664, Val Loss: 0.0645\n",
      "Epoch 32/300 - Train Loss: 0.0663, Val Loss: 0.0652\n",
      "Epoch 33/300 - Train Loss: 0.0649, Val Loss: 0.0650\n",
      "Epoch 34/300 - Train Loss: 0.0642, Val Loss: 0.0640\n",
      "Epoch 35/300 - Train Loss: 0.0648, Val Loss: 0.0638\n",
      "Epoch 36/300 - Train Loss: 0.0619, Val Loss: 0.0632\n",
      "Epoch 37/300 - Train Loss: 0.0610, Val Loss: 0.0740\n",
      "Epoch 38/300 - Train Loss: 0.0614, Val Loss: 0.0624\n",
      "Epoch 39/300 - Train Loss: 0.0624, Val Loss: 0.0649\n",
      "Epoch 40/300 - Train Loss: 0.0602, Val Loss: 0.0635\n",
      "Epoch 41/300 - Train Loss: 0.0603, Val Loss: 0.0630\n",
      "Epoch 42/300 - Train Loss: 0.0604, Val Loss: 0.0620\n",
      "Epoch 43/300 - Train Loss: 0.0583, Val Loss: 0.0618\n",
      "Epoch 44/300 - Train Loss: 0.0587, Val Loss: 0.0676\n",
      "Epoch 45/300 - Train Loss: 0.0576, Val Loss: 0.0669\n",
      "Epoch 46/300 - Train Loss: 0.0578, Val Loss: 0.0669\n",
      "Epoch 47/300 - Train Loss: 0.0564, Val Loss: 0.0646\n",
      "Epoch 48/300 - Train Loss: 0.0598, Val Loss: 0.0664\n",
      "Epoch 49/300 - Train Loss: 0.0551, Val Loss: 0.0647\n",
      "Epoch 50/300 - Train Loss: 0.0559, Val Loss: 0.0702\n",
      "Epoch 51/300 - Train Loss: 0.0541, Val Loss: 0.0640\n",
      "Epoch 52/300 - Train Loss: 0.0552, Val Loss: 0.0688\n",
      "Epoch 53/300 - Train Loss: 0.0511, Val Loss: 0.0642\n",
      "Epoch 54/300 - Train Loss: 0.0541, Val Loss: 0.0684\n",
      "Epoch 55/300 - Train Loss: 0.0510, Val Loss: 0.0657\n",
      "Epoch 56/300 - Train Loss: 0.0506, Val Loss: 0.0706\n",
      "Epoch 57/300 - Train Loss: 0.0532, Val Loss: 0.0672\n",
      "Epoch 58/300 - Train Loss: 0.0498, Val Loss: 0.0672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 13:57:16,519] Trial 229 finished with value: 0.9731611531271973 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.13787192622422761, 'learning_rate': 8.499921317662713e-05, 'batch_size': 32, 'weight_decay': 8.894017009179826e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300 - Train Loss: 0.0552, Val Loss: 0.0747\n",
      "Early stopping at epoch 59\n",
      "Macro F1 Score: 0.9732, Macro Precision: 0.9691, Macro Recall: 0.9774\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 231\n",
      "Training with F1=16, F2=32, D=8, dropout=0.4106350954587748, LR=8.568202131995956e-05, BS=32, WD=9.037929992871111e-05\n",
      "Epoch 1/300 - Train Loss: 0.3054, Val Loss: 0.1682\n",
      "Epoch 2/300 - Train Loss: 0.1366, Val Loss: 0.0979\n",
      "Epoch 3/300 - Train Loss: 0.1113, Val Loss: 0.0855\n",
      "Epoch 4/300 - Train Loss: 0.1033, Val Loss: 0.0849\n",
      "Epoch 5/300 - Train Loss: 0.1012, Val Loss: 0.0809\n",
      "Epoch 6/300 - Train Loss: 0.0967, Val Loss: 0.0743\n",
      "Epoch 7/300 - Train Loss: 0.0957, Val Loss: 0.0796\n",
      "Epoch 8/300 - Train Loss: 0.0937, Val Loss: 0.0859\n",
      "Epoch 9/300 - Train Loss: 0.0926, Val Loss: 0.0788\n",
      "Epoch 10/300 - Train Loss: 0.0880, Val Loss: 0.0793\n",
      "Epoch 11/300 - Train Loss: 0.0917, Val Loss: 0.0705\n",
      "Epoch 12/300 - Train Loss: 0.0894, Val Loss: 0.0769\n",
      "Epoch 13/300 - Train Loss: 0.0867, Val Loss: 0.0740\n",
      "Epoch 14/300 - Train Loss: 0.0852, Val Loss: 0.0811\n",
      "Epoch 15/300 - Train Loss: 0.0858, Val Loss: 0.0762\n",
      "Epoch 16/300 - Train Loss: 0.0859, Val Loss: 0.0752\n",
      "Epoch 17/300 - Train Loss: 0.0856, Val Loss: 0.0710\n",
      "Epoch 18/300 - Train Loss: 0.0846, Val Loss: 0.0770\n",
      "Epoch 19/300 - Train Loss: 0.0831, Val Loss: 0.0742\n",
      "Epoch 20/300 - Train Loss: 0.0828, Val Loss: 0.0750\n",
      "Epoch 21/300 - Train Loss: 0.0824, Val Loss: 0.0669\n",
      "Epoch 22/300 - Train Loss: 0.0822, Val Loss: 0.0709\n",
      "Epoch 23/300 - Train Loss: 0.0816, Val Loss: 0.0697\n",
      "Epoch 24/300 - Train Loss: 0.0824, Val Loss: 0.0741\n",
      "Epoch 25/300 - Train Loss: 0.0799, Val Loss: 0.0673\n",
      "Epoch 26/300 - Train Loss: 0.0801, Val Loss: 0.0682\n",
      "Epoch 27/300 - Train Loss: 0.0807, Val Loss: 0.0708\n",
      "Epoch 28/300 - Train Loss: 0.0785, Val Loss: 0.0669\n",
      "Epoch 29/300 - Train Loss: 0.0781, Val Loss: 0.0747\n",
      "Epoch 30/300 - Train Loss: 0.0805, Val Loss: 0.0710\n",
      "Epoch 31/300 - Train Loss: 0.0783, Val Loss: 0.0677\n",
      "Epoch 32/300 - Train Loss: 0.0778, Val Loss: 0.0679\n",
      "Epoch 33/300 - Train Loss: 0.0756, Val Loss: 0.0668\n",
      "Epoch 34/300 - Train Loss: 0.0767, Val Loss: 0.0640\n",
      "Epoch 35/300 - Train Loss: 0.0750, Val Loss: 0.0679\n",
      "Epoch 36/300 - Train Loss: 0.0752, Val Loss: 0.0747\n",
      "Epoch 37/300 - Train Loss: 0.0763, Val Loss: 0.0754\n",
      "Epoch 38/300 - Train Loss: 0.0741, Val Loss: 0.0680\n",
      "Epoch 39/300 - Train Loss: 0.0768, Val Loss: 0.0790\n",
      "Epoch 40/300 - Train Loss: 0.0760, Val Loss: 0.0684\n",
      "Epoch 41/300 - Train Loss: 0.0731, Val Loss: 0.0721\n",
      "Epoch 42/300 - Train Loss: 0.0734, Val Loss: 0.0717\n",
      "Epoch 43/300 - Train Loss: 0.0762, Val Loss: 0.0733\n",
      "Epoch 44/300 - Train Loss: 0.0729, Val Loss: 0.0669\n",
      "Epoch 45/300 - Train Loss: 0.0727, Val Loss: 0.0670\n",
      "Epoch 46/300 - Train Loss: 0.0741, Val Loss: 0.0695\n",
      "Epoch 47/300 - Train Loss: 0.0730, Val Loss: 0.0698\n",
      "Epoch 48/300 - Train Loss: 0.0728, Val Loss: 0.0707\n",
      "Epoch 49/300 - Train Loss: 0.0723, Val Loss: 0.0683\n",
      "Epoch 50/300 - Train Loss: 0.0708, Val Loss: 0.0705\n",
      "Epoch 51/300 - Train Loss: 0.0725, Val Loss: 0.0661\n",
      "Epoch 52/300 - Train Loss: 0.0739, Val Loss: 0.0697\n",
      "Epoch 53/300 - Train Loss: 0.0728, Val Loss: 0.0721\n",
      "Epoch 54/300 - Train Loss: 0.0720, Val Loss: 0.0687\n",
      "Epoch 55/300 - Train Loss: 0.0731, Val Loss: 0.0688\n",
      "Epoch 56/300 - Train Loss: 0.0700, Val Loss: 0.0687\n",
      "Epoch 57/300 - Train Loss: 0.0708, Val Loss: 0.0721\n",
      "Epoch 58/300 - Train Loss: 0.0716, Val Loss: 0.0683\n",
      "Epoch 59/300 - Train Loss: 0.0709, Val Loss: 0.0701\n",
      "Epoch 60/300 - Train Loss: 0.0701, Val Loss: 0.0707\n",
      "Epoch 61/300 - Train Loss: 0.0721, Val Loss: 0.0748\n",
      "Epoch 62/300 - Train Loss: 0.0720, Val Loss: 0.0740\n",
      "Epoch 63/300 - Train Loss: 0.0688, Val Loss: 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:01:23,391] Trial 230 finished with value: 0.9711707143381645 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.4106350954587748, 'learning_rate': 8.568202131995956e-05, 'batch_size': 32, 'weight_decay': 9.037929992871111e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300 - Train Loss: 0.0694, Val Loss: 0.0679\n",
      "Early stopping at epoch 64\n",
      "Macro F1 Score: 0.9712, Macro Precision: 0.9678, Macro Recall: 0.9749\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 232\n",
      "Training with F1=16, F2=32, D=8, dropout=0.15969526413785062, LR=7.795006786136878e-05, BS=32, WD=7.516701202517976e-05\n",
      "Epoch 1/300 - Train Loss: 0.2550, Val Loss: 0.1102\n",
      "Epoch 2/300 - Train Loss: 0.1133, Val Loss: 0.0915\n",
      "Epoch 3/300 - Train Loss: 0.0993, Val Loss: 0.0809\n",
      "Epoch 4/300 - Train Loss: 0.0978, Val Loss: 0.0847\n",
      "Epoch 5/300 - Train Loss: 0.0910, Val Loss: 0.0831\n",
      "Epoch 6/300 - Train Loss: 0.0902, Val Loss: 0.0734\n",
      "Epoch 7/300 - Train Loss: 0.0871, Val Loss: 0.0727\n",
      "Epoch 8/300 - Train Loss: 0.0840, Val Loss: 0.0732\n",
      "Epoch 9/300 - Train Loss: 0.0832, Val Loss: 0.0710\n",
      "Epoch 10/300 - Train Loss: 0.0821, Val Loss: 0.0742\n",
      "Epoch 11/300 - Train Loss: 0.0806, Val Loss: 0.0674\n",
      "Epoch 12/300 - Train Loss: 0.0792, Val Loss: 0.0691\n",
      "Epoch 13/300 - Train Loss: 0.0781, Val Loss: 0.0696\n",
      "Epoch 14/300 - Train Loss: 0.0764, Val Loss: 0.0755\n",
      "Epoch 15/300 - Train Loss: 0.0774, Val Loss: 0.0738\n",
      "Epoch 16/300 - Train Loss: 0.0756, Val Loss: 0.0688\n",
      "Epoch 17/300 - Train Loss: 0.0755, Val Loss: 0.0678\n",
      "Epoch 18/300 - Train Loss: 0.0768, Val Loss: 0.0798\n",
      "Epoch 19/300 - Train Loss: 0.0734, Val Loss: 0.0698\n",
      "Epoch 20/300 - Train Loss: 0.0735, Val Loss: 0.0660\n",
      "Epoch 21/300 - Train Loss: 0.0722, Val Loss: 0.0694\n",
      "Epoch 22/300 - Train Loss: 0.0711, Val Loss: 0.0704\n",
      "Epoch 23/300 - Train Loss: 0.0722, Val Loss: 0.0775\n",
      "Epoch 24/300 - Train Loss: 0.0710, Val Loss: 0.0685\n",
      "Epoch 25/300 - Train Loss: 0.0700, Val Loss: 0.0678\n",
      "Epoch 26/300 - Train Loss: 0.0691, Val Loss: 0.0671\n",
      "Epoch 27/300 - Train Loss: 0.0679, Val Loss: 0.0679\n",
      "Epoch 28/300 - Train Loss: 0.0696, Val Loss: 0.0691\n",
      "Epoch 29/300 - Train Loss: 0.0665, Val Loss: 0.0681\n",
      "Epoch 30/300 - Train Loss: 0.0669, Val Loss: 0.0739\n",
      "Epoch 31/300 - Train Loss: 0.0662, Val Loss: 0.0689\n",
      "Epoch 32/300 - Train Loss: 0.0648, Val Loss: 0.0683\n",
      "Epoch 33/300 - Train Loss: 0.0652, Val Loss: 0.0711\n",
      "Epoch 34/300 - Train Loss: 0.0663, Val Loss: 0.0645\n",
      "Epoch 35/300 - Train Loss: 0.0657, Val Loss: 0.0645\n",
      "Epoch 36/300 - Train Loss: 0.0630, Val Loss: 0.0643\n",
      "Epoch 37/300 - Train Loss: 0.0617, Val Loss: 0.0642\n",
      "Epoch 38/300 - Train Loss: 0.0639, Val Loss: 0.0689\n",
      "Epoch 39/300 - Train Loss: 0.0617, Val Loss: 0.0689\n",
      "Epoch 40/300 - Train Loss: 0.0622, Val Loss: 0.0688\n",
      "Epoch 41/300 - Train Loss: 0.0614, Val Loss: 0.0733\n",
      "Epoch 42/300 - Train Loss: 0.0596, Val Loss: 0.0688\n",
      "Epoch 43/300 - Train Loss: 0.0629, Val Loss: 0.0688\n",
      "Epoch 44/300 - Train Loss: 0.0607, Val Loss: 0.0664\n",
      "Epoch 45/300 - Train Loss: 0.0600, Val Loss: 0.0658\n",
      "Epoch 46/300 - Train Loss: 0.0587, Val Loss: 0.0716\n",
      "Epoch 47/300 - Train Loss: 0.0579, Val Loss: 0.0647\n",
      "Epoch 48/300 - Train Loss: 0.0608, Val Loss: 0.0632\n",
      "Epoch 49/300 - Train Loss: 0.0588, Val Loss: 0.0731\n",
      "Epoch 50/300 - Train Loss: 0.0578, Val Loss: 0.0694\n",
      "Epoch 51/300 - Train Loss: 0.0576, Val Loss: 0.0720\n",
      "Epoch 52/300 - Train Loss: 0.0558, Val Loss: 0.0777\n",
      "Epoch 53/300 - Train Loss: 0.0559, Val Loss: 0.0698\n",
      "Epoch 54/300 - Train Loss: 0.0553, Val Loss: 0.0731\n",
      "Epoch 55/300 - Train Loss: 0.0565, Val Loss: 0.0695\n",
      "Epoch 56/300 - Train Loss: 0.0594, Val Loss: 0.0669\n",
      "Epoch 57/300 - Train Loss: 0.0560, Val Loss: 0.0714\n",
      "Epoch 58/300 - Train Loss: 0.0549, Val Loss: 0.0698\n",
      "Epoch 59/300 - Train Loss: 0.0565, Val Loss: 0.0724\n",
      "Epoch 60/300 - Train Loss: 0.0529, Val Loss: 0.0670\n",
      "Epoch 61/300 - Train Loss: 0.0529, Val Loss: 0.0675\n",
      "Epoch 62/300 - Train Loss: 0.0557, Val Loss: 0.0715\n",
      "Epoch 63/300 - Train Loss: 0.0536, Val Loss: 0.0692\n",
      "Epoch 64/300 - Train Loss: 0.0537, Val Loss: 0.0706\n",
      "Epoch 65/300 - Train Loss: 0.0530, Val Loss: 0.0696\n",
      "Epoch 66/300 - Train Loss: 0.0504, Val Loss: 0.0702\n",
      "Epoch 67/300 - Train Loss: 0.0537, Val Loss: 0.0746\n",
      "Epoch 68/300 - Train Loss: 0.0547, Val Loss: 0.0708\n",
      "Epoch 69/300 - Train Loss: 0.0554, Val Loss: 0.0731\n",
      "Epoch 70/300 - Train Loss: 0.0511, Val Loss: 0.0709\n",
      "Epoch 71/300 - Train Loss: 0.0504, Val Loss: 0.0732\n",
      "Epoch 72/300 - Train Loss: 0.0501, Val Loss: 0.0728\n",
      "Epoch 73/300 - Train Loss: 0.0540, Val Loss: 0.0820\n",
      "Epoch 74/300 - Train Loss: 0.0502, Val Loss: 0.0683\n",
      "Epoch 75/300 - Train Loss: 0.0509, Val Loss: 0.0674\n",
      "Epoch 76/300 - Train Loss: 0.0506, Val Loss: 0.0809\n",
      "Epoch 77/300 - Train Loss: 0.0500, Val Loss: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:06:24,074] Trial 231 finished with value: 0.9706880167595714 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.15969526413785062, 'learning_rate': 7.795006786136878e-05, 'batch_size': 32, 'weight_decay': 7.516701202517976e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300 - Train Loss: 0.0487, Val Loss: 0.0718\n",
      "Early stopping at epoch 78\n",
      "Macro F1 Score: 0.9707, Macro Precision: 0.9609, Macro Recall: 0.9816\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 233\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1315412803679775, LR=0.00010024086624881712, BS=32, WD=9.912405492201295e-05\n",
      "Epoch 1/300 - Train Loss: 0.2407, Val Loss: 0.0951\n",
      "Epoch 2/300 - Train Loss: 0.1048, Val Loss: 0.0798\n",
      "Epoch 3/300 - Train Loss: 0.0960, Val Loss: 0.0833\n",
      "Epoch 4/300 - Train Loss: 0.0917, Val Loss: 0.0869\n",
      "Epoch 5/300 - Train Loss: 0.0886, Val Loss: 0.0764\n",
      "Epoch 6/300 - Train Loss: 0.0863, Val Loss: 0.0770\n",
      "Epoch 7/300 - Train Loss: 0.0845, Val Loss: 0.0695\n",
      "Epoch 8/300 - Train Loss: 0.0834, Val Loss: 0.0718\n",
      "Epoch 9/300 - Train Loss: 0.0796, Val Loss: 0.0716\n",
      "Epoch 10/300 - Train Loss: 0.0808, Val Loss: 0.0746\n",
      "Epoch 11/300 - Train Loss: 0.0778, Val Loss: 0.0711\n",
      "Epoch 12/300 - Train Loss: 0.0784, Val Loss: 0.0696\n",
      "Epoch 13/300 - Train Loss: 0.0753, Val Loss: 0.0701\n",
      "Epoch 14/300 - Train Loss: 0.0747, Val Loss: 0.0801\n",
      "Epoch 15/300 - Train Loss: 0.0754, Val Loss: 0.0735\n",
      "Epoch 16/300 - Train Loss: 0.0740, Val Loss: 0.0758\n",
      "Epoch 17/300 - Train Loss: 0.0739, Val Loss: 0.0751\n",
      "Epoch 18/300 - Train Loss: 0.0725, Val Loss: 0.0724\n",
      "Epoch 19/300 - Train Loss: 0.0742, Val Loss: 0.0690\n",
      "Epoch 20/300 - Train Loss: 0.0734, Val Loss: 0.0713\n",
      "Epoch 21/300 - Train Loss: 0.0701, Val Loss: 0.0675\n",
      "Epoch 22/300 - Train Loss: 0.0704, Val Loss: 0.0711\n",
      "Epoch 23/300 - Train Loss: 0.0693, Val Loss: 0.0685\n",
      "Epoch 24/300 - Train Loss: 0.0684, Val Loss: 0.0646\n",
      "Epoch 25/300 - Train Loss: 0.0677, Val Loss: 0.0747\n",
      "Epoch 26/300 - Train Loss: 0.0661, Val Loss: 0.0702\n",
      "Epoch 27/300 - Train Loss: 0.0663, Val Loss: 0.0673\n",
      "Epoch 28/300 - Train Loss: 0.0640, Val Loss: 0.0721\n",
      "Epoch 29/300 - Train Loss: 0.0648, Val Loss: 0.0656\n",
      "Epoch 30/300 - Train Loss: 0.0646, Val Loss: 0.0678\n",
      "Epoch 31/300 - Train Loss: 0.0629, Val Loss: 0.0692\n",
      "Epoch 32/300 - Train Loss: 0.0623, Val Loss: 0.0682\n",
      "Epoch 33/300 - Train Loss: 0.0626, Val Loss: 0.0722\n",
      "Epoch 34/300 - Train Loss: 0.0599, Val Loss: 0.0667\n",
      "Epoch 35/300 - Train Loss: 0.0631, Val Loss: 0.0685\n",
      "Epoch 36/300 - Train Loss: 0.0616, Val Loss: 0.0647\n",
      "Epoch 37/300 - Train Loss: 0.0598, Val Loss: 0.0693\n",
      "Epoch 38/300 - Train Loss: 0.0589, Val Loss: 0.0757\n",
      "Epoch 39/300 - Train Loss: 0.0584, Val Loss: 0.0722\n",
      "Epoch 40/300 - Train Loss: 0.0578, Val Loss: 0.0694\n",
      "Epoch 41/300 - Train Loss: 0.0590, Val Loss: 0.0668\n",
      "Epoch 42/300 - Train Loss: 0.0591, Val Loss: 0.0744\n",
      "Epoch 43/300 - Train Loss: 0.0559, Val Loss: 0.0686\n",
      "Epoch 44/300 - Train Loss: 0.0573, Val Loss: 0.0732\n",
      "Epoch 45/300 - Train Loss: 0.0574, Val Loss: 0.0724\n",
      "Epoch 46/300 - Train Loss: 0.0567, Val Loss: 0.0671\n",
      "Epoch 47/300 - Train Loss: 0.0542, Val Loss: 0.0668\n",
      "Epoch 48/300 - Train Loss: 0.0542, Val Loss: 0.0666\n",
      "Epoch 49/300 - Train Loss: 0.0528, Val Loss: 0.0713\n",
      "Epoch 50/300 - Train Loss: 0.0537, Val Loss: 0.0735\n",
      "Epoch 51/300 - Train Loss: 0.0534, Val Loss: 0.0682\n",
      "Epoch 52/300 - Train Loss: 0.0520, Val Loss: 0.0681\n",
      "Epoch 53/300 - Train Loss: 0.0528, Val Loss: 0.0693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:09:52,139] Trial 232 finished with value: 0.9659493091800059 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1315412803679775, 'learning_rate': 0.00010024086624881712, 'batch_size': 32, 'weight_decay': 9.912405492201295e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300 - Train Loss: 0.0534, Val Loss: 0.0687\n",
      "Early stopping at epoch 54\n",
      "Macro F1 Score: 0.9659, Macro Precision: 0.9670, Macro Recall: 0.9651\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 234\n",
      "Training with F1=16, F2=32, D=8, dropout=0.48735027075200626, LR=7.156720394604653e-05, BS=32, WD=0.00011328443940733491\n",
      "Epoch 1/300 - Train Loss: 0.3392, Val Loss: 0.1894\n",
      "Epoch 2/300 - Train Loss: 0.1738, Val Loss: 0.1290\n",
      "Epoch 3/300 - Train Loss: 0.1305, Val Loss: 0.0976\n",
      "Epoch 4/300 - Train Loss: 0.1149, Val Loss: 0.0902\n",
      "Epoch 5/300 - Train Loss: 0.1063, Val Loss: 0.0817\n",
      "Epoch 6/300 - Train Loss: 0.1040, Val Loss: 0.0818\n",
      "Epoch 7/300 - Train Loss: 0.1022, Val Loss: 0.0763\n",
      "Epoch 8/300 - Train Loss: 0.0981, Val Loss: 0.0796\n",
      "Epoch 9/300 - Train Loss: 0.0974, Val Loss: 0.0792\n",
      "Epoch 10/300 - Train Loss: 0.0966, Val Loss: 0.0726\n",
      "Epoch 11/300 - Train Loss: 0.0948, Val Loss: 0.0736\n",
      "Epoch 12/300 - Train Loss: 0.0952, Val Loss: 0.0739\n",
      "Epoch 13/300 - Train Loss: 0.0928, Val Loss: 0.0773\n",
      "Epoch 14/300 - Train Loss: 0.0932, Val Loss: 0.0699\n",
      "Epoch 15/300 - Train Loss: 0.0914, Val Loss: 0.0705\n",
      "Epoch 16/300 - Train Loss: 0.0910, Val Loss: 0.0707\n",
      "Epoch 17/300 - Train Loss: 0.0894, Val Loss: 0.0749\n",
      "Epoch 18/300 - Train Loss: 0.0879, Val Loss: 0.0675\n",
      "Epoch 19/300 - Train Loss: 0.0879, Val Loss: 0.0773\n",
      "Epoch 20/300 - Train Loss: 0.0861, Val Loss: 0.0715\n",
      "Epoch 21/300 - Train Loss: 0.0846, Val Loss: 0.0698\n",
      "Epoch 22/300 - Train Loss: 0.0864, Val Loss: 0.0663\n",
      "Epoch 23/300 - Train Loss: 0.0872, Val Loss: 0.0695\n",
      "Epoch 24/300 - Train Loss: 0.0874, Val Loss: 0.0748\n",
      "Epoch 25/300 - Train Loss: 0.0830, Val Loss: 0.0738\n",
      "Epoch 26/300 - Train Loss: 0.0836, Val Loss: 0.0685\n",
      "Epoch 27/300 - Train Loss: 0.0841, Val Loss: 0.0682\n",
      "Epoch 28/300 - Train Loss: 0.0850, Val Loss: 0.0689\n",
      "Epoch 29/300 - Train Loss: 0.0816, Val Loss: 0.0746\n",
      "Epoch 30/300 - Train Loss: 0.0814, Val Loss: 0.0701\n",
      "Epoch 31/300 - Train Loss: 0.0844, Val Loss: 0.0782\n",
      "Epoch 32/300 - Train Loss: 0.0818, Val Loss: 0.0701\n",
      "Epoch 33/300 - Train Loss: 0.0823, Val Loss: 0.0724\n",
      "Epoch 34/300 - Train Loss: 0.0816, Val Loss: 0.0716\n",
      "Epoch 35/300 - Train Loss: 0.0788, Val Loss: 0.0702\n",
      "Epoch 36/300 - Train Loss: 0.0825, Val Loss: 0.0718\n",
      "Epoch 37/300 - Train Loss: 0.0818, Val Loss: 0.0716\n",
      "Epoch 38/300 - Train Loss: 0.0816, Val Loss: 0.0715\n",
      "Epoch 39/300 - Train Loss: 0.0791, Val Loss: 0.0735\n",
      "Epoch 40/300 - Train Loss: 0.0809, Val Loss: 0.0707\n",
      "Epoch 41/300 - Train Loss: 0.0820, Val Loss: 0.0694\n",
      "Epoch 42/300 - Train Loss: 0.0798, Val Loss: 0.0715\n",
      "Epoch 43/300 - Train Loss: 0.0804, Val Loss: 0.0683\n",
      "Epoch 44/300 - Train Loss: 0.0797, Val Loss: 0.0777\n",
      "Epoch 45/300 - Train Loss: 0.0797, Val Loss: 0.0678\n",
      "Epoch 46/300 - Train Loss: 0.0817, Val Loss: 0.0711\n",
      "Epoch 47/300 - Train Loss: 0.0764, Val Loss: 0.0672\n",
      "Epoch 48/300 - Train Loss: 0.0795, Val Loss: 0.0656\n",
      "Epoch 49/300 - Train Loss: 0.0808, Val Loss: 0.0704\n",
      "Epoch 50/300 - Train Loss: 0.0793, Val Loss: 0.0812\n",
      "Epoch 51/300 - Train Loss: 0.0800, Val Loss: 0.0730\n",
      "Epoch 52/300 - Train Loss: 0.0778, Val Loss: 0.0716\n",
      "Epoch 53/300 - Train Loss: 0.0809, Val Loss: 0.0699\n",
      "Epoch 54/300 - Train Loss: 0.0783, Val Loss: 0.0703\n",
      "Epoch 55/300 - Train Loss: 0.0777, Val Loss: 0.0690\n",
      "Epoch 56/300 - Train Loss: 0.0775, Val Loss: 0.0974\n",
      "Epoch 57/300 - Train Loss: 0.0776, Val Loss: 0.0724\n",
      "Epoch 58/300 - Train Loss: 0.0781, Val Loss: 0.0734\n",
      "Epoch 59/300 - Train Loss: 0.0779, Val Loss: 0.0685\n",
      "Epoch 60/300 - Train Loss: 0.0739, Val Loss: 0.0757\n",
      "Epoch 61/300 - Train Loss: 0.0814, Val Loss: 0.0814\n",
      "Epoch 62/300 - Train Loss: 0.0755, Val Loss: 0.0736\n",
      "Epoch 63/300 - Train Loss: 0.0742, Val Loss: 0.0683\n",
      "Epoch 64/300 - Train Loss: 0.0760, Val Loss: 0.0698\n",
      "Epoch 65/300 - Train Loss: 0.0754, Val Loss: 0.0717\n",
      "Epoch 66/300 - Train Loss: 0.0753, Val Loss: 0.0687\n",
      "Epoch 67/300 - Train Loss: 0.0756, Val Loss: 0.0710\n",
      "Epoch 68/300 - Train Loss: 0.0757, Val Loss: 0.0691\n",
      "Epoch 69/300 - Train Loss: 0.0740, Val Loss: 0.0699\n",
      "Epoch 70/300 - Train Loss: 0.0755, Val Loss: 0.0659\n",
      "Epoch 71/300 - Train Loss: 0.0764, Val Loss: 0.0691\n",
      "Epoch 72/300 - Train Loss: 0.0754, Val Loss: 0.0692\n",
      "Epoch 73/300 - Train Loss: 0.0751, Val Loss: 0.0705\n",
      "Epoch 74/300 - Train Loss: 0.0769, Val Loss: 0.0664\n",
      "Epoch 75/300 - Train Loss: 0.0758, Val Loss: 0.0702\n",
      "Epoch 76/300 - Train Loss: 0.0756, Val Loss: 0.0672\n",
      "Epoch 77/300 - Train Loss: 0.0763, Val Loss: 0.0679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:14:52,993] Trial 233 finished with value: 0.965992733490673 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.48735027075200626, 'learning_rate': 7.156720394604653e-05, 'batch_size': 32, 'weight_decay': 0.00011328443940733491}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300 - Train Loss: 0.0757, Val Loss: 0.0718\n",
      "Early stopping at epoch 78\n",
      "Macro F1 Score: 0.9660, Macro Precision: 0.9599, Macro Recall: 0.9725\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 235\n",
      "Training with F1=16, F2=32, D=8, dropout=0.14932724407459658, LR=9.315741048562412e-05, BS=32, WD=6.618638381087309e-05\n",
      "Epoch 1/300 - Train Loss: 0.2691, Val Loss: 0.1240\n",
      "Epoch 2/300 - Train Loss: 0.1205, Val Loss: 0.1034\n",
      "Epoch 3/300 - Train Loss: 0.1014, Val Loss: 0.0728\n",
      "Epoch 4/300 - Train Loss: 0.0940, Val Loss: 0.0748\n",
      "Epoch 5/300 - Train Loss: 0.0898, Val Loss: 0.0709\n",
      "Epoch 6/300 - Train Loss: 0.0879, Val Loss: 0.0820\n",
      "Epoch 7/300 - Train Loss: 0.0865, Val Loss: 0.0740\n",
      "Epoch 8/300 - Train Loss: 0.0849, Val Loss: 0.0739\n",
      "Epoch 9/300 - Train Loss: 0.0826, Val Loss: 0.0753\n",
      "Epoch 10/300 - Train Loss: 0.0810, Val Loss: 0.0797\n",
      "Epoch 11/300 - Train Loss: 0.0819, Val Loss: 0.0678\n",
      "Epoch 12/300 - Train Loss: 0.0794, Val Loss: 0.0652\n",
      "Epoch 13/300 - Train Loss: 0.0764, Val Loss: 0.0692\n",
      "Epoch 14/300 - Train Loss: 0.0753, Val Loss: 0.0716\n",
      "Epoch 15/300 - Train Loss: 0.0759, Val Loss: 0.0667\n",
      "Epoch 16/300 - Train Loss: 0.0754, Val Loss: 0.0723\n",
      "Epoch 17/300 - Train Loss: 0.0736, Val Loss: 0.0688\n",
      "Epoch 18/300 - Train Loss: 0.0727, Val Loss: 0.0681\n",
      "Epoch 19/300 - Train Loss: 0.0723, Val Loss: 0.0739\n",
      "Epoch 20/300 - Train Loss: 0.0711, Val Loss: 0.0677\n",
      "Epoch 21/300 - Train Loss: 0.0714, Val Loss: 0.0682\n",
      "Epoch 22/300 - Train Loss: 0.0711, Val Loss: 0.0714\n",
      "Epoch 23/300 - Train Loss: 0.0713, Val Loss: 0.0688\n",
      "Epoch 24/300 - Train Loss: 0.0693, Val Loss: 0.0641\n",
      "Epoch 25/300 - Train Loss: 0.0692, Val Loss: 0.0742\n",
      "Epoch 26/300 - Train Loss: 0.0673, Val Loss: 0.0683\n",
      "Epoch 27/300 - Train Loss: 0.0672, Val Loss: 0.0635\n",
      "Epoch 28/300 - Train Loss: 0.0655, Val Loss: 0.0686\n",
      "Epoch 29/300 - Train Loss: 0.0671, Val Loss: 0.0674\n",
      "Epoch 30/300 - Train Loss: 0.0667, Val Loss: 0.0688\n",
      "Epoch 31/300 - Train Loss: 0.0660, Val Loss: 0.0654\n",
      "Epoch 32/300 - Train Loss: 0.0646, Val Loss: 0.0652\n",
      "Epoch 33/300 - Train Loss: 0.0631, Val Loss: 0.0651\n",
      "Epoch 34/300 - Train Loss: 0.0638, Val Loss: 0.0628\n",
      "Epoch 35/300 - Train Loss: 0.0615, Val Loss: 0.0666\n",
      "Epoch 36/300 - Train Loss: 0.0630, Val Loss: 0.0650\n",
      "Epoch 37/300 - Train Loss: 0.0624, Val Loss: 0.0704\n",
      "Epoch 38/300 - Train Loss: 0.0621, Val Loss: 0.0659\n",
      "Epoch 39/300 - Train Loss: 0.0606, Val Loss: 0.0641\n",
      "Epoch 40/300 - Train Loss: 0.0601, Val Loss: 0.0760\n",
      "Epoch 41/300 - Train Loss: 0.0593, Val Loss: 0.0636\n",
      "Epoch 42/300 - Train Loss: 0.0593, Val Loss: 0.0704\n",
      "Epoch 43/300 - Train Loss: 0.0599, Val Loss: 0.0646\n",
      "Epoch 44/300 - Train Loss: 0.0574, Val Loss: 0.0657\n",
      "Epoch 45/300 - Train Loss: 0.0579, Val Loss: 0.0704\n",
      "Epoch 46/300 - Train Loss: 0.0559, Val Loss: 0.0670\n",
      "Epoch 47/300 - Train Loss: 0.0545, Val Loss: 0.0649\n",
      "Epoch 48/300 - Train Loss: 0.0560, Val Loss: 0.0667\n",
      "Epoch 49/300 - Train Loss: 0.0556, Val Loss: 0.0656\n",
      "Epoch 50/300 - Train Loss: 0.0553, Val Loss: 0.0718\n",
      "Epoch 51/300 - Train Loss: 0.0568, Val Loss: 0.0668\n",
      "Epoch 52/300 - Train Loss: 0.0569, Val Loss: 0.0704\n",
      "Epoch 53/300 - Train Loss: 0.0549, Val Loss: 0.0628\n",
      "Epoch 54/300 - Train Loss: 0.0524, Val Loss: 0.0681\n",
      "Epoch 55/300 - Train Loss: 0.0538, Val Loss: 0.0668\n",
      "Epoch 56/300 - Train Loss: 0.0535, Val Loss: 0.0656\n",
      "Epoch 57/300 - Train Loss: 0.0538, Val Loss: 0.0640\n",
      "Epoch 58/300 - Train Loss: 0.0526, Val Loss: 0.0630\n",
      "Epoch 59/300 - Train Loss: 0.0500, Val Loss: 0.0669\n",
      "Epoch 60/300 - Train Loss: 0.0512, Val Loss: 0.0715\n",
      "Epoch 61/300 - Train Loss: 0.0517, Val Loss: 0.0669\n",
      "Epoch 62/300 - Train Loss: 0.0496, Val Loss: 0.0629\n",
      "Epoch 63/300 - Train Loss: 0.0532, Val Loss: 0.0615\n",
      "Epoch 64/300 - Train Loss: 0.0512, Val Loss: 0.0660\n",
      "Epoch 65/300 - Train Loss: 0.0507, Val Loss: 0.0680\n",
      "Epoch 66/300 - Train Loss: 0.0495, Val Loss: 0.0657\n",
      "Epoch 67/300 - Train Loss: 0.0512, Val Loss: 0.0655\n",
      "Epoch 68/300 - Train Loss: 0.0519, Val Loss: 0.0665\n",
      "Epoch 69/300 - Train Loss: 0.0467, Val Loss: 0.0706\n",
      "Epoch 70/300 - Train Loss: 0.0469, Val Loss: 0.0669\n",
      "Epoch 71/300 - Train Loss: 0.0485, Val Loss: 0.0692\n",
      "Epoch 72/300 - Train Loss: 0.0465, Val Loss: 0.0700\n",
      "Epoch 73/300 - Train Loss: 0.0467, Val Loss: 0.0719\n",
      "Epoch 74/300 - Train Loss: 0.0446, Val Loss: 0.0714\n",
      "Epoch 75/300 - Train Loss: 0.0465, Val Loss: 0.0692\n",
      "Epoch 76/300 - Train Loss: 0.0474, Val Loss: 0.0696\n",
      "Epoch 77/300 - Train Loss: 0.0484, Val Loss: 0.0685\n",
      "Epoch 78/300 - Train Loss: 0.0468, Val Loss: 0.0663\n",
      "Epoch 79/300 - Train Loss: 0.0455, Val Loss: 0.0722\n",
      "Epoch 80/300 - Train Loss: 0.0444, Val Loss: 0.0693\n",
      "Epoch 81/300 - Train Loss: 0.0438, Val Loss: 0.0700\n",
      "Epoch 82/300 - Train Loss: 0.0443, Val Loss: 0.0661\n",
      "Epoch 83/300 - Train Loss: 0.0424, Val Loss: 0.0663\n",
      "Epoch 84/300 - Train Loss: 0.0428, Val Loss: 0.0702\n",
      "Epoch 85/300 - Train Loss: 0.0450, Val Loss: 0.0741\n",
      "Epoch 86/300 - Train Loss: 0.0434, Val Loss: 0.0687\n",
      "Epoch 87/300 - Train Loss: 0.0446, Val Loss: 0.0698\n",
      "Epoch 88/300 - Train Loss: 0.0443, Val Loss: 0.0732\n",
      "Epoch 89/300 - Train Loss: 0.0425, Val Loss: 0.0703\n",
      "Epoch 90/300 - Train Loss: 0.0433, Val Loss: 0.0695\n",
      "Epoch 91/300 - Train Loss: 0.0424, Val Loss: 0.0764\n",
      "Epoch 92/300 - Train Loss: 0.0444, Val Loss: 0.0710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:20:51,470] Trial 234 finished with value: 0.9760302874975748 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.14932724407459658, 'learning_rate': 9.315741048562412e-05, 'batch_size': 32, 'weight_decay': 6.618638381087309e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/300 - Train Loss: 0.0463, Val Loss: 0.0895\n",
      "Early stopping at epoch 93\n",
      "Macro F1 Score: 0.9760, Macro Precision: 0.9740, Macro Recall: 0.9781\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.97      0.96        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 236\n",
      "Training with F1=16, F2=32, D=8, dropout=0.11115108625291638, LR=8.401987005704267e-05, BS=32, WD=6.999018839982805e-05\n",
      "Epoch 1/300 - Train Loss: 0.2605, Val Loss: 0.1027\n",
      "Epoch 2/300 - Train Loss: 0.1183, Val Loss: 0.0833\n",
      "Epoch 3/300 - Train Loss: 0.1037, Val Loss: 0.0747\n",
      "Epoch 4/300 - Train Loss: 0.1011, Val Loss: 0.0959\n",
      "Epoch 5/300 - Train Loss: 0.0950, Val Loss: 0.0849\n",
      "Epoch 6/300 - Train Loss: 0.0886, Val Loss: 0.0738\n",
      "Epoch 7/300 - Train Loss: 0.0887, Val Loss: 0.0743\n",
      "Epoch 8/300 - Train Loss: 0.0844, Val Loss: 0.0811\n",
      "Epoch 9/300 - Train Loss: 0.0818, Val Loss: 0.0738\n",
      "Epoch 10/300 - Train Loss: 0.0832, Val Loss: 0.0675\n",
      "Epoch 11/300 - Train Loss: 0.0805, Val Loss: 0.0715\n",
      "Epoch 12/300 - Train Loss: 0.0774, Val Loss: 0.0657\n",
      "Epoch 13/300 - Train Loss: 0.0786, Val Loss: 0.0674\n",
      "Epoch 14/300 - Train Loss: 0.0771, Val Loss: 0.0724\n",
      "Epoch 15/300 - Train Loss: 0.0759, Val Loss: 0.0674\n",
      "Epoch 16/300 - Train Loss: 0.0764, Val Loss: 0.0654\n",
      "Epoch 17/300 - Train Loss: 0.0739, Val Loss: 0.0742\n",
      "Epoch 18/300 - Train Loss: 0.0755, Val Loss: 0.0826\n",
      "Epoch 19/300 - Train Loss: 0.0715, Val Loss: 0.0690\n",
      "Epoch 20/300 - Train Loss: 0.0707, Val Loss: 0.0660\n",
      "Epoch 21/300 - Train Loss: 0.0724, Val Loss: 0.0672\n",
      "Epoch 22/300 - Train Loss: 0.0693, Val Loss: 0.0684\n",
      "Epoch 23/300 - Train Loss: 0.0687, Val Loss: 0.0669\n",
      "Epoch 24/300 - Train Loss: 0.0668, Val Loss: 0.0691\n",
      "Epoch 25/300 - Train Loss: 0.0675, Val Loss: 0.0703\n",
      "Epoch 26/300 - Train Loss: 0.0671, Val Loss: 0.0685\n",
      "Epoch 27/300 - Train Loss: 0.0671, Val Loss: 0.0672\n",
      "Epoch 28/300 - Train Loss: 0.0646, Val Loss: 0.0670\n",
      "Epoch 29/300 - Train Loss: 0.0658, Val Loss: 0.0647\n",
      "Epoch 30/300 - Train Loss: 0.0648, Val Loss: 0.0678\n",
      "Epoch 31/300 - Train Loss: 0.0635, Val Loss: 0.0637\n",
      "Epoch 32/300 - Train Loss: 0.0631, Val Loss: 0.0769\n",
      "Epoch 33/300 - Train Loss: 0.0620, Val Loss: 0.0804\n",
      "Epoch 34/300 - Train Loss: 0.0609, Val Loss: 0.0634\n",
      "Epoch 35/300 - Train Loss: 0.0610, Val Loss: 0.0700\n",
      "Epoch 36/300 - Train Loss: 0.0598, Val Loss: 0.0696\n",
      "Epoch 37/300 - Train Loss: 0.0595, Val Loss: 0.0693\n",
      "Epoch 38/300 - Train Loss: 0.0605, Val Loss: 0.0675\n",
      "Epoch 39/300 - Train Loss: 0.0557, Val Loss: 0.0717\n",
      "Epoch 40/300 - Train Loss: 0.0604, Val Loss: 0.0677\n",
      "Epoch 41/300 - Train Loss: 0.0579, Val Loss: 0.0673\n",
      "Epoch 42/300 - Train Loss: 0.0595, Val Loss: 0.0646\n",
      "Epoch 43/300 - Train Loss: 0.0561, Val Loss: 0.0680\n",
      "Epoch 44/300 - Train Loss: 0.0558, Val Loss: 0.0689\n",
      "Epoch 45/300 - Train Loss: 0.0550, Val Loss: 0.0672\n",
      "Epoch 46/300 - Train Loss: 0.0566, Val Loss: 0.0726\n",
      "Epoch 47/300 - Train Loss: 0.0541, Val Loss: 0.0787\n",
      "Epoch 48/300 - Train Loss: 0.0518, Val Loss: 0.0649\n",
      "Epoch 49/300 - Train Loss: 0.0543, Val Loss: 0.0666\n",
      "Epoch 50/300 - Train Loss: 0.0552, Val Loss: 0.0716\n",
      "Epoch 51/300 - Train Loss: 0.0549, Val Loss: 0.0887\n",
      "Epoch 52/300 - Train Loss: 0.0547, Val Loss: 0.0662\n",
      "Epoch 53/300 - Train Loss: 0.0536, Val Loss: 0.0685\n",
      "Epoch 54/300 - Train Loss: 0.0514, Val Loss: 0.0685\n",
      "Epoch 55/300 - Train Loss: 0.0517, Val Loss: 0.0685\n",
      "Epoch 56/300 - Train Loss: 0.0525, Val Loss: 0.0671\n",
      "Epoch 57/300 - Train Loss: 0.0521, Val Loss: 0.0732\n",
      "Epoch 58/300 - Train Loss: 0.0506, Val Loss: 0.0659\n",
      "Epoch 59/300 - Train Loss: 0.0518, Val Loss: 0.0678\n",
      "Epoch 60/300 - Train Loss: 0.0519, Val Loss: 0.0663\n",
      "Epoch 61/300 - Train Loss: 0.0487, Val Loss: 0.0762\n",
      "Epoch 62/300 - Train Loss: 0.0483, Val Loss: 0.0747\n",
      "Epoch 63/300 - Train Loss: 0.0502, Val Loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:24:58,107] Trial 235 finished with value: 0.9709540969483809 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.11115108625291638, 'learning_rate': 8.401987005704267e-05, 'batch_size': 32, 'weight_decay': 6.999018839982805e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300 - Train Loss: 0.0481, Val Loss: 0.0662\n",
      "Early stopping at epoch 64\n",
      "Macro F1 Score: 0.9710, Macro Precision: 0.9692, Macro Recall: 0.9728\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 237\n",
      "Training with F1=16, F2=32, D=8, dropout=0.14774916503400576, LR=9.192240530088825e-05, BS=256, WD=4.984521466950829e-05\n",
      "Epoch 1/300 - Train Loss: 0.4899, Val Loss: 0.2469\n",
      "Epoch 2/300 - Train Loss: 0.2157, Val Loss: 0.1886\n",
      "Epoch 3/300 - Train Loss: 0.1734, Val Loss: 0.1516\n",
      "Epoch 4/300 - Train Loss: 0.1498, Val Loss: 0.1248\n",
      "Epoch 5/300 - Train Loss: 0.1296, Val Loss: 0.1116\n",
      "Epoch 6/300 - Train Loss: 0.1159, Val Loss: 0.0973\n",
      "Epoch 7/300 - Train Loss: 0.1043, Val Loss: 0.0933\n",
      "Epoch 8/300 - Train Loss: 0.0992, Val Loss: 0.0865\n",
      "Epoch 9/300 - Train Loss: 0.0949, Val Loss: 0.0894\n",
      "Epoch 10/300 - Train Loss: 0.0911, Val Loss: 0.0798\n",
      "Epoch 11/300 - Train Loss: 0.0894, Val Loss: 0.0877\n",
      "Epoch 12/300 - Train Loss: 0.0855, Val Loss: 0.0809\n",
      "Epoch 13/300 - Train Loss: 0.0869, Val Loss: 0.0794\n",
      "Epoch 14/300 - Train Loss: 0.0829, Val Loss: 0.0744\n",
      "Epoch 15/300 - Train Loss: 0.0815, Val Loss: 0.0905\n",
      "Epoch 16/300 - Train Loss: 0.0796, Val Loss: 0.0720\n",
      "Epoch 17/300 - Train Loss: 0.0773, Val Loss: 0.0765\n",
      "Epoch 18/300 - Train Loss: 0.0786, Val Loss: 0.0795\n",
      "Epoch 19/300 - Train Loss: 0.0756, Val Loss: 0.0757\n",
      "Epoch 20/300 - Train Loss: 0.0748, Val Loss: 0.0735\n",
      "Epoch 21/300 - Train Loss: 0.0753, Val Loss: 0.0723\n",
      "Epoch 22/300 - Train Loss: 0.0730, Val Loss: 0.0822\n",
      "Epoch 23/300 - Train Loss: 0.0724, Val Loss: 0.0749\n",
      "Epoch 24/300 - Train Loss: 0.0731, Val Loss: 0.0692\n",
      "Epoch 25/300 - Train Loss: 0.0718, Val Loss: 0.0722\n",
      "Epoch 26/300 - Train Loss: 0.0711, Val Loss: 0.0746\n",
      "Epoch 27/300 - Train Loss: 0.0715, Val Loss: 0.0709\n",
      "Epoch 28/300 - Train Loss: 0.0720, Val Loss: 0.0732\n",
      "Epoch 29/300 - Train Loss: 0.0701, Val Loss: 0.0687\n",
      "Epoch 30/300 - Train Loss: 0.0690, Val Loss: 0.0755\n",
      "Epoch 31/300 - Train Loss: 0.0708, Val Loss: 0.0731\n",
      "Epoch 32/300 - Train Loss: 0.0682, Val Loss: 0.0670\n",
      "Epoch 33/300 - Train Loss: 0.0690, Val Loss: 0.0700\n",
      "Epoch 34/300 - Train Loss: 0.0689, Val Loss: 0.0856\n",
      "Epoch 35/300 - Train Loss: 0.0677, Val Loss: 0.0751\n",
      "Epoch 36/300 - Train Loss: 0.0665, Val Loss: 0.0736\n",
      "Epoch 37/300 - Train Loss: 0.0674, Val Loss: 0.0674\n",
      "Epoch 38/300 - Train Loss: 0.0664, Val Loss: 0.0735\n",
      "Epoch 39/300 - Train Loss: 0.0666, Val Loss: 0.0690\n",
      "Epoch 40/300 - Train Loss: 0.0663, Val Loss: 0.0697\n",
      "Epoch 41/300 - Train Loss: 0.0678, Val Loss: 0.0660\n",
      "Epoch 42/300 - Train Loss: 0.0643, Val Loss: 0.0783\n",
      "Epoch 43/300 - Train Loss: 0.0641, Val Loss: 0.0699\n",
      "Epoch 44/300 - Train Loss: 0.0634, Val Loss: 0.0669\n",
      "Epoch 45/300 - Train Loss: 0.0638, Val Loss: 0.0727\n",
      "Epoch 46/300 - Train Loss: 0.0639, Val Loss: 0.0694\n",
      "Epoch 47/300 - Train Loss: 0.0636, Val Loss: 0.0697\n",
      "Epoch 48/300 - Train Loss: 0.0635, Val Loss: 0.0724\n",
      "Epoch 49/300 - Train Loss: 0.0631, Val Loss: 0.0708\n",
      "Epoch 50/300 - Train Loss: 0.0628, Val Loss: 0.0693\n",
      "Epoch 51/300 - Train Loss: 0.0620, Val Loss: 0.0701\n",
      "Epoch 52/300 - Train Loss: 0.0612, Val Loss: 0.0701\n",
      "Epoch 53/300 - Train Loss: 0.0621, Val Loss: 0.0741\n",
      "Epoch 54/300 - Train Loss: 0.0612, Val Loss: 0.0706\n",
      "Epoch 55/300 - Train Loss: 0.0633, Val Loss: 0.0688\n",
      "Epoch 56/300 - Train Loss: 0.0603, Val Loss: 0.0704\n",
      "Epoch 57/300 - Train Loss: 0.0612, Val Loss: 0.0667\n",
      "Epoch 58/300 - Train Loss: 0.0596, Val Loss: 0.0707\n",
      "Epoch 59/300 - Train Loss: 0.0596, Val Loss: 0.0738\n",
      "Epoch 60/300 - Train Loss: 0.0602, Val Loss: 0.0753\n",
      "Epoch 61/300 - Train Loss: 0.0581, Val Loss: 0.0719\n",
      "Epoch 62/300 - Train Loss: 0.0584, Val Loss: 0.0741\n",
      "Epoch 63/300 - Train Loss: 0.0609, Val Loss: 0.0716\n",
      "Epoch 64/300 - Train Loss: 0.0590, Val Loss: 0.0676\n",
      "Epoch 65/300 - Train Loss: 0.0582, Val Loss: 0.0705\n",
      "Epoch 66/300 - Train Loss: 0.0577, Val Loss: 0.0689\n",
      "Epoch 67/300 - Train Loss: 0.0595, Val Loss: 0.0688\n",
      "Epoch 68/300 - Train Loss: 0.0585, Val Loss: 0.0765\n",
      "Epoch 69/300 - Train Loss: 0.0566, Val Loss: 0.0652\n",
      "Epoch 70/300 - Train Loss: 0.0559, Val Loss: 0.0692\n",
      "Epoch 71/300 - Train Loss: 0.0553, Val Loss: 0.0676\n",
      "Epoch 72/300 - Train Loss: 0.0571, Val Loss: 0.0682\n",
      "Epoch 73/300 - Train Loss: 0.0559, Val Loss: 0.0707\n",
      "Epoch 74/300 - Train Loss: 0.0554, Val Loss: 0.0707\n",
      "Epoch 75/300 - Train Loss: 0.0549, Val Loss: 0.0669\n",
      "Epoch 76/300 - Train Loss: 0.0557, Val Loss: 0.0714\n",
      "Epoch 77/300 - Train Loss: 0.0544, Val Loss: 0.0700\n",
      "Epoch 78/300 - Train Loss: 0.0538, Val Loss: 0.0701\n",
      "Epoch 79/300 - Train Loss: 0.0554, Val Loss: 0.0714\n",
      "Epoch 80/300 - Train Loss: 0.0546, Val Loss: 0.0700\n",
      "Epoch 81/300 - Train Loss: 0.0570, Val Loss: 0.0670\n",
      "Epoch 82/300 - Train Loss: 0.0546, Val Loss: 0.0747\n",
      "Epoch 83/300 - Train Loss: 0.0538, Val Loss: 0.0675\n",
      "Epoch 84/300 - Train Loss: 0.0526, Val Loss: 0.0723\n",
      "Epoch 85/300 - Train Loss: 0.0542, Val Loss: 0.0710\n",
      "Epoch 86/300 - Train Loss: 0.0548, Val Loss: 0.0757\n",
      "Epoch 87/300 - Train Loss: 0.0519, Val Loss: 0.0702\n",
      "Epoch 88/300 - Train Loss: 0.0538, Val Loss: 0.0736\n",
      "Epoch 89/300 - Train Loss: 0.0533, Val Loss: 0.0703\n",
      "Epoch 90/300 - Train Loss: 0.0520, Val Loss: 0.0676\n",
      "Epoch 91/300 - Train Loss: 0.0532, Val Loss: 0.0708\n",
      "Epoch 92/300 - Train Loss: 0.0515, Val Loss: 0.0699\n",
      "Epoch 93/300 - Train Loss: 0.0510, Val Loss: 0.0719\n",
      "Epoch 94/300 - Train Loss: 0.0507, Val Loss: 0.0717\n",
      "Epoch 95/300 - Train Loss: 0.0511, Val Loss: 0.0677\n",
      "Epoch 96/300 - Train Loss: 0.0509, Val Loss: 0.0684\n",
      "Epoch 97/300 - Train Loss: 0.0506, Val Loss: 0.0693\n",
      "Epoch 98/300 - Train Loss: 0.0501, Val Loss: 0.0704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:30:03,444] Trial 236 finished with value: 0.9759428100992181 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.14774916503400576, 'learning_rate': 9.192240530088825e-05, 'batch_size': 256, 'weight_decay': 4.984521466950829e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/300 - Train Loss: 0.0496, Val Loss: 0.0699\n",
      "Early stopping at epoch 99\n",
      "Macro F1 Score: 0.9759, Macro Precision: 0.9747, Macro Recall: 0.9774\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.97      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 238\n",
      "Training with F1=16, F2=32, D=8, dropout=0.14604277323156176, LR=0.00011907244437277382, BS=256, WD=5.154317628280708e-05\n",
      "Epoch 1/300 - Train Loss: 0.4455, Val Loss: 0.2102\n",
      "Epoch 2/300 - Train Loss: 0.1708, Val Loss: 0.1369\n",
      "Epoch 3/300 - Train Loss: 0.1240, Val Loss: 0.1044\n",
      "Epoch 4/300 - Train Loss: 0.1051, Val Loss: 0.0897\n",
      "Epoch 5/300 - Train Loss: 0.0969, Val Loss: 0.0828\n",
      "Epoch 6/300 - Train Loss: 0.0918, Val Loss: 0.0869\n",
      "Epoch 7/300 - Train Loss: 0.0882, Val Loss: 0.0827\n",
      "Epoch 8/300 - Train Loss: 0.0853, Val Loss: 0.0758\n",
      "Epoch 9/300 - Train Loss: 0.0830, Val Loss: 0.0820\n",
      "Epoch 10/300 - Train Loss: 0.0806, Val Loss: 0.0748\n",
      "Epoch 11/300 - Train Loss: 0.0815, Val Loss: 0.0752\n",
      "Epoch 12/300 - Train Loss: 0.0789, Val Loss: 0.0934\n",
      "Epoch 13/300 - Train Loss: 0.0762, Val Loss: 0.0762\n",
      "Epoch 14/300 - Train Loss: 0.0761, Val Loss: 0.0737\n",
      "Epoch 15/300 - Train Loss: 0.0742, Val Loss: 0.0760\n",
      "Epoch 16/300 - Train Loss: 0.0724, Val Loss: 0.0735\n",
      "Epoch 17/300 - Train Loss: 0.0754, Val Loss: 0.0894\n",
      "Epoch 18/300 - Train Loss: 0.0719, Val Loss: 0.0724\n",
      "Epoch 19/300 - Train Loss: 0.0715, Val Loss: 0.0809\n",
      "Epoch 20/300 - Train Loss: 0.0732, Val Loss: 0.0772\n",
      "Epoch 21/300 - Train Loss: 0.0717, Val Loss: 0.0835\n",
      "Epoch 22/300 - Train Loss: 0.0698, Val Loss: 0.0743\n",
      "Epoch 23/300 - Train Loss: 0.0692, Val Loss: 0.0719\n",
      "Epoch 24/300 - Train Loss: 0.0713, Val Loss: 0.0771\n",
      "Epoch 25/300 - Train Loss: 0.0683, Val Loss: 0.0814\n",
      "Epoch 26/300 - Train Loss: 0.0689, Val Loss: 0.0711\n",
      "Epoch 27/300 - Train Loss: 0.0676, Val Loss: 0.0725\n",
      "Epoch 28/300 - Train Loss: 0.0691, Val Loss: 0.0766\n",
      "Epoch 29/300 - Train Loss: 0.0662, Val Loss: 0.0729\n",
      "Epoch 30/300 - Train Loss: 0.0665, Val Loss: 0.0718\n",
      "Epoch 31/300 - Train Loss: 0.0665, Val Loss: 0.0785\n",
      "Epoch 32/300 - Train Loss: 0.0677, Val Loss: 0.0837\n",
      "Epoch 33/300 - Train Loss: 0.0682, Val Loss: 0.0815\n",
      "Epoch 34/300 - Train Loss: 0.0647, Val Loss: 0.0732\n",
      "Epoch 35/300 - Train Loss: 0.0642, Val Loss: 0.0733\n",
      "Epoch 36/300 - Train Loss: 0.0636, Val Loss: 0.0781\n",
      "Epoch 37/300 - Train Loss: 0.0649, Val Loss: 0.0750\n",
      "Epoch 38/300 - Train Loss: 0.0640, Val Loss: 0.0680\n",
      "Epoch 39/300 - Train Loss: 0.0628, Val Loss: 0.0710\n",
      "Epoch 40/300 - Train Loss: 0.0641, Val Loss: 0.0760\n",
      "Epoch 41/300 - Train Loss: 0.0628, Val Loss: 0.0712\n",
      "Epoch 42/300 - Train Loss: 0.0606, Val Loss: 0.0708\n",
      "Epoch 43/300 - Train Loss: 0.0619, Val Loss: 0.0699\n",
      "Epoch 44/300 - Train Loss: 0.0620, Val Loss: 0.0773\n",
      "Epoch 45/300 - Train Loss: 0.0604, Val Loss: 0.0726\n",
      "Epoch 46/300 - Train Loss: 0.0615, Val Loss: 0.0744\n",
      "Epoch 47/300 - Train Loss: 0.0614, Val Loss: 0.0701\n",
      "Epoch 48/300 - Train Loss: 0.0593, Val Loss: 0.0762\n",
      "Epoch 49/300 - Train Loss: 0.0594, Val Loss: 0.0692\n",
      "Epoch 50/300 - Train Loss: 0.0581, Val Loss: 0.0718\n",
      "Epoch 51/300 - Train Loss: 0.0578, Val Loss: 0.0703\n",
      "Epoch 52/300 - Train Loss: 0.0592, Val Loss: 0.0692\n",
      "Epoch 53/300 - Train Loss: 0.0580, Val Loss: 0.0705\n",
      "Epoch 54/300 - Train Loss: 0.0585, Val Loss: 0.0703\n",
      "Epoch 55/300 - Train Loss: 0.0588, Val Loss: 0.0656\n",
      "Epoch 56/300 - Train Loss: 0.0579, Val Loss: 0.0704\n",
      "Epoch 57/300 - Train Loss: 0.0576, Val Loss: 0.0719\n",
      "Epoch 58/300 - Train Loss: 0.0564, Val Loss: 0.0777\n",
      "Epoch 59/300 - Train Loss: 0.0552, Val Loss: 0.0694\n",
      "Epoch 60/300 - Train Loss: 0.0545, Val Loss: 0.0665\n",
      "Epoch 61/300 - Train Loss: 0.0555, Val Loss: 0.0672\n",
      "Epoch 62/300 - Train Loss: 0.0550, Val Loss: 0.0686\n",
      "Epoch 63/300 - Train Loss: 0.0543, Val Loss: 0.0688\n",
      "Epoch 64/300 - Train Loss: 0.0545, Val Loss: 0.0654\n",
      "Epoch 65/300 - Train Loss: 0.0541, Val Loss: 0.0674\n",
      "Epoch 66/300 - Train Loss: 0.0549, Val Loss: 0.0717\n",
      "Epoch 67/300 - Train Loss: 0.0538, Val Loss: 0.0685\n",
      "Epoch 68/300 - Train Loss: 0.0542, Val Loss: 0.0721\n",
      "Epoch 69/300 - Train Loss: 0.0519, Val Loss: 0.0737\n",
      "Epoch 70/300 - Train Loss: 0.0529, Val Loss: 0.0658\n",
      "Epoch 71/300 - Train Loss: 0.0524, Val Loss: 0.0721\n",
      "Epoch 72/300 - Train Loss: 0.0519, Val Loss: 0.0744\n",
      "Epoch 73/300 - Train Loss: 0.0525, Val Loss: 0.0688\n",
      "Epoch 74/300 - Train Loss: 0.0515, Val Loss: 0.0754\n",
      "Epoch 75/300 - Train Loss: 0.0512, Val Loss: 0.0715\n",
      "Epoch 76/300 - Train Loss: 0.0527, Val Loss: 0.0660\n",
      "Epoch 77/300 - Train Loss: 0.0502, Val Loss: 0.0702\n",
      "Epoch 78/300 - Train Loss: 0.0496, Val Loss: 0.0717\n",
      "Epoch 79/300 - Train Loss: 0.0493, Val Loss: 0.0725\n",
      "Epoch 80/300 - Train Loss: 0.0493, Val Loss: 0.0711\n",
      "Epoch 81/300 - Train Loss: 0.0487, Val Loss: 0.0703\n",
      "Epoch 82/300 - Train Loss: 0.0486, Val Loss: 0.0720\n",
      "Epoch 83/300 - Train Loss: 0.0473, Val Loss: 0.0694\n",
      "Epoch 84/300 - Train Loss: 0.0481, Val Loss: 0.0711\n",
      "Epoch 85/300 - Train Loss: 0.0479, Val Loss: 0.0693\n",
      "Epoch 86/300 - Train Loss: 0.0466, Val Loss: 0.0684\n",
      "Epoch 87/300 - Train Loss: 0.0480, Val Loss: 0.0700\n",
      "Epoch 88/300 - Train Loss: 0.0459, Val Loss: 0.0682\n",
      "Epoch 89/300 - Train Loss: 0.0473, Val Loss: 0.0697\n",
      "Epoch 90/300 - Train Loss: 0.0457, Val Loss: 0.0700\n",
      "Epoch 91/300 - Train Loss: 0.0448, Val Loss: 0.0751\n",
      "Epoch 92/300 - Train Loss: 0.0458, Val Loss: 0.0772\n",
      "Epoch 93/300 - Train Loss: 0.0471, Val Loss: 0.0825\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:34:53,312] Trial 237 finished with value: 0.968041518357647 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.14604277323156176, 'learning_rate': 0.00011907244437277382, 'batch_size': 256, 'weight_decay': 5.154317628280708e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300 - Train Loss: 0.0460, Val Loss: 0.0691\n",
      "Early stopping at epoch 94\n",
      "Macro F1 Score: 0.9680, Macro Precision: 0.9601, Macro Recall: 0.9767\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 239\n",
      "Training with F1=16, F2=32, D=8, dropout=0.14488172293073204, LR=0.00010433180611865077, BS=256, WD=4.950113672655184e-05\n",
      "Epoch 1/300 - Train Loss: 0.4978, Val Loss: 0.2301\n",
      "Epoch 2/300 - Train Loss: 0.2070, Val Loss: 0.1678\n",
      "Epoch 3/300 - Train Loss: 0.1562, Val Loss: 0.1455\n",
      "Epoch 4/300 - Train Loss: 0.1260, Val Loss: 0.1350\n",
      "Epoch 5/300 - Train Loss: 0.1072, Val Loss: 0.1060\n",
      "Epoch 6/300 - Train Loss: 0.0994, Val Loss: 0.0939\n",
      "Epoch 7/300 - Train Loss: 0.0939, Val Loss: 0.0823\n",
      "Epoch 8/300 - Train Loss: 0.0884, Val Loss: 0.0979\n",
      "Epoch 9/300 - Train Loss: 0.0870, Val Loss: 0.0826\n",
      "Epoch 10/300 - Train Loss: 0.0847, Val Loss: 0.0777\n",
      "Epoch 11/300 - Train Loss: 0.0858, Val Loss: 0.0801\n",
      "Epoch 12/300 - Train Loss: 0.0811, Val Loss: 0.0777\n",
      "Epoch 13/300 - Train Loss: 0.0810, Val Loss: 0.0801\n",
      "Epoch 14/300 - Train Loss: 0.0779, Val Loss: 0.0787\n",
      "Epoch 15/300 - Train Loss: 0.0754, Val Loss: 0.0748\n",
      "Epoch 16/300 - Train Loss: 0.0750, Val Loss: 0.0827\n",
      "Epoch 17/300 - Train Loss: 0.0741, Val Loss: 0.0769\n",
      "Epoch 18/300 - Train Loss: 0.0725, Val Loss: 0.0752\n",
      "Epoch 19/300 - Train Loss: 0.0731, Val Loss: 0.0780\n",
      "Epoch 20/300 - Train Loss: 0.0714, Val Loss: 0.0742\n",
      "Epoch 21/300 - Train Loss: 0.0711, Val Loss: 0.0699\n",
      "Epoch 22/300 - Train Loss: 0.0695, Val Loss: 0.0765\n",
      "Epoch 23/300 - Train Loss: 0.0700, Val Loss: 0.0706\n",
      "Epoch 24/300 - Train Loss: 0.0692, Val Loss: 0.0690\n",
      "Epoch 25/300 - Train Loss: 0.0674, Val Loss: 0.0703\n",
      "Epoch 26/300 - Train Loss: 0.0671, Val Loss: 0.0721\n",
      "Epoch 27/300 - Train Loss: 0.0652, Val Loss: 0.0707\n",
      "Epoch 28/300 - Train Loss: 0.0655, Val Loss: 0.0736\n",
      "Epoch 29/300 - Train Loss: 0.0667, Val Loss: 0.0724\n",
      "Epoch 30/300 - Train Loss: 0.0652, Val Loss: 0.0801\n",
      "Epoch 31/300 - Train Loss: 0.0652, Val Loss: 0.0686\n",
      "Epoch 32/300 - Train Loss: 0.0628, Val Loss: 0.0696\n",
      "Epoch 33/300 - Train Loss: 0.0628, Val Loss: 0.0736\n",
      "Epoch 34/300 - Train Loss: 0.0620, Val Loss: 0.0732\n",
      "Epoch 35/300 - Train Loss: 0.0631, Val Loss: 0.0675\n",
      "Epoch 36/300 - Train Loss: 0.0626, Val Loss: 0.0699\n",
      "Epoch 37/300 - Train Loss: 0.0616, Val Loss: 0.0713\n",
      "Epoch 38/300 - Train Loss: 0.0619, Val Loss: 0.0762\n",
      "Epoch 39/300 - Train Loss: 0.0618, Val Loss: 0.0703\n",
      "Epoch 40/300 - Train Loss: 0.0601, Val Loss: 0.0710\n",
      "Epoch 41/300 - Train Loss: 0.0601, Val Loss: 0.0743\n",
      "Epoch 42/300 - Train Loss: 0.0609, Val Loss: 0.0760\n",
      "Epoch 43/300 - Train Loss: 0.0586, Val Loss: 0.0681\n",
      "Epoch 44/300 - Train Loss: 0.0593, Val Loss: 0.0692\n",
      "Epoch 45/300 - Train Loss: 0.0588, Val Loss: 0.0750\n",
      "Epoch 46/300 - Train Loss: 0.0584, Val Loss: 0.0711\n",
      "Epoch 47/300 - Train Loss: 0.0580, Val Loss: 0.0735\n",
      "Epoch 48/300 - Train Loss: 0.0581, Val Loss: 0.0690\n",
      "Epoch 49/300 - Train Loss: 0.0595, Val Loss: 0.0719\n",
      "Epoch 50/300 - Train Loss: 0.0575, Val Loss: 0.0687\n",
      "Epoch 51/300 - Train Loss: 0.0579, Val Loss: 0.0682\n",
      "Epoch 52/300 - Train Loss: 0.0567, Val Loss: 0.0666\n",
      "Epoch 53/300 - Train Loss: 0.0563, Val Loss: 0.0684\n",
      "Epoch 54/300 - Train Loss: 0.0559, Val Loss: 0.0721\n",
      "Epoch 55/300 - Train Loss: 0.0538, Val Loss: 0.0699\n",
      "Epoch 56/300 - Train Loss: 0.0556, Val Loss: 0.0694\n",
      "Epoch 57/300 - Train Loss: 0.0551, Val Loss: 0.0695\n",
      "Epoch 58/300 - Train Loss: 0.0546, Val Loss: 0.0664\n",
      "Epoch 59/300 - Train Loss: 0.0538, Val Loss: 0.0744\n",
      "Epoch 60/300 - Train Loss: 0.0535, Val Loss: 0.0681\n",
      "Epoch 61/300 - Train Loss: 0.0540, Val Loss: 0.0705\n",
      "Epoch 62/300 - Train Loss: 0.0527, Val Loss: 0.0696\n",
      "Epoch 63/300 - Train Loss: 0.0518, Val Loss: 0.0778\n",
      "Epoch 64/300 - Train Loss: 0.0545, Val Loss: 0.0691\n",
      "Epoch 65/300 - Train Loss: 0.0531, Val Loss: 0.0717\n",
      "Epoch 66/300 - Train Loss: 0.0530, Val Loss: 0.0735\n",
      "Epoch 67/300 - Train Loss: 0.0522, Val Loss: 0.0727\n",
      "Epoch 68/300 - Train Loss: 0.0513, Val Loss: 0.0681\n",
      "Epoch 69/300 - Train Loss: 0.0511, Val Loss: 0.0683\n",
      "Epoch 70/300 - Train Loss: 0.0505, Val Loss: 0.0665\n",
      "Epoch 71/300 - Train Loss: 0.0494, Val Loss: 0.0685\n",
      "Epoch 72/300 - Train Loss: 0.0503, Val Loss: 0.0681\n",
      "Epoch 73/300 - Train Loss: 0.0500, Val Loss: 0.0695\n",
      "Epoch 74/300 - Train Loss: 0.0499, Val Loss: 0.0751\n",
      "Epoch 75/300 - Train Loss: 0.0498, Val Loss: 0.0744\n",
      "Epoch 76/300 - Train Loss: 0.0496, Val Loss: 0.0710\n",
      "Epoch 77/300 - Train Loss: 0.0500, Val Loss: 0.0741\n",
      "Epoch 78/300 - Train Loss: 0.0503, Val Loss: 0.0773\n",
      "Epoch 79/300 - Train Loss: 0.0495, Val Loss: 0.0673\n",
      "Epoch 80/300 - Train Loss: 0.0482, Val Loss: 0.0709\n",
      "Epoch 81/300 - Train Loss: 0.0498, Val Loss: 0.0710\n",
      "Epoch 82/300 - Train Loss: 0.0471, Val Loss: 0.0744\n",
      "Epoch 83/300 - Train Loss: 0.0481, Val Loss: 0.0700\n",
      "Epoch 84/300 - Train Loss: 0.0493, Val Loss: 0.0684\n",
      "Epoch 85/300 - Train Loss: 0.0461, Val Loss: 0.0723\n",
      "Epoch 86/300 - Train Loss: 0.0479, Val Loss: 0.0726\n",
      "Epoch 87/300 - Train Loss: 0.0470, Val Loss: 0.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:39:24,681] Trial 238 finished with value: 0.9668726997903986 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.14488172293073204, 'learning_rate': 0.00010433180611865077, 'batch_size': 256, 'weight_decay': 4.950113672655184e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/300 - Train Loss: 0.0464, Val Loss: 0.0670\n",
      "Early stopping at epoch 88\n",
      "Macro F1 Score: 0.9669, Macro Precision: 0.9675, Macro Recall: 0.9663\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 240\n",
      "Training with F1=16, F2=32, D=8, dropout=0.13071174000634245, LR=9.423406722219498e-05, BS=256, WD=3.53062402243763e-05\n",
      "Epoch 1/300 - Train Loss: 0.4721, Val Loss: 0.2615\n",
      "Epoch 2/300 - Train Loss: 0.2185, Val Loss: 0.1783\n",
      "Epoch 3/300 - Train Loss: 0.1654, Val Loss: 0.1410\n",
      "Epoch 4/300 - Train Loss: 0.1274, Val Loss: 0.1417\n",
      "Epoch 5/300 - Train Loss: 0.1074, Val Loss: 0.0911\n",
      "Epoch 6/300 - Train Loss: 0.0969, Val Loss: 0.0938\n",
      "Epoch 7/300 - Train Loss: 0.0945, Val Loss: 0.0837\n",
      "Epoch 8/300 - Train Loss: 0.0887, Val Loss: 0.0910\n",
      "Epoch 9/300 - Train Loss: 0.0866, Val Loss: 0.0875\n",
      "Epoch 10/300 - Train Loss: 0.0854, Val Loss: 0.0782\n",
      "Epoch 11/300 - Train Loss: 0.0816, Val Loss: 0.0759\n",
      "Epoch 12/300 - Train Loss: 0.0809, Val Loss: 0.0823\n",
      "Epoch 13/300 - Train Loss: 0.0785, Val Loss: 0.0718\n",
      "Epoch 14/300 - Train Loss: 0.0775, Val Loss: 0.0751\n",
      "Epoch 15/300 - Train Loss: 0.0769, Val Loss: 0.0754\n",
      "Epoch 16/300 - Train Loss: 0.0751, Val Loss: 0.0706\n",
      "Epoch 17/300 - Train Loss: 0.0740, Val Loss: 0.0730\n",
      "Epoch 18/300 - Train Loss: 0.0742, Val Loss: 0.0734\n",
      "Epoch 19/300 - Train Loss: 0.0743, Val Loss: 0.0731\n",
      "Epoch 20/300 - Train Loss: 0.0720, Val Loss: 0.0729\n",
      "Epoch 21/300 - Train Loss: 0.0722, Val Loss: 0.0740\n",
      "Epoch 22/300 - Train Loss: 0.0725, Val Loss: 0.0763\n",
      "Epoch 23/300 - Train Loss: 0.0696, Val Loss: 0.0769\n",
      "Epoch 24/300 - Train Loss: 0.0703, Val Loss: 0.0768\n",
      "Epoch 25/300 - Train Loss: 0.0701, Val Loss: 0.0715\n",
      "Epoch 26/300 - Train Loss: 0.0705, Val Loss: 0.0746\n",
      "Epoch 27/300 - Train Loss: 0.0696, Val Loss: 0.0771\n",
      "Epoch 28/300 - Train Loss: 0.0687, Val Loss: 0.0804\n",
      "Epoch 29/300 - Train Loss: 0.0691, Val Loss: 0.0739\n",
      "Epoch 30/300 - Train Loss: 0.0665, Val Loss: 0.0747\n",
      "Epoch 31/300 - Train Loss: 0.0664, Val Loss: 0.0682\n",
      "Epoch 32/300 - Train Loss: 0.0670, Val Loss: 0.0712\n",
      "Epoch 33/300 - Train Loss: 0.0667, Val Loss: 0.0697\n",
      "Epoch 34/300 - Train Loss: 0.0654, Val Loss: 0.0703\n",
      "Epoch 35/300 - Train Loss: 0.0653, Val Loss: 0.0756\n",
      "Epoch 36/300 - Train Loss: 0.0649, Val Loss: 0.0733\n",
      "Epoch 37/300 - Train Loss: 0.0637, Val Loss: 0.0765\n",
      "Epoch 38/300 - Train Loss: 0.0651, Val Loss: 0.0803\n",
      "Epoch 39/300 - Train Loss: 0.0645, Val Loss: 0.0790\n",
      "Epoch 40/300 - Train Loss: 0.0645, Val Loss: 0.0731\n",
      "Epoch 41/300 - Train Loss: 0.0633, Val Loss: 0.0695\n",
      "Epoch 42/300 - Train Loss: 0.0625, Val Loss: 0.0736\n",
      "Epoch 43/300 - Train Loss: 0.0626, Val Loss: 0.0722\n",
      "Epoch 44/300 - Train Loss: 0.0626, Val Loss: 0.0703\n",
      "Epoch 45/300 - Train Loss: 0.0618, Val Loss: 0.0789\n",
      "Epoch 46/300 - Train Loss: 0.0626, Val Loss: 0.0788\n",
      "Epoch 47/300 - Train Loss: 0.0612, Val Loss: 0.0713\n",
      "Epoch 48/300 - Train Loss: 0.0605, Val Loss: 0.0751\n",
      "Epoch 49/300 - Train Loss: 0.0608, Val Loss: 0.0753\n",
      "Epoch 50/300 - Train Loss: 0.0601, Val Loss: 0.0746\n",
      "Epoch 51/300 - Train Loss: 0.0584, Val Loss: 0.0728\n",
      "Epoch 52/300 - Train Loss: 0.0603, Val Loss: 0.0696\n",
      "Epoch 53/300 - Train Loss: 0.0581, Val Loss: 0.0709\n",
      "Epoch 54/300 - Train Loss: 0.0595, Val Loss: 0.0705\n",
      "Epoch 55/300 - Train Loss: 0.0586, Val Loss: 0.0719\n",
      "Epoch 56/300 - Train Loss: 0.0572, Val Loss: 0.0706\n",
      "Epoch 57/300 - Train Loss: 0.0576, Val Loss: 0.0678\n",
      "Epoch 58/300 - Train Loss: 0.0585, Val Loss: 0.0685\n",
      "Epoch 59/300 - Train Loss: 0.0566, Val Loss: 0.0733\n",
      "Epoch 60/300 - Train Loss: 0.0564, Val Loss: 0.0686\n",
      "Epoch 61/300 - Train Loss: 0.0573, Val Loss: 0.0758\n",
      "Epoch 62/300 - Train Loss: 0.0560, Val Loss: 0.0717\n",
      "Epoch 63/300 - Train Loss: 0.0557, Val Loss: 0.0695\n",
      "Epoch 64/300 - Train Loss: 0.0554, Val Loss: 0.0715\n",
      "Epoch 65/300 - Train Loss: 0.0563, Val Loss: 0.0700\n",
      "Epoch 66/300 - Train Loss: 0.0541, Val Loss: 0.0756\n",
      "Epoch 67/300 - Train Loss: 0.0538, Val Loss: 0.0771\n",
      "Epoch 68/300 - Train Loss: 0.0542, Val Loss: 0.0722\n",
      "Epoch 69/300 - Train Loss: 0.0540, Val Loss: 0.0724\n",
      "Epoch 70/300 - Train Loss: 0.0537, Val Loss: 0.0733\n",
      "Epoch 71/300 - Train Loss: 0.0541, Val Loss: 0.0728\n",
      "Epoch 72/300 - Train Loss: 0.0532, Val Loss: 0.0738\n",
      "Epoch 73/300 - Train Loss: 0.0511, Val Loss: 0.0685\n",
      "Epoch 74/300 - Train Loss: 0.0515, Val Loss: 0.0680\n",
      "Epoch 75/300 - Train Loss: 0.0508, Val Loss: 0.0751\n",
      "Epoch 76/300 - Train Loss: 0.0514, Val Loss: 0.0732\n",
      "Epoch 77/300 - Train Loss: 0.0511, Val Loss: 0.0712\n",
      "Epoch 78/300 - Train Loss: 0.0508, Val Loss: 0.0730\n",
      "Epoch 79/300 - Train Loss: 0.0494, Val Loss: 0.0701\n",
      "Epoch 80/300 - Train Loss: 0.0512, Val Loss: 0.0758\n",
      "Epoch 81/300 - Train Loss: 0.0515, Val Loss: 0.0718\n",
      "Epoch 82/300 - Train Loss: 0.0500, Val Loss: 0.0714\n",
      "Epoch 83/300 - Train Loss: 0.0490, Val Loss: 0.0759\n",
      "Epoch 84/300 - Train Loss: 0.0488, Val Loss: 0.0735\n",
      "Epoch 85/300 - Train Loss: 0.0497, Val Loss: 0.0733\n",
      "Epoch 86/300 - Train Loss: 0.0493, Val Loss: 0.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:43:52,958] Trial 239 finished with value: 0.9703438592962833 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.13071174000634245, 'learning_rate': 9.423406722219498e-05, 'batch_size': 256, 'weight_decay': 3.53062402243763e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300 - Train Loss: 0.0476, Val Loss: 0.0715\n",
      "Early stopping at epoch 87\n",
      "Macro F1 Score: 0.9703, Macro Precision: 0.9716, Macro Recall: 0.9693\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 241\n",
      "Training with F1=4, F2=32, D=8, dropout=0.14921798791302862, LR=8.048440352665408e-05, BS=256, WD=8.764740463170605e-05\n",
      "Epoch 1/300 - Train Loss: 0.7362, Val Loss: 0.4410\n",
      "Epoch 2/300 - Train Loss: 0.3429, Val Loss: 0.2622\n",
      "Epoch 3/300 - Train Loss: 0.2540, Val Loss: 0.2206\n",
      "Epoch 4/300 - Train Loss: 0.2199, Val Loss: 0.1984\n",
      "Epoch 5/300 - Train Loss: 0.1986, Val Loss: 0.1805\n",
      "Epoch 6/300 - Train Loss: 0.1839, Val Loss: 0.1675\n",
      "Epoch 7/300 - Train Loss: 0.1709, Val Loss: 0.1565\n",
      "Epoch 8/300 - Train Loss: 0.1630, Val Loss: 0.1511\n",
      "Epoch 9/300 - Train Loss: 0.1561, Val Loss: 0.1414\n",
      "Epoch 10/300 - Train Loss: 0.1549, Val Loss: 0.1381\n",
      "Epoch 11/300 - Train Loss: 0.1448, Val Loss: 0.1328\n",
      "Epoch 12/300 - Train Loss: 0.1415, Val Loss: 0.1257\n",
      "Epoch 13/300 - Train Loss: 0.1366, Val Loss: 0.1228\n",
      "Epoch 14/300 - Train Loss: 0.1307, Val Loss: 0.1166\n",
      "Epoch 15/300 - Train Loss: 0.1233, Val Loss: 0.1076\n",
      "Epoch 16/300 - Train Loss: 0.1161, Val Loss: 0.0998\n",
      "Epoch 17/300 - Train Loss: 0.1104, Val Loss: 0.0982\n",
      "Epoch 18/300 - Train Loss: 0.1057, Val Loss: 0.0980\n",
      "Epoch 19/300 - Train Loss: 0.1019, Val Loss: 0.0897\n",
      "Epoch 20/300 - Train Loss: 0.0981, Val Loss: 0.0911\n",
      "Epoch 21/300 - Train Loss: 0.0961, Val Loss: 0.0857\n",
      "Epoch 22/300 - Train Loss: 0.0933, Val Loss: 0.0841\n",
      "Epoch 23/300 - Train Loss: 0.0906, Val Loss: 0.0872\n",
      "Epoch 24/300 - Train Loss: 0.0901, Val Loss: 0.0855\n",
      "Epoch 25/300 - Train Loss: 0.0918, Val Loss: 0.0852\n",
      "Epoch 26/300 - Train Loss: 0.0914, Val Loss: 0.0808\n",
      "Epoch 27/300 - Train Loss: 0.0881, Val Loss: 0.0836\n",
      "Epoch 28/300 - Train Loss: 0.0866, Val Loss: 0.0817\n",
      "Epoch 29/300 - Train Loss: 0.0857, Val Loss: 0.0850\n",
      "Epoch 30/300 - Train Loss: 0.0864, Val Loss: 0.0806\n",
      "Epoch 31/300 - Train Loss: 0.0838, Val Loss: 0.0810\n",
      "Epoch 32/300 - Train Loss: 0.0837, Val Loss: 0.0813\n",
      "Epoch 33/300 - Train Loss: 0.0822, Val Loss: 0.0786\n",
      "Epoch 34/300 - Train Loss: 0.0849, Val Loss: 0.0826\n",
      "Epoch 35/300 - Train Loss: 0.0831, Val Loss: 0.0792\n",
      "Epoch 36/300 - Train Loss: 0.0818, Val Loss: 0.0793\n",
      "Epoch 37/300 - Train Loss: 0.0806, Val Loss: 0.0808\n",
      "Epoch 38/300 - Train Loss: 0.0792, Val Loss: 0.0792\n",
      "Epoch 39/300 - Train Loss: 0.0797, Val Loss: 0.0778\n",
      "Epoch 40/300 - Train Loss: 0.0799, Val Loss: 0.0834\n",
      "Epoch 41/300 - Train Loss: 0.0780, Val Loss: 0.0788\n",
      "Epoch 42/300 - Train Loss: 0.0793, Val Loss: 0.0812\n",
      "Epoch 43/300 - Train Loss: 0.0777, Val Loss: 0.0814\n",
      "Epoch 44/300 - Train Loss: 0.0778, Val Loss: 0.0780\n",
      "Epoch 45/300 - Train Loss: 0.0788, Val Loss: 0.0809\n",
      "Epoch 46/300 - Train Loss: 0.0770, Val Loss: 0.0765\n",
      "Epoch 47/300 - Train Loss: 0.0761, Val Loss: 0.0781\n",
      "Epoch 48/300 - Train Loss: 0.0759, Val Loss: 0.0775\n",
      "Epoch 49/300 - Train Loss: 0.0778, Val Loss: 0.0778\n",
      "Epoch 50/300 - Train Loss: 0.0769, Val Loss: 0.0784\n",
      "Epoch 51/300 - Train Loss: 0.0752, Val Loss: 0.0778\n",
      "Epoch 52/300 - Train Loss: 0.0739, Val Loss: 0.0763\n",
      "Epoch 53/300 - Train Loss: 0.0738, Val Loss: 0.0790\n",
      "Epoch 54/300 - Train Loss: 0.0742, Val Loss: 0.0771\n",
      "Epoch 55/300 - Train Loss: 0.0742, Val Loss: 0.0767\n",
      "Epoch 56/300 - Train Loss: 0.0738, Val Loss: 0.0794\n",
      "Epoch 57/300 - Train Loss: 0.0736, Val Loss: 0.0764\n",
      "Epoch 58/300 - Train Loss: 0.0730, Val Loss: 0.0765\n",
      "Epoch 59/300 - Train Loss: 0.0727, Val Loss: 0.0767\n",
      "Epoch 60/300 - Train Loss: 0.0729, Val Loss: 0.0786\n",
      "Epoch 61/300 - Train Loss: 0.0723, Val Loss: 0.0764\n",
      "Epoch 62/300 - Train Loss: 0.0732, Val Loss: 0.0768\n",
      "Epoch 63/300 - Train Loss: 0.0716, Val Loss: 0.0785\n",
      "Epoch 64/300 - Train Loss: 0.0738, Val Loss: 0.0762\n",
      "Epoch 65/300 - Train Loss: 0.0699, Val Loss: 0.0778\n",
      "Epoch 66/300 - Train Loss: 0.0703, Val Loss: 0.0765\n",
      "Epoch 67/300 - Train Loss: 0.0710, Val Loss: 0.0775\n",
      "Epoch 68/300 - Train Loss: 0.0698, Val Loss: 0.0776\n",
      "Epoch 69/300 - Train Loss: 0.0708, Val Loss: 0.0781\n",
      "Epoch 70/300 - Train Loss: 0.0683, Val Loss: 0.0742\n",
      "Epoch 71/300 - Train Loss: 0.0704, Val Loss: 0.0776\n",
      "Epoch 72/300 - Train Loss: 0.0700, Val Loss: 0.0783\n",
      "Epoch 73/300 - Train Loss: 0.0696, Val Loss: 0.0767\n",
      "Epoch 74/300 - Train Loss: 0.0676, Val Loss: 0.0775\n",
      "Epoch 75/300 - Train Loss: 0.0677, Val Loss: 0.0749\n",
      "Epoch 76/300 - Train Loss: 0.0700, Val Loss: 0.0771\n",
      "Epoch 77/300 - Train Loss: 0.0683, Val Loss: 0.0749\n",
      "Epoch 78/300 - Train Loss: 0.0699, Val Loss: 0.0774\n",
      "Epoch 79/300 - Train Loss: 0.0688, Val Loss: 0.0738\n",
      "Epoch 80/300 - Train Loss: 0.0669, Val Loss: 0.0753\n",
      "Epoch 81/300 - Train Loss: 0.0692, Val Loss: 0.0756\n",
      "Epoch 82/300 - Train Loss: 0.0679, Val Loss: 0.0761\n",
      "Epoch 83/300 - Train Loss: 0.0691, Val Loss: 0.0743\n",
      "Epoch 84/300 - Train Loss: 0.0679, Val Loss: 0.0765\n",
      "Epoch 85/300 - Train Loss: 0.0659, Val Loss: 0.0746\n",
      "Epoch 86/300 - Train Loss: 0.0671, Val Loss: 0.0753\n",
      "Epoch 87/300 - Train Loss: 0.0665, Val Loss: 0.0742\n",
      "Epoch 88/300 - Train Loss: 0.0654, Val Loss: 0.0754\n",
      "Epoch 89/300 - Train Loss: 0.0657, Val Loss: 0.0753\n",
      "Epoch 90/300 - Train Loss: 0.0659, Val Loss: 0.0744\n",
      "Epoch 91/300 - Train Loss: 0.0658, Val Loss: 0.0775\n",
      "Epoch 92/300 - Train Loss: 0.0659, Val Loss: 0.0745\n",
      "Epoch 93/300 - Train Loss: 0.0664, Val Loss: 0.0761\n",
      "Epoch 94/300 - Train Loss: 0.0657, Val Loss: 0.0730\n",
      "Epoch 95/300 - Train Loss: 0.0661, Val Loss: 0.0752\n",
      "Epoch 96/300 - Train Loss: 0.0644, Val Loss: 0.0744\n",
      "Epoch 97/300 - Train Loss: 0.0654, Val Loss: 0.0745\n",
      "Epoch 98/300 - Train Loss: 0.0656, Val Loss: 0.0750\n",
      "Epoch 99/300 - Train Loss: 0.0661, Val Loss: 0.0730\n",
      "Epoch 100/300 - Train Loss: 0.0632, Val Loss: 0.0732\n",
      "Epoch 101/300 - Train Loss: 0.0640, Val Loss: 0.0747\n",
      "Epoch 102/300 - Train Loss: 0.0627, Val Loss: 0.0740\n",
      "Epoch 103/300 - Train Loss: 0.0640, Val Loss: 0.0736\n",
      "Epoch 104/300 - Train Loss: 0.0657, Val Loss: 0.0755\n",
      "Epoch 105/300 - Train Loss: 0.0635, Val Loss: 0.0740\n",
      "Epoch 106/300 - Train Loss: 0.0624, Val Loss: 0.0723\n",
      "Epoch 107/300 - Train Loss: 0.0661, Val Loss: 0.0725\n",
      "Epoch 108/300 - Train Loss: 0.0644, Val Loss: 0.0728\n",
      "Epoch 109/300 - Train Loss: 0.0621, Val Loss: 0.0742\n",
      "Epoch 110/300 - Train Loss: 0.0640, Val Loss: 0.0745\n",
      "Epoch 111/300 - Train Loss: 0.0620, Val Loss: 0.0730\n",
      "Epoch 112/300 - Train Loss: 0.0646, Val Loss: 0.0738\n",
      "Epoch 113/300 - Train Loss: 0.0633, Val Loss: 0.0719\n",
      "Epoch 114/300 - Train Loss: 0.0626, Val Loss: 0.0727\n",
      "Epoch 115/300 - Train Loss: 0.0620, Val Loss: 0.0753\n",
      "Epoch 116/300 - Train Loss: 0.0619, Val Loss: 0.0746\n",
      "Epoch 117/300 - Train Loss: 0.0626, Val Loss: 0.0736\n",
      "Epoch 118/300 - Train Loss: 0.0618, Val Loss: 0.0723\n",
      "Epoch 119/300 - Train Loss: 0.0633, Val Loss: 0.0736\n",
      "Epoch 120/300 - Train Loss: 0.0623, Val Loss: 0.0754\n",
      "Epoch 121/300 - Train Loss: 0.0620, Val Loss: 0.0734\n",
      "Epoch 122/300 - Train Loss: 0.0609, Val Loss: 0.0715\n",
      "Epoch 123/300 - Train Loss: 0.0622, Val Loss: 0.0738\n",
      "Epoch 124/300 - Train Loss: 0.0616, Val Loss: 0.0731\n",
      "Epoch 125/300 - Train Loss: 0.0618, Val Loss: 0.0722\n",
      "Epoch 126/300 - Train Loss: 0.0611, Val Loss: 0.0736\n",
      "Epoch 127/300 - Train Loss: 0.0613, Val Loss: 0.0747\n",
      "Epoch 128/300 - Train Loss: 0.0595, Val Loss: 0.0728\n",
      "Epoch 129/300 - Train Loss: 0.0610, Val Loss: 0.0726\n",
      "Epoch 130/300 - Train Loss: 0.0593, Val Loss: 0.0727\n",
      "Epoch 131/300 - Train Loss: 0.0606, Val Loss: 0.0723\n",
      "Epoch 132/300 - Train Loss: 0.0596, Val Loss: 0.0730\n",
      "Epoch 133/300 - Train Loss: 0.0610, Val Loss: 0.0728\n",
      "Epoch 134/300 - Train Loss: 0.0607, Val Loss: 0.0708\n",
      "Epoch 135/300 - Train Loss: 0.0612, Val Loss: 0.0725\n",
      "Epoch 136/300 - Train Loss: 0.0597, Val Loss: 0.0733\n",
      "Epoch 137/300 - Train Loss: 0.0614, Val Loss: 0.0720\n",
      "Epoch 138/300 - Train Loss: 0.0609, Val Loss: 0.0732\n",
      "Epoch 139/300 - Train Loss: 0.0591, Val Loss: 0.0703\n",
      "Epoch 140/300 - Train Loss: 0.0591, Val Loss: 0.0719\n",
      "Epoch 141/300 - Train Loss: 0.0600, Val Loss: 0.0730\n",
      "Epoch 142/300 - Train Loss: 0.0594, Val Loss: 0.0721\n",
      "Epoch 143/300 - Train Loss: 0.0585, Val Loss: 0.0730\n",
      "Epoch 144/300 - Train Loss: 0.0598, Val Loss: 0.0705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0580, Val Loss: 0.0722\n",
      "Epoch 146/300 - Train Loss: 0.0581, Val Loss: 0.0716\n",
      "Epoch 147/300 - Train Loss: 0.0582, Val Loss: 0.0730\n",
      "Epoch 148/300 - Train Loss: 0.0579, Val Loss: 0.0715\n",
      "Epoch 149/300 - Train Loss: 0.0580, Val Loss: 0.0730\n",
      "Epoch 150/300 - Train Loss: 0.0572, Val Loss: 0.0730\n",
      "Epoch 151/300 - Train Loss: 0.0576, Val Loss: 0.0722\n",
      "Epoch 152/300 - Train Loss: 0.0581, Val Loss: 0.0726\n",
      "Epoch 153/300 - Train Loss: 0.0583, Val Loss: 0.0717\n",
      "Epoch 154/300 - Train Loss: 0.0570, Val Loss: 0.0702\n",
      "Epoch 155/300 - Train Loss: 0.0568, Val Loss: 0.0705\n",
      "Epoch 156/300 - Train Loss: 0.0564, Val Loss: 0.0696\n",
      "Epoch 157/300 - Train Loss: 0.0573, Val Loss: 0.0697\n",
      "Epoch 158/300 - Train Loss: 0.0587, Val Loss: 0.0709\n",
      "Epoch 159/300 - Train Loss: 0.0579, Val Loss: 0.0709\n",
      "Epoch 160/300 - Train Loss: 0.0569, Val Loss: 0.0706\n",
      "Epoch 161/300 - Train Loss: 0.0560, Val Loss: 0.0708\n",
      "Epoch 162/300 - Train Loss: 0.0564, Val Loss: 0.0731\n",
      "Epoch 163/300 - Train Loss: 0.0569, Val Loss: 0.0703\n",
      "Epoch 164/300 - Train Loss: 0.0561, Val Loss: 0.0723\n",
      "Epoch 165/300 - Train Loss: 0.0568, Val Loss: 0.0713\n",
      "Epoch 166/300 - Train Loss: 0.0560, Val Loss: 0.0698\n",
      "Epoch 167/300 - Train Loss: 0.0540, Val Loss: 0.0702\n",
      "Epoch 168/300 - Train Loss: 0.0556, Val Loss: 0.0708\n",
      "Epoch 169/300 - Train Loss: 0.0566, Val Loss: 0.0704\n",
      "Epoch 170/300 - Train Loss: 0.0565, Val Loss: 0.0725\n",
      "Epoch 171/300 - Train Loss: 0.0547, Val Loss: 0.0704\n",
      "Epoch 172/300 - Train Loss: 0.0563, Val Loss: 0.0712\n",
      "Epoch 173/300 - Train Loss: 0.0560, Val Loss: 0.0717\n",
      "Epoch 174/300 - Train Loss: 0.0557, Val Loss: 0.0699\n",
      "Epoch 175/300 - Train Loss: 0.0557, Val Loss: 0.0698\n",
      "Epoch 176/300 - Train Loss: 0.0552, Val Loss: 0.0692\n",
      "Epoch 177/300 - Train Loss: 0.0577, Val Loss: 0.0691\n",
      "Epoch 178/300 - Train Loss: 0.0563, Val Loss: 0.0711\n",
      "Epoch 179/300 - Train Loss: 0.0550, Val Loss: 0.0700\n",
      "Epoch 180/300 - Train Loss: 0.0553, Val Loss: 0.0700\n",
      "Epoch 181/300 - Train Loss: 0.0548, Val Loss: 0.0707\n",
      "Epoch 182/300 - Train Loss: 0.0537, Val Loss: 0.0705\n",
      "Epoch 183/300 - Train Loss: 0.0541, Val Loss: 0.0713\n",
      "Epoch 184/300 - Train Loss: 0.0540, Val Loss: 0.0688\n",
      "Epoch 185/300 - Train Loss: 0.0563, Val Loss: 0.0712\n",
      "Epoch 186/300 - Train Loss: 0.0538, Val Loss: 0.0712\n",
      "Epoch 187/300 - Train Loss: 0.0534, Val Loss: 0.0706\n",
      "Epoch 188/300 - Train Loss: 0.0544, Val Loss: 0.0696\n",
      "Epoch 189/300 - Train Loss: 0.0539, Val Loss: 0.0719\n",
      "Epoch 190/300 - Train Loss: 0.0545, Val Loss: 0.0689\n",
      "Epoch 191/300 - Train Loss: 0.0531, Val Loss: 0.0691\n",
      "Epoch 192/300 - Train Loss: 0.0533, Val Loss: 0.0687\n",
      "Epoch 193/300 - Train Loss: 0.0534, Val Loss: 0.0708\n",
      "Epoch 194/300 - Train Loss: 0.0535, Val Loss: 0.0681\n",
      "Epoch 195/300 - Train Loss: 0.0567, Val Loss: 0.0707\n",
      "Epoch 196/300 - Train Loss: 0.0540, Val Loss: 0.0699\n",
      "Epoch 197/300 - Train Loss: 0.0541, Val Loss: 0.0707\n",
      "Epoch 198/300 - Train Loss: 0.0546, Val Loss: 0.0703\n",
      "Epoch 199/300 - Train Loss: 0.0531, Val Loss: 0.0689\n",
      "Epoch 200/300 - Train Loss: 0.0538, Val Loss: 0.0676\n",
      "Epoch 201/300 - Train Loss: 0.0543, Val Loss: 0.0689\n",
      "Epoch 202/300 - Train Loss: 0.0537, Val Loss: 0.0694\n",
      "Epoch 203/300 - Train Loss: 0.0531, Val Loss: 0.0689\n",
      "Epoch 204/300 - Train Loss: 0.0513, Val Loss: 0.0681\n",
      "Epoch 205/300 - Train Loss: 0.0547, Val Loss: 0.0689\n",
      "Epoch 206/300 - Train Loss: 0.0528, Val Loss: 0.0708\n",
      "Epoch 207/300 - Train Loss: 0.0513, Val Loss: 0.0690\n",
      "Epoch 208/300 - Train Loss: 0.0516, Val Loss: 0.0687\n",
      "Epoch 209/300 - Train Loss: 0.0524, Val Loss: 0.0692\n",
      "Epoch 210/300 - Train Loss: 0.0520, Val Loss: 0.0703\n",
      "Epoch 211/300 - Train Loss: 0.0529, Val Loss: 0.0701\n",
      "Epoch 212/300 - Train Loss: 0.0526, Val Loss: 0.0693\n",
      "Epoch 213/300 - Train Loss: 0.0528, Val Loss: 0.0708\n",
      "Epoch 214/300 - Train Loss: 0.0514, Val Loss: 0.0683\n",
      "Epoch 215/300 - Train Loss: 0.0521, Val Loss: 0.0714\n",
      "Epoch 216/300 - Train Loss: 0.0514, Val Loss: 0.0701\n",
      "Epoch 217/300 - Train Loss: 0.0517, Val Loss: 0.0712\n",
      "Epoch 218/300 - Train Loss: 0.0512, Val Loss: 0.0697\n",
      "Epoch 219/300 - Train Loss: 0.0526, Val Loss: 0.0688\n",
      "Epoch 220/300 - Train Loss: 0.0515, Val Loss: 0.0702\n",
      "Epoch 221/300 - Train Loss: 0.0517, Val Loss: 0.0694\n",
      "Epoch 222/300 - Train Loss: 0.0508, Val Loss: 0.0689\n",
      "Epoch 223/300 - Train Loss: 0.0511, Val Loss: 0.0687\n",
      "Epoch 224/300 - Train Loss: 0.0511, Val Loss: 0.0680\n",
      "Epoch 225/300 - Train Loss: 0.0508, Val Loss: 0.0702\n",
      "Epoch 226/300 - Train Loss: 0.0502, Val Loss: 0.0679\n",
      "Epoch 227/300 - Train Loss: 0.0513, Val Loss: 0.0698\n",
      "Epoch 228/300 - Train Loss: 0.0518, Val Loss: 0.0681\n",
      "Epoch 229/300 - Train Loss: 0.0519, Val Loss: 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:48:30,469] Trial 240 finished with value: 0.9580981625025012 and parameters: {'F1': 4, 'F2': 32, 'D': 8, 'dropout': 0.14921798791302862, 'learning_rate': 8.048440352665408e-05, 'batch_size': 256, 'weight_decay': 8.764740463170605e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 230/300 - Train Loss: 0.0505, Val Loss: 0.0676\n",
      "Early stopping at epoch 230\n",
      "Macro F1 Score: 0.9581, Macro Precision: 0.9465, Macro Recall: 0.9711\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.87      0.95      0.91        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 242\n",
      "Training with F1=16, F2=32, D=8, dropout=0.15803858610131982, LR=9.283532819532693e-05, BS=256, WD=6.0880184640174806e-05\n",
      "Epoch 1/300 - Train Loss: 0.5082, Val Loss: 0.2311\n",
      "Epoch 2/300 - Train Loss: 0.1839, Val Loss: 0.1522\n",
      "Epoch 3/300 - Train Loss: 0.1350, Val Loss: 0.1192\n",
      "Epoch 4/300 - Train Loss: 0.1139, Val Loss: 0.1055\n",
      "Epoch 5/300 - Train Loss: 0.1018, Val Loss: 0.0924\n",
      "Epoch 6/300 - Train Loss: 0.0955, Val Loss: 0.0942\n",
      "Epoch 7/300 - Train Loss: 0.0922, Val Loss: 0.0952\n",
      "Epoch 8/300 - Train Loss: 0.0905, Val Loss: 0.0781\n",
      "Epoch 9/300 - Train Loss: 0.0865, Val Loss: 0.0771\n",
      "Epoch 10/300 - Train Loss: 0.0856, Val Loss: 0.0759\n",
      "Epoch 11/300 - Train Loss: 0.0844, Val Loss: 0.0779\n",
      "Epoch 12/300 - Train Loss: 0.0823, Val Loss: 0.0737\n",
      "Epoch 13/300 - Train Loss: 0.0807, Val Loss: 0.0800\n",
      "Epoch 14/300 - Train Loss: 0.0806, Val Loss: 0.0799\n",
      "Epoch 15/300 - Train Loss: 0.0791, Val Loss: 0.0757\n",
      "Epoch 16/300 - Train Loss: 0.0773, Val Loss: 0.0771\n",
      "Epoch 17/300 - Train Loss: 0.0777, Val Loss: 0.0682\n",
      "Epoch 18/300 - Train Loss: 0.0764, Val Loss: 0.0700\n",
      "Epoch 19/300 - Train Loss: 0.0761, Val Loss: 0.0725\n",
      "Epoch 20/300 - Train Loss: 0.0747, Val Loss: 0.0772\n",
      "Epoch 21/300 - Train Loss: 0.0751, Val Loss: 0.0755\n",
      "Epoch 22/300 - Train Loss: 0.0728, Val Loss: 0.0681\n",
      "Epoch 23/300 - Train Loss: 0.0732, Val Loss: 0.0699\n",
      "Epoch 24/300 - Train Loss: 0.0707, Val Loss: 0.0677\n",
      "Epoch 25/300 - Train Loss: 0.0705, Val Loss: 0.0667\n",
      "Epoch 26/300 - Train Loss: 0.0715, Val Loss: 0.0706\n",
      "Epoch 27/300 - Train Loss: 0.0705, Val Loss: 0.0806\n",
      "Epoch 28/300 - Train Loss: 0.0698, Val Loss: 0.0696\n",
      "Epoch 29/300 - Train Loss: 0.0693, Val Loss: 0.0722\n",
      "Epoch 30/300 - Train Loss: 0.0692, Val Loss: 0.0687\n",
      "Epoch 31/300 - Train Loss: 0.0672, Val Loss: 0.0722\n",
      "Epoch 32/300 - Train Loss: 0.0682, Val Loss: 0.0769\n",
      "Epoch 33/300 - Train Loss: 0.0682, Val Loss: 0.0735\n",
      "Epoch 34/300 - Train Loss: 0.0665, Val Loss: 0.0678\n",
      "Epoch 35/300 - Train Loss: 0.0666, Val Loss: 0.0666\n",
      "Epoch 36/300 - Train Loss: 0.0673, Val Loss: 0.0709\n",
      "Epoch 37/300 - Train Loss: 0.0667, Val Loss: 0.0690\n",
      "Epoch 38/300 - Train Loss: 0.0662, Val Loss: 0.0696\n",
      "Epoch 39/300 - Train Loss: 0.0649, Val Loss: 0.0761\n",
      "Epoch 40/300 - Train Loss: 0.0656, Val Loss: 0.0687\n",
      "Epoch 41/300 - Train Loss: 0.0649, Val Loss: 0.0664\n",
      "Epoch 42/300 - Train Loss: 0.0637, Val Loss: 0.0668\n",
      "Epoch 43/300 - Train Loss: 0.0635, Val Loss: 0.0684\n",
      "Epoch 44/300 - Train Loss: 0.0638, Val Loss: 0.0719\n",
      "Epoch 45/300 - Train Loss: 0.0641, Val Loss: 0.0666\n",
      "Epoch 46/300 - Train Loss: 0.0632, Val Loss: 0.0730\n",
      "Epoch 47/300 - Train Loss: 0.0621, Val Loss: 0.0783\n",
      "Epoch 48/300 - Train Loss: 0.0623, Val Loss: 0.0720\n",
      "Epoch 49/300 - Train Loss: 0.0624, Val Loss: 0.0766\n",
      "Epoch 50/300 - Train Loss: 0.0612, Val Loss: 0.0675\n",
      "Epoch 51/300 - Train Loss: 0.0620, Val Loss: 0.0704\n",
      "Epoch 52/300 - Train Loss: 0.0599, Val Loss: 0.0702\n",
      "Epoch 53/300 - Train Loss: 0.0605, Val Loss: 0.0676\n",
      "Epoch 54/300 - Train Loss: 0.0612, Val Loss: 0.0685\n",
      "Epoch 55/300 - Train Loss: 0.0593, Val Loss: 0.0671\n",
      "Epoch 56/300 - Train Loss: 0.0589, Val Loss: 0.0699\n",
      "Epoch 57/300 - Train Loss: 0.0585, Val Loss: 0.0681\n",
      "Epoch 58/300 - Train Loss: 0.0592, Val Loss: 0.0692\n",
      "Epoch 59/300 - Train Loss: 0.0588, Val Loss: 0.0726\n",
      "Epoch 60/300 - Train Loss: 0.0596, Val Loss: 0.0688\n",
      "Epoch 61/300 - Train Loss: 0.0586, Val Loss: 0.0665\n",
      "Epoch 62/300 - Train Loss: 0.0577, Val Loss: 0.0667\n",
      "Epoch 63/300 - Train Loss: 0.0573, Val Loss: 0.0698\n",
      "Epoch 64/300 - Train Loss: 0.0579, Val Loss: 0.0710\n",
      "Epoch 65/300 - Train Loss: 0.0574, Val Loss: 0.0734\n",
      "Epoch 66/300 - Train Loss: 0.0577, Val Loss: 0.0692\n",
      "Epoch 67/300 - Train Loss: 0.0576, Val Loss: 0.0756\n",
      "Epoch 68/300 - Train Loss: 0.0572, Val Loss: 0.0746\n",
      "Epoch 69/300 - Train Loss: 0.0565, Val Loss: 0.0673\n",
      "Epoch 70/300 - Train Loss: 0.0549, Val Loss: 0.0679\n",
      "Epoch 71/300 - Train Loss: 0.0557, Val Loss: 0.0649\n",
      "Epoch 72/300 - Train Loss: 0.0553, Val Loss: 0.0721\n",
      "Epoch 73/300 - Train Loss: 0.0553, Val Loss: 0.0713\n",
      "Epoch 74/300 - Train Loss: 0.0549, Val Loss: 0.0661\n",
      "Epoch 75/300 - Train Loss: 0.0538, Val Loss: 0.0683\n",
      "Epoch 76/300 - Train Loss: 0.0541, Val Loss: 0.0680\n",
      "Epoch 77/300 - Train Loss: 0.0537, Val Loss: 0.0683\n",
      "Epoch 78/300 - Train Loss: 0.0539, Val Loss: 0.0686\n",
      "Epoch 79/300 - Train Loss: 0.0541, Val Loss: 0.0746\n",
      "Epoch 80/300 - Train Loss: 0.0520, Val Loss: 0.0671\n",
      "Epoch 81/300 - Train Loss: 0.0533, Val Loss: 0.0667\n",
      "Epoch 82/300 - Train Loss: 0.0535, Val Loss: 0.0716\n",
      "Epoch 83/300 - Train Loss: 0.0526, Val Loss: 0.0678\n",
      "Epoch 84/300 - Train Loss: 0.0554, Val Loss: 0.0705\n",
      "Epoch 85/300 - Train Loss: 0.0541, Val Loss: 0.0698\n",
      "Epoch 86/300 - Train Loss: 0.0510, Val Loss: 0.0719\n",
      "Epoch 87/300 - Train Loss: 0.0519, Val Loss: 0.0731\n",
      "Epoch 88/300 - Train Loss: 0.0512, Val Loss: 0.0719\n",
      "Epoch 89/300 - Train Loss: 0.0502, Val Loss: 0.0699\n",
      "Epoch 90/300 - Train Loss: 0.0503, Val Loss: 0.0712\n",
      "Epoch 91/300 - Train Loss: 0.0498, Val Loss: 0.0737\n",
      "Epoch 92/300 - Train Loss: 0.0509, Val Loss: 0.0669\n",
      "Epoch 93/300 - Train Loss: 0.0487, Val Loss: 0.0706\n",
      "Epoch 94/300 - Train Loss: 0.0497, Val Loss: 0.0712\n",
      "Epoch 95/300 - Train Loss: 0.0496, Val Loss: 0.0702\n",
      "Epoch 96/300 - Train Loss: 0.0488, Val Loss: 0.0719\n",
      "Epoch 97/300 - Train Loss: 0.0492, Val Loss: 0.0725\n",
      "Epoch 98/300 - Train Loss: 0.0484, Val Loss: 0.0710\n",
      "Epoch 99/300 - Train Loss: 0.0483, Val Loss: 0.0706\n",
      "Epoch 100/300 - Train Loss: 0.0483, Val Loss: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:53:42,184] Trial 241 finished with value: 0.9707563799993707 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.15803858610131982, 'learning_rate': 9.283532819532693e-05, 'batch_size': 256, 'weight_decay': 6.0880184640174806e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/300 - Train Loss: 0.0474, Val Loss: 0.0718\n",
      "Early stopping at epoch 101\n",
      "Macro F1 Score: 0.9708, Macro Precision: 0.9647, Macro Recall: 0.9773\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 243\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1260074750057825, LR=8.76406155784669e-05, BS=32, WD=4.559014541490907e-05\n",
      "Epoch 1/300 - Train Loss: 0.2643, Val Loss: 0.1171\n",
      "Epoch 2/300 - Train Loss: 0.1141, Val Loss: 0.0807\n",
      "Epoch 3/300 - Train Loss: 0.0995, Val Loss: 0.0793\n",
      "Epoch 4/300 - Train Loss: 0.0945, Val Loss: 0.0715\n",
      "Epoch 5/300 - Train Loss: 0.0908, Val Loss: 0.0789\n",
      "Epoch 6/300 - Train Loss: 0.0870, Val Loss: 0.0748\n",
      "Epoch 7/300 - Train Loss: 0.0843, Val Loss: 0.0766\n",
      "Epoch 8/300 - Train Loss: 0.0849, Val Loss: 0.0826\n",
      "Epoch 9/300 - Train Loss: 0.0840, Val Loss: 0.0716\n",
      "Epoch 10/300 - Train Loss: 0.0809, Val Loss: 0.0808\n",
      "Epoch 11/300 - Train Loss: 0.0779, Val Loss: 0.0700\n",
      "Epoch 12/300 - Train Loss: 0.0796, Val Loss: 0.0702\n",
      "Epoch 13/300 - Train Loss: 0.0756, Val Loss: 0.0683\n",
      "Epoch 14/300 - Train Loss: 0.0757, Val Loss: 0.0759\n",
      "Epoch 15/300 - Train Loss: 0.0751, Val Loss: 0.0764\n",
      "Epoch 16/300 - Train Loss: 0.0744, Val Loss: 0.0726\n",
      "Epoch 17/300 - Train Loss: 0.0836, Val Loss: 0.0665\n",
      "Epoch 18/300 - Train Loss: 0.0730, Val Loss: 0.0694\n",
      "Epoch 19/300 - Train Loss: 0.0730, Val Loss: 0.0685\n",
      "Epoch 20/300 - Train Loss: 0.0711, Val Loss: 0.0711\n",
      "Epoch 21/300 - Train Loss: 0.0700, Val Loss: 0.0752\n",
      "Epoch 22/300 - Train Loss: 0.0706, Val Loss: 0.0708\n",
      "Epoch 23/300 - Train Loss: 0.0703, Val Loss: 0.0748\n",
      "Epoch 24/300 - Train Loss: 0.0687, Val Loss: 0.0727\n",
      "Epoch 25/300 - Train Loss: 0.0686, Val Loss: 0.0755\n",
      "Epoch 26/300 - Train Loss: 0.0691, Val Loss: 0.0709\n",
      "Epoch 27/300 - Train Loss: 0.0665, Val Loss: 0.0668\n",
      "Epoch 28/300 - Train Loss: 0.0663, Val Loss: 0.0646\n",
      "Epoch 29/300 - Train Loss: 0.0650, Val Loss: 0.0762\n",
      "Epoch 30/300 - Train Loss: 0.0643, Val Loss: 0.0676\n",
      "Epoch 31/300 - Train Loss: 0.0621, Val Loss: 0.0641\n",
      "Epoch 32/300 - Train Loss: 0.0637, Val Loss: 0.0710\n",
      "Epoch 33/300 - Train Loss: 0.0642, Val Loss: 0.0698\n",
      "Epoch 34/300 - Train Loss: 0.0615, Val Loss: 0.0675\n",
      "Epoch 35/300 - Train Loss: 0.0622, Val Loss: 0.0699\n",
      "Epoch 36/300 - Train Loss: 0.0610, Val Loss: 0.0649\n",
      "Epoch 37/300 - Train Loss: 0.0617, Val Loss: 0.0642\n",
      "Epoch 38/300 - Train Loss: 0.0614, Val Loss: 0.0652\n",
      "Epoch 39/300 - Train Loss: 0.0585, Val Loss: 0.0627\n",
      "Epoch 40/300 - Train Loss: 0.0599, Val Loss: 0.0657\n",
      "Epoch 41/300 - Train Loss: 0.0575, Val Loss: 0.0691\n",
      "Epoch 42/300 - Train Loss: 0.0590, Val Loss: 0.0665\n",
      "Epoch 43/300 - Train Loss: 0.0578, Val Loss: 0.0685\n",
      "Epoch 44/300 - Train Loss: 0.0569, Val Loss: 0.0648\n",
      "Epoch 45/300 - Train Loss: 0.0575, Val Loss: 0.0683\n",
      "Epoch 46/300 - Train Loss: 0.0553, Val Loss: 0.0682\n",
      "Epoch 47/300 - Train Loss: 0.0553, Val Loss: 0.0691\n",
      "Epoch 48/300 - Train Loss: 0.0538, Val Loss: 0.0649\n",
      "Epoch 49/300 - Train Loss: 0.0559, Val Loss: 0.0682\n",
      "Epoch 50/300 - Train Loss: 0.0537, Val Loss: 0.0659\n",
      "Epoch 51/300 - Train Loss: 0.0534, Val Loss: 0.0659\n",
      "Epoch 52/300 - Train Loss: 0.0516, Val Loss: 0.0707\n",
      "Epoch 53/300 - Train Loss: 0.0533, Val Loss: 0.0691\n",
      "Epoch 54/300 - Train Loss: 0.0508, Val Loss: 0.0686\n",
      "Epoch 55/300 - Train Loss: 0.0515, Val Loss: 0.0696\n",
      "Epoch 56/300 - Train Loss: 0.0512, Val Loss: 0.0666\n",
      "Epoch 57/300 - Train Loss: 0.0494, Val Loss: 0.0643\n",
      "Epoch 58/300 - Train Loss: 0.0523, Val Loss: 0.0655\n",
      "Epoch 59/300 - Train Loss: 0.0482, Val Loss: 0.0665\n",
      "Epoch 60/300 - Train Loss: 0.0493, Val Loss: 0.0657\n",
      "Epoch 61/300 - Train Loss: 0.0499, Val Loss: 0.0691\n",
      "Epoch 62/300 - Train Loss: 0.0503, Val Loss: 0.0689\n",
      "Epoch 63/300 - Train Loss: 0.0459, Val Loss: 0.0722\n",
      "Epoch 64/300 - Train Loss: 0.0477, Val Loss: 0.0660\n",
      "Epoch 65/300 - Train Loss: 0.0506, Val Loss: 0.0651\n",
      "Epoch 66/300 - Train Loss: 0.0471, Val Loss: 0.0714\n",
      "Epoch 67/300 - Train Loss: 0.0481, Val Loss: 0.0658\n",
      "Epoch 68/300 - Train Loss: 0.0485, Val Loss: 0.0662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 14:58:08,085] Trial 242 finished with value: 0.9712397648509269 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1260074750057825, 'learning_rate': 8.76406155784669e-05, 'batch_size': 32, 'weight_decay': 4.559014541490907e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.0470, Val Loss: 0.0724\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9712, Macro Precision: 0.9656, Macro Recall: 0.9774\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 244\n",
      "Training with F1=16, F2=32, D=8, dropout=0.15271817927393064, LR=6.806110128391836e-05, BS=32, WD=6.319830149085237e-05\n",
      "Epoch 1/300 - Train Loss: 0.3029, Val Loss: 0.1568\n",
      "Epoch 2/300 - Train Loss: 0.1459, Val Loss: 0.0950\n",
      "Epoch 3/300 - Train Loss: 0.1094, Val Loss: 0.0964\n",
      "Epoch 4/300 - Train Loss: 0.0982, Val Loss: 0.0759\n",
      "Epoch 5/300 - Train Loss: 0.0922, Val Loss: 0.0738\n",
      "Epoch 6/300 - Train Loss: 0.0873, Val Loss: 0.0729\n",
      "Epoch 7/300 - Train Loss: 0.0887, Val Loss: 0.0933\n",
      "Epoch 8/300 - Train Loss: 0.0837, Val Loss: 0.0758\n",
      "Epoch 9/300 - Train Loss: 0.0836, Val Loss: 0.0765\n",
      "Epoch 10/300 - Train Loss: 0.0813, Val Loss: 0.0704\n",
      "Epoch 11/300 - Train Loss: 0.0808, Val Loss: 0.0697\n",
      "Epoch 12/300 - Train Loss: 0.0802, Val Loss: 0.0712\n",
      "Epoch 13/300 - Train Loss: 0.0792, Val Loss: 0.0727\n",
      "Epoch 14/300 - Train Loss: 0.0766, Val Loss: 0.0720\n",
      "Epoch 15/300 - Train Loss: 0.0769, Val Loss: 0.0693\n",
      "Epoch 16/300 - Train Loss: 0.0767, Val Loss: 0.0683\n",
      "Epoch 17/300 - Train Loss: 0.0770, Val Loss: 0.0710\n",
      "Epoch 18/300 - Train Loss: 0.0745, Val Loss: 0.0709\n",
      "Epoch 19/300 - Train Loss: 0.0734, Val Loss: 0.0680\n",
      "Epoch 20/300 - Train Loss: 0.0743, Val Loss: 0.0723\n",
      "Epoch 21/300 - Train Loss: 0.0721, Val Loss: 0.0727\n",
      "Epoch 22/300 - Train Loss: 0.0732, Val Loss: 0.0690\n",
      "Epoch 23/300 - Train Loss: 0.0740, Val Loss: 0.0738\n",
      "Epoch 24/300 - Train Loss: 0.0712, Val Loss: 0.0767\n",
      "Epoch 25/300 - Train Loss: 0.0710, Val Loss: 0.0767\n",
      "Epoch 26/300 - Train Loss: 0.0690, Val Loss: 0.0708\n",
      "Epoch 27/300 - Train Loss: 0.0701, Val Loss: 0.0721\n",
      "Epoch 28/300 - Train Loss: 0.0685, Val Loss: 0.0734\n",
      "Epoch 29/300 - Train Loss: 0.0686, Val Loss: 0.0732\n",
      "Epoch 30/300 - Train Loss: 0.0704, Val Loss: 0.0728\n",
      "Epoch 31/300 - Train Loss: 0.0683, Val Loss: 0.0692\n",
      "Epoch 32/300 - Train Loss: 0.0694, Val Loss: 0.0716\n",
      "Epoch 33/300 - Train Loss: 0.0669, Val Loss: 0.0691\n",
      "Epoch 34/300 - Train Loss: 0.0677, Val Loss: 0.0690\n",
      "Epoch 35/300 - Train Loss: 0.0644, Val Loss: 0.0747\n",
      "Epoch 36/300 - Train Loss: 0.0661, Val Loss: 0.0713\n",
      "Epoch 37/300 - Train Loss: 0.0663, Val Loss: 0.0782\n",
      "Epoch 38/300 - Train Loss: 0.0624, Val Loss: 0.0684\n",
      "Epoch 39/300 - Train Loss: 0.0658, Val Loss: 0.0726\n",
      "Epoch 40/300 - Train Loss: 0.0628, Val Loss: 0.0698\n",
      "Epoch 41/300 - Train Loss: 0.0651, Val Loss: 0.0772\n",
      "Epoch 42/300 - Train Loss: 0.0632, Val Loss: 0.0755\n",
      "Epoch 43/300 - Train Loss: 0.0610, Val Loss: 0.0700\n",
      "Epoch 44/300 - Train Loss: 0.0614, Val Loss: 0.0729\n",
      "Epoch 45/300 - Train Loss: 0.0604, Val Loss: 0.0716\n",
      "Epoch 46/300 - Train Loss: 0.0600, Val Loss: 0.0703\n",
      "Epoch 47/300 - Train Loss: 0.0619, Val Loss: 0.0733\n",
      "Epoch 48/300 - Train Loss: 0.0606, Val Loss: 0.0800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:01:17,062] Trial 243 finished with value: 0.9670752126448329 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.15271817927393064, 'learning_rate': 6.806110128391836e-05, 'batch_size': 32, 'weight_decay': 6.319830149085237e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300 - Train Loss: 0.0589, Val Loss: 0.0689\n",
      "Early stopping at epoch 49\n",
      "Macro F1 Score: 0.9671, Macro Precision: 0.9584, Macro Recall: 0.9764\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 245\n",
      "Training with F1=16, F2=32, D=8, dropout=0.11405651683399112, LR=0.00010807190950558543, BS=32, WD=8.051633156027789e-05\n",
      "Epoch 1/300 - Train Loss: 0.2088, Val Loss: 0.1153\n",
      "Epoch 2/300 - Train Loss: 0.1082, Val Loss: 0.0803\n",
      "Epoch 3/300 - Train Loss: 0.0967, Val Loss: 0.0712\n",
      "Epoch 4/300 - Train Loss: 0.0907, Val Loss: 0.0716\n",
      "Epoch 5/300 - Train Loss: 0.0870, Val Loss: 0.0707\n",
      "Epoch 6/300 - Train Loss: 0.0844, Val Loss: 0.0690\n",
      "Epoch 7/300 - Train Loss: 0.0819, Val Loss: 0.0664\n",
      "Epoch 8/300 - Train Loss: 0.0806, Val Loss: 0.0701\n",
      "Epoch 9/300 - Train Loss: 0.0799, Val Loss: 0.0672\n",
      "Epoch 10/300 - Train Loss: 0.0769, Val Loss: 0.0693\n",
      "Epoch 11/300 - Train Loss: 0.0796, Val Loss: 0.0706\n",
      "Epoch 12/300 - Train Loss: 0.0755, Val Loss: 0.0651\n",
      "Epoch 13/300 - Train Loss: 0.0758, Val Loss: 0.0668\n",
      "Epoch 14/300 - Train Loss: 0.0723, Val Loss: 0.0731\n",
      "Epoch 15/300 - Train Loss: 0.0736, Val Loss: 0.0771\n",
      "Epoch 16/300 - Train Loss: 0.0692, Val Loss: 0.0663\n",
      "Epoch 17/300 - Train Loss: 0.0712, Val Loss: 0.0620\n",
      "Epoch 18/300 - Train Loss: 0.0711, Val Loss: 0.0753\n",
      "Epoch 19/300 - Train Loss: 0.0689, Val Loss: 0.0667\n",
      "Epoch 20/300 - Train Loss: 0.0691, Val Loss: 0.0639\n",
      "Epoch 21/300 - Train Loss: 0.0696, Val Loss: 0.0652\n",
      "Epoch 22/300 - Train Loss: 0.0685, Val Loss: 0.0672\n",
      "Epoch 23/300 - Train Loss: 0.0669, Val Loss: 0.0675\n",
      "Epoch 24/300 - Train Loss: 0.0645, Val Loss: 0.0653\n",
      "Epoch 25/300 - Train Loss: 0.0693, Val Loss: 0.0659\n",
      "Epoch 26/300 - Train Loss: 0.0650, Val Loss: 0.0638\n",
      "Epoch 27/300 - Train Loss: 0.0628, Val Loss: 0.0686\n",
      "Epoch 28/300 - Train Loss: 0.0631, Val Loss: 0.0679\n",
      "Epoch 29/300 - Train Loss: 0.0627, Val Loss: 0.0676\n",
      "Epoch 30/300 - Train Loss: 0.0620, Val Loss: 0.0692\n",
      "Epoch 31/300 - Train Loss: 0.0610, Val Loss: 0.0764\n",
      "Epoch 32/300 - Train Loss: 0.0585, Val Loss: 0.0631\n",
      "Epoch 33/300 - Train Loss: 0.0600, Val Loss: 0.0655\n",
      "Epoch 34/300 - Train Loss: 0.0595, Val Loss: 0.0674\n",
      "Epoch 35/300 - Train Loss: 0.0603, Val Loss: 0.0674\n",
      "Epoch 36/300 - Train Loss: 0.0581, Val Loss: 0.0741\n",
      "Epoch 37/300 - Train Loss: 0.0582, Val Loss: 0.0661\n",
      "Epoch 38/300 - Train Loss: 0.0576, Val Loss: 0.0716\n",
      "Epoch 39/300 - Train Loss: 0.0583, Val Loss: 0.0664\n",
      "Epoch 40/300 - Train Loss: 0.0570, Val Loss: 0.0698\n",
      "Epoch 41/300 - Train Loss: 0.0557, Val Loss: 0.0675\n",
      "Epoch 42/300 - Train Loss: 0.0551, Val Loss: 0.0748\n",
      "Epoch 43/300 - Train Loss: 0.0562, Val Loss: 0.0667\n",
      "Epoch 44/300 - Train Loss: 0.0553, Val Loss: 0.0720\n",
      "Epoch 45/300 - Train Loss: 0.0538, Val Loss: 0.0652\n",
      "Epoch 46/300 - Train Loss: 0.0536, Val Loss: 0.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:04:18,034] Trial 244 finished with value: 0.969538970913621 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.11405651683399112, 'learning_rate': 0.00010807190950558543, 'batch_size': 32, 'weight_decay': 8.051633156027789e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300 - Train Loss: 0.0534, Val Loss: 0.0676\n",
      "Early stopping at epoch 47\n",
      "Macro F1 Score: 0.9695, Macro Precision: 0.9681, Macro Recall: 0.9711\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 246\n",
      "Training with F1=16, F2=32, D=8, dropout=0.17585925270181227, LR=7.565823307074054e-05, BS=32, WD=5.704760645816568e-05\n",
      "Epoch 1/300 - Train Loss: 0.2881, Val Loss: 0.1351\n",
      "Epoch 2/300 - Train Loss: 0.1265, Val Loss: 0.0915\n",
      "Epoch 3/300 - Train Loss: 0.1017, Val Loss: 0.0824\n",
      "Epoch 4/300 - Train Loss: 0.0988, Val Loss: 0.0797\n",
      "Epoch 5/300 - Train Loss: 0.0948, Val Loss: 0.0809\n",
      "Epoch 6/300 - Train Loss: 0.0923, Val Loss: 0.0758\n",
      "Epoch 7/300 - Train Loss: 0.0872, Val Loss: 0.0796\n",
      "Epoch 8/300 - Train Loss: 0.0868, Val Loss: 0.0805\n",
      "Epoch 9/300 - Train Loss: 0.0868, Val Loss: 0.0710\n",
      "Epoch 10/300 - Train Loss: 0.0837, Val Loss: 0.0763\n",
      "Epoch 11/300 - Train Loss: 0.0867, Val Loss: 0.0793\n",
      "Epoch 12/300 - Train Loss: 0.0825, Val Loss: 0.0775\n",
      "Epoch 13/300 - Train Loss: 0.0802, Val Loss: 0.0709\n",
      "Epoch 14/300 - Train Loss: 0.0813, Val Loss: 0.0702\n",
      "Epoch 15/300 - Train Loss: 0.0781, Val Loss: 0.0771\n",
      "Epoch 16/300 - Train Loss: 0.0767, Val Loss: 0.0699\n",
      "Epoch 17/300 - Train Loss: 0.0763, Val Loss: 0.0718\n",
      "Epoch 18/300 - Train Loss: 0.0760, Val Loss: 0.0730\n",
      "Epoch 19/300 - Train Loss: 0.0753, Val Loss: 0.0710\n",
      "Epoch 20/300 - Train Loss: 0.0722, Val Loss: 0.0714\n",
      "Epoch 21/300 - Train Loss: 0.0736, Val Loss: 0.0774\n",
      "Epoch 22/300 - Train Loss: 0.0727, Val Loss: 0.0678\n",
      "Epoch 23/300 - Train Loss: 0.0725, Val Loss: 0.0704\n",
      "Epoch 24/300 - Train Loss: 0.0706, Val Loss: 0.0656\n",
      "Epoch 25/300 - Train Loss: 0.0708, Val Loss: 0.0726\n",
      "Epoch 26/300 - Train Loss: 0.0695, Val Loss: 0.0675\n",
      "Epoch 27/300 - Train Loss: 0.0711, Val Loss: 0.0764\n",
      "Epoch 28/300 - Train Loss: 0.0697, Val Loss: 0.0666\n",
      "Epoch 29/300 - Train Loss: 0.0695, Val Loss: 0.0714\n",
      "Epoch 30/300 - Train Loss: 0.0712, Val Loss: 0.0692\n",
      "Epoch 31/300 - Train Loss: 0.0665, Val Loss: 0.0657\n",
      "Epoch 32/300 - Train Loss: 0.0684, Val Loss: 0.0708\n",
      "Epoch 33/300 - Train Loss: 0.0660, Val Loss: 0.0707\n",
      "Epoch 34/300 - Train Loss: 0.0677, Val Loss: 0.0715\n",
      "Epoch 35/300 - Train Loss: 0.0635, Val Loss: 0.0730\n",
      "Epoch 36/300 - Train Loss: 0.0653, Val Loss: 0.0690\n",
      "Epoch 37/300 - Train Loss: 0.0653, Val Loss: 0.0697\n",
      "Epoch 38/300 - Train Loss: 0.0642, Val Loss: 0.0756\n",
      "Epoch 39/300 - Train Loss: 0.0645, Val Loss: 0.0700\n",
      "Epoch 40/300 - Train Loss: 0.0617, Val Loss: 0.0690\n",
      "Epoch 41/300 - Train Loss: 0.0638, Val Loss: 0.0678\n",
      "Epoch 42/300 - Train Loss: 0.0610, Val Loss: 0.0819\n",
      "Epoch 43/300 - Train Loss: 0.0622, Val Loss: 0.0703\n",
      "Epoch 44/300 - Train Loss: 0.0622, Val Loss: 0.0677\n",
      "Epoch 45/300 - Train Loss: 0.0615, Val Loss: 0.0804\n",
      "Epoch 46/300 - Train Loss: 0.0605, Val Loss: 0.0680\n",
      "Epoch 47/300 - Train Loss: 0.0606, Val Loss: 0.0711\n",
      "Epoch 48/300 - Train Loss: 0.0597, Val Loss: 0.0688\n",
      "Epoch 49/300 - Train Loss: 0.0596, Val Loss: 0.0699\n",
      "Epoch 50/300 - Train Loss: 0.0587, Val Loss: 0.0700\n",
      "Epoch 51/300 - Train Loss: 0.0587, Val Loss: 0.0700\n",
      "Epoch 52/300 - Train Loss: 0.0593, Val Loss: 0.0689\n",
      "Epoch 53/300 - Train Loss: 0.0595, Val Loss: 0.0666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:07:46,636] Trial 245 finished with value: 0.972836252153025 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.17585925270181227, 'learning_rate': 7.565823307074054e-05, 'batch_size': 32, 'weight_decay': 5.704760645816568e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300 - Train Loss: 0.0583, Val Loss: 0.0706\n",
      "Early stopping at epoch 54\n",
      "Macro F1 Score: 0.9728, Macro Precision: 0.9732, Macro Recall: 0.9725\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 247\n",
      "Training with F1=16, F2=32, D=8, dropout=0.18749454833924342, LR=6.173217918832794e-05, BS=32, WD=5.522747940494886e-05\n",
      "Epoch 1/300 - Train Loss: 0.3027, Val Loss: 0.1327\n",
      "Epoch 2/300 - Train Loss: 0.1263, Val Loss: 0.1004\n",
      "Epoch 3/300 - Train Loss: 0.1059, Val Loss: 0.0898\n",
      "Epoch 4/300 - Train Loss: 0.0999, Val Loss: 0.0774\n",
      "Epoch 5/300 - Train Loss: 0.0949, Val Loss: 0.0777\n",
      "Epoch 6/300 - Train Loss: 0.0938, Val Loss: 0.0773\n",
      "Epoch 7/300 - Train Loss: 0.0916, Val Loss: 0.0735\n",
      "Epoch 8/300 - Train Loss: 0.0896, Val Loss: 0.0732\n",
      "Epoch 9/300 - Train Loss: 0.0870, Val Loss: 0.0711\n",
      "Epoch 10/300 - Train Loss: 0.0860, Val Loss: 0.0737\n",
      "Epoch 11/300 - Train Loss: 0.0868, Val Loss: 0.0728\n",
      "Epoch 12/300 - Train Loss: 0.0846, Val Loss: 0.0768\n",
      "Epoch 13/300 - Train Loss: 0.0844, Val Loss: 0.0827\n",
      "Epoch 14/300 - Train Loss: 0.0822, Val Loss: 0.0752\n",
      "Epoch 15/300 - Train Loss: 0.0821, Val Loss: 0.0722\n",
      "Epoch 16/300 - Train Loss: 0.0814, Val Loss: 0.0708\n",
      "Epoch 17/300 - Train Loss: 0.0786, Val Loss: 0.0704\n",
      "Epoch 18/300 - Train Loss: 0.0797, Val Loss: 0.0720\n",
      "Epoch 19/300 - Train Loss: 0.0818, Val Loss: 0.0756\n",
      "Epoch 20/300 - Train Loss: 0.0771, Val Loss: 0.0646\n",
      "Epoch 21/300 - Train Loss: 0.0796, Val Loss: 0.0722\n",
      "Epoch 22/300 - Train Loss: 0.0767, Val Loss: 0.0679\n",
      "Epoch 23/300 - Train Loss: 0.0796, Val Loss: 0.0637\n",
      "Epoch 24/300 - Train Loss: 0.0787, Val Loss: 0.0686\n",
      "Epoch 25/300 - Train Loss: 0.0768, Val Loss: 0.0683\n",
      "Epoch 26/300 - Train Loss: 0.0724, Val Loss: 0.0752\n",
      "Epoch 27/300 - Train Loss: 0.0730, Val Loss: 0.0673\n",
      "Epoch 28/300 - Train Loss: 0.0756, Val Loss: 0.0662\n",
      "Epoch 29/300 - Train Loss: 0.0738, Val Loss: 0.0735\n",
      "Epoch 30/300 - Train Loss: 0.0726, Val Loss: 0.0651\n",
      "Epoch 31/300 - Train Loss: 0.0701, Val Loss: 0.0696\n",
      "Epoch 32/300 - Train Loss: 0.0716, Val Loss: 0.0644\n",
      "Epoch 33/300 - Train Loss: 0.0730, Val Loss: 0.0673\n",
      "Epoch 34/300 - Train Loss: 0.0724, Val Loss: 0.0649\n",
      "Epoch 35/300 - Train Loss: 0.0707, Val Loss: 0.0669\n",
      "Epoch 36/300 - Train Loss: 0.0727, Val Loss: 0.0644\n",
      "Epoch 37/300 - Train Loss: 0.0687, Val Loss: 0.0652\n",
      "Epoch 38/300 - Train Loss: 0.0683, Val Loss: 0.0641\n",
      "Epoch 39/300 - Train Loss: 0.0683, Val Loss: 0.0730\n",
      "Epoch 40/300 - Train Loss: 0.0688, Val Loss: 0.0664\n",
      "Epoch 41/300 - Train Loss: 0.0674, Val Loss: 0.0661\n",
      "Epoch 42/300 - Train Loss: 0.0662, Val Loss: 0.0665\n",
      "Epoch 43/300 - Train Loss: 0.0677, Val Loss: 0.0659\n",
      "Epoch 44/300 - Train Loss: 0.0652, Val Loss: 0.0720\n",
      "Epoch 45/300 - Train Loss: 0.0661, Val Loss: 0.0669\n",
      "Epoch 46/300 - Train Loss: 0.0652, Val Loss: 0.0671\n",
      "Epoch 47/300 - Train Loss: 0.0644, Val Loss: 0.0675\n",
      "Epoch 48/300 - Train Loss: 0.0643, Val Loss: 0.0663\n",
      "Epoch 49/300 - Train Loss: 0.0642, Val Loss: 0.0654\n",
      "Epoch 50/300 - Train Loss: 0.0629, Val Loss: 0.0661\n",
      "Epoch 51/300 - Train Loss: 0.0626, Val Loss: 0.0631\n",
      "Epoch 52/300 - Train Loss: 0.0626, Val Loss: 0.0651\n",
      "Epoch 53/300 - Train Loss: 0.0612, Val Loss: 0.0770\n",
      "Epoch 54/300 - Train Loss: 0.0627, Val Loss: 0.0699\n",
      "Epoch 55/300 - Train Loss: 0.0635, Val Loss: 0.0834\n",
      "Epoch 56/300 - Train Loss: 0.0620, Val Loss: 0.0665\n",
      "Epoch 57/300 - Train Loss: 0.0598, Val Loss: 0.0716\n",
      "Epoch 58/300 - Train Loss: 0.0606, Val Loss: 0.0689\n",
      "Epoch 59/300 - Train Loss: 0.0599, Val Loss: 0.0668\n",
      "Epoch 60/300 - Train Loss: 0.0619, Val Loss: 0.0634\n",
      "Epoch 61/300 - Train Loss: 0.0613, Val Loss: 0.0643\n",
      "Epoch 62/300 - Train Loss: 0.0617, Val Loss: 0.0633\n",
      "Epoch 63/300 - Train Loss: 0.0600, Val Loss: 0.0667\n",
      "Epoch 64/300 - Train Loss: 0.0579, Val Loss: 0.0618\n",
      "Epoch 65/300 - Train Loss: 0.0597, Val Loss: 0.0677\n",
      "Epoch 66/300 - Train Loss: 0.0586, Val Loss: 0.0684\n",
      "Epoch 67/300 - Train Loss: 0.0579, Val Loss: 0.0747\n",
      "Epoch 68/300 - Train Loss: 0.0581, Val Loss: 0.0673\n",
      "Epoch 69/300 - Train Loss: 0.0567, Val Loss: 0.0647\n",
      "Epoch 70/300 - Train Loss: 0.0581, Val Loss: 0.0668\n",
      "Epoch 71/300 - Train Loss: 0.0580, Val Loss: 0.0642\n",
      "Epoch 72/300 - Train Loss: 0.0543, Val Loss: 0.0665\n",
      "Epoch 73/300 - Train Loss: 0.0579, Val Loss: 0.0667\n",
      "Epoch 74/300 - Train Loss: 0.0539, Val Loss: 0.0661\n",
      "Epoch 75/300 - Train Loss: 0.0559, Val Loss: 0.0627\n",
      "Epoch 76/300 - Train Loss: 0.0561, Val Loss: 0.0638\n",
      "Epoch 77/300 - Train Loss: 0.0544, Val Loss: 0.0673\n",
      "Epoch 78/300 - Train Loss: 0.0555, Val Loss: 0.0682\n",
      "Epoch 79/300 - Train Loss: 0.0564, Val Loss: 0.0701\n",
      "Epoch 80/300 - Train Loss: 0.0551, Val Loss: 0.0678\n",
      "Epoch 81/300 - Train Loss: 0.0538, Val Loss: 0.0687\n",
      "Epoch 82/300 - Train Loss: 0.0544, Val Loss: 0.0683\n",
      "Epoch 83/300 - Train Loss: 0.0550, Val Loss: 0.0787\n",
      "Epoch 84/300 - Train Loss: 0.0561, Val Loss: 0.0674\n",
      "Epoch 85/300 - Train Loss: 0.0522, Val Loss: 0.0678\n",
      "Epoch 86/300 - Train Loss: 0.0547, Val Loss: 0.0678\n",
      "Epoch 87/300 - Train Loss: 0.0516, Val Loss: 0.0639\n",
      "Epoch 88/300 - Train Loss: 0.0520, Val Loss: 0.0645\n",
      "Epoch 89/300 - Train Loss: 0.0533, Val Loss: 0.0657\n",
      "Epoch 90/300 - Train Loss: 0.0530, Val Loss: 0.0669\n",
      "Epoch 91/300 - Train Loss: 0.0522, Val Loss: 0.0678\n",
      "Epoch 92/300 - Train Loss: 0.0514, Val Loss: 0.0685\n",
      "Epoch 93/300 - Train Loss: 0.0532, Val Loss: 0.0716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:13:49,062] Trial 246 finished with value: 0.9698538787761564 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.18749454833924342, 'learning_rate': 6.173217918832794e-05, 'batch_size': 32, 'weight_decay': 5.522747940494886e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300 - Train Loss: 0.0521, Val Loss: 0.0667\n",
      "Early stopping at epoch 94\n",
      "Macro F1 Score: 0.9699, Macro Precision: 0.9577, Macro Recall: 0.9834\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 248\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1677232321162758, LR=7.338216918897203e-05, BS=256, WD=6.956428959005247e-05\n",
      "Epoch 1/300 - Train Loss: 0.5285, Val Loss: 0.3037\n",
      "Epoch 2/300 - Train Loss: 0.2357, Val Loss: 0.2120\n",
      "Epoch 3/300 - Train Loss: 0.1820, Val Loss: 0.1566\n",
      "Epoch 4/300 - Train Loss: 0.1448, Val Loss: 0.1291\n",
      "Epoch 5/300 - Train Loss: 0.1223, Val Loss: 0.1138\n",
      "Epoch 6/300 - Train Loss: 0.1069, Val Loss: 0.0910\n",
      "Epoch 7/300 - Train Loss: 0.0996, Val Loss: 0.0898\n",
      "Epoch 8/300 - Train Loss: 0.0960, Val Loss: 0.0891\n",
      "Epoch 9/300 - Train Loss: 0.0900, Val Loss: 0.0832\n",
      "Epoch 10/300 - Train Loss: 0.0868, Val Loss: 0.0788\n",
      "Epoch 11/300 - Train Loss: 0.0849, Val Loss: 0.0832\n",
      "Epoch 12/300 - Train Loss: 0.0838, Val Loss: 0.0797\n",
      "Epoch 13/300 - Train Loss: 0.0818, Val Loss: 0.0742\n",
      "Epoch 14/300 - Train Loss: 0.0816, Val Loss: 0.0737\n",
      "Epoch 15/300 - Train Loss: 0.0798, Val Loss: 0.0752\n",
      "Epoch 16/300 - Train Loss: 0.0782, Val Loss: 0.0718\n",
      "Epoch 17/300 - Train Loss: 0.0791, Val Loss: 0.0742\n",
      "Epoch 18/300 - Train Loss: 0.0762, Val Loss: 0.0741\n",
      "Epoch 19/300 - Train Loss: 0.0760, Val Loss: 0.0702\n",
      "Epoch 20/300 - Train Loss: 0.0762, Val Loss: 0.0728\n",
      "Epoch 21/300 - Train Loss: 0.0741, Val Loss: 0.0782\n",
      "Epoch 22/300 - Train Loss: 0.0751, Val Loss: 0.0705\n",
      "Epoch 23/300 - Train Loss: 0.0746, Val Loss: 0.0760\n",
      "Epoch 24/300 - Train Loss: 0.0745, Val Loss: 0.0720\n",
      "Epoch 25/300 - Train Loss: 0.0720, Val Loss: 0.0711\n",
      "Epoch 26/300 - Train Loss: 0.0723, Val Loss: 0.0705\n",
      "Epoch 27/300 - Train Loss: 0.0719, Val Loss: 0.0684\n",
      "Epoch 28/300 - Train Loss: 0.0715, Val Loss: 0.0700\n",
      "Epoch 29/300 - Train Loss: 0.0712, Val Loss: 0.0734\n",
      "Epoch 30/300 - Train Loss: 0.0712, Val Loss: 0.0679\n",
      "Epoch 31/300 - Train Loss: 0.0713, Val Loss: 0.0712\n",
      "Epoch 32/300 - Train Loss: 0.0692, Val Loss: 0.0708\n",
      "Epoch 33/300 - Train Loss: 0.0681, Val Loss: 0.0682\n",
      "Epoch 34/300 - Train Loss: 0.0697, Val Loss: 0.0719\n",
      "Epoch 35/300 - Train Loss: 0.0698, Val Loss: 0.0718\n",
      "Epoch 36/300 - Train Loss: 0.0684, Val Loss: 0.0709\n",
      "Epoch 37/300 - Train Loss: 0.0679, Val Loss: 0.0676\n",
      "Epoch 38/300 - Train Loss: 0.0684, Val Loss: 0.0695\n",
      "Epoch 39/300 - Train Loss: 0.0677, Val Loss: 0.0782\n",
      "Epoch 40/300 - Train Loss: 0.0667, Val Loss: 0.0715\n",
      "Epoch 41/300 - Train Loss: 0.0672, Val Loss: 0.0730\n",
      "Epoch 42/300 - Train Loss: 0.0666, Val Loss: 0.0747\n",
      "Epoch 43/300 - Train Loss: 0.0658, Val Loss: 0.0782\n",
      "Epoch 44/300 - Train Loss: 0.0653, Val Loss: 0.0676\n",
      "Epoch 45/300 - Train Loss: 0.0646, Val Loss: 0.0739\n",
      "Epoch 46/300 - Train Loss: 0.0651, Val Loss: 0.0725\n",
      "Epoch 47/300 - Train Loss: 0.0646, Val Loss: 0.0733\n",
      "Epoch 48/300 - Train Loss: 0.0633, Val Loss: 0.0733\n",
      "Epoch 49/300 - Train Loss: 0.0650, Val Loss: 0.0723\n",
      "Epoch 50/300 - Train Loss: 0.0638, Val Loss: 0.0661\n",
      "Epoch 51/300 - Train Loss: 0.0629, Val Loss: 0.0738\n",
      "Epoch 52/300 - Train Loss: 0.0649, Val Loss: 0.0732\n",
      "Epoch 53/300 - Train Loss: 0.0635, Val Loss: 0.0724\n",
      "Epoch 54/300 - Train Loss: 0.0617, Val Loss: 0.0729\n",
      "Epoch 55/300 - Train Loss: 0.0624, Val Loss: 0.0731\n",
      "Epoch 56/300 - Train Loss: 0.0624, Val Loss: 0.0665\n",
      "Epoch 57/300 - Train Loss: 0.0627, Val Loss: 0.0717\n",
      "Epoch 58/300 - Train Loss: 0.0638, Val Loss: 0.0685\n",
      "Epoch 59/300 - Train Loss: 0.0635, Val Loss: 0.0706\n",
      "Epoch 60/300 - Train Loss: 0.0620, Val Loss: 0.0790\n",
      "Epoch 61/300 - Train Loss: 0.0600, Val Loss: 0.0676\n",
      "Epoch 62/300 - Train Loss: 0.0599, Val Loss: 0.0753\n",
      "Epoch 63/300 - Train Loss: 0.0601, Val Loss: 0.0659\n",
      "Epoch 64/300 - Train Loss: 0.0607, Val Loss: 0.0737\n",
      "Epoch 65/300 - Train Loss: 0.0598, Val Loss: 0.0765\n",
      "Epoch 66/300 - Train Loss: 0.0596, Val Loss: 0.0704\n",
      "Epoch 67/300 - Train Loss: 0.0591, Val Loss: 0.0687\n",
      "Epoch 68/300 - Train Loss: 0.0605, Val Loss: 0.0666\n",
      "Epoch 69/300 - Train Loss: 0.0590, Val Loss: 0.0713\n",
      "Epoch 70/300 - Train Loss: 0.0595, Val Loss: 0.0736\n",
      "Epoch 71/300 - Train Loss: 0.0589, Val Loss: 0.0726\n",
      "Epoch 72/300 - Train Loss: 0.0579, Val Loss: 0.0687\n",
      "Epoch 73/300 - Train Loss: 0.0573, Val Loss: 0.0695\n",
      "Epoch 74/300 - Train Loss: 0.0583, Val Loss: 0.0695\n",
      "Epoch 75/300 - Train Loss: 0.0580, Val Loss: 0.0702\n",
      "Epoch 76/300 - Train Loss: 0.0575, Val Loss: 0.0683\n",
      "Epoch 77/300 - Train Loss: 0.0570, Val Loss: 0.0764\n",
      "Epoch 78/300 - Train Loss: 0.0575, Val Loss: 0.0690\n",
      "Epoch 79/300 - Train Loss: 0.0562, Val Loss: 0.0722\n",
      "Epoch 80/300 - Train Loss: 0.0567, Val Loss: 0.0752\n",
      "Epoch 81/300 - Train Loss: 0.0571, Val Loss: 0.0767\n",
      "Epoch 82/300 - Train Loss: 0.0560, Val Loss: 0.0731\n",
      "Epoch 83/300 - Train Loss: 0.0554, Val Loss: 0.0703\n",
      "Epoch 84/300 - Train Loss: 0.0562, Val Loss: 0.0730\n",
      "Epoch 85/300 - Train Loss: 0.0555, Val Loss: 0.0695\n",
      "Epoch 86/300 - Train Loss: 0.0553, Val Loss: 0.0696\n",
      "Epoch 87/300 - Train Loss: 0.0554, Val Loss: 0.0730\n",
      "Epoch 88/300 - Train Loss: 0.0545, Val Loss: 0.0732\n",
      "Epoch 89/300 - Train Loss: 0.0536, Val Loss: 0.0722\n",
      "Epoch 90/300 - Train Loss: 0.0527, Val Loss: 0.0745\n",
      "Epoch 91/300 - Train Loss: 0.0542, Val Loss: 0.0689\n",
      "Epoch 92/300 - Train Loss: 0.0530, Val Loss: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:18:35,774] Trial 247 finished with value: 0.9635866383497144 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1677232321162758, 'learning_rate': 7.338216918897203e-05, 'batch_size': 256, 'weight_decay': 6.956428959005247e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93/300 - Train Loss: 0.0546, Val Loss: 0.0693\n",
      "Early stopping at epoch 93\n",
      "Macro F1 Score: 0.9636, Macro Precision: 0.9579, Macro Recall: 0.9697\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 249\n",
      "Training with F1=32, F2=16, D=8, dropout=0.14222832827783147, LR=7.844409866842567e-05, BS=32, WD=3.874271236488998e-05\n",
      "Epoch 1/300 - Train Loss: 0.2873, Val Loss: 0.1146\n",
      "Epoch 2/300 - Train Loss: 0.1188, Val Loss: 0.0956\n",
      "Epoch 3/300 - Train Loss: 0.1063, Val Loss: 0.0921\n",
      "Epoch 4/300 - Train Loss: 0.0970, Val Loss: 0.0779\n",
      "Epoch 5/300 - Train Loss: 0.0941, Val Loss: 0.0794\n",
      "Epoch 6/300 - Train Loss: 0.0920, Val Loss: 0.0779\n",
      "Epoch 7/300 - Train Loss: 0.0919, Val Loss: 0.0792\n",
      "Epoch 8/300 - Train Loss: 0.0858, Val Loss: 0.0701\n",
      "Epoch 9/300 - Train Loss: 0.0846, Val Loss: 0.0708\n",
      "Epoch 10/300 - Train Loss: 0.0824, Val Loss: 0.0706\n",
      "Epoch 11/300 - Train Loss: 0.0809, Val Loss: 0.0663\n",
      "Epoch 12/300 - Train Loss: 0.0813, Val Loss: 0.0692\n",
      "Epoch 13/300 - Train Loss: 0.0794, Val Loss: 0.0682\n",
      "Epoch 14/300 - Train Loss: 0.0793, Val Loss: 0.0782\n",
      "Epoch 15/300 - Train Loss: 0.0775, Val Loss: 0.0772\n",
      "Epoch 16/300 - Train Loss: 0.0787, Val Loss: 0.0673\n",
      "Epoch 17/300 - Train Loss: 0.0760, Val Loss: 0.0719\n",
      "Epoch 18/300 - Train Loss: 0.0750, Val Loss: 0.0659\n",
      "Epoch 19/300 - Train Loss: 0.0731, Val Loss: 0.0727\n",
      "Epoch 20/300 - Train Loss: 0.0739, Val Loss: 0.0658\n",
      "Epoch 21/300 - Train Loss: 0.0740, Val Loss: 0.0715\n",
      "Epoch 22/300 - Train Loss: 0.0738, Val Loss: 0.0680\n",
      "Epoch 23/300 - Train Loss: 0.0715, Val Loss: 0.0632\n",
      "Epoch 24/300 - Train Loss: 0.0705, Val Loss: 0.0752\n",
      "Epoch 25/300 - Train Loss: 0.0695, Val Loss: 0.0684\n",
      "Epoch 26/300 - Train Loss: 0.0691, Val Loss: 0.0663\n",
      "Epoch 27/300 - Train Loss: 0.0684, Val Loss: 0.0677\n",
      "Epoch 28/300 - Train Loss: 0.0682, Val Loss: 0.0661\n",
      "Epoch 29/300 - Train Loss: 0.0666, Val Loss: 0.0678\n",
      "Epoch 30/300 - Train Loss: 0.0673, Val Loss: 0.0674\n",
      "Epoch 31/300 - Train Loss: 0.0661, Val Loss: 0.0650\n",
      "Epoch 32/300 - Train Loss: 0.0668, Val Loss: 0.0639\n",
      "Epoch 33/300 - Train Loss: 0.0650, Val Loss: 0.0705\n",
      "Epoch 34/300 - Train Loss: 0.0658, Val Loss: 0.0777\n",
      "Epoch 35/300 - Train Loss: 0.0654, Val Loss: 0.0624\n",
      "Epoch 36/300 - Train Loss: 0.0650, Val Loss: 0.0646\n",
      "Epoch 37/300 - Train Loss: 0.0653, Val Loss: 0.0739\n",
      "Epoch 38/300 - Train Loss: 0.0643, Val Loss: 0.0658\n",
      "Epoch 39/300 - Train Loss: 0.0627, Val Loss: 0.0642\n",
      "Epoch 40/300 - Train Loss: 0.0656, Val Loss: 0.0664\n",
      "Epoch 41/300 - Train Loss: 0.0631, Val Loss: 0.0636\n",
      "Epoch 42/300 - Train Loss: 0.0607, Val Loss: 0.0662\n",
      "Epoch 43/300 - Train Loss: 0.0612, Val Loss: 0.0638\n",
      "Epoch 44/300 - Train Loss: 0.0621, Val Loss: 0.0623\n",
      "Epoch 45/300 - Train Loss: 0.0611, Val Loss: 0.0625\n",
      "Epoch 46/300 - Train Loss: 0.0615, Val Loss: 0.0628\n",
      "Epoch 47/300 - Train Loss: 0.0613, Val Loss: 0.0648\n",
      "Epoch 48/300 - Train Loss: 0.0595, Val Loss: 0.0643\n",
      "Epoch 49/300 - Train Loss: 0.0604, Val Loss: 0.0669\n",
      "Epoch 50/300 - Train Loss: 0.0584, Val Loss: 0.0647\n",
      "Epoch 51/300 - Train Loss: 0.0596, Val Loss: 0.0738\n",
      "Epoch 52/300 - Train Loss: 0.0598, Val Loss: 0.0675\n",
      "Epoch 53/300 - Train Loss: 0.0588, Val Loss: 0.0629\n",
      "Epoch 54/300 - Train Loss: 0.0588, Val Loss: 0.0719\n",
      "Epoch 55/300 - Train Loss: 0.0566, Val Loss: 0.0643\n",
      "Epoch 56/300 - Train Loss: 0.0567, Val Loss: 0.0725\n",
      "Epoch 57/300 - Train Loss: 0.0585, Val Loss: 0.0678\n",
      "Epoch 58/300 - Train Loss: 0.0565, Val Loss: 0.0600\n",
      "Epoch 59/300 - Train Loss: 0.0554, Val Loss: 0.0619\n",
      "Epoch 60/300 - Train Loss: 0.0582, Val Loss: 0.0639\n",
      "Epoch 61/300 - Train Loss: 0.0560, Val Loss: 0.0685\n",
      "Epoch 62/300 - Train Loss: 0.0538, Val Loss: 0.0625\n",
      "Epoch 63/300 - Train Loss: 0.0575, Val Loss: 0.0676\n",
      "Epoch 64/300 - Train Loss: 0.0552, Val Loss: 0.0648\n",
      "Epoch 65/300 - Train Loss: 0.0545, Val Loss: 0.0699\n",
      "Epoch 66/300 - Train Loss: 0.0557, Val Loss: 0.0643\n",
      "Epoch 67/300 - Train Loss: 0.0558, Val Loss: 0.0664\n",
      "Epoch 68/300 - Train Loss: 0.0528, Val Loss: 0.0642\n",
      "Epoch 69/300 - Train Loss: 0.0510, Val Loss: 0.0641\n",
      "Epoch 70/300 - Train Loss: 0.0524, Val Loss: 0.0636\n",
      "Epoch 71/300 - Train Loss: 0.0527, Val Loss: 0.0644\n",
      "Epoch 72/300 - Train Loss: 0.0523, Val Loss: 0.0672\n",
      "Epoch 73/300 - Train Loss: 0.0536, Val Loss: 0.0614\n",
      "Epoch 74/300 - Train Loss: 0.0557, Val Loss: 0.0648\n",
      "Epoch 75/300 - Train Loss: 0.0543, Val Loss: 0.0649\n",
      "Epoch 76/300 - Train Loss: 0.0514, Val Loss: 0.0690\n",
      "Epoch 77/300 - Train Loss: 0.0511, Val Loss: 0.0735\n",
      "Epoch 78/300 - Train Loss: 0.0525, Val Loss: 0.0641\n",
      "Epoch 79/300 - Train Loss: 0.0515, Val Loss: 0.0654\n",
      "Epoch 80/300 - Train Loss: 0.0521, Val Loss: 0.0677\n",
      "Epoch 81/300 - Train Loss: 0.0508, Val Loss: 0.0655\n",
      "Epoch 82/300 - Train Loss: 0.0504, Val Loss: 0.0627\n",
      "Epoch 83/300 - Train Loss: 0.0511, Val Loss: 0.0654\n",
      "Epoch 84/300 - Train Loss: 0.0489, Val Loss: 0.0660\n",
      "Epoch 85/300 - Train Loss: 0.0467, Val Loss: 0.0657\n",
      "Epoch 86/300 - Train Loss: 0.0501, Val Loss: 0.0697\n",
      "Epoch 87/300 - Train Loss: 0.0485, Val Loss: 0.0661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:27:50,381] Trial 248 finished with value: 0.969546946223293 and parameters: {'F1': 32, 'F2': 16, 'D': 8, 'dropout': 0.14222832827783147, 'learning_rate': 7.844409866842567e-05, 'batch_size': 32, 'weight_decay': 3.874271236488998e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/300 - Train Loss: 0.0479, Val Loss: 0.0683\n",
      "Early stopping at epoch 88\n",
      "Macro F1 Score: 0.9695, Macro Precision: 0.9678, Macro Recall: 0.9714\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 250\n",
      "Training with F1=16, F2=32, D=8, dropout=0.17190678214867663, LR=0.0001276189102428213, BS=32, WD=4.337361253682249e-05\n",
      "Epoch 1/300 - Train Loss: 0.2236, Val Loss: 0.0979\n",
      "Epoch 2/300 - Train Loss: 0.1092, Val Loss: 0.0948\n",
      "Epoch 3/300 - Train Loss: 0.0992, Val Loss: 0.0807\n",
      "Epoch 4/300 - Train Loss: 0.0930, Val Loss: 0.0830\n",
      "Epoch 5/300 - Train Loss: 0.0902, Val Loss: 0.0813\n",
      "Epoch 6/300 - Train Loss: 0.0872, Val Loss: 0.0747\n",
      "Epoch 7/300 - Train Loss: 0.0862, Val Loss: 0.0740\n",
      "Epoch 8/300 - Train Loss: 0.0836, Val Loss: 0.0762\n",
      "Epoch 9/300 - Train Loss: 0.0822, Val Loss: 0.0769\n",
      "Epoch 10/300 - Train Loss: 0.0790, Val Loss: 0.0752\n",
      "Epoch 11/300 - Train Loss: 0.0796, Val Loss: 0.0692\n",
      "Epoch 12/300 - Train Loss: 0.0780, Val Loss: 0.0744\n",
      "Epoch 13/300 - Train Loss: 0.0760, Val Loss: 0.0790\n",
      "Epoch 14/300 - Train Loss: 0.0748, Val Loss: 0.0797\n",
      "Epoch 15/300 - Train Loss: 0.0730, Val Loss: 0.0748\n",
      "Epoch 16/300 - Train Loss: 0.0729, Val Loss: 0.0681\n",
      "Epoch 17/300 - Train Loss: 0.0713, Val Loss: 0.0727\n",
      "Epoch 18/300 - Train Loss: 0.0732, Val Loss: 0.0737\n",
      "Epoch 19/300 - Train Loss: 0.0722, Val Loss: 0.0685\n",
      "Epoch 20/300 - Train Loss: 0.0715, Val Loss: 0.0710\n",
      "Epoch 21/300 - Train Loss: 0.0693, Val Loss: 0.0721\n",
      "Epoch 22/300 - Train Loss: 0.0695, Val Loss: 0.0649\n",
      "Epoch 23/300 - Train Loss: 0.0679, Val Loss: 0.0727\n",
      "Epoch 24/300 - Train Loss: 0.0681, Val Loss: 0.0730\n",
      "Epoch 25/300 - Train Loss: 0.0657, Val Loss: 0.0679\n",
      "Epoch 26/300 - Train Loss: 0.0668, Val Loss: 0.0667\n",
      "Epoch 27/300 - Train Loss: 0.0662, Val Loss: 0.0709\n",
      "Epoch 28/300 - Train Loss: 0.0634, Val Loss: 0.0705\n",
      "Epoch 29/300 - Train Loss: 0.0627, Val Loss: 0.0695\n",
      "Epoch 30/300 - Train Loss: 0.0632, Val Loss: 0.0669\n",
      "Epoch 31/300 - Train Loss: 0.0621, Val Loss: 0.0673\n",
      "Epoch 32/300 - Train Loss: 0.0615, Val Loss: 0.0679\n",
      "Epoch 33/300 - Train Loss: 0.0617, Val Loss: 0.0776\n",
      "Epoch 34/300 - Train Loss: 0.0613, Val Loss: 0.0769\n",
      "Epoch 35/300 - Train Loss: 0.0597, Val Loss: 0.0675\n",
      "Epoch 36/300 - Train Loss: 0.0599, Val Loss: 0.0733\n",
      "Epoch 37/300 - Train Loss: 0.0602, Val Loss: 0.0719\n",
      "Epoch 38/300 - Train Loss: 0.0605, Val Loss: 0.0698\n",
      "Epoch 39/300 - Train Loss: 0.0583, Val Loss: 0.0655\n",
      "Epoch 40/300 - Train Loss: 0.0580, Val Loss: 0.0715\n",
      "Epoch 41/300 - Train Loss: 0.0575, Val Loss: 0.0656\n",
      "Epoch 42/300 - Train Loss: 0.0578, Val Loss: 0.0719\n",
      "Epoch 43/300 - Train Loss: 0.0549, Val Loss: 0.0681\n",
      "Epoch 44/300 - Train Loss: 0.0565, Val Loss: 0.0687\n",
      "Epoch 45/300 - Train Loss: 0.0570, Val Loss: 0.0706\n",
      "Epoch 46/300 - Train Loss: 0.0553, Val Loss: 0.0697\n",
      "Epoch 47/300 - Train Loss: 0.0533, Val Loss: 0.0727\n",
      "Epoch 48/300 - Train Loss: 0.0538, Val Loss: 0.0726\n",
      "Epoch 49/300 - Train Loss: 0.0553, Val Loss: 0.0748\n",
      "Epoch 50/300 - Train Loss: 0.0546, Val Loss: 0.0671\n",
      "Epoch 51/300 - Train Loss: 0.0543, Val Loss: 0.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:31:10,292] Trial 249 finished with value: 0.975233165255586 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.17190678214867663, 'learning_rate': 0.0001276189102428213, 'batch_size': 32, 'weight_decay': 4.337361253682249e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300 - Train Loss: 0.0511, Val Loss: 0.0709\n",
      "Early stopping at epoch 52\n",
      "Macro F1 Score: 0.9752, Macro Precision: 0.9787, Macro Recall: 0.9719\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 251\n",
      "Training with F1=16, F2=32, D=8, dropout=0.17613674539491217, LR=6.819110119423862e-05, BS=32, WD=4.1599759842737015e-05\n",
      "Epoch 1/300 - Train Loss: 0.2896, Val Loss: 0.1569\n",
      "Epoch 2/300 - Train Loss: 0.1365, Val Loss: 0.0896\n",
      "Epoch 3/300 - Train Loss: 0.1075, Val Loss: 0.0804\n",
      "Epoch 4/300 - Train Loss: 0.1012, Val Loss: 0.0802\n",
      "Epoch 5/300 - Train Loss: 0.0958, Val Loss: 0.0828\n",
      "Epoch 6/300 - Train Loss: 0.0928, Val Loss: 0.0743\n",
      "Epoch 7/300 - Train Loss: 0.0879, Val Loss: 0.0747\n",
      "Epoch 8/300 - Train Loss: 0.0873, Val Loss: 0.0722\n",
      "Epoch 9/300 - Train Loss: 0.0881, Val Loss: 0.0755\n",
      "Epoch 10/300 - Train Loss: 0.0844, Val Loss: 0.0821\n",
      "Epoch 11/300 - Train Loss: 0.0843, Val Loss: 0.0936\n",
      "Epoch 12/300 - Train Loss: 0.0831, Val Loss: 0.0874\n",
      "Epoch 13/300 - Train Loss: 0.0810, Val Loss: 0.0817\n",
      "Epoch 14/300 - Train Loss: 0.0804, Val Loss: 0.0844\n",
      "Epoch 15/300 - Train Loss: 0.0802, Val Loss: 0.0723\n",
      "Epoch 16/300 - Train Loss: 0.0798, Val Loss: 0.0708\n",
      "Epoch 17/300 - Train Loss: 0.0779, Val Loss: 0.0888\n",
      "Epoch 18/300 - Train Loss: 0.0762, Val Loss: 0.0775\n",
      "Epoch 19/300 - Train Loss: 0.0775, Val Loss: 0.0733\n",
      "Epoch 20/300 - Train Loss: 0.0766, Val Loss: 0.0738\n",
      "Epoch 21/300 - Train Loss: 0.0767, Val Loss: 0.0705\n",
      "Epoch 22/300 - Train Loss: 0.0759, Val Loss: 0.0739\n",
      "Epoch 23/300 - Train Loss: 0.0726, Val Loss: 0.0693\n",
      "Epoch 24/300 - Train Loss: 0.0740, Val Loss: 0.0732\n",
      "Epoch 25/300 - Train Loss: 0.0735, Val Loss: 0.0712\n",
      "Epoch 26/300 - Train Loss: 0.0712, Val Loss: 0.0708\n",
      "Epoch 27/300 - Train Loss: 0.0718, Val Loss: 0.0735\n",
      "Epoch 28/300 - Train Loss: 0.0739, Val Loss: 0.0753\n",
      "Epoch 29/300 - Train Loss: 0.0727, Val Loss: 0.0711\n",
      "Epoch 30/300 - Train Loss: 0.0707, Val Loss: 0.0711\n",
      "Epoch 31/300 - Train Loss: 0.0693, Val Loss: 0.0733\n",
      "Epoch 32/300 - Train Loss: 0.0708, Val Loss: 0.0770\n",
      "Epoch 33/300 - Train Loss: 0.0666, Val Loss: 0.0669\n",
      "Epoch 34/300 - Train Loss: 0.0702, Val Loss: 0.0713\n",
      "Epoch 35/300 - Train Loss: 0.0689, Val Loss: 0.0746\n",
      "Epoch 36/300 - Train Loss: 0.0668, Val Loss: 0.0633\n",
      "Epoch 37/300 - Train Loss: 0.0676, Val Loss: 0.0677\n",
      "Epoch 38/300 - Train Loss: 0.0704, Val Loss: 0.0685\n",
      "Epoch 39/300 - Train Loss: 0.0679, Val Loss: 0.0656\n",
      "Epoch 40/300 - Train Loss: 0.0646, Val Loss: 0.0685\n",
      "Epoch 41/300 - Train Loss: 0.0641, Val Loss: 0.0682\n",
      "Epoch 42/300 - Train Loss: 0.0639, Val Loss: 0.0633\n",
      "Epoch 43/300 - Train Loss: 0.0621, Val Loss: 0.0685\n",
      "Epoch 44/300 - Train Loss: 0.0625, Val Loss: 0.0684\n",
      "Epoch 45/300 - Train Loss: 0.0637, Val Loss: 0.0740\n",
      "Epoch 46/300 - Train Loss: 0.0645, Val Loss: 0.0742\n",
      "Epoch 47/300 - Train Loss: 0.0599, Val Loss: 0.0663\n",
      "Epoch 48/300 - Train Loss: 0.0617, Val Loss: 0.0711\n",
      "Epoch 49/300 - Train Loss: 0.0613, Val Loss: 0.0669\n",
      "Epoch 50/300 - Train Loss: 0.0597, Val Loss: 0.0693\n",
      "Epoch 51/300 - Train Loss: 0.0603, Val Loss: 0.0771\n",
      "Epoch 52/300 - Train Loss: 0.0630, Val Loss: 0.0727\n",
      "Epoch 53/300 - Train Loss: 0.0577, Val Loss: 0.0650\n",
      "Epoch 54/300 - Train Loss: 0.0611, Val Loss: 0.0823\n",
      "Epoch 55/300 - Train Loss: 0.0595, Val Loss: 0.0735\n",
      "Epoch 56/300 - Train Loss: 0.0595, Val Loss: 0.0671\n",
      "Epoch 57/300 - Train Loss: 0.0588, Val Loss: 0.0674\n",
      "Epoch 58/300 - Train Loss: 0.0571, Val Loss: 0.0731\n",
      "Epoch 59/300 - Train Loss: 0.0570, Val Loss: 0.0672\n",
      "Epoch 60/300 - Train Loss: 0.0559, Val Loss: 0.0689\n",
      "Epoch 61/300 - Train Loss: 0.0553, Val Loss: 0.0685\n",
      "Epoch 62/300 - Train Loss: 0.0562, Val Loss: 0.0680\n",
      "Epoch 63/300 - Train Loss: 0.0569, Val Loss: 0.0700\n",
      "Epoch 64/300 - Train Loss: 0.0555, Val Loss: 0.0714\n",
      "Epoch 65/300 - Train Loss: 0.0562, Val Loss: 0.0779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:35:24,035] Trial 250 finished with value: 0.9778656378828181 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.17613674539491217, 'learning_rate': 6.819110119423862e-05, 'batch_size': 32, 'weight_decay': 4.1599759842737015e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/300 - Train Loss: 0.0533, Val Loss: 0.0708\n",
      "Early stopping at epoch 66\n",
      "Macro F1 Score: 0.9779, Macro Precision: 0.9789, Macro Recall: 0.9770\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.97      0.97        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 252\n",
      "Training with F1=16, F2=32, D=8, dropout=0.17520321869640187, LR=6.884675258198733e-05, BS=32, WD=4.2851641435022956e-05\n",
      "Epoch 1/300 - Train Loss: 0.2840, Val Loss: 0.1150\n",
      "Epoch 2/300 - Train Loss: 0.1155, Val Loss: 0.0951\n",
      "Epoch 3/300 - Train Loss: 0.1003, Val Loss: 0.0956\n",
      "Epoch 4/300 - Train Loss: 0.0966, Val Loss: 0.0800\n",
      "Epoch 5/300 - Train Loss: 0.0903, Val Loss: 0.0750\n",
      "Epoch 6/300 - Train Loss: 0.0899, Val Loss: 0.0748\n",
      "Epoch 7/300 - Train Loss: 0.0890, Val Loss: 0.0836\n",
      "Epoch 8/300 - Train Loss: 0.0865, Val Loss: 0.0724\n",
      "Epoch 9/300 - Train Loss: 0.0850, Val Loss: 0.0748\n",
      "Epoch 10/300 - Train Loss: 0.0838, Val Loss: 0.0805\n",
      "Epoch 11/300 - Train Loss: 0.0829, Val Loss: 0.0718\n",
      "Epoch 12/300 - Train Loss: 0.0802, Val Loss: 0.0701\n",
      "Epoch 13/300 - Train Loss: 0.0814, Val Loss: 0.0669\n",
      "Epoch 14/300 - Train Loss: 0.0791, Val Loss: 0.0735\n",
      "Epoch 15/300 - Train Loss: 0.0770, Val Loss: 0.0701\n",
      "Epoch 16/300 - Train Loss: 0.0777, Val Loss: 0.0728\n",
      "Epoch 17/300 - Train Loss: 0.0763, Val Loss: 0.0745\n",
      "Epoch 18/300 - Train Loss: 0.0773, Val Loss: 0.0772\n",
      "Epoch 19/300 - Train Loss: 0.0744, Val Loss: 0.0720\n",
      "Epoch 20/300 - Train Loss: 0.0738, Val Loss: 0.0688\n",
      "Epoch 21/300 - Train Loss: 0.0729, Val Loss: 0.0666\n",
      "Epoch 22/300 - Train Loss: 0.0732, Val Loss: 0.0732\n",
      "Epoch 23/300 - Train Loss: 0.0707, Val Loss: 0.0715\n",
      "Epoch 24/300 - Train Loss: 0.0724, Val Loss: 0.0720\n",
      "Epoch 25/300 - Train Loss: 0.0712, Val Loss: 0.0710\n",
      "Epoch 26/300 - Train Loss: 0.0711, Val Loss: 0.0688\n",
      "Epoch 27/300 - Train Loss: 0.0700, Val Loss: 0.0678\n",
      "Epoch 28/300 - Train Loss: 0.0693, Val Loss: 0.0712\n",
      "Epoch 29/300 - Train Loss: 0.0683, Val Loss: 0.0671\n",
      "Epoch 30/300 - Train Loss: 0.0682, Val Loss: 0.0678\n",
      "Epoch 31/300 - Train Loss: 0.0669, Val Loss: 0.0662\n",
      "Epoch 32/300 - Train Loss: 0.0691, Val Loss: 0.0714\n",
      "Epoch 33/300 - Train Loss: 0.0660, Val Loss: 0.0694\n",
      "Epoch 34/300 - Train Loss: 0.0669, Val Loss: 0.0773\n",
      "Epoch 35/300 - Train Loss: 0.0647, Val Loss: 0.0689\n",
      "Epoch 36/300 - Train Loss: 0.0639, Val Loss: 0.0707\n",
      "Epoch 37/300 - Train Loss: 0.0641, Val Loss: 0.0675\n",
      "Epoch 38/300 - Train Loss: 0.0654, Val Loss: 0.0699\n",
      "Epoch 39/300 - Train Loss: 0.0643, Val Loss: 0.0664\n",
      "Epoch 40/300 - Train Loss: 0.0620, Val Loss: 0.0667\n",
      "Epoch 41/300 - Train Loss: 0.0624, Val Loss: 0.0674\n",
      "Epoch 42/300 - Train Loss: 0.0634, Val Loss: 0.0667\n",
      "Epoch 43/300 - Train Loss: 0.0623, Val Loss: 0.0665\n",
      "Epoch 44/300 - Train Loss: 0.0643, Val Loss: 0.0665\n",
      "Epoch 45/300 - Train Loss: 0.0616, Val Loss: 0.0688\n",
      "Epoch 46/300 - Train Loss: 0.0612, Val Loss: 0.0679\n",
      "Epoch 47/300 - Train Loss: 0.0601, Val Loss: 0.0672\n",
      "Epoch 48/300 - Train Loss: 0.0611, Val Loss: 0.0651\n",
      "Epoch 49/300 - Train Loss: 0.0595, Val Loss: 0.0704\n",
      "Epoch 50/300 - Train Loss: 0.0598, Val Loss: 0.0674\n",
      "Epoch 51/300 - Train Loss: 0.0582, Val Loss: 0.0673\n",
      "Epoch 52/300 - Train Loss: 0.0615, Val Loss: 0.0692\n",
      "Epoch 53/300 - Train Loss: 0.0585, Val Loss: 0.0676\n",
      "Epoch 54/300 - Train Loss: 0.0571, Val Loss: 0.0678\n",
      "Epoch 55/300 - Train Loss: 0.0579, Val Loss: 0.0657\n",
      "Epoch 56/300 - Train Loss: 0.0586, Val Loss: 0.0642\n",
      "Epoch 57/300 - Train Loss: 0.0546, Val Loss: 0.0737\n",
      "Epoch 58/300 - Train Loss: 0.0562, Val Loss: 0.0672\n",
      "Epoch 59/300 - Train Loss: 0.0571, Val Loss: 0.0724\n",
      "Epoch 60/300 - Train Loss: 0.0563, Val Loss: 0.0704\n",
      "Epoch 61/300 - Train Loss: 0.0546, Val Loss: 0.0667\n",
      "Epoch 62/300 - Train Loss: 0.0550, Val Loss: 0.0706\n",
      "Epoch 63/300 - Train Loss: 0.0579, Val Loss: 0.0687\n",
      "Epoch 64/300 - Train Loss: 0.0575, Val Loss: 0.0676\n",
      "Epoch 65/300 - Train Loss: 0.0566, Val Loss: 0.0661\n",
      "Epoch 66/300 - Train Loss: 0.0542, Val Loss: 0.0690\n",
      "Epoch 67/300 - Train Loss: 0.0554, Val Loss: 0.0649\n",
      "Epoch 68/300 - Train Loss: 0.0533, Val Loss: 0.0671\n",
      "Epoch 69/300 - Train Loss: 0.0547, Val Loss: 0.0708\n",
      "Epoch 70/300 - Train Loss: 0.0533, Val Loss: 0.0686\n",
      "Epoch 71/300 - Train Loss: 0.0523, Val Loss: 0.0693\n",
      "Epoch 72/300 - Train Loss: 0.0546, Val Loss: 0.0670\n",
      "Epoch 73/300 - Train Loss: 0.0518, Val Loss: 0.0704\n",
      "Epoch 74/300 - Train Loss: 0.0507, Val Loss: 0.0679\n",
      "Epoch 75/300 - Train Loss: 0.0510, Val Loss: 0.0679\n",
      "Epoch 76/300 - Train Loss: 0.0498, Val Loss: 0.0751\n",
      "Epoch 77/300 - Train Loss: 0.0506, Val Loss: 0.0677\n",
      "Epoch 78/300 - Train Loss: 0.0501, Val Loss: 0.0716\n",
      "Epoch 79/300 - Train Loss: 0.0510, Val Loss: 0.0717\n",
      "Epoch 80/300 - Train Loss: 0.0477, Val Loss: 0.0704\n",
      "Epoch 81/300 - Train Loss: 0.0484, Val Loss: 0.0708\n",
      "Epoch 82/300 - Train Loss: 0.0505, Val Loss: 0.0767\n",
      "Epoch 83/300 - Train Loss: 0.0514, Val Loss: 0.0735\n",
      "Epoch 84/300 - Train Loss: 0.0484, Val Loss: 0.0715\n",
      "Epoch 85/300 - Train Loss: 0.0491, Val Loss: 0.0689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:40:54,903] Trial 251 finished with value: 0.9668810127732259 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.17520321869640187, 'learning_rate': 6.884675258198733e-05, 'batch_size': 32, 'weight_decay': 4.2851641435022956e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/300 - Train Loss: 0.0463, Val Loss: 0.0719\n",
      "Early stopping at epoch 86\n",
      "Macro F1 Score: 0.9669, Macro Precision: 0.9672, Macro Recall: 0.9666\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 253\n",
      "Training with F1=16, F2=32, D=8, dropout=0.16255446813389246, LR=5.849434856205025e-05, BS=32, WD=4.61221711812075e-05\n",
      "Epoch 1/300 - Train Loss: 0.2730, Val Loss: 0.1214\n",
      "Epoch 2/300 - Train Loss: 0.1192, Val Loss: 0.0852\n",
      "Epoch 3/300 - Train Loss: 0.1044, Val Loss: 0.1196\n",
      "Epoch 4/300 - Train Loss: 0.0950, Val Loss: 0.0755\n",
      "Epoch 5/300 - Train Loss: 0.0924, Val Loss: 0.0734\n",
      "Epoch 6/300 - Train Loss: 0.0895, Val Loss: 0.0818\n",
      "Epoch 7/300 - Train Loss: 0.0890, Val Loss: 0.0830\n",
      "Epoch 8/300 - Train Loss: 0.0850, Val Loss: 0.0737\n",
      "Epoch 9/300 - Train Loss: 0.0847, Val Loss: 0.0688\n",
      "Epoch 10/300 - Train Loss: 0.0831, Val Loss: 0.0744\n",
      "Epoch 11/300 - Train Loss: 0.0818, Val Loss: 0.0831\n",
      "Epoch 12/300 - Train Loss: 0.0815, Val Loss: 0.0746\n",
      "Epoch 13/300 - Train Loss: 0.0804, Val Loss: 0.0745\n",
      "Epoch 14/300 - Train Loss: 0.0797, Val Loss: 0.0728\n",
      "Epoch 15/300 - Train Loss: 0.0791, Val Loss: 0.0737\n",
      "Epoch 16/300 - Train Loss: 0.0778, Val Loss: 0.0706\n",
      "Epoch 17/300 - Train Loss: 0.0766, Val Loss: 0.0763\n",
      "Epoch 18/300 - Train Loss: 0.0774, Val Loss: 0.0723\n",
      "Epoch 19/300 - Train Loss: 0.0758, Val Loss: 0.0685\n",
      "Epoch 20/300 - Train Loss: 0.0764, Val Loss: 0.0738\n",
      "Epoch 21/300 - Train Loss: 0.0737, Val Loss: 0.0755\n",
      "Epoch 22/300 - Train Loss: 0.0737, Val Loss: 0.0680\n",
      "Epoch 23/300 - Train Loss: 0.0716, Val Loss: 0.0695\n",
      "Epoch 24/300 - Train Loss: 0.0699, Val Loss: 0.0810\n",
      "Epoch 25/300 - Train Loss: 0.0714, Val Loss: 0.0667\n",
      "Epoch 26/300 - Train Loss: 0.0697, Val Loss: 0.0664\n",
      "Epoch 27/300 - Train Loss: 0.0695, Val Loss: 0.0670\n",
      "Epoch 28/300 - Train Loss: 0.0688, Val Loss: 0.0714\n",
      "Epoch 29/300 - Train Loss: 0.0684, Val Loss: 0.0744\n",
      "Epoch 30/300 - Train Loss: 0.0681, Val Loss: 0.0671\n",
      "Epoch 31/300 - Train Loss: 0.0672, Val Loss: 0.0721\n",
      "Epoch 32/300 - Train Loss: 0.0674, Val Loss: 0.0734\n",
      "Epoch 33/300 - Train Loss: 0.0671, Val Loss: 0.0716\n",
      "Epoch 34/300 - Train Loss: 0.0661, Val Loss: 0.0702\n",
      "Epoch 35/300 - Train Loss: 0.0657, Val Loss: 0.0711\n",
      "Epoch 36/300 - Train Loss: 0.0655, Val Loss: 0.0674\n",
      "Epoch 37/300 - Train Loss: 0.0686, Val Loss: 0.0762\n",
      "Epoch 38/300 - Train Loss: 0.0660, Val Loss: 0.0677\n",
      "Epoch 39/300 - Train Loss: 0.0649, Val Loss: 0.0754\n",
      "Epoch 40/300 - Train Loss: 0.0653, Val Loss: 0.0624\n",
      "Epoch 41/300 - Train Loss: 0.0616, Val Loss: 0.0659\n",
      "Epoch 42/300 - Train Loss: 0.0628, Val Loss: 0.0658\n",
      "Epoch 43/300 - Train Loss: 0.0619, Val Loss: 0.0635\n",
      "Epoch 44/300 - Train Loss: 0.0622, Val Loss: 0.0665\n",
      "Epoch 45/300 - Train Loss: 0.0648, Val Loss: 0.0711\n",
      "Epoch 46/300 - Train Loss: 0.0610, Val Loss: 0.0730\n",
      "Epoch 47/300 - Train Loss: 0.0610, Val Loss: 0.0648\n",
      "Epoch 48/300 - Train Loss: 0.0586, Val Loss: 0.0645\n",
      "Epoch 49/300 - Train Loss: 0.0602, Val Loss: 0.0639\n",
      "Epoch 50/300 - Train Loss: 0.0604, Val Loss: 0.0760\n",
      "Epoch 51/300 - Train Loss: 0.0620, Val Loss: 0.0670\n",
      "Epoch 52/300 - Train Loss: 0.0609, Val Loss: 0.0699\n",
      "Epoch 53/300 - Train Loss: 0.0588, Val Loss: 0.0731\n",
      "Epoch 54/300 - Train Loss: 0.0608, Val Loss: 0.0696\n",
      "Epoch 55/300 - Train Loss: 0.0586, Val Loss: 0.0664\n",
      "Epoch 56/300 - Train Loss: 0.0604, Val Loss: 0.0679\n",
      "Epoch 57/300 - Train Loss: 0.0588, Val Loss: 0.0638\n",
      "Epoch 58/300 - Train Loss: 0.0575, Val Loss: 0.0698\n",
      "Epoch 59/300 - Train Loss: 0.0569, Val Loss: 0.0668\n",
      "Epoch 60/300 - Train Loss: 0.0555, Val Loss: 0.0643\n",
      "Epoch 61/300 - Train Loss: 0.0555, Val Loss: 0.0681\n",
      "Epoch 62/300 - Train Loss: 0.0574, Val Loss: 0.0655\n",
      "Epoch 63/300 - Train Loss: 0.0561, Val Loss: 0.0736\n",
      "Epoch 64/300 - Train Loss: 0.0541, Val Loss: 0.0676\n",
      "Epoch 65/300 - Train Loss: 0.0579, Val Loss: 0.0701\n",
      "Epoch 66/300 - Train Loss: 0.0583, Val Loss: 0.0733\n",
      "Epoch 67/300 - Train Loss: 0.0567, Val Loss: 0.0658\n",
      "Epoch 68/300 - Train Loss: 0.0524, Val Loss: 0.0723\n",
      "Epoch 69/300 - Train Loss: 0.0540, Val Loss: 0.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:45:24,310] Trial 252 finished with value: 0.9726625232438638 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.16255446813389246, 'learning_rate': 5.849434856205025e-05, 'batch_size': 32, 'weight_decay': 4.61221711812075e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/300 - Train Loss: 0.0543, Val Loss: 0.0647\n",
      "Early stopping at epoch 70\n",
      "Macro F1 Score: 0.9727, Macro Precision: 0.9689, Macro Recall: 0.9767\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 254\n",
      "Training with F1=16, F2=32, D=8, dropout=0.15992129914549036, LR=5.7652463013141055e-05, BS=32, WD=3.393800286917706e-05\n",
      "Epoch 1/300 - Train Loss: 0.3328, Val Loss: 0.1614\n",
      "Epoch 2/300 - Train Loss: 0.1623, Val Loss: 0.1214\n",
      "Epoch 3/300 - Train Loss: 0.1174, Val Loss: 0.0903\n",
      "Epoch 4/300 - Train Loss: 0.1016, Val Loss: 0.0769\n",
      "Epoch 5/300 - Train Loss: 0.0962, Val Loss: 0.0775\n",
      "Epoch 6/300 - Train Loss: 0.0925, Val Loss: 0.0705\n",
      "Epoch 7/300 - Train Loss: 0.0937, Val Loss: 0.0752\n",
      "Epoch 8/300 - Train Loss: 0.0904, Val Loss: 0.0733\n",
      "Epoch 9/300 - Train Loss: 0.0886, Val Loss: 0.0734\n",
      "Epoch 10/300 - Train Loss: 0.0862, Val Loss: 0.0732\n",
      "Epoch 11/300 - Train Loss: 0.0833, Val Loss: 0.0720\n",
      "Epoch 12/300 - Train Loss: 0.0847, Val Loss: 0.0686\n",
      "Epoch 13/300 - Train Loss: 0.0822, Val Loss: 0.0705\n",
      "Epoch 14/300 - Train Loss: 0.0813, Val Loss: 0.0751\n",
      "Epoch 15/300 - Train Loss: 0.0812, Val Loss: 0.0734\n",
      "Epoch 16/300 - Train Loss: 0.0784, Val Loss: 0.0695\n",
      "Epoch 17/300 - Train Loss: 0.0798, Val Loss: 0.0690\n",
      "Epoch 18/300 - Train Loss: 0.0796, Val Loss: 0.0682\n",
      "Epoch 19/300 - Train Loss: 0.0754, Val Loss: 0.0707\n",
      "Epoch 20/300 - Train Loss: 0.0780, Val Loss: 0.0723\n",
      "Epoch 21/300 - Train Loss: 0.0745, Val Loss: 0.0736\n",
      "Epoch 22/300 - Train Loss: 0.0759, Val Loss: 0.0761\n",
      "Epoch 23/300 - Train Loss: 0.0782, Val Loss: 0.0704\n",
      "Epoch 24/300 - Train Loss: 0.0744, Val Loss: 0.0722\n",
      "Epoch 25/300 - Train Loss: 0.0748, Val Loss: 0.0698\n",
      "Epoch 26/300 - Train Loss: 0.0735, Val Loss: 0.0664\n",
      "Epoch 27/300 - Train Loss: 0.0717, Val Loss: 0.0667\n",
      "Epoch 28/300 - Train Loss: 0.0735, Val Loss: 0.0699\n",
      "Epoch 29/300 - Train Loss: 0.0713, Val Loss: 0.0692\n",
      "Epoch 30/300 - Train Loss: 0.0707, Val Loss: 0.0682\n",
      "Epoch 31/300 - Train Loss: 0.0694, Val Loss: 0.0639\n",
      "Epoch 32/300 - Train Loss: 0.0699, Val Loss: 0.0670\n",
      "Epoch 33/300 - Train Loss: 0.0672, Val Loss: 0.0691\n",
      "Epoch 34/300 - Train Loss: 0.0693, Val Loss: 0.0662\n",
      "Epoch 35/300 - Train Loss: 0.0680, Val Loss: 0.0657\n",
      "Epoch 36/300 - Train Loss: 0.0680, Val Loss: 0.0651\n",
      "Epoch 37/300 - Train Loss: 0.0674, Val Loss: 0.0689\n",
      "Epoch 38/300 - Train Loss: 0.0681, Val Loss: 0.0676\n",
      "Epoch 39/300 - Train Loss: 0.0667, Val Loss: 0.0722\n",
      "Epoch 40/300 - Train Loss: 0.0657, Val Loss: 0.0702\n",
      "Epoch 41/300 - Train Loss: 0.0675, Val Loss: 0.0688\n",
      "Epoch 42/300 - Train Loss: 0.0676, Val Loss: 0.0665\n",
      "Epoch 43/300 - Train Loss: 0.0658, Val Loss: 0.0754\n",
      "Epoch 44/300 - Train Loss: 0.0648, Val Loss: 0.0665\n",
      "Epoch 45/300 - Train Loss: 0.0644, Val Loss: 0.0656\n",
      "Epoch 46/300 - Train Loss: 0.0628, Val Loss: 0.0687\n",
      "Epoch 47/300 - Train Loss: 0.0631, Val Loss: 0.0694\n",
      "Epoch 48/300 - Train Loss: 0.0629, Val Loss: 0.0668\n",
      "Epoch 49/300 - Train Loss: 0.0619, Val Loss: 0.0668\n",
      "Epoch 50/300 - Train Loss: 0.0616, Val Loss: 0.0696\n",
      "Epoch 51/300 - Train Loss: 0.0631, Val Loss: 0.0718\n",
      "Epoch 52/300 - Train Loss: 0.0603, Val Loss: 0.0650\n",
      "Epoch 53/300 - Train Loss: 0.0620, Val Loss: 0.0681\n",
      "Epoch 54/300 - Train Loss: 0.0586, Val Loss: 0.0687\n",
      "Epoch 55/300 - Train Loss: 0.0607, Val Loss: 0.0658\n",
      "Epoch 56/300 - Train Loss: 0.0594, Val Loss: 0.0687\n",
      "Epoch 57/300 - Train Loss: 0.0589, Val Loss: 0.0671\n",
      "Epoch 58/300 - Train Loss: 0.0581, Val Loss: 0.0691\n",
      "Epoch 59/300 - Train Loss: 0.0596, Val Loss: 0.0703\n",
      "Epoch 60/300 - Train Loss: 0.0582, Val Loss: 0.0794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:49:19,285] Trial 253 finished with value: 0.9758015085349268 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.15992129914549036, 'learning_rate': 5.7652463013141055e-05, 'batch_size': 32, 'weight_decay': 3.393800286917706e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/300 - Train Loss: 0.0595, Val Loss: 0.0691\n",
      "Early stopping at epoch 61\n",
      "Macro F1 Score: 0.9758, Macro Precision: 0.9793, Macro Recall: 0.9725\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 255\n",
      "Training with F1=16, F2=32, D=8, dropout=0.16386566936920066, LR=5.7397032894169355e-05, BS=32, WD=3.4104717867496246e-05\n",
      "Epoch 1/300 - Train Loss: 0.3190, Val Loss: 0.2055\n",
      "Epoch 2/300 - Train Loss: 0.1339, Val Loss: 0.0983\n",
      "Epoch 3/300 - Train Loss: 0.1121, Val Loss: 0.0838\n",
      "Epoch 4/300 - Train Loss: 0.1045, Val Loss: 0.0840\n",
      "Epoch 5/300 - Train Loss: 0.0979, Val Loss: 0.0793\n",
      "Epoch 6/300 - Train Loss: 0.0954, Val Loss: 0.0766\n",
      "Epoch 7/300 - Train Loss: 0.0929, Val Loss: 0.0711\n",
      "Epoch 8/300 - Train Loss: 0.0933, Val Loss: 0.0815\n",
      "Epoch 9/300 - Train Loss: 0.0871, Val Loss: 0.0730\n",
      "Epoch 10/300 - Train Loss: 0.0844, Val Loss: 0.0764\n",
      "Epoch 11/300 - Train Loss: 0.0853, Val Loss: 0.0751\n",
      "Epoch 12/300 - Train Loss: 0.0820, Val Loss: 0.0728\n",
      "Epoch 13/300 - Train Loss: 0.0819, Val Loss: 0.0772\n",
      "Epoch 14/300 - Train Loss: 0.0804, Val Loss: 0.0706\n",
      "Epoch 15/300 - Train Loss: 0.0785, Val Loss: 0.0749\n",
      "Epoch 16/300 - Train Loss: 0.0789, Val Loss: 0.0721\n",
      "Epoch 17/300 - Train Loss: 0.0773, Val Loss: 0.0693\n",
      "Epoch 18/300 - Train Loss: 0.0750, Val Loss: 0.0673\n",
      "Epoch 19/300 - Train Loss: 0.0750, Val Loss: 0.0656\n",
      "Epoch 20/300 - Train Loss: 0.0759, Val Loss: 0.0714\n",
      "Epoch 21/300 - Train Loss: 0.0747, Val Loss: 0.0703\n",
      "Epoch 22/300 - Train Loss: 0.0750, Val Loss: 0.0690\n",
      "Epoch 23/300 - Train Loss: 0.0730, Val Loss: 0.0689\n",
      "Epoch 24/300 - Train Loss: 0.0724, Val Loss: 0.0649\n",
      "Epoch 25/300 - Train Loss: 0.0719, Val Loss: 0.0680\n",
      "Epoch 26/300 - Train Loss: 0.0704, Val Loss: 0.0813\n",
      "Epoch 27/300 - Train Loss: 0.0705, Val Loss: 0.0665\n",
      "Epoch 28/300 - Train Loss: 0.0692, Val Loss: 0.0645\n",
      "Epoch 29/300 - Train Loss: 0.0672, Val Loss: 0.0709\n",
      "Epoch 30/300 - Train Loss: 0.0686, Val Loss: 0.0651\n",
      "Epoch 31/300 - Train Loss: 0.0676, Val Loss: 0.0650\n",
      "Epoch 32/300 - Train Loss: 0.0681, Val Loss: 0.0683\n",
      "Epoch 33/300 - Train Loss: 0.0673, Val Loss: 0.0650\n",
      "Epoch 34/300 - Train Loss: 0.0669, Val Loss: 0.0700\n",
      "Epoch 35/300 - Train Loss: 0.0668, Val Loss: 0.0634\n",
      "Epoch 36/300 - Train Loss: 0.0641, Val Loss: 0.0665\n",
      "Epoch 37/300 - Train Loss: 0.0660, Val Loss: 0.0710\n",
      "Epoch 38/300 - Train Loss: 0.0632, Val Loss: 0.0691\n",
      "Epoch 39/300 - Train Loss: 0.0660, Val Loss: 0.0649\n",
      "Epoch 40/300 - Train Loss: 0.0647, Val Loss: 0.0736\n",
      "Epoch 41/300 - Train Loss: 0.0641, Val Loss: 0.0683\n",
      "Epoch 42/300 - Train Loss: 0.0639, Val Loss: 0.0716\n",
      "Epoch 43/300 - Train Loss: 0.0624, Val Loss: 0.0652\n",
      "Epoch 44/300 - Train Loss: 0.0629, Val Loss: 0.0630\n",
      "Epoch 45/300 - Train Loss: 0.0644, Val Loss: 0.0656\n",
      "Epoch 46/300 - Train Loss: 0.0620, Val Loss: 0.0652\n",
      "Epoch 47/300 - Train Loss: 0.0627, Val Loss: 0.0639\n",
      "Epoch 48/300 - Train Loss: 0.0604, Val Loss: 0.0641\n",
      "Epoch 49/300 - Train Loss: 0.0615, Val Loss: 0.0737\n",
      "Epoch 50/300 - Train Loss: 0.0609, Val Loss: 0.0792\n",
      "Epoch 51/300 - Train Loss: 0.0597, Val Loss: 0.0659\n",
      "Epoch 52/300 - Train Loss: 0.0597, Val Loss: 0.0655\n",
      "Epoch 53/300 - Train Loss: 0.0580, Val Loss: 0.0709\n",
      "Epoch 54/300 - Train Loss: 0.0598, Val Loss: 0.0729\n",
      "Epoch 55/300 - Train Loss: 0.0575, Val Loss: 0.0724\n",
      "Epoch 56/300 - Train Loss: 0.0605, Val Loss: 0.0710\n",
      "Epoch 57/300 - Train Loss: 0.0587, Val Loss: 0.0652\n",
      "Epoch 58/300 - Train Loss: 0.0576, Val Loss: 0.0641\n",
      "Epoch 59/300 - Train Loss: 0.0563, Val Loss: 0.0637\n",
      "Epoch 60/300 - Train Loss: 0.0554, Val Loss: 0.0649\n",
      "Epoch 61/300 - Train Loss: 0.0572, Val Loss: 0.0716\n",
      "Epoch 62/300 - Train Loss: 0.0540, Val Loss: 0.0673\n",
      "Epoch 63/300 - Train Loss: 0.0553, Val Loss: 0.0643\n",
      "Epoch 64/300 - Train Loss: 0.0551, Val Loss: 0.0660\n",
      "Epoch 65/300 - Train Loss: 0.0553, Val Loss: 0.0661\n",
      "Epoch 66/300 - Train Loss: 0.0557, Val Loss: 0.0656\n",
      "Epoch 67/300 - Train Loss: 0.0550, Val Loss: 0.0686\n",
      "Epoch 68/300 - Train Loss: 0.0553, Val Loss: 0.0652\n",
      "Epoch 69/300 - Train Loss: 0.0543, Val Loss: 0.0651\n",
      "Epoch 70/300 - Train Loss: 0.0548, Val Loss: 0.0723\n",
      "Epoch 71/300 - Train Loss: 0.0528, Val Loss: 0.0683\n",
      "Epoch 72/300 - Train Loss: 0.0546, Val Loss: 0.0687\n",
      "Epoch 73/300 - Train Loss: 0.0533, Val Loss: 0.0687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 15:54:04,474] Trial 254 finished with value: 0.9709581092084892 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.16386566936920066, 'learning_rate': 5.7397032894169355e-05, 'batch_size': 32, 'weight_decay': 3.4104717867496246e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300 - Train Loss: 0.0534, Val Loss: 0.0696\n",
      "Early stopping at epoch 74\n",
      "Macro F1 Score: 0.9710, Macro Precision: 0.9691, Macro Recall: 0.9729\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 256\n",
      "Training with F1=16, F2=32, D=8, dropout=0.15273473676969762, LR=5.1837557304147856e-05, BS=64, WD=4.662845138886554e-05\n",
      "Epoch 1/300 - Train Loss: 0.3803, Val Loss: 0.2023\n",
      "Epoch 2/300 - Train Loss: 0.1693, Val Loss: 0.1339\n",
      "Epoch 3/300 - Train Loss: 0.1215, Val Loss: 0.0993\n",
      "Epoch 4/300 - Train Loss: 0.1054, Val Loss: 0.0904\n",
      "Epoch 5/300 - Train Loss: 0.0978, Val Loss: 0.0897\n",
      "Epoch 6/300 - Train Loss: 0.0935, Val Loss: 0.0825\n",
      "Epoch 7/300 - Train Loss: 0.0918, Val Loss: 0.0809\n",
      "Epoch 8/300 - Train Loss: 0.0896, Val Loss: 0.0797\n",
      "Epoch 9/300 - Train Loss: 0.0863, Val Loss: 0.0859\n",
      "Epoch 10/300 - Train Loss: 0.0864, Val Loss: 0.0783\n",
      "Epoch 11/300 - Train Loss: 0.0853, Val Loss: 0.0747\n",
      "Epoch 12/300 - Train Loss: 0.0828, Val Loss: 0.0750\n",
      "Epoch 13/300 - Train Loss: 0.0824, Val Loss: 0.0779\n",
      "Epoch 14/300 - Train Loss: 0.0804, Val Loss: 0.0872\n",
      "Epoch 15/300 - Train Loss: 0.0797, Val Loss: 0.0750\n",
      "Epoch 16/300 - Train Loss: 0.0794, Val Loss: 0.0754\n",
      "Epoch 17/300 - Train Loss: 0.0780, Val Loss: 0.0792\n",
      "Epoch 18/300 - Train Loss: 0.0783, Val Loss: 0.0703\n",
      "Epoch 19/300 - Train Loss: 0.0760, Val Loss: 0.0767\n",
      "Epoch 20/300 - Train Loss: 0.0741, Val Loss: 0.0759\n",
      "Epoch 21/300 - Train Loss: 0.0761, Val Loss: 0.0760\n",
      "Epoch 22/300 - Train Loss: 0.0741, Val Loss: 0.0718\n",
      "Epoch 23/300 - Train Loss: 0.0738, Val Loss: 0.0743\n",
      "Epoch 24/300 - Train Loss: 0.0730, Val Loss: 0.0793\n",
      "Epoch 25/300 - Train Loss: 0.0727, Val Loss: 0.0721\n",
      "Epoch 26/300 - Train Loss: 0.0703, Val Loss: 0.0753\n",
      "Epoch 27/300 - Train Loss: 0.0726, Val Loss: 0.0735\n",
      "Epoch 28/300 - Train Loss: 0.0707, Val Loss: 0.0715\n",
      "Epoch 29/300 - Train Loss: 0.0705, Val Loss: 0.0774\n",
      "Epoch 30/300 - Train Loss: 0.0688, Val Loss: 0.0727\n",
      "Epoch 31/300 - Train Loss: 0.0702, Val Loss: 0.0766\n",
      "Epoch 32/300 - Train Loss: 0.0685, Val Loss: 0.0722\n",
      "Epoch 33/300 - Train Loss: 0.0686, Val Loss: 0.0786\n",
      "Epoch 34/300 - Train Loss: 0.0676, Val Loss: 0.0789\n",
      "Epoch 35/300 - Train Loss: 0.0676, Val Loss: 0.0796\n",
      "Epoch 36/300 - Train Loss: 0.0685, Val Loss: 0.0727\n",
      "Epoch 37/300 - Train Loss: 0.0653, Val Loss: 0.0733\n",
      "Epoch 38/300 - Train Loss: 0.0660, Val Loss: 0.0745\n",
      "Epoch 39/300 - Train Loss: 0.0645, Val Loss: 0.0698\n",
      "Epoch 40/300 - Train Loss: 0.0655, Val Loss: 0.0756\n",
      "Epoch 41/300 - Train Loss: 0.0666, Val Loss: 0.0734\n",
      "Epoch 42/300 - Train Loss: 0.0652, Val Loss: 0.0742\n",
      "Epoch 43/300 - Train Loss: 0.0649, Val Loss: 0.0718\n",
      "Epoch 44/300 - Train Loss: 0.0648, Val Loss: 0.0714\n",
      "Epoch 45/300 - Train Loss: 0.0622, Val Loss: 0.0775\n",
      "Epoch 46/300 - Train Loss: 0.0627, Val Loss: 0.0726\n",
      "Epoch 47/300 - Train Loss: 0.0629, Val Loss: 0.0748\n",
      "Epoch 48/300 - Train Loss: 0.0608, Val Loss: 0.0831\n",
      "Epoch 49/300 - Train Loss: 0.0631, Val Loss: 0.0712\n",
      "Epoch 50/300 - Train Loss: 0.0610, Val Loss: 0.0727\n",
      "Epoch 51/300 - Train Loss: 0.0602, Val Loss: 0.0697\n",
      "Epoch 52/300 - Train Loss: 0.0615, Val Loss: 0.0763\n",
      "Epoch 53/300 - Train Loss: 0.0606, Val Loss: 0.0751\n",
      "Epoch 54/300 - Train Loss: 0.0607, Val Loss: 0.0717\n",
      "Epoch 55/300 - Train Loss: 0.0596, Val Loss: 0.0751\n",
      "Epoch 56/300 - Train Loss: 0.0584, Val Loss: 0.0713\n",
      "Epoch 57/300 - Train Loss: 0.0590, Val Loss: 0.0733\n",
      "Epoch 58/300 - Train Loss: 0.0588, Val Loss: 0.0711\n",
      "Epoch 59/300 - Train Loss: 0.0598, Val Loss: 0.0718\n",
      "Epoch 60/300 - Train Loss: 0.0586, Val Loss: 0.0700\n",
      "Epoch 61/300 - Train Loss: 0.0578, Val Loss: 0.0738\n",
      "Epoch 62/300 - Train Loss: 0.0584, Val Loss: 0.0724\n",
      "Epoch 63/300 - Train Loss: 0.0581, Val Loss: 0.0719\n",
      "Epoch 64/300 - Train Loss: 0.0570, Val Loss: 0.0736\n",
      "Epoch 65/300 - Train Loss: 0.0571, Val Loss: 0.0763\n",
      "Epoch 66/300 - Train Loss: 0.0573, Val Loss: 0.0755\n",
      "Epoch 67/300 - Train Loss: 0.0562, Val Loss: 0.0728\n",
      "Epoch 68/300 - Train Loss: 0.0553, Val Loss: 0.0713\n",
      "Epoch 69/300 - Train Loss: 0.0562, Val Loss: 0.0713\n",
      "Epoch 70/300 - Train Loss: 0.0558, Val Loss: 0.0729\n",
      "Epoch 71/300 - Train Loss: 0.0538, Val Loss: 0.0696\n",
      "Epoch 72/300 - Train Loss: 0.0539, Val Loss: 0.0724\n",
      "Epoch 73/300 - Train Loss: 0.0544, Val Loss: 0.0695\n",
      "Epoch 74/300 - Train Loss: 0.0546, Val Loss: 0.0716\n",
      "Epoch 75/300 - Train Loss: 0.0537, Val Loss: 0.0749\n",
      "Epoch 76/300 - Train Loss: 0.0542, Val Loss: 0.0701\n",
      "Epoch 77/300 - Train Loss: 0.0535, Val Loss: 0.0721\n",
      "Epoch 78/300 - Train Loss: 0.0541, Val Loss: 0.0728\n",
      "Epoch 79/300 - Train Loss: 0.0523, Val Loss: 0.0695\n",
      "Epoch 80/300 - Train Loss: 0.0500, Val Loss: 0.0702\n",
      "Epoch 81/300 - Train Loss: 0.0535, Val Loss: 0.0713\n",
      "Epoch 82/300 - Train Loss: 0.0519, Val Loss: 0.0737\n",
      "Epoch 83/300 - Train Loss: 0.0501, Val Loss: 0.0720\n",
      "Epoch 84/300 - Train Loss: 0.0516, Val Loss: 0.0715\n",
      "Epoch 85/300 - Train Loss: 0.0497, Val Loss: 0.0734\n",
      "Epoch 86/300 - Train Loss: 0.0511, Val Loss: 0.0724\n",
      "Epoch 87/300 - Train Loss: 0.0510, Val Loss: 0.0725\n",
      "Epoch 88/300 - Train Loss: 0.0495, Val Loss: 0.0695\n",
      "Epoch 89/300 - Train Loss: 0.0502, Val Loss: 0.0730\n",
      "Epoch 90/300 - Train Loss: 0.0498, Val Loss: 0.0732\n",
      "Epoch 91/300 - Train Loss: 0.0495, Val Loss: 0.0705\n",
      "Epoch 92/300 - Train Loss: 0.0483, Val Loss: 0.0748\n",
      "Epoch 93/300 - Train Loss: 0.0505, Val Loss: 0.0739\n",
      "Epoch 94/300 - Train Loss: 0.0490, Val Loss: 0.0683\n",
      "Epoch 95/300 - Train Loss: 0.0474, Val Loss: 0.0739\n",
      "Epoch 96/300 - Train Loss: 0.0485, Val Loss: 0.0720\n",
      "Epoch 97/300 - Train Loss: 0.0482, Val Loss: 0.0709\n",
      "Epoch 98/300 - Train Loss: 0.0483, Val Loss: 0.0735\n",
      "Epoch 99/300 - Train Loss: 0.0475, Val Loss: 0.0741\n",
      "Epoch 100/300 - Train Loss: 0.0485, Val Loss: 0.0730\n",
      "Epoch 101/300 - Train Loss: 0.0474, Val Loss: 0.0779\n",
      "Epoch 102/300 - Train Loss: 0.0482, Val Loss: 0.0707\n",
      "Epoch 103/300 - Train Loss: 0.0462, Val Loss: 0.0749\n",
      "Epoch 104/300 - Train Loss: 0.0475, Val Loss: 0.0731\n",
      "Epoch 105/300 - Train Loss: 0.0476, Val Loss: 0.0705\n",
      "Epoch 106/300 - Train Loss: 0.0458, Val Loss: 0.0756\n",
      "Epoch 107/300 - Train Loss: 0.0443, Val Loss: 0.0744\n",
      "Epoch 108/300 - Train Loss: 0.0454, Val Loss: 0.0725\n",
      "Epoch 109/300 - Train Loss: 0.0454, Val Loss: 0.0731\n",
      "Epoch 110/300 - Train Loss: 0.0459, Val Loss: 0.0700\n",
      "Epoch 111/300 - Train Loss: 0.0426, Val Loss: 0.0737\n",
      "Epoch 112/300 - Train Loss: 0.0444, Val Loss: 0.0705\n",
      "Epoch 113/300 - Train Loss: 0.0446, Val Loss: 0.0725\n",
      "Epoch 114/300 - Train Loss: 0.0452, Val Loss: 0.0724\n",
      "Epoch 115/300 - Train Loss: 0.0439, Val Loss: 0.0746\n",
      "Epoch 116/300 - Train Loss: 0.0441, Val Loss: 0.0729\n",
      "Epoch 117/300 - Train Loss: 0.0435, Val Loss: 0.0741\n",
      "Epoch 118/300 - Train Loss: 0.0420, Val Loss: 0.0748\n",
      "Epoch 119/300 - Train Loss: 0.0417, Val Loss: 0.0736\n",
      "Epoch 120/300 - Train Loss: 0.0420, Val Loss: 0.0760\n",
      "Epoch 121/300 - Train Loss: 0.0427, Val Loss: 0.0780\n",
      "Epoch 122/300 - Train Loss: 0.0416, Val Loss: 0.0733\n",
      "Epoch 123/300 - Train Loss: 0.0420, Val Loss: 0.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:00:52,509] Trial 255 finished with value: 0.9643859197565118 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.15273473676969762, 'learning_rate': 5.1837557304147856e-05, 'batch_size': 64, 'weight_decay': 4.662845138886554e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 124/300 - Train Loss: 0.0415, Val Loss: 0.0743\n",
      "Early stopping at epoch 124\n",
      "Macro F1 Score: 0.9644, Macro Precision: 0.9543, Macro Recall: 0.9754\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 257\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1772670142289011, LR=6.163012204471619e-05, BS=32, WD=3.322465363296297e-05\n",
      "Epoch 1/300 - Train Loss: 0.2903, Val Loss: 0.1534\n",
      "Epoch 2/300 - Train Loss: 0.1323, Val Loss: 0.1111\n",
      "Epoch 3/300 - Train Loss: 0.1069, Val Loss: 0.0831\n",
      "Epoch 4/300 - Train Loss: 0.0990, Val Loss: 0.0872\n",
      "Epoch 5/300 - Train Loss: 0.0971, Val Loss: 0.0773\n",
      "Epoch 6/300 - Train Loss: 0.0956, Val Loss: 0.0759\n",
      "Epoch 7/300 - Train Loss: 0.0891, Val Loss: 0.0733\n",
      "Epoch 8/300 - Train Loss: 0.0884, Val Loss: 0.0769\n",
      "Epoch 9/300 - Train Loss: 0.0885, Val Loss: 0.0761\n",
      "Epoch 10/300 - Train Loss: 0.0858, Val Loss: 0.0729\n",
      "Epoch 11/300 - Train Loss: 0.0843, Val Loss: 0.0712\n",
      "Epoch 12/300 - Train Loss: 0.0819, Val Loss: 0.0787\n",
      "Epoch 13/300 - Train Loss: 0.0818, Val Loss: 0.0713\n",
      "Epoch 14/300 - Train Loss: 0.0797, Val Loss: 0.0727\n",
      "Epoch 15/300 - Train Loss: 0.0800, Val Loss: 0.0755\n",
      "Epoch 16/300 - Train Loss: 0.0794, Val Loss: 0.0773\n",
      "Epoch 17/300 - Train Loss: 0.0777, Val Loss: 0.0670\n",
      "Epoch 18/300 - Train Loss: 0.0791, Val Loss: 0.0736\n",
      "Epoch 19/300 - Train Loss: 0.0764, Val Loss: 0.0676\n",
      "Epoch 20/300 - Train Loss: 0.0769, Val Loss: 0.0818\n",
      "Epoch 21/300 - Train Loss: 0.0760, Val Loss: 0.0748\n",
      "Epoch 22/300 - Train Loss: 0.0751, Val Loss: 0.0726\n",
      "Epoch 23/300 - Train Loss: 0.0740, Val Loss: 0.0699\n",
      "Epoch 24/300 - Train Loss: 0.0773, Val Loss: 0.0699\n",
      "Epoch 25/300 - Train Loss: 0.0732, Val Loss: 0.0708\n",
      "Epoch 26/300 - Train Loss: 0.0737, Val Loss: 0.0746\n",
      "Epoch 27/300 - Train Loss: 0.0718, Val Loss: 0.0695\n",
      "Epoch 28/300 - Train Loss: 0.0727, Val Loss: 0.0708\n",
      "Epoch 29/300 - Train Loss: 0.0707, Val Loss: 0.0700\n",
      "Epoch 30/300 - Train Loss: 0.0733, Val Loss: 0.0827\n",
      "Epoch 31/300 - Train Loss: 0.0745, Val Loss: 0.0679\n",
      "Epoch 32/300 - Train Loss: 0.0712, Val Loss: 0.0687\n",
      "Epoch 33/300 - Train Loss: 0.0715, Val Loss: 0.0698\n",
      "Epoch 34/300 - Train Loss: 0.0706, Val Loss: 0.0725\n",
      "Epoch 35/300 - Train Loss: 0.0715, Val Loss: 0.0672\n",
      "Epoch 36/300 - Train Loss: 0.0698, Val Loss: 0.0666\n",
      "Epoch 37/300 - Train Loss: 0.0695, Val Loss: 0.0679\n",
      "Epoch 38/300 - Train Loss: 0.0682, Val Loss: 0.0685\n",
      "Epoch 39/300 - Train Loss: 0.0677, Val Loss: 0.0710\n",
      "Epoch 40/300 - Train Loss: 0.0658, Val Loss: 0.0657\n",
      "Epoch 41/300 - Train Loss: 0.0676, Val Loss: 0.0730\n",
      "Epoch 42/300 - Train Loss: 0.0654, Val Loss: 0.0659\n",
      "Epoch 43/300 - Train Loss: 0.0657, Val Loss: 0.0668\n",
      "Epoch 44/300 - Train Loss: 0.0649, Val Loss: 0.0678\n",
      "Epoch 45/300 - Train Loss: 0.0663, Val Loss: 0.0691\n",
      "Epoch 46/300 - Train Loss: 0.0643, Val Loss: 0.0690\n",
      "Epoch 47/300 - Train Loss: 0.0648, Val Loss: 0.0698\n",
      "Epoch 48/300 - Train Loss: 0.0664, Val Loss: 0.0699\n",
      "Epoch 49/300 - Train Loss: 0.0648, Val Loss: 0.0772\n",
      "Epoch 50/300 - Train Loss: 0.0641, Val Loss: 0.0648\n",
      "Epoch 51/300 - Train Loss: 0.0618, Val Loss: 0.0716\n",
      "Epoch 52/300 - Train Loss: 0.0643, Val Loss: 0.0676\n",
      "Epoch 53/300 - Train Loss: 0.0615, Val Loss: 0.0656\n",
      "Epoch 54/300 - Train Loss: 0.0621, Val Loss: 0.0659\n",
      "Epoch 55/300 - Train Loss: 0.0620, Val Loss: 0.0652\n",
      "Epoch 56/300 - Train Loss: 0.0616, Val Loss: 0.0670\n",
      "Epoch 57/300 - Train Loss: 0.0605, Val Loss: 0.0670\n",
      "Epoch 58/300 - Train Loss: 0.0590, Val Loss: 0.0706\n",
      "Epoch 59/300 - Train Loss: 0.0609, Val Loss: 0.0642\n",
      "Epoch 60/300 - Train Loss: 0.0614, Val Loss: 0.0719\n",
      "Epoch 61/300 - Train Loss: 0.0608, Val Loss: 0.0644\n",
      "Epoch 62/300 - Train Loss: 0.0603, Val Loss: 0.0685\n",
      "Epoch 63/300 - Train Loss: 0.0589, Val Loss: 0.0680\n",
      "Epoch 64/300 - Train Loss: 0.0605, Val Loss: 0.0721\n",
      "Epoch 65/300 - Train Loss: 0.0578, Val Loss: 0.0658\n",
      "Epoch 66/300 - Train Loss: 0.0589, Val Loss: 0.0676\n",
      "Epoch 67/300 - Train Loss: 0.0586, Val Loss: 0.0696\n",
      "Epoch 68/300 - Train Loss: 0.0580, Val Loss: 0.0707\n",
      "Epoch 69/300 - Train Loss: 0.0574, Val Loss: 0.0673\n",
      "Epoch 70/300 - Train Loss: 0.0563, Val Loss: 0.0756\n",
      "Epoch 71/300 - Train Loss: 0.0575, Val Loss: 0.0668\n",
      "Epoch 72/300 - Train Loss: 0.0567, Val Loss: 0.0692\n",
      "Epoch 73/300 - Train Loss: 0.0560, Val Loss: 0.0675\n",
      "Epoch 74/300 - Train Loss: 0.0563, Val Loss: 0.0671\n",
      "Epoch 75/300 - Train Loss: 0.0563, Val Loss: 0.0663\n",
      "Epoch 76/300 - Train Loss: 0.0552, Val Loss: 0.0653\n",
      "Epoch 77/300 - Train Loss: 0.0523, Val Loss: 0.0672\n",
      "Epoch 78/300 - Train Loss: 0.0540, Val Loss: 0.0672\n",
      "Epoch 79/300 - Train Loss: 0.0566, Val Loss: 0.0714\n",
      "Epoch 80/300 - Train Loss: 0.0551, Val Loss: 0.0699\n",
      "Epoch 81/300 - Train Loss: 0.0540, Val Loss: 0.0679\n",
      "Epoch 82/300 - Train Loss: 0.0537, Val Loss: 0.0705\n",
      "Epoch 83/300 - Train Loss: 0.0533, Val Loss: 0.0639\n",
      "Epoch 84/300 - Train Loss: 0.0540, Val Loss: 0.0677\n",
      "Epoch 85/300 - Train Loss: 0.0530, Val Loss: 0.0720\n",
      "Epoch 86/300 - Train Loss: 0.0546, Val Loss: 0.0776\n",
      "Epoch 87/300 - Train Loss: 0.0531, Val Loss: 0.0698\n",
      "Epoch 88/300 - Train Loss: 0.0530, Val Loss: 0.0658\n",
      "Epoch 89/300 - Train Loss: 0.0512, Val Loss: 0.0694\n",
      "Epoch 90/300 - Train Loss: 0.0509, Val Loss: 0.0676\n",
      "Epoch 91/300 - Train Loss: 0.0498, Val Loss: 0.0731\n",
      "Epoch 92/300 - Train Loss: 0.0494, Val Loss: 0.0681\n",
      "Epoch 93/300 - Train Loss: 0.0500, Val Loss: 0.0676\n",
      "Epoch 94/300 - Train Loss: 0.0536, Val Loss: 0.0783\n",
      "Epoch 95/300 - Train Loss: 0.0491, Val Loss: 0.0694\n",
      "Epoch 96/300 - Train Loss: 0.0508, Val Loss: 0.0709\n",
      "Epoch 97/300 - Train Loss: 0.0479, Val Loss: 0.0695\n",
      "Epoch 98/300 - Train Loss: 0.0489, Val Loss: 0.0663\n",
      "Epoch 99/300 - Train Loss: 0.0507, Val Loss: 0.0697\n",
      "Epoch 100/300 - Train Loss: 0.0515, Val Loss: 0.0679\n",
      "Epoch 101/300 - Train Loss: 0.0473, Val Loss: 0.0737\n",
      "Epoch 102/300 - Train Loss: 0.0493, Val Loss: 0.0709\n",
      "Epoch 103/300 - Train Loss: 0.0474, Val Loss: 0.0707\n",
      "Epoch 104/300 - Train Loss: 0.0459, Val Loss: 0.0721\n",
      "Epoch 105/300 - Train Loss: 0.0480, Val Loss: 0.0659\n",
      "Epoch 106/300 - Train Loss: 0.0481, Val Loss: 0.0673\n",
      "Epoch 107/300 - Train Loss: 0.0464, Val Loss: 0.0664\n",
      "Epoch 108/300 - Train Loss: 0.0470, Val Loss: 0.0700\n",
      "Epoch 109/300 - Train Loss: 0.0477, Val Loss: 0.0690\n",
      "Epoch 110/300 - Train Loss: 0.0460, Val Loss: 0.0652\n",
      "Epoch 111/300 - Train Loss: 0.0442, Val Loss: 0.0717\n",
      "Epoch 112/300 - Train Loss: 0.0466, Val Loss: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:08:07,811] Trial 256 finished with value: 0.971028396670735 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1772670142289011, 'learning_rate': 6.163012204471619e-05, 'batch_size': 32, 'weight_decay': 3.322465363296297e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 113/300 - Train Loss: 0.0489, Val Loss: 0.0799\n",
      "Early stopping at epoch 113\n",
      "Macro F1 Score: 0.9710, Macro Precision: 0.9692, Macro Recall: 0.9729\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 258\n",
      "Training with F1=16, F2=32, D=8, dropout=0.16239007365726701, LR=5.7226255919804693e-05, BS=32, WD=2.7708087572988178e-05\n",
      "Epoch 1/300 - Train Loss: 0.3111, Val Loss: 0.1430\n",
      "Epoch 2/300 - Train Loss: 0.1326, Val Loss: 0.0880\n",
      "Epoch 3/300 - Train Loss: 0.1083, Val Loss: 0.0851\n",
      "Epoch 4/300 - Train Loss: 0.1010, Val Loss: 0.0733\n",
      "Epoch 5/300 - Train Loss: 0.0973, Val Loss: 0.0743\n",
      "Epoch 6/300 - Train Loss: 0.0916, Val Loss: 0.0828\n",
      "Epoch 7/300 - Train Loss: 0.0889, Val Loss: 0.0796\n",
      "Epoch 8/300 - Train Loss: 0.0880, Val Loss: 0.0745\n",
      "Epoch 9/300 - Train Loss: 0.0873, Val Loss: 0.0785\n",
      "Epoch 10/300 - Train Loss: 0.0871, Val Loss: 0.0778\n",
      "Epoch 11/300 - Train Loss: 0.0821, Val Loss: 0.0719\n",
      "Epoch 12/300 - Train Loss: 0.0842, Val Loss: 0.0707\n",
      "Epoch 13/300 - Train Loss: 0.0808, Val Loss: 0.0677\n",
      "Epoch 14/300 - Train Loss: 0.0796, Val Loss: 0.0719\n",
      "Epoch 15/300 - Train Loss: 0.0787, Val Loss: 0.0729\n",
      "Epoch 16/300 - Train Loss: 0.0808, Val Loss: 0.0710\n",
      "Epoch 17/300 - Train Loss: 0.0788, Val Loss: 0.0706\n",
      "Epoch 18/300 - Train Loss: 0.0770, Val Loss: 0.0686\n",
      "Epoch 19/300 - Train Loss: 0.0776, Val Loss: 0.0676\n",
      "Epoch 20/300 - Train Loss: 0.0776, Val Loss: 0.0703\n",
      "Epoch 21/300 - Train Loss: 0.0750, Val Loss: 0.0689\n",
      "Epoch 22/300 - Train Loss: 0.0747, Val Loss: 0.0720\n",
      "Epoch 23/300 - Train Loss: 0.0713, Val Loss: 0.0734\n",
      "Epoch 24/300 - Train Loss: 0.0745, Val Loss: 0.0680\n",
      "Epoch 25/300 - Train Loss: 0.0736, Val Loss: 0.0673\n",
      "Epoch 26/300 - Train Loss: 0.0730, Val Loss: 0.0669\n",
      "Epoch 27/300 - Train Loss: 0.0734, Val Loss: 0.0743\n",
      "Epoch 28/300 - Train Loss: 0.0715, Val Loss: 0.0673\n",
      "Epoch 29/300 - Train Loss: 0.0688, Val Loss: 0.0674\n",
      "Epoch 30/300 - Train Loss: 0.0710, Val Loss: 0.0692\n",
      "Epoch 31/300 - Train Loss: 0.0714, Val Loss: 0.0644\n",
      "Epoch 32/300 - Train Loss: 0.0689, Val Loss: 0.0664\n",
      "Epoch 33/300 - Train Loss: 0.0676, Val Loss: 0.0666\n",
      "Epoch 34/300 - Train Loss: 0.0701, Val Loss: 0.0678\n",
      "Epoch 35/300 - Train Loss: 0.0679, Val Loss: 0.0633\n",
      "Epoch 36/300 - Train Loss: 0.0654, Val Loss: 0.0656\n",
      "Epoch 37/300 - Train Loss: 0.0663, Val Loss: 0.0643\n",
      "Epoch 38/300 - Train Loss: 0.0680, Val Loss: 0.0691\n",
      "Epoch 39/300 - Train Loss: 0.0682, Val Loss: 0.0633\n",
      "Epoch 40/300 - Train Loss: 0.0645, Val Loss: 0.0629\n",
      "Epoch 41/300 - Train Loss: 0.0637, Val Loss: 0.0674\n",
      "Epoch 42/300 - Train Loss: 0.0643, Val Loss: 0.0734\n",
      "Epoch 43/300 - Train Loss: 0.0630, Val Loss: 0.0645\n",
      "Epoch 44/300 - Train Loss: 0.0626, Val Loss: 0.0641\n",
      "Epoch 45/300 - Train Loss: 0.0635, Val Loss: 0.0686\n",
      "Epoch 46/300 - Train Loss: 0.0617, Val Loss: 0.0653\n",
      "Epoch 47/300 - Train Loss: 0.0615, Val Loss: 0.0647\n",
      "Epoch 48/300 - Train Loss: 0.0611, Val Loss: 0.0627\n",
      "Epoch 49/300 - Train Loss: 0.0599, Val Loss: 0.0638\n",
      "Epoch 50/300 - Train Loss: 0.0601, Val Loss: 0.0682\n",
      "Epoch 51/300 - Train Loss: 0.0629, Val Loss: 0.0698\n",
      "Epoch 52/300 - Train Loss: 0.0610, Val Loss: 0.0647\n",
      "Epoch 53/300 - Train Loss: 0.0596, Val Loss: 0.0654\n",
      "Epoch 54/300 - Train Loss: 0.0598, Val Loss: 0.0631\n",
      "Epoch 55/300 - Train Loss: 0.0593, Val Loss: 0.0637\n",
      "Epoch 56/300 - Train Loss: 0.0563, Val Loss: 0.0623\n",
      "Epoch 57/300 - Train Loss: 0.0573, Val Loss: 0.0620\n",
      "Epoch 58/300 - Train Loss: 0.0591, Val Loss: 0.0650\n",
      "Epoch 59/300 - Train Loss: 0.0577, Val Loss: 0.0641\n",
      "Epoch 60/300 - Train Loss: 0.0565, Val Loss: 0.0636\n",
      "Epoch 61/300 - Train Loss: 0.0584, Val Loss: 0.0668\n",
      "Epoch 62/300 - Train Loss: 0.0575, Val Loss: 0.0634\n",
      "Epoch 63/300 - Train Loss: 0.0586, Val Loss: 0.0697\n",
      "Epoch 64/300 - Train Loss: 0.0557, Val Loss: 0.0651\n",
      "Epoch 65/300 - Train Loss: 0.0562, Val Loss: 0.0696\n",
      "Epoch 66/300 - Train Loss: 0.0552, Val Loss: 0.0654\n",
      "Epoch 67/300 - Train Loss: 0.0534, Val Loss: 0.0640\n",
      "Epoch 68/300 - Train Loss: 0.0555, Val Loss: 0.0638\n",
      "Epoch 69/300 - Train Loss: 0.0550, Val Loss: 0.0691\n",
      "Epoch 70/300 - Train Loss: 0.0544, Val Loss: 0.0656\n",
      "Epoch 71/300 - Train Loss: 0.0543, Val Loss: 0.0651\n",
      "Epoch 72/300 - Train Loss: 0.0545, Val Loss: 0.0671\n",
      "Epoch 73/300 - Train Loss: 0.0528, Val Loss: 0.0652\n",
      "Epoch 74/300 - Train Loss: 0.0536, Val Loss: 0.0651\n",
      "Epoch 75/300 - Train Loss: 0.0521, Val Loss: 0.0719\n",
      "Epoch 76/300 - Train Loss: 0.0552, Val Loss: 0.0667\n",
      "Epoch 77/300 - Train Loss: 0.0523, Val Loss: 0.0657\n",
      "Epoch 78/300 - Train Loss: 0.0523, Val Loss: 0.0733\n",
      "Epoch 79/300 - Train Loss: 0.0507, Val Loss: 0.0672\n",
      "Epoch 80/300 - Train Loss: 0.0514, Val Loss: 0.0670\n",
      "Epoch 81/300 - Train Loss: 0.0524, Val Loss: 0.0653\n",
      "Epoch 82/300 - Train Loss: 0.0532, Val Loss: 0.0750\n",
      "Epoch 83/300 - Train Loss: 0.0510, Val Loss: 0.0682\n",
      "Epoch 84/300 - Train Loss: 0.0522, Val Loss: 0.0688\n",
      "Epoch 85/300 - Train Loss: 0.0508, Val Loss: 0.0682\n",
      "Epoch 86/300 - Train Loss: 0.0515, Val Loss: 0.0681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:13:43,071] Trial 257 finished with value: 0.9706999772160864 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.16239007365726701, 'learning_rate': 5.7226255919804693e-05, 'batch_size': 32, 'weight_decay': 2.7708087572988178e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300 - Train Loss: 0.0510, Val Loss: 0.0652\n",
      "Early stopping at epoch 87\n",
      "Macro F1 Score: 0.9707, Macro Precision: 0.9602, Macro Recall: 0.9822\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 259\n",
      "Training with F1=16, F2=32, D=8, dropout=0.13884356970964942, LR=6.510249811496238e-05, BS=32, WD=4.528762007846347e-05\n",
      "Epoch 1/300 - Train Loss: 0.2811, Val Loss: 0.1225\n",
      "Epoch 2/300 - Train Loss: 0.1213, Val Loss: 0.0960\n",
      "Epoch 3/300 - Train Loss: 0.1028, Val Loss: 0.0835\n",
      "Epoch 4/300 - Train Loss: 0.0979, Val Loss: 0.0737\n",
      "Epoch 5/300 - Train Loss: 0.0958, Val Loss: 0.0692\n",
      "Epoch 6/300 - Train Loss: 0.0901, Val Loss: 0.0822\n",
      "Epoch 7/300 - Train Loss: 0.0870, Val Loss: 0.0708\n",
      "Epoch 8/300 - Train Loss: 0.0864, Val Loss: 0.0687\n",
      "Epoch 9/300 - Train Loss: 0.0856, Val Loss: 0.0820\n",
      "Epoch 10/300 - Train Loss: 0.0829, Val Loss: 0.0729\n",
      "Epoch 11/300 - Train Loss: 0.0812, Val Loss: 0.0691\n",
      "Epoch 12/300 - Train Loss: 0.0778, Val Loss: 0.0689\n",
      "Epoch 13/300 - Train Loss: 0.0786, Val Loss: 0.0674\n",
      "Epoch 14/300 - Train Loss: 0.0769, Val Loss: 0.0759\n",
      "Epoch 15/300 - Train Loss: 0.0776, Val Loss: 0.0683\n",
      "Epoch 16/300 - Train Loss: 0.0798, Val Loss: 0.0674\n",
      "Epoch 17/300 - Train Loss: 0.0747, Val Loss: 0.0709\n",
      "Epoch 18/300 - Train Loss: 0.0768, Val Loss: 0.0777\n",
      "Epoch 19/300 - Train Loss: 0.0751, Val Loss: 0.0761\n",
      "Epoch 20/300 - Train Loss: 0.0728, Val Loss: 0.0650\n",
      "Epoch 21/300 - Train Loss: 0.0715, Val Loss: 0.0701\n",
      "Epoch 22/300 - Train Loss: 0.0705, Val Loss: 0.0660\n",
      "Epoch 23/300 - Train Loss: 0.0720, Val Loss: 0.0642\n",
      "Epoch 24/300 - Train Loss: 0.0716, Val Loss: 0.0687\n",
      "Epoch 25/300 - Train Loss: 0.0704, Val Loss: 0.0655\n",
      "Epoch 26/300 - Train Loss: 0.0708, Val Loss: 0.0778\n",
      "Epoch 27/300 - Train Loss: 0.0662, Val Loss: 0.0766\n",
      "Epoch 28/300 - Train Loss: 0.0705, Val Loss: 0.0658\n",
      "Epoch 29/300 - Train Loss: 0.0702, Val Loss: 0.0742\n",
      "Epoch 30/300 - Train Loss: 0.0675, Val Loss: 0.0704\n",
      "Epoch 31/300 - Train Loss: 0.0667, Val Loss: 0.0674\n",
      "Epoch 32/300 - Train Loss: 0.0667, Val Loss: 0.0675\n",
      "Epoch 33/300 - Train Loss: 0.0657, Val Loss: 0.0655\n",
      "Epoch 34/300 - Train Loss: 0.0656, Val Loss: 0.0689\n",
      "Epoch 35/300 - Train Loss: 0.0684, Val Loss: 0.0672\n",
      "Epoch 36/300 - Train Loss: 0.0636, Val Loss: 0.0676\n",
      "Epoch 37/300 - Train Loss: 0.0624, Val Loss: 0.0680\n",
      "Epoch 38/300 - Train Loss: 0.0634, Val Loss: 0.0703\n",
      "Epoch 39/300 - Train Loss: 0.0645, Val Loss: 0.0639\n",
      "Epoch 40/300 - Train Loss: 0.0639, Val Loss: 0.0670\n",
      "Epoch 41/300 - Train Loss: 0.0622, Val Loss: 0.0692\n",
      "Epoch 42/300 - Train Loss: 0.0623, Val Loss: 0.0676\n",
      "Epoch 43/300 - Train Loss: 0.0602, Val Loss: 0.0729\n",
      "Epoch 44/300 - Train Loss: 0.0625, Val Loss: 0.0659\n",
      "Epoch 45/300 - Train Loss: 0.0612, Val Loss: 0.0669\n",
      "Epoch 46/300 - Train Loss: 0.0620, Val Loss: 0.0690\n",
      "Epoch 47/300 - Train Loss: 0.0614, Val Loss: 0.0770\n",
      "Epoch 48/300 - Train Loss: 0.0592, Val Loss: 0.0704\n",
      "Epoch 49/300 - Train Loss: 0.0606, Val Loss: 0.0641\n",
      "Epoch 50/300 - Train Loss: 0.0591, Val Loss: 0.0713\n",
      "Epoch 51/300 - Train Loss: 0.0596, Val Loss: 0.0656\n",
      "Epoch 52/300 - Train Loss: 0.0601, Val Loss: 0.0672\n",
      "Epoch 53/300 - Train Loss: 0.0573, Val Loss: 0.0634\n",
      "Epoch 54/300 - Train Loss: 0.0595, Val Loss: 0.0678\n",
      "Epoch 55/300 - Train Loss: 0.0581, Val Loss: 0.0715\n",
      "Epoch 56/300 - Train Loss: 0.0549, Val Loss: 0.0663\n",
      "Epoch 57/300 - Train Loss: 0.0564, Val Loss: 0.0707\n",
      "Epoch 58/300 - Train Loss: 0.0557, Val Loss: 0.0691\n",
      "Epoch 59/300 - Train Loss: 0.0572, Val Loss: 0.0638\n",
      "Epoch 60/300 - Train Loss: 0.0560, Val Loss: 0.0753\n",
      "Epoch 61/300 - Train Loss: 0.0555, Val Loss: 0.0673\n",
      "Epoch 62/300 - Train Loss: 0.0541, Val Loss: 0.0679\n",
      "Epoch 63/300 - Train Loss: 0.0545, Val Loss: 0.0682\n",
      "Epoch 64/300 - Train Loss: 0.0530, Val Loss: 0.0667\n",
      "Epoch 65/300 - Train Loss: 0.0523, Val Loss: 0.0657\n",
      "Epoch 66/300 - Train Loss: 0.0544, Val Loss: 0.0649\n",
      "Epoch 67/300 - Train Loss: 0.0534, Val Loss: 0.0690\n",
      "Epoch 68/300 - Train Loss: 0.0523, Val Loss: 0.0671\n",
      "Epoch 69/300 - Train Loss: 0.0526, Val Loss: 0.0677\n",
      "Epoch 70/300 - Train Loss: 0.0525, Val Loss: 0.0660\n",
      "Epoch 71/300 - Train Loss: 0.0521, Val Loss: 0.0702\n",
      "Epoch 72/300 - Train Loss: 0.0520, Val Loss: 0.0739\n",
      "Epoch 73/300 - Train Loss: 0.0492, Val Loss: 0.0644\n",
      "Epoch 74/300 - Train Loss: 0.0502, Val Loss: 0.0692\n",
      "Epoch 75/300 - Train Loss: 0.0502, Val Loss: 0.0660\n",
      "Epoch 76/300 - Train Loss: 0.0494, Val Loss: 0.0743\n",
      "Epoch 77/300 - Train Loss: 0.0492, Val Loss: 0.0692\n",
      "Epoch 78/300 - Train Loss: 0.0496, Val Loss: 0.0692\n",
      "Epoch 79/300 - Train Loss: 0.0482, Val Loss: 0.0672\n",
      "Epoch 80/300 - Train Loss: 0.0499, Val Loss: 0.0671\n",
      "Epoch 81/300 - Train Loss: 0.0499, Val Loss: 0.0731\n",
      "Epoch 82/300 - Train Loss: 0.0484, Val Loss: 0.0706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:19:02,930] Trial 258 finished with value: 0.961913332311406 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.13884356970964942, 'learning_rate': 6.510249811496238e-05, 'batch_size': 32, 'weight_decay': 4.528762007846347e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83/300 - Train Loss: 0.0475, Val Loss: 0.0704\n",
      "Early stopping at epoch 83\n",
      "Macro F1 Score: 0.9619, Macro Precision: 0.9581, Macro Recall: 0.9659\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.90      0.93      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 260\n",
      "Training with F1=16, F2=8, D=8, dropout=0.18149827253070852, LR=4.933974225152024e-05, BS=32, WD=3.9971380423008694e-05\n",
      "Epoch 1/300 - Train Loss: 0.4914, Val Loss: 0.2088\n",
      "Epoch 2/300 - Train Loss: 0.1845, Val Loss: 0.1316\n",
      "Epoch 3/300 - Train Loss: 0.1370, Val Loss: 0.1015\n",
      "Epoch 4/300 - Train Loss: 0.1203, Val Loss: 0.0957\n",
      "Epoch 5/300 - Train Loss: 0.1119, Val Loss: 0.0866\n",
      "Epoch 6/300 - Train Loss: 0.1104, Val Loss: 0.0811\n",
      "Epoch 7/300 - Train Loss: 0.1053, Val Loss: 0.0791\n",
      "Epoch 8/300 - Train Loss: 0.1022, Val Loss: 0.0838\n",
      "Epoch 9/300 - Train Loss: 0.1005, Val Loss: 0.0843\n",
      "Epoch 10/300 - Train Loss: 0.0978, Val Loss: 0.0788\n",
      "Epoch 11/300 - Train Loss: 0.0965, Val Loss: 0.0829\n",
      "Epoch 12/300 - Train Loss: 0.0942, Val Loss: 0.0796\n",
      "Epoch 13/300 - Train Loss: 0.0916, Val Loss: 0.0759\n",
      "Epoch 14/300 - Train Loss: 0.0899, Val Loss: 0.0724\n",
      "Epoch 15/300 - Train Loss: 0.0916, Val Loss: 0.0703\n",
      "Epoch 16/300 - Train Loss: 0.0932, Val Loss: 0.0742\n",
      "Epoch 17/300 - Train Loss: 0.0889, Val Loss: 0.0710\n",
      "Epoch 18/300 - Train Loss: 0.0884, Val Loss: 0.0735\n",
      "Epoch 19/300 - Train Loss: 0.0885, Val Loss: 0.0741\n",
      "Epoch 20/300 - Train Loss: 0.0895, Val Loss: 0.0705\n",
      "Epoch 21/300 - Train Loss: 0.0858, Val Loss: 0.0766\n",
      "Epoch 22/300 - Train Loss: 0.0861, Val Loss: 0.0726\n",
      "Epoch 23/300 - Train Loss: 0.0860, Val Loss: 0.0698\n",
      "Epoch 24/300 - Train Loss: 0.0852, Val Loss: 0.0759\n",
      "Epoch 25/300 - Train Loss: 0.0842, Val Loss: 0.0718\n",
      "Epoch 26/300 - Train Loss: 0.0851, Val Loss: 0.0715\n",
      "Epoch 27/300 - Train Loss: 0.0872, Val Loss: 0.0702\n",
      "Epoch 28/300 - Train Loss: 0.0827, Val Loss: 0.0718\n",
      "Epoch 29/300 - Train Loss: 0.0853, Val Loss: 0.0727\n",
      "Epoch 30/300 - Train Loss: 0.0839, Val Loss: 0.0710\n",
      "Epoch 31/300 - Train Loss: 0.0838, Val Loss: 0.0657\n",
      "Epoch 32/300 - Train Loss: 0.0840, Val Loss: 0.0675\n",
      "Epoch 33/300 - Train Loss: 0.0812, Val Loss: 0.0784\n",
      "Epoch 34/300 - Train Loss: 0.0832, Val Loss: 0.0698\n",
      "Epoch 35/300 - Train Loss: 0.0823, Val Loss: 0.0707\n",
      "Epoch 36/300 - Train Loss: 0.0810, Val Loss: 0.0660\n",
      "Epoch 37/300 - Train Loss: 0.0821, Val Loss: 0.0672\n",
      "Epoch 38/300 - Train Loss: 0.0811, Val Loss: 0.0713\n",
      "Epoch 39/300 - Train Loss: 0.0801, Val Loss: 0.0714\n",
      "Epoch 40/300 - Train Loss: 0.0808, Val Loss: 0.0686\n",
      "Epoch 41/300 - Train Loss: 0.0793, Val Loss: 0.0732\n",
      "Epoch 42/300 - Train Loss: 0.0791, Val Loss: 0.0697\n",
      "Epoch 43/300 - Train Loss: 0.0798, Val Loss: 0.0667\n",
      "Epoch 44/300 - Train Loss: 0.0804, Val Loss: 0.0692\n",
      "Epoch 45/300 - Train Loss: 0.0802, Val Loss: 0.0710\n",
      "Epoch 46/300 - Train Loss: 0.0771, Val Loss: 0.0732\n",
      "Epoch 47/300 - Train Loss: 0.0785, Val Loss: 0.0681\n",
      "Epoch 48/300 - Train Loss: 0.0799, Val Loss: 0.0730\n",
      "Epoch 49/300 - Train Loss: 0.0771, Val Loss: 0.0710\n",
      "Epoch 50/300 - Train Loss: 0.0786, Val Loss: 0.0642\n",
      "Epoch 51/300 - Train Loss: 0.0767, Val Loss: 0.0688\n",
      "Epoch 52/300 - Train Loss: 0.0788, Val Loss: 0.0659\n",
      "Epoch 53/300 - Train Loss: 0.0785, Val Loss: 0.0673\n",
      "Epoch 54/300 - Train Loss: 0.0785, Val Loss: 0.0693\n",
      "Epoch 55/300 - Train Loss: 0.0762, Val Loss: 0.0749\n",
      "Epoch 56/300 - Train Loss: 0.0773, Val Loss: 0.0665\n",
      "Epoch 57/300 - Train Loss: 0.0784, Val Loss: 0.0739\n",
      "Epoch 58/300 - Train Loss: 0.0794, Val Loss: 0.0761\n",
      "Epoch 59/300 - Train Loss: 0.0760, Val Loss: 0.0677\n",
      "Epoch 60/300 - Train Loss: 0.0778, Val Loss: 0.0647\n",
      "Epoch 61/300 - Train Loss: 0.0771, Val Loss: 0.0648\n",
      "Epoch 62/300 - Train Loss: 0.0767, Val Loss: 0.0768\n",
      "Epoch 63/300 - Train Loss: 0.0773, Val Loss: 0.0677\n",
      "Epoch 64/300 - Train Loss: 0.0744, Val Loss: 0.0728\n",
      "Epoch 65/300 - Train Loss: 0.0784, Val Loss: 0.0764\n",
      "Epoch 66/300 - Train Loss: 0.0761, Val Loss: 0.0668\n",
      "Epoch 67/300 - Train Loss: 0.0746, Val Loss: 0.0714\n",
      "Epoch 68/300 - Train Loss: 0.0744, Val Loss: 0.0743\n",
      "Epoch 69/300 - Train Loss: 0.0754, Val Loss: 0.0716\n",
      "Epoch 70/300 - Train Loss: 0.0762, Val Loss: 0.0655\n",
      "Epoch 71/300 - Train Loss: 0.0760, Val Loss: 0.0666\n",
      "Epoch 72/300 - Train Loss: 0.0752, Val Loss: 0.0697\n",
      "Epoch 73/300 - Train Loss: 0.0748, Val Loss: 0.0770\n",
      "Epoch 74/300 - Train Loss: 0.0740, Val Loss: 0.0744\n",
      "Epoch 75/300 - Train Loss: 0.0747, Val Loss: 0.0650\n",
      "Epoch 76/300 - Train Loss: 0.0743, Val Loss: 0.0657\n",
      "Epoch 77/300 - Train Loss: 0.0749, Val Loss: 0.0665\n",
      "Epoch 78/300 - Train Loss: 0.0728, Val Loss: 0.0684\n",
      "Epoch 79/300 - Train Loss: 0.0740, Val Loss: 0.0733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:23:36,507] Trial 259 finished with value: 0.9653737390389302 and parameters: {'F1': 16, 'F2': 8, 'D': 8, 'dropout': 0.18149827253070852, 'learning_rate': 4.933974225152024e-05, 'batch_size': 32, 'weight_decay': 3.9971380423008694e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300 - Train Loss: 0.0738, Val Loss: 0.0718\n",
      "Early stopping at epoch 80\n",
      "Macro F1 Score: 0.9654, Macro Precision: 0.9553, Macro Recall: 0.9764\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 261\n",
      "Training with F1=16, F2=32, D=8, dropout=0.15940894832056215, LR=6.397068560208728e-05, BS=32, WD=5.259012640808581e-05\n",
      "Epoch 1/300 - Train Loss: 0.2989, Val Loss: 0.1461\n",
      "Epoch 2/300 - Train Loss: 0.1344, Val Loss: 0.0940\n",
      "Epoch 3/300 - Train Loss: 0.1074, Val Loss: 0.0892\n",
      "Epoch 4/300 - Train Loss: 0.1018, Val Loss: 0.0810\n",
      "Epoch 5/300 - Train Loss: 0.0959, Val Loss: 0.0753\n",
      "Epoch 6/300 - Train Loss: 0.0922, Val Loss: 0.0805\n",
      "Epoch 7/300 - Train Loss: 0.0916, Val Loss: 0.0712\n",
      "Epoch 8/300 - Train Loss: 0.0884, Val Loss: 0.0785\n",
      "Epoch 9/300 - Train Loss: 0.0880, Val Loss: 0.0701\n",
      "Epoch 10/300 - Train Loss: 0.0845, Val Loss: 0.0725\n",
      "Epoch 11/300 - Train Loss: 0.0821, Val Loss: 0.0798\n",
      "Epoch 12/300 - Train Loss: 0.0823, Val Loss: 0.0827\n",
      "Epoch 13/300 - Train Loss: 0.0811, Val Loss: 0.0765\n",
      "Epoch 14/300 - Train Loss: 0.0804, Val Loss: 0.0668\n",
      "Epoch 15/300 - Train Loss: 0.0804, Val Loss: 0.0708\n",
      "Epoch 16/300 - Train Loss: 0.0803, Val Loss: 0.0753\n",
      "Epoch 17/300 - Train Loss: 0.0778, Val Loss: 0.0828\n",
      "Epoch 18/300 - Train Loss: 0.0777, Val Loss: 0.0724\n",
      "Epoch 19/300 - Train Loss: 0.0784, Val Loss: 0.0697\n",
      "Epoch 20/300 - Train Loss: 0.0767, Val Loss: 0.0711\n",
      "Epoch 21/300 - Train Loss: 0.0783, Val Loss: 0.0774\n",
      "Epoch 22/300 - Train Loss: 0.0732, Val Loss: 0.0720\n",
      "Epoch 23/300 - Train Loss: 0.0748, Val Loss: 0.0790\n",
      "Epoch 24/300 - Train Loss: 0.0732, Val Loss: 0.0699\n",
      "Epoch 25/300 - Train Loss: 0.0726, Val Loss: 0.0696\n",
      "Epoch 26/300 - Train Loss: 0.0703, Val Loss: 0.0670\n",
      "Epoch 27/300 - Train Loss: 0.0706, Val Loss: 0.0692\n",
      "Epoch 28/300 - Train Loss: 0.0750, Val Loss: 0.0720\n",
      "Epoch 29/300 - Train Loss: 0.0709, Val Loss: 0.0727\n",
      "Epoch 30/300 - Train Loss: 0.0709, Val Loss: 0.0689\n",
      "Epoch 31/300 - Train Loss: 0.0684, Val Loss: 0.0668\n",
      "Epoch 32/300 - Train Loss: 0.0700, Val Loss: 0.0682\n",
      "Epoch 33/300 - Train Loss: 0.0682, Val Loss: 0.0657\n",
      "Epoch 34/300 - Train Loss: 0.0669, Val Loss: 0.0683\n",
      "Epoch 35/300 - Train Loss: 0.0660, Val Loss: 0.0721\n",
      "Epoch 36/300 - Train Loss: 0.0691, Val Loss: 0.0648\n",
      "Epoch 37/300 - Train Loss: 0.0668, Val Loss: 0.0689\n",
      "Epoch 38/300 - Train Loss: 0.0653, Val Loss: 0.0712\n",
      "Epoch 39/300 - Train Loss: 0.0641, Val Loss: 0.0661\n",
      "Epoch 40/300 - Train Loss: 0.0635, Val Loss: 0.0675\n",
      "Epoch 41/300 - Train Loss: 0.0663, Val Loss: 0.0676\n",
      "Epoch 42/300 - Train Loss: 0.0661, Val Loss: 0.0720\n",
      "Epoch 43/300 - Train Loss: 0.0636, Val Loss: 0.0693\n",
      "Epoch 44/300 - Train Loss: 0.0640, Val Loss: 0.0739\n",
      "Epoch 45/300 - Train Loss: 0.0618, Val Loss: 0.0692\n",
      "Epoch 46/300 - Train Loss: 0.0643, Val Loss: 0.0695\n",
      "Epoch 47/300 - Train Loss: 0.0617, Val Loss: 0.0714\n",
      "Epoch 48/300 - Train Loss: 0.0598, Val Loss: 0.0638\n",
      "Epoch 49/300 - Train Loss: 0.0609, Val Loss: 0.0681\n",
      "Epoch 50/300 - Train Loss: 0.0613, Val Loss: 0.0682\n",
      "Epoch 51/300 - Train Loss: 0.0623, Val Loss: 0.0742\n",
      "Epoch 52/300 - Train Loss: 0.0605, Val Loss: 0.0733\n",
      "Epoch 53/300 - Train Loss: 0.0603, Val Loss: 0.0725\n",
      "Epoch 54/300 - Train Loss: 0.0571, Val Loss: 0.0697\n",
      "Epoch 55/300 - Train Loss: 0.0589, Val Loss: 0.0660\n",
      "Epoch 56/300 - Train Loss: 0.0592, Val Loss: 0.0726\n",
      "Epoch 57/300 - Train Loss: 0.0586, Val Loss: 0.0643\n",
      "Epoch 58/300 - Train Loss: 0.0573, Val Loss: 0.0667\n",
      "Epoch 59/300 - Train Loss: 0.0576, Val Loss: 0.0662\n",
      "Epoch 60/300 - Train Loss: 0.0577, Val Loss: 0.0670\n",
      "Epoch 61/300 - Train Loss: 0.0567, Val Loss: 0.0711\n",
      "Epoch 62/300 - Train Loss: 0.0561, Val Loss: 0.0698\n",
      "Epoch 63/300 - Train Loss: 0.0555, Val Loss: 0.0708\n",
      "Epoch 64/300 - Train Loss: 0.0563, Val Loss: 0.0693\n",
      "Epoch 65/300 - Train Loss: 0.0542, Val Loss: 0.0668\n",
      "Epoch 66/300 - Train Loss: 0.0545, Val Loss: 0.0668\n",
      "Epoch 67/300 - Train Loss: 0.0528, Val Loss: 0.0714\n",
      "Epoch 68/300 - Train Loss: 0.0528, Val Loss: 0.0663\n",
      "Epoch 69/300 - Train Loss: 0.0537, Val Loss: 0.0694\n",
      "Epoch 70/300 - Train Loss: 0.0560, Val Loss: 0.0683\n",
      "Epoch 71/300 - Train Loss: 0.0540, Val Loss: 0.0721\n",
      "Epoch 72/300 - Train Loss: 0.0532, Val Loss: 0.0687\n",
      "Epoch 73/300 - Train Loss: 0.0530, Val Loss: 0.0719\n",
      "Epoch 74/300 - Train Loss: 0.0525, Val Loss: 0.0700\n",
      "Epoch 75/300 - Train Loss: 0.0528, Val Loss: 0.0697\n",
      "Epoch 76/300 - Train Loss: 0.0502, Val Loss: 0.0723\n",
      "Epoch 77/300 - Train Loss: 0.0516, Val Loss: 0.0685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:28:37,275] Trial 260 finished with value: 0.9700307593241448 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.15940894832056215, 'learning_rate': 6.397068560208728e-05, 'batch_size': 32, 'weight_decay': 5.259012640808581e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300 - Train Loss: 0.0512, Val Loss: 0.0765\n",
      "Early stopping at epoch 78\n",
      "Macro F1 Score: 0.9700, Macro Precision: 0.9687, Macro Recall: 0.9715\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 262\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1942615598412463, LR=5.370696836344095e-05, BS=32, WD=5.784048843678919e-05\n",
      "Epoch 1/300 - Train Loss: 0.3092, Val Loss: 0.1350\n",
      "Epoch 2/300 - Train Loss: 0.1287, Val Loss: 0.0990\n",
      "Epoch 3/300 - Train Loss: 0.1131, Val Loss: 0.0810\n",
      "Epoch 4/300 - Train Loss: 0.1052, Val Loss: 0.0847\n",
      "Epoch 5/300 - Train Loss: 0.1008, Val Loss: 0.0749\n",
      "Epoch 6/300 - Train Loss: 0.0968, Val Loss: 0.0747\n",
      "Epoch 7/300 - Train Loss: 0.0918, Val Loss: 0.0812\n",
      "Epoch 8/300 - Train Loss: 0.0902, Val Loss: 0.0765\n",
      "Epoch 9/300 - Train Loss: 0.0913, Val Loss: 0.0893\n",
      "Epoch 10/300 - Train Loss: 0.0891, Val Loss: 0.0736\n",
      "Epoch 11/300 - Train Loss: 0.0881, Val Loss: 0.0797\n",
      "Epoch 12/300 - Train Loss: 0.0851, Val Loss: 0.0718\n",
      "Epoch 13/300 - Train Loss: 0.0829, Val Loss: 0.0711\n",
      "Epoch 14/300 - Train Loss: 0.0849, Val Loss: 0.0749\n",
      "Epoch 15/300 - Train Loss: 0.0825, Val Loss: 0.0721\n",
      "Epoch 16/300 - Train Loss: 0.0822, Val Loss: 0.0670\n",
      "Epoch 17/300 - Train Loss: 0.0779, Val Loss: 0.0702\n",
      "Epoch 18/300 - Train Loss: 0.0800, Val Loss: 0.0700\n",
      "Epoch 19/300 - Train Loss: 0.0768, Val Loss: 0.0717\n",
      "Epoch 20/300 - Train Loss: 0.0787, Val Loss: 0.0735\n",
      "Epoch 21/300 - Train Loss: 0.0770, Val Loss: 0.0699\n",
      "Epoch 22/300 - Train Loss: 0.0741, Val Loss: 0.0677\n",
      "Epoch 23/300 - Train Loss: 0.0777, Val Loss: 0.0719\n",
      "Epoch 24/300 - Train Loss: 0.0753, Val Loss: 0.0721\n",
      "Epoch 25/300 - Train Loss: 0.0746, Val Loss: 0.0674\n",
      "Epoch 26/300 - Train Loss: 0.0737, Val Loss: 0.0676\n",
      "Epoch 27/300 - Train Loss: 0.0734, Val Loss: 0.0705\n",
      "Epoch 28/300 - Train Loss: 0.0733, Val Loss: 0.0676\n",
      "Epoch 29/300 - Train Loss: 0.0742, Val Loss: 0.0661\n",
      "Epoch 30/300 - Train Loss: 0.0731, Val Loss: 0.0694\n",
      "Epoch 31/300 - Train Loss: 0.0710, Val Loss: 0.0686\n",
      "Epoch 32/300 - Train Loss: 0.0727, Val Loss: 0.0687\n",
      "Epoch 33/300 - Train Loss: 0.0701, Val Loss: 0.0672\n",
      "Epoch 34/300 - Train Loss: 0.0712, Val Loss: 0.0727\n",
      "Epoch 35/300 - Train Loss: 0.0710, Val Loss: 0.0696\n",
      "Epoch 36/300 - Train Loss: 0.0699, Val Loss: 0.0740\n",
      "Epoch 37/300 - Train Loss: 0.0694, Val Loss: 0.0756\n",
      "Epoch 38/300 - Train Loss: 0.0700, Val Loss: 0.0647\n",
      "Epoch 39/300 - Train Loss: 0.0677, Val Loss: 0.0670\n",
      "Epoch 40/300 - Train Loss: 0.0675, Val Loss: 0.0752\n",
      "Epoch 41/300 - Train Loss: 0.0672, Val Loss: 0.0681\n",
      "Epoch 42/300 - Train Loss: 0.0680, Val Loss: 0.0681\n",
      "Epoch 43/300 - Train Loss: 0.0676, Val Loss: 0.0646\n",
      "Epoch 44/300 - Train Loss: 0.0647, Val Loss: 0.0685\n",
      "Epoch 45/300 - Train Loss: 0.0653, Val Loss: 0.0646\n",
      "Epoch 46/300 - Train Loss: 0.0648, Val Loss: 0.0655\n",
      "Epoch 47/300 - Train Loss: 0.0671, Val Loss: 0.0669\n",
      "Epoch 48/300 - Train Loss: 0.0641, Val Loss: 0.0638\n",
      "Epoch 49/300 - Train Loss: 0.0648, Val Loss: 0.0671\n",
      "Epoch 50/300 - Train Loss: 0.0641, Val Loss: 0.0663\n",
      "Epoch 51/300 - Train Loss: 0.0648, Val Loss: 0.0665\n",
      "Epoch 52/300 - Train Loss: 0.0652, Val Loss: 0.0633\n",
      "Epoch 53/300 - Train Loss: 0.0630, Val Loss: 0.0648\n",
      "Epoch 54/300 - Train Loss: 0.0627, Val Loss: 0.0671\n",
      "Epoch 55/300 - Train Loss: 0.0627, Val Loss: 0.0657\n",
      "Epoch 56/300 - Train Loss: 0.0628, Val Loss: 0.0624\n",
      "Epoch 57/300 - Train Loss: 0.0621, Val Loss: 0.0626\n",
      "Epoch 58/300 - Train Loss: 0.0608, Val Loss: 0.0627\n",
      "Epoch 59/300 - Train Loss: 0.0612, Val Loss: 0.0635\n",
      "Epoch 60/300 - Train Loss: 0.0623, Val Loss: 0.0632\n",
      "Epoch 61/300 - Train Loss: 0.0607, Val Loss: 0.0654\n",
      "Epoch 62/300 - Train Loss: 0.0600, Val Loss: 0.0629\n",
      "Epoch 63/300 - Train Loss: 0.0596, Val Loss: 0.0669\n",
      "Epoch 64/300 - Train Loss: 0.0592, Val Loss: 0.0649\n",
      "Epoch 65/300 - Train Loss: 0.0576, Val Loss: 0.0686\n",
      "Epoch 66/300 - Train Loss: 0.0611, Val Loss: 0.0674\n",
      "Epoch 67/300 - Train Loss: 0.0594, Val Loss: 0.0653\n",
      "Epoch 68/300 - Train Loss: 0.0566, Val Loss: 0.0710\n",
      "Epoch 69/300 - Train Loss: 0.0578, Val Loss: 0.0652\n",
      "Epoch 70/300 - Train Loss: 0.0578, Val Loss: 0.0649\n",
      "Epoch 71/300 - Train Loss: 0.0591, Val Loss: 0.0661\n",
      "Epoch 72/300 - Train Loss: 0.0568, Val Loss: 0.0656\n",
      "Epoch 73/300 - Train Loss: 0.0579, Val Loss: 0.0722\n",
      "Epoch 74/300 - Train Loss: 0.0581, Val Loss: 0.0719\n",
      "Epoch 75/300 - Train Loss: 0.0563, Val Loss: 0.0684\n",
      "Epoch 76/300 - Train Loss: 0.0570, Val Loss: 0.0650\n",
      "Epoch 77/300 - Train Loss: 0.0562, Val Loss: 0.0663\n",
      "Epoch 78/300 - Train Loss: 0.0566, Val Loss: 0.0664\n",
      "Epoch 79/300 - Train Loss: 0.0573, Val Loss: 0.0658\n",
      "Epoch 80/300 - Train Loss: 0.0559, Val Loss: 0.0690\n",
      "Epoch 81/300 - Train Loss: 0.0543, Val Loss: 0.0660\n",
      "Epoch 82/300 - Train Loss: 0.0558, Val Loss: 0.0640\n",
      "Epoch 83/300 - Train Loss: 0.0540, Val Loss: 0.0682\n",
      "Epoch 84/300 - Train Loss: 0.0566, Val Loss: 0.0661\n",
      "Epoch 85/300 - Train Loss: 0.0550, Val Loss: 0.0638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:34:08,832] Trial 261 finished with value: 0.9762236650656781 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1942615598412463, 'learning_rate': 5.370696836344095e-05, 'batch_size': 32, 'weight_decay': 5.784048843678919e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86/300 - Train Loss: 0.0572, Val Loss: 0.0719\n",
      "Early stopping at epoch 86\n",
      "Macro F1 Score: 0.9762, Macro Precision: 0.9795, Macro Recall: 0.9731\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 263\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1701766086509992, LR=5.4123652131718706e-05, BS=128, WD=5.917793030412445e-05\n",
      "Epoch 1/300 - Train Loss: 0.4938, Val Loss: 0.2617\n",
      "Epoch 2/300 - Train Loss: 0.2116, Val Loss: 0.1660\n",
      "Epoch 3/300 - Train Loss: 0.1548, Val Loss: 0.1333\n",
      "Epoch 4/300 - Train Loss: 0.1203, Val Loss: 0.1071\n",
      "Epoch 5/300 - Train Loss: 0.1065, Val Loss: 0.1115\n",
      "Epoch 6/300 - Train Loss: 0.1012, Val Loss: 0.0943\n",
      "Epoch 7/300 - Train Loss: 0.0942, Val Loss: 0.1067\n",
      "Epoch 8/300 - Train Loss: 0.0914, Val Loss: 0.0833\n",
      "Epoch 9/300 - Train Loss: 0.0899, Val Loss: 0.0883\n",
      "Epoch 10/300 - Train Loss: 0.0872, Val Loss: 0.0838\n",
      "Epoch 11/300 - Train Loss: 0.0860, Val Loss: 0.0904\n",
      "Epoch 12/300 - Train Loss: 0.0844, Val Loss: 0.0821\n",
      "Epoch 13/300 - Train Loss: 0.0830, Val Loss: 0.0812\n",
      "Epoch 14/300 - Train Loss: 0.0837, Val Loss: 0.0916\n",
      "Epoch 15/300 - Train Loss: 0.0808, Val Loss: 0.0820\n",
      "Epoch 16/300 - Train Loss: 0.0788, Val Loss: 0.0795\n",
      "Epoch 17/300 - Train Loss: 0.0795, Val Loss: 0.0783\n",
      "Epoch 18/300 - Train Loss: 0.0774, Val Loss: 0.0845\n",
      "Epoch 19/300 - Train Loss: 0.0777, Val Loss: 0.0788\n",
      "Epoch 20/300 - Train Loss: 0.0759, Val Loss: 0.0768\n",
      "Epoch 21/300 - Train Loss: 0.0756, Val Loss: 0.0803\n",
      "Epoch 22/300 - Train Loss: 0.0759, Val Loss: 0.0763\n",
      "Epoch 23/300 - Train Loss: 0.0752, Val Loss: 0.0808\n",
      "Epoch 24/300 - Train Loss: 0.0759, Val Loss: 0.0724\n",
      "Epoch 25/300 - Train Loss: 0.0738, Val Loss: 0.0760\n",
      "Epoch 26/300 - Train Loss: 0.0729, Val Loss: 0.0745\n",
      "Epoch 27/300 - Train Loss: 0.0746, Val Loss: 0.0759\n",
      "Epoch 28/300 - Train Loss: 0.0718, Val Loss: 0.0734\n",
      "Epoch 29/300 - Train Loss: 0.0700, Val Loss: 0.0717\n",
      "Epoch 30/300 - Train Loss: 0.0705, Val Loss: 0.0717\n",
      "Epoch 31/300 - Train Loss: 0.0705, Val Loss: 0.0730\n",
      "Epoch 32/300 - Train Loss: 0.0699, Val Loss: 0.0711\n",
      "Epoch 33/300 - Train Loss: 0.0697, Val Loss: 0.0740\n",
      "Epoch 34/300 - Train Loss: 0.0695, Val Loss: 0.0743\n",
      "Epoch 35/300 - Train Loss: 0.0696, Val Loss: 0.0694\n",
      "Epoch 36/300 - Train Loss: 0.0689, Val Loss: 0.0749\n",
      "Epoch 37/300 - Train Loss: 0.0686, Val Loss: 0.0692\n",
      "Epoch 38/300 - Train Loss: 0.0670, Val Loss: 0.0698\n",
      "Epoch 39/300 - Train Loss: 0.0678, Val Loss: 0.0706\n",
      "Epoch 40/300 - Train Loss: 0.0681, Val Loss: 0.0704\n",
      "Epoch 41/300 - Train Loss: 0.0666, Val Loss: 0.0717\n",
      "Epoch 42/300 - Train Loss: 0.0664, Val Loss: 0.0694\n",
      "Epoch 43/300 - Train Loss: 0.0664, Val Loss: 0.0690\n",
      "Epoch 44/300 - Train Loss: 0.0653, Val Loss: 0.0699\n",
      "Epoch 45/300 - Train Loss: 0.0645, Val Loss: 0.0711\n",
      "Epoch 46/300 - Train Loss: 0.0644, Val Loss: 0.0736\n",
      "Epoch 47/300 - Train Loss: 0.0641, Val Loss: 0.0688\n",
      "Epoch 48/300 - Train Loss: 0.0658, Val Loss: 0.0693\n",
      "Epoch 49/300 - Train Loss: 0.0652, Val Loss: 0.0685\n",
      "Epoch 50/300 - Train Loss: 0.0643, Val Loss: 0.0677\n",
      "Epoch 51/300 - Train Loss: 0.0630, Val Loss: 0.0693\n",
      "Epoch 52/300 - Train Loss: 0.0635, Val Loss: 0.0690\n",
      "Epoch 53/300 - Train Loss: 0.0635, Val Loss: 0.0708\n",
      "Epoch 54/300 - Train Loss: 0.0637, Val Loss: 0.0723\n",
      "Epoch 55/300 - Train Loss: 0.0628, Val Loss: 0.0721\n",
      "Epoch 56/300 - Train Loss: 0.0625, Val Loss: 0.0687\n",
      "Epoch 57/300 - Train Loss: 0.0621, Val Loss: 0.0688\n",
      "Epoch 58/300 - Train Loss: 0.0607, Val Loss: 0.0688\n",
      "Epoch 59/300 - Train Loss: 0.0620, Val Loss: 0.0688\n",
      "Epoch 60/300 - Train Loss: 0.0607, Val Loss: 0.0674\n",
      "Epoch 61/300 - Train Loss: 0.0621, Val Loss: 0.0711\n",
      "Epoch 62/300 - Train Loss: 0.0597, Val Loss: 0.0706\n",
      "Epoch 63/300 - Train Loss: 0.0602, Val Loss: 0.0684\n",
      "Epoch 64/300 - Train Loss: 0.0599, Val Loss: 0.0693\n",
      "Epoch 65/300 - Train Loss: 0.0599, Val Loss: 0.0686\n",
      "Epoch 66/300 - Train Loss: 0.0595, Val Loss: 0.0716\n",
      "Epoch 67/300 - Train Loss: 0.0588, Val Loss: 0.0721\n",
      "Epoch 68/300 - Train Loss: 0.0584, Val Loss: 0.0727\n",
      "Epoch 69/300 - Train Loss: 0.0581, Val Loss: 0.0705\n",
      "Epoch 70/300 - Train Loss: 0.0591, Val Loss: 0.0684\n",
      "Epoch 71/300 - Train Loss: 0.0579, Val Loss: 0.0685\n",
      "Epoch 72/300 - Train Loss: 0.0585, Val Loss: 0.0667\n",
      "Epoch 73/300 - Train Loss: 0.0576, Val Loss: 0.0676\n",
      "Epoch 74/300 - Train Loss: 0.0588, Val Loss: 0.0672\n",
      "Epoch 75/300 - Train Loss: 0.0593, Val Loss: 0.0695\n",
      "Epoch 76/300 - Train Loss: 0.0573, Val Loss: 0.0691\n",
      "Epoch 77/300 - Train Loss: 0.0581, Val Loss: 0.0672\n",
      "Epoch 78/300 - Train Loss: 0.0552, Val Loss: 0.0687\n",
      "Epoch 79/300 - Train Loss: 0.0566, Val Loss: 0.0697\n",
      "Epoch 80/300 - Train Loss: 0.0562, Val Loss: 0.0733\n",
      "Epoch 81/300 - Train Loss: 0.0568, Val Loss: 0.0694\n",
      "Epoch 82/300 - Train Loss: 0.0552, Val Loss: 0.0683\n",
      "Epoch 83/300 - Train Loss: 0.0552, Val Loss: 0.0718\n",
      "Epoch 84/300 - Train Loss: 0.0554, Val Loss: 0.0678\n",
      "Epoch 85/300 - Train Loss: 0.0558, Val Loss: 0.0683\n",
      "Epoch 86/300 - Train Loss: 0.0561, Val Loss: 0.0690\n",
      "Epoch 87/300 - Train Loss: 0.0539, Val Loss: 0.0660\n",
      "Epoch 88/300 - Train Loss: 0.0561, Val Loss: 0.0700\n",
      "Epoch 89/300 - Train Loss: 0.0560, Val Loss: 0.0693\n",
      "Epoch 90/300 - Train Loss: 0.0542, Val Loss: 0.0655\n",
      "Epoch 91/300 - Train Loss: 0.0541, Val Loss: 0.0689\n",
      "Epoch 92/300 - Train Loss: 0.0520, Val Loss: 0.0719\n",
      "Epoch 93/300 - Train Loss: 0.0541, Val Loss: 0.0690\n",
      "Epoch 94/300 - Train Loss: 0.0526, Val Loss: 0.0679\n",
      "Epoch 95/300 - Train Loss: 0.0532, Val Loss: 0.0736\n",
      "Epoch 96/300 - Train Loss: 0.0525, Val Loss: 0.0679\n",
      "Epoch 97/300 - Train Loss: 0.0513, Val Loss: 0.0685\n",
      "Epoch 98/300 - Train Loss: 0.0517, Val Loss: 0.0686\n",
      "Epoch 99/300 - Train Loss: 0.0514, Val Loss: 0.0692\n",
      "Epoch 100/300 - Train Loss: 0.0514, Val Loss: 0.0705\n",
      "Epoch 101/300 - Train Loss: 0.0519, Val Loss: 0.0685\n",
      "Epoch 102/300 - Train Loss: 0.0522, Val Loss: 0.0707\n",
      "Epoch 103/300 - Train Loss: 0.0525, Val Loss: 0.0691\n",
      "Epoch 104/300 - Train Loss: 0.0517, Val Loss: 0.0675\n",
      "Epoch 105/300 - Train Loss: 0.0515, Val Loss: 0.0723\n",
      "Epoch 106/300 - Train Loss: 0.0518, Val Loss: 0.0700\n",
      "Epoch 107/300 - Train Loss: 0.0501, Val Loss: 0.0699\n",
      "Epoch 108/300 - Train Loss: 0.0506, Val Loss: 0.0700\n",
      "Epoch 109/300 - Train Loss: 0.0509, Val Loss: 0.0696\n",
      "Epoch 110/300 - Train Loss: 0.0502, Val Loss: 0.0689\n",
      "Epoch 111/300 - Train Loss: 0.0502, Val Loss: 0.0689\n",
      "Epoch 112/300 - Train Loss: 0.0482, Val Loss: 0.0708\n",
      "Epoch 113/300 - Train Loss: 0.0489, Val Loss: 0.0709\n",
      "Epoch 114/300 - Train Loss: 0.0492, Val Loss: 0.0698\n",
      "Epoch 115/300 - Train Loss: 0.0489, Val Loss: 0.0689\n",
      "Epoch 116/300 - Train Loss: 0.0485, Val Loss: 0.0676\n",
      "Epoch 117/300 - Train Loss: 0.0493, Val Loss: 0.0736\n",
      "Epoch 118/300 - Train Loss: 0.0511, Val Loss: 0.0684\n",
      "Epoch 119/300 - Train Loss: 0.0485, Val Loss: 0.0696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:40:17,617] Trial 262 finished with value: 0.9597092443688452 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1701766086509992, 'learning_rate': 5.4123652131718706e-05, 'batch_size': 128, 'weight_decay': 5.917793030412445e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/300 - Train Loss: 0.0484, Val Loss: 0.0680\n",
      "Early stopping at epoch 120\n",
      "Macro F1 Score: 0.9597, Macro Precision: 0.9496, Macro Recall: 0.9708\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.88      0.95      0.91        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 264\n",
      "Training with F1=16, F2=32, D=2, dropout=0.19623203837401446, LR=4.8977457440312573e-05, BS=32, WD=3.8913994508000886e-05\n",
      "Epoch 1/300 - Train Loss: 0.3922, Val Loss: 0.1735\n",
      "Epoch 2/300 - Train Loss: 0.1694, Val Loss: 0.1182\n",
      "Epoch 3/300 - Train Loss: 0.1380, Val Loss: 0.0998\n",
      "Epoch 4/300 - Train Loss: 0.1199, Val Loss: 0.0902\n",
      "Epoch 5/300 - Train Loss: 0.1107, Val Loss: 0.0932\n",
      "Epoch 6/300 - Train Loss: 0.1093, Val Loss: 0.0883\n",
      "Epoch 7/300 - Train Loss: 0.1033, Val Loss: 0.0798\n",
      "Epoch 8/300 - Train Loss: 0.1003, Val Loss: 0.0854\n",
      "Epoch 9/300 - Train Loss: 0.1007, Val Loss: 0.0780\n",
      "Epoch 10/300 - Train Loss: 0.1004, Val Loss: 0.0887\n",
      "Epoch 11/300 - Train Loss: 0.0982, Val Loss: 0.0763\n",
      "Epoch 12/300 - Train Loss: 0.0958, Val Loss: 0.0754\n",
      "Epoch 13/300 - Train Loss: 0.0920, Val Loss: 0.0763\n",
      "Epoch 14/300 - Train Loss: 0.0944, Val Loss: 0.0844\n",
      "Epoch 15/300 - Train Loss: 0.0927, Val Loss: 0.0774\n",
      "Epoch 16/300 - Train Loss: 0.0925, Val Loss: 0.0750\n",
      "Epoch 17/300 - Train Loss: 0.0911, Val Loss: 0.0798\n",
      "Epoch 18/300 - Train Loss: 0.0909, Val Loss: 0.0744\n",
      "Epoch 19/300 - Train Loss: 0.0894, Val Loss: 0.0785\n",
      "Epoch 20/300 - Train Loss: 0.0898, Val Loss: 0.0757\n",
      "Epoch 21/300 - Train Loss: 0.0900, Val Loss: 0.0815\n",
      "Epoch 22/300 - Train Loss: 0.0852, Val Loss: 0.0747\n",
      "Epoch 23/300 - Train Loss: 0.0863, Val Loss: 0.0723\n",
      "Epoch 24/300 - Train Loss: 0.0850, Val Loss: 0.0726\n",
      "Epoch 25/300 - Train Loss: 0.0851, Val Loss: 0.0758\n",
      "Epoch 26/300 - Train Loss: 0.0845, Val Loss: 0.0750\n",
      "Epoch 27/300 - Train Loss: 0.0842, Val Loss: 0.0776\n",
      "Epoch 28/300 - Train Loss: 0.0811, Val Loss: 0.0753\n",
      "Epoch 29/300 - Train Loss: 0.0830, Val Loss: 0.0775\n",
      "Epoch 30/300 - Train Loss: 0.0849, Val Loss: 0.0683\n",
      "Epoch 31/300 - Train Loss: 0.0819, Val Loss: 0.0740\n",
      "Epoch 32/300 - Train Loss: 0.0821, Val Loss: 0.0808\n",
      "Epoch 33/300 - Train Loss: 0.0819, Val Loss: 0.0740\n",
      "Epoch 34/300 - Train Loss: 0.0790, Val Loss: 0.0775\n",
      "Epoch 35/300 - Train Loss: 0.0831, Val Loss: 0.0747\n",
      "Epoch 36/300 - Train Loss: 0.0822, Val Loss: 0.0752\n",
      "Epoch 37/300 - Train Loss: 0.0814, Val Loss: 0.0768\n",
      "Epoch 38/300 - Train Loss: 0.0809, Val Loss: 0.0742\n",
      "Epoch 39/300 - Train Loss: 0.0797, Val Loss: 0.0763\n",
      "Epoch 40/300 - Train Loss: 0.0783, Val Loss: 0.0767\n",
      "Epoch 41/300 - Train Loss: 0.0773, Val Loss: 0.0741\n",
      "Epoch 42/300 - Train Loss: 0.0785, Val Loss: 0.0767\n",
      "Epoch 43/300 - Train Loss: 0.0789, Val Loss: 0.0752\n",
      "Epoch 44/300 - Train Loss: 0.0792, Val Loss: 0.0738\n",
      "Epoch 45/300 - Train Loss: 0.0783, Val Loss: 0.0714\n",
      "Epoch 46/300 - Train Loss: 0.0779, Val Loss: 0.0763\n",
      "Epoch 47/300 - Train Loss: 0.0766, Val Loss: 0.0752\n",
      "Epoch 48/300 - Train Loss: 0.0785, Val Loss: 0.0814\n",
      "Epoch 49/300 - Train Loss: 0.0787, Val Loss: 0.0789\n",
      "Epoch 50/300 - Train Loss: 0.0784, Val Loss: 0.0760\n",
      "Epoch 51/300 - Train Loss: 0.0763, Val Loss: 0.0758\n",
      "Epoch 52/300 - Train Loss: 0.0757, Val Loss: 0.0740\n",
      "Epoch 53/300 - Train Loss: 0.0767, Val Loss: 0.0701\n",
      "Epoch 54/300 - Train Loss: 0.0752, Val Loss: 0.0709\n",
      "Epoch 55/300 - Train Loss: 0.0748, Val Loss: 0.0738\n",
      "Epoch 56/300 - Train Loss: 0.0744, Val Loss: 0.0792\n",
      "Epoch 57/300 - Train Loss: 0.0737, Val Loss: 0.0767\n",
      "Epoch 58/300 - Train Loss: 0.0736, Val Loss: 0.0774\n",
      "Epoch 59/300 - Train Loss: 0.0762, Val Loss: 0.0707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:42:26,710] Trial 263 finished with value: 0.9663582522042901 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.19623203837401446, 'learning_rate': 4.8977457440312573e-05, 'batch_size': 32, 'weight_decay': 3.8913994508000886e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300 - Train Loss: 0.0741, Val Loss: 0.0721\n",
      "Early stopping at epoch 60\n",
      "Macro F1 Score: 0.9664, Macro Precision: 0.9565, Macro Recall: 0.9773\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 265\n",
      "Training with F1=16, F2=32, D=8, dropout=0.182499186080261, LR=5.740697213177942e-05, BS=32, WD=5.802115643184476e-05\n",
      "Epoch 1/300 - Train Loss: 0.2977, Val Loss: 0.1364\n",
      "Epoch 2/300 - Train Loss: 0.1233, Val Loss: 0.0839\n",
      "Epoch 3/300 - Train Loss: 0.1057, Val Loss: 0.0864\n",
      "Epoch 4/300 - Train Loss: 0.0987, Val Loss: 0.0920\n",
      "Epoch 5/300 - Train Loss: 0.0949, Val Loss: 0.0716\n",
      "Epoch 6/300 - Train Loss: 0.0903, Val Loss: 0.0799\n",
      "Epoch 7/300 - Train Loss: 0.0883, Val Loss: 0.0693\n",
      "Epoch 8/300 - Train Loss: 0.0869, Val Loss: 0.0739\n",
      "Epoch 9/300 - Train Loss: 0.0880, Val Loss: 0.0741\n",
      "Epoch 10/300 - Train Loss: 0.0870, Val Loss: 0.0743\n",
      "Epoch 11/300 - Train Loss: 0.0844, Val Loss: 0.0758\n",
      "Epoch 12/300 - Train Loss: 0.0852, Val Loss: 0.0699\n",
      "Epoch 13/300 - Train Loss: 0.0811, Val Loss: 0.0809\n",
      "Epoch 14/300 - Train Loss: 0.0822, Val Loss: 0.0730\n",
      "Epoch 15/300 - Train Loss: 0.0797, Val Loss: 0.0790\n",
      "Epoch 16/300 - Train Loss: 0.0789, Val Loss: 0.0738\n",
      "Epoch 17/300 - Train Loss: 0.0778, Val Loss: 0.0722\n",
      "Epoch 18/300 - Train Loss: 0.0796, Val Loss: 0.0664\n",
      "Epoch 19/300 - Train Loss: 0.0768, Val Loss: 0.0704\n",
      "Epoch 20/300 - Train Loss: 0.0807, Val Loss: 0.0717\n",
      "Epoch 21/300 - Train Loss: 0.0760, Val Loss: 0.0730\n",
      "Epoch 22/300 - Train Loss: 0.0754, Val Loss: 0.0722\n",
      "Epoch 23/300 - Train Loss: 0.0757, Val Loss: 0.0727\n",
      "Epoch 24/300 - Train Loss: 0.0754, Val Loss: 0.0725\n",
      "Epoch 25/300 - Train Loss: 0.0761, Val Loss: 0.0706\n",
      "Epoch 26/300 - Train Loss: 0.0786, Val Loss: 0.0895\n",
      "Epoch 27/300 - Train Loss: 0.0742, Val Loss: 0.0725\n",
      "Epoch 28/300 - Train Loss: 0.0738, Val Loss: 0.0713\n",
      "Epoch 29/300 - Train Loss: 0.0763, Val Loss: 0.0669\n",
      "Epoch 30/300 - Train Loss: 0.0748, Val Loss: 0.0671\n",
      "Epoch 31/300 - Train Loss: 0.0729, Val Loss: 0.0755\n",
      "Epoch 32/300 - Train Loss: 0.0736, Val Loss: 0.0733\n",
      "Epoch 33/300 - Train Loss: 0.0684, Val Loss: 0.0725\n",
      "Epoch 34/300 - Train Loss: 0.0699, Val Loss: 0.0671\n",
      "Epoch 35/300 - Train Loss: 0.0693, Val Loss: 0.0711\n",
      "Epoch 36/300 - Train Loss: 0.0698, Val Loss: 0.0689\n",
      "Epoch 37/300 - Train Loss: 0.0693, Val Loss: 0.0704\n",
      "Epoch 38/300 - Train Loss: 0.0681, Val Loss: 0.0726\n",
      "Epoch 39/300 - Train Loss: 0.0704, Val Loss: 0.0702\n",
      "Epoch 40/300 - Train Loss: 0.0700, Val Loss: 0.0735\n",
      "Epoch 41/300 - Train Loss: 0.0677, Val Loss: 0.0729\n",
      "Epoch 42/300 - Train Loss: 0.0669, Val Loss: 0.0706\n",
      "Epoch 43/300 - Train Loss: 0.0674, Val Loss: 0.0724\n",
      "Epoch 44/300 - Train Loss: 0.0668, Val Loss: 0.0712\n",
      "Epoch 45/300 - Train Loss: 0.0687, Val Loss: 0.0782\n",
      "Epoch 46/300 - Train Loss: 0.0657, Val Loss: 0.0694\n",
      "Epoch 47/300 - Train Loss: 0.0677, Val Loss: 0.0736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:45:31,854] Trial 264 finished with value: 0.9603405303168566 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.182499186080261, 'learning_rate': 5.740697213177942e-05, 'batch_size': 32, 'weight_decay': 5.802115643184476e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300 - Train Loss: 0.0670, Val Loss: 0.0665\n",
      "Early stopping at epoch 48\n",
      "Macro F1 Score: 0.9603, Macro Precision: 0.9531, Macro Recall: 0.9684\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 266\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1705305421499055, LR=5.36105407945334e-05, BS=32, WD=4.688821216326767e-05\n",
      "Epoch 1/300 - Train Loss: 0.3129, Val Loss: 0.1878\n",
      "Epoch 2/300 - Train Loss: 0.1513, Val Loss: 0.1090\n",
      "Epoch 3/300 - Train Loss: 0.1132, Val Loss: 0.0841\n",
      "Epoch 4/300 - Train Loss: 0.1031, Val Loss: 0.0865\n",
      "Epoch 5/300 - Train Loss: 0.0965, Val Loss: 0.0765\n",
      "Epoch 6/300 - Train Loss: 0.0958, Val Loss: 0.0830\n",
      "Epoch 7/300 - Train Loss: 0.0923, Val Loss: 0.0817\n",
      "Epoch 8/300 - Train Loss: 0.0903, Val Loss: 0.0742\n",
      "Epoch 9/300 - Train Loss: 0.0874, Val Loss: 0.0761\n",
      "Epoch 10/300 - Train Loss: 0.0873, Val Loss: 0.0726\n",
      "Epoch 11/300 - Train Loss: 0.0875, Val Loss: 0.0731\n",
      "Epoch 12/300 - Train Loss: 0.0871, Val Loss: 0.0755\n",
      "Epoch 13/300 - Train Loss: 0.0854, Val Loss: 0.0725\n",
      "Epoch 14/300 - Train Loss: 0.0831, Val Loss: 0.0745\n",
      "Epoch 15/300 - Train Loss: 0.0806, Val Loss: 0.0658\n",
      "Epoch 16/300 - Train Loss: 0.0804, Val Loss: 0.0697\n",
      "Epoch 17/300 - Train Loss: 0.0808, Val Loss: 0.0708\n",
      "Epoch 18/300 - Train Loss: 0.0797, Val Loss: 0.0701\n",
      "Epoch 19/300 - Train Loss: 0.0766, Val Loss: 0.0745\n",
      "Epoch 20/300 - Train Loss: 0.0792, Val Loss: 0.0701\n",
      "Epoch 21/300 - Train Loss: 0.0763, Val Loss: 0.0718\n",
      "Epoch 22/300 - Train Loss: 0.0761, Val Loss: 0.0717\n",
      "Epoch 23/300 - Train Loss: 0.0772, Val Loss: 0.0699\n",
      "Epoch 24/300 - Train Loss: 0.0744, Val Loss: 0.0665\n",
      "Epoch 25/300 - Train Loss: 0.0754, Val Loss: 0.0804\n",
      "Epoch 26/300 - Train Loss: 0.0751, Val Loss: 0.0739\n",
      "Epoch 27/300 - Train Loss: 0.0740, Val Loss: 0.0649\n",
      "Epoch 28/300 - Train Loss: 0.0737, Val Loss: 0.0697\n",
      "Epoch 29/300 - Train Loss: 0.0741, Val Loss: 0.0717\n",
      "Epoch 30/300 - Train Loss: 0.0728, Val Loss: 0.0707\n",
      "Epoch 31/300 - Train Loss: 0.0715, Val Loss: 0.0697\n",
      "Epoch 32/300 - Train Loss: 0.0707, Val Loss: 0.0658\n",
      "Epoch 33/300 - Train Loss: 0.0698, Val Loss: 0.0691\n",
      "Epoch 34/300 - Train Loss: 0.0702, Val Loss: 0.0789\n",
      "Epoch 35/300 - Train Loss: 0.0704, Val Loss: 0.0634\n",
      "Epoch 36/300 - Train Loss: 0.0685, Val Loss: 0.0671\n",
      "Epoch 37/300 - Train Loss: 0.0685, Val Loss: 0.0653\n",
      "Epoch 38/300 - Train Loss: 0.0688, Val Loss: 0.0720\n",
      "Epoch 39/300 - Train Loss: 0.0675, Val Loss: 0.0729\n",
      "Epoch 40/300 - Train Loss: 0.0666, Val Loss: 0.0707\n",
      "Epoch 41/300 - Train Loss: 0.0667, Val Loss: 0.0693\n",
      "Epoch 42/300 - Train Loss: 0.0674, Val Loss: 0.0681\n",
      "Epoch 43/300 - Train Loss: 0.0662, Val Loss: 0.0671\n",
      "Epoch 44/300 - Train Loss: 0.0666, Val Loss: 0.0624\n",
      "Epoch 45/300 - Train Loss: 0.0679, Val Loss: 0.0679\n",
      "Epoch 46/300 - Train Loss: 0.0671, Val Loss: 0.0653\n",
      "Epoch 47/300 - Train Loss: 0.0661, Val Loss: 0.0650\n",
      "Epoch 48/300 - Train Loss: 0.0644, Val Loss: 0.0675\n",
      "Epoch 49/300 - Train Loss: 0.0648, Val Loss: 0.0683\n",
      "Epoch 50/300 - Train Loss: 0.0645, Val Loss: 0.0720\n",
      "Epoch 51/300 - Train Loss: 0.0634, Val Loss: 0.0645\n",
      "Epoch 52/300 - Train Loss: 0.0624, Val Loss: 0.0630\n",
      "Epoch 53/300 - Train Loss: 0.0628, Val Loss: 0.0719\n",
      "Epoch 54/300 - Train Loss: 0.0637, Val Loss: 0.0672\n",
      "Epoch 55/300 - Train Loss: 0.0625, Val Loss: 0.0685\n",
      "Epoch 56/300 - Train Loss: 0.0624, Val Loss: 0.0663\n",
      "Epoch 57/300 - Train Loss: 0.0617, Val Loss: 0.0657\n",
      "Epoch 58/300 - Train Loss: 0.0599, Val Loss: 0.0685\n",
      "Epoch 59/300 - Train Loss: 0.0619, Val Loss: 0.0676\n",
      "Epoch 60/300 - Train Loss: 0.0605, Val Loss: 0.0654\n",
      "Epoch 61/300 - Train Loss: 0.0595, Val Loss: 0.0655\n",
      "Epoch 62/300 - Train Loss: 0.0601, Val Loss: 0.0631\n",
      "Epoch 63/300 - Train Loss: 0.0591, Val Loss: 0.0664\n",
      "Epoch 64/300 - Train Loss: 0.0602, Val Loss: 0.0766\n",
      "Epoch 65/300 - Train Loss: 0.0595, Val Loss: 0.0659\n",
      "Epoch 66/300 - Train Loss: 0.0578, Val Loss: 0.0725\n",
      "Epoch 67/300 - Train Loss: 0.0604, Val Loss: 0.0699\n",
      "Epoch 68/300 - Train Loss: 0.0559, Val Loss: 0.0653\n",
      "Epoch 69/300 - Train Loss: 0.0568, Val Loss: 0.0679\n",
      "Epoch 70/300 - Train Loss: 0.0587, Val Loss: 0.0695\n",
      "Epoch 71/300 - Train Loss: 0.0569, Val Loss: 0.0651\n",
      "Epoch 72/300 - Train Loss: 0.0570, Val Loss: 0.0643\n",
      "Epoch 73/300 - Train Loss: 0.0551, Val Loss: 0.0664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:50:17,251] Trial 265 finished with value: 0.9695318237905665 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1705305421499055, 'learning_rate': 5.36105407945334e-05, 'batch_size': 32, 'weight_decay': 4.688821216326767e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300 - Train Loss: 0.0560, Val Loss: 0.0629\n",
      "Early stopping at epoch 74\n",
      "Macro F1 Score: 0.9695, Macro Precision: 0.9612, Macro Recall: 0.9785\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 267\n",
      "Training with F1=16, F2=32, D=8, dropout=0.15409806760468966, LR=7.005815675623969e-05, BS=32, WD=5.046829868232453e-05\n",
      "Epoch 1/300 - Train Loss: 0.2676, Val Loss: 0.1140\n",
      "Epoch 2/300 - Train Loss: 0.1181, Val Loss: 0.1063\n",
      "Epoch 3/300 - Train Loss: 0.1052, Val Loss: 0.0891\n",
      "Epoch 4/300 - Train Loss: 0.0952, Val Loss: 0.0842\n",
      "Epoch 5/300 - Train Loss: 0.0948, Val Loss: 0.0823\n",
      "Epoch 6/300 - Train Loss: 0.0922, Val Loss: 0.0846\n",
      "Epoch 7/300 - Train Loss: 0.0889, Val Loss: 0.0721\n",
      "Epoch 8/300 - Train Loss: 0.0876, Val Loss: 0.0722\n",
      "Epoch 9/300 - Train Loss: 0.0841, Val Loss: 0.0780\n",
      "Epoch 10/300 - Train Loss: 0.0838, Val Loss: 0.0716\n",
      "Epoch 11/300 - Train Loss: 0.0813, Val Loss: 0.0740\n",
      "Epoch 12/300 - Train Loss: 0.0797, Val Loss: 0.0741\n",
      "Epoch 13/300 - Train Loss: 0.0789, Val Loss: 0.0739\n",
      "Epoch 14/300 - Train Loss: 0.0780, Val Loss: 0.0790\n",
      "Epoch 15/300 - Train Loss: 0.0779, Val Loss: 0.0775\n",
      "Epoch 16/300 - Train Loss: 0.0740, Val Loss: 0.0761\n",
      "Epoch 17/300 - Train Loss: 0.0744, Val Loss: 0.0780\n",
      "Epoch 18/300 - Train Loss: 0.0761, Val Loss: 0.0712\n",
      "Epoch 19/300 - Train Loss: 0.0748, Val Loss: 0.0678\n",
      "Epoch 20/300 - Train Loss: 0.0731, Val Loss: 0.0743\n",
      "Epoch 21/300 - Train Loss: 0.0697, Val Loss: 0.0694\n",
      "Epoch 22/300 - Train Loss: 0.0706, Val Loss: 0.0730\n",
      "Epoch 23/300 - Train Loss: 0.0692, Val Loss: 0.0683\n",
      "Epoch 24/300 - Train Loss: 0.0698, Val Loss: 0.0726\n",
      "Epoch 25/300 - Train Loss: 0.0703, Val Loss: 0.0707\n",
      "Epoch 26/300 - Train Loss: 0.0693, Val Loss: 0.0711\n",
      "Epoch 27/300 - Train Loss: 0.0690, Val Loss: 0.0755\n",
      "Epoch 28/300 - Train Loss: 0.0657, Val Loss: 0.0687\n",
      "Epoch 29/300 - Train Loss: 0.0668, Val Loss: 0.0692\n",
      "Epoch 30/300 - Train Loss: 0.0648, Val Loss: 0.0660\n",
      "Epoch 31/300 - Train Loss: 0.0662, Val Loss: 0.0710\n",
      "Epoch 32/300 - Train Loss: 0.0659, Val Loss: 0.0787\n",
      "Epoch 33/300 - Train Loss: 0.0633, Val Loss: 0.0670\n",
      "Epoch 34/300 - Train Loss: 0.0636, Val Loss: 0.0682\n",
      "Epoch 35/300 - Train Loss: 0.0641, Val Loss: 0.0682\n",
      "Epoch 36/300 - Train Loss: 0.0636, Val Loss: 0.0786\n",
      "Epoch 37/300 - Train Loss: 0.0646, Val Loss: 0.0662\n",
      "Epoch 38/300 - Train Loss: 0.0615, Val Loss: 0.0701\n",
      "Epoch 39/300 - Train Loss: 0.0625, Val Loss: 0.0663\n",
      "Epoch 40/300 - Train Loss: 0.0608, Val Loss: 0.0679\n",
      "Epoch 41/300 - Train Loss: 0.0624, Val Loss: 0.0678\n",
      "Epoch 42/300 - Train Loss: 0.0604, Val Loss: 0.0686\n",
      "Epoch 43/300 - Train Loss: 0.0595, Val Loss: 0.0670\n",
      "Epoch 44/300 - Train Loss: 0.0597, Val Loss: 0.0695\n",
      "Epoch 45/300 - Train Loss: 0.0593, Val Loss: 0.0688\n",
      "Epoch 46/300 - Train Loss: 0.0580, Val Loss: 0.0663\n",
      "Epoch 47/300 - Train Loss: 0.0591, Val Loss: 0.0774\n",
      "Epoch 48/300 - Train Loss: 0.0572, Val Loss: 0.0660\n",
      "Epoch 49/300 - Train Loss: 0.0591, Val Loss: 0.0690\n",
      "Epoch 50/300 - Train Loss: 0.0588, Val Loss: 0.0707\n",
      "Epoch 51/300 - Train Loss: 0.0578, Val Loss: 0.0627\n",
      "Epoch 52/300 - Train Loss: 0.0578, Val Loss: 0.0654\n",
      "Epoch 53/300 - Train Loss: 0.0568, Val Loss: 0.0743\n",
      "Epoch 54/300 - Train Loss: 0.0570, Val Loss: 0.0708\n",
      "Epoch 55/300 - Train Loss: 0.0556, Val Loss: 0.0722\n",
      "Epoch 56/300 - Train Loss: 0.0560, Val Loss: 0.0677\n",
      "Epoch 57/300 - Train Loss: 0.0546, Val Loss: 0.0702\n",
      "Epoch 58/300 - Train Loss: 0.0544, Val Loss: 0.0706\n",
      "Epoch 59/300 - Train Loss: 0.0519, Val Loss: 0.0679\n",
      "Epoch 60/300 - Train Loss: 0.0527, Val Loss: 0.0676\n",
      "Epoch 61/300 - Train Loss: 0.0524, Val Loss: 0.0669\n",
      "Epoch 62/300 - Train Loss: 0.0525, Val Loss: 0.0681\n",
      "Epoch 63/300 - Train Loss: 0.0540, Val Loss: 0.0703\n",
      "Epoch 64/300 - Train Loss: 0.0507, Val Loss: 0.0663\n",
      "Epoch 65/300 - Train Loss: 0.0510, Val Loss: 0.0670\n",
      "Epoch 66/300 - Train Loss: 0.0529, Val Loss: 0.0684\n",
      "Epoch 67/300 - Train Loss: 0.0502, Val Loss: 0.0660\n",
      "Epoch 68/300 - Train Loss: 0.0498, Val Loss: 0.0683\n",
      "Epoch 69/300 - Train Loss: 0.0502, Val Loss: 0.0690\n",
      "Epoch 70/300 - Train Loss: 0.0507, Val Loss: 0.0691\n",
      "Epoch 71/300 - Train Loss: 0.0509, Val Loss: 0.0667\n",
      "Epoch 72/300 - Train Loss: 0.0472, Val Loss: 0.0671\n",
      "Epoch 73/300 - Train Loss: 0.0493, Val Loss: 0.0665\n",
      "Epoch 74/300 - Train Loss: 0.0486, Val Loss: 0.0688\n",
      "Epoch 75/300 - Train Loss: 0.0485, Val Loss: 0.0675\n",
      "Epoch 76/300 - Train Loss: 0.0481, Val Loss: 0.0662\n",
      "Epoch 77/300 - Train Loss: 0.0482, Val Loss: 0.0679\n",
      "Epoch 78/300 - Train Loss: 0.0458, Val Loss: 0.0680\n",
      "Epoch 79/300 - Train Loss: 0.0476, Val Loss: 0.0708\n",
      "Epoch 80/300 - Train Loss: 0.0469, Val Loss: 0.0713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 16:55:29,337] Trial 266 finished with value: 0.9724082359831551 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.15409806760468966, 'learning_rate': 7.005815675623969e-05, 'batch_size': 32, 'weight_decay': 5.046829868232453e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300 - Train Loss: 0.0467, Val Loss: 0.0704\n",
      "Early stopping at epoch 81\n",
      "Macro F1 Score: 0.9724, Macro Precision: 0.9731, Macro Recall: 0.9718\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 268\n",
      "Training with F1=16, F2=32, D=8, dropout=0.14915994301557217, LR=6.70466005203829e-05, BS=32, WD=6.785141349256642e-05\n",
      "Epoch 1/300 - Train Loss: 0.2744, Val Loss: 0.1134\n",
      "Epoch 2/300 - Train Loss: 0.1189, Val Loss: 0.1129\n",
      "Epoch 3/300 - Train Loss: 0.1061, Val Loss: 0.0829\n",
      "Epoch 4/300 - Train Loss: 0.0982, Val Loss: 0.0771\n",
      "Epoch 5/300 - Train Loss: 0.0950, Val Loss: 0.0828\n",
      "Epoch 6/300 - Train Loss: 0.0917, Val Loss: 0.0744\n",
      "Epoch 7/300 - Train Loss: 0.0899, Val Loss: 0.0800\n",
      "Epoch 8/300 - Train Loss: 0.0858, Val Loss: 0.0751\n",
      "Epoch 9/300 - Train Loss: 0.0862, Val Loss: 0.0714\n",
      "Epoch 10/300 - Train Loss: 0.0839, Val Loss: 0.0740\n",
      "Epoch 11/300 - Train Loss: 0.0858, Val Loss: 0.0720\n",
      "Epoch 12/300 - Train Loss: 0.0823, Val Loss: 0.0733\n",
      "Epoch 13/300 - Train Loss: 0.0829, Val Loss: 0.0798\n",
      "Epoch 14/300 - Train Loss: 0.0764, Val Loss: 0.0740\n",
      "Epoch 15/300 - Train Loss: 0.0802, Val Loss: 0.0726\n",
      "Epoch 16/300 - Train Loss: 0.0786, Val Loss: 0.0728\n",
      "Epoch 17/300 - Train Loss: 0.0778, Val Loss: 0.0761\n",
      "Epoch 18/300 - Train Loss: 0.0762, Val Loss: 0.0760\n",
      "Epoch 19/300 - Train Loss: 0.0763, Val Loss: 0.0686\n",
      "Epoch 20/300 - Train Loss: 0.0751, Val Loss: 0.0700\n",
      "Epoch 21/300 - Train Loss: 0.0750, Val Loss: 0.0693\n",
      "Epoch 22/300 - Train Loss: 0.0743, Val Loss: 0.0702\n",
      "Epoch 23/300 - Train Loss: 0.0756, Val Loss: 0.0681\n",
      "Epoch 24/300 - Train Loss: 0.0709, Val Loss: 0.0680\n",
      "Epoch 25/300 - Train Loss: 0.0701, Val Loss: 0.0718\n",
      "Epoch 26/300 - Train Loss: 0.0696, Val Loss: 0.0757\n",
      "Epoch 27/300 - Train Loss: 0.0713, Val Loss: 0.0725\n",
      "Epoch 28/300 - Train Loss: 0.0697, Val Loss: 0.0722\n",
      "Epoch 29/300 - Train Loss: 0.0687, Val Loss: 0.0742\n",
      "Epoch 30/300 - Train Loss: 0.0684, Val Loss: 0.0668\n",
      "Epoch 31/300 - Train Loss: 0.0668, Val Loss: 0.0767\n",
      "Epoch 32/300 - Train Loss: 0.0692, Val Loss: 0.0690\n",
      "Epoch 33/300 - Train Loss: 0.0675, Val Loss: 0.0679\n",
      "Epoch 34/300 - Train Loss: 0.0663, Val Loss: 0.0664\n",
      "Epoch 35/300 - Train Loss: 0.0676, Val Loss: 0.0723\n",
      "Epoch 36/300 - Train Loss: 0.0654, Val Loss: 0.0673\n",
      "Epoch 37/300 - Train Loss: 0.0670, Val Loss: 0.0693\n",
      "Epoch 38/300 - Train Loss: 0.0660, Val Loss: 0.0665\n",
      "Epoch 39/300 - Train Loss: 0.0633, Val Loss: 0.0705\n",
      "Epoch 40/300 - Train Loss: 0.0634, Val Loss: 0.0654\n",
      "Epoch 41/300 - Train Loss: 0.0628, Val Loss: 0.0657\n",
      "Epoch 42/300 - Train Loss: 0.0632, Val Loss: 0.0740\n",
      "Epoch 43/300 - Train Loss: 0.0626, Val Loss: 0.0651\n",
      "Epoch 44/300 - Train Loss: 0.0656, Val Loss: 0.0671\n",
      "Epoch 45/300 - Train Loss: 0.0617, Val Loss: 0.0680\n",
      "Epoch 46/300 - Train Loss: 0.0615, Val Loss: 0.0666\n",
      "Epoch 47/300 - Train Loss: 0.0602, Val Loss: 0.0635\n",
      "Epoch 48/300 - Train Loss: 0.0598, Val Loss: 0.0684\n",
      "Epoch 49/300 - Train Loss: 0.0579, Val Loss: 0.0674\n",
      "Epoch 50/300 - Train Loss: 0.0634, Val Loss: 0.0668\n",
      "Epoch 51/300 - Train Loss: 0.0589, Val Loss: 0.0682\n",
      "Epoch 52/300 - Train Loss: 0.0582, Val Loss: 0.0689\n",
      "Epoch 53/300 - Train Loss: 0.0568, Val Loss: 0.0663\n",
      "Epoch 54/300 - Train Loss: 0.0558, Val Loss: 0.0689\n",
      "Epoch 55/300 - Train Loss: 0.0575, Val Loss: 0.0740\n",
      "Epoch 56/300 - Train Loss: 0.0579, Val Loss: 0.0707\n",
      "Epoch 57/300 - Train Loss: 0.0565, Val Loss: 0.0740\n",
      "Epoch 58/300 - Train Loss: 0.0565, Val Loss: 0.0752\n",
      "Epoch 59/300 - Train Loss: 0.0546, Val Loss: 0.0723\n",
      "Epoch 60/300 - Train Loss: 0.0568, Val Loss: 0.0720\n",
      "Epoch 61/300 - Train Loss: 0.0550, Val Loss: 0.0690\n",
      "Epoch 62/300 - Train Loss: 0.0542, Val Loss: 0.0709\n",
      "Epoch 63/300 - Train Loss: 0.0540, Val Loss: 0.0728\n",
      "Epoch 64/300 - Train Loss: 0.0525, Val Loss: 0.0675\n",
      "Epoch 65/300 - Train Loss: 0.0584, Val Loss: 0.0683\n",
      "Epoch 66/300 - Train Loss: 0.0514, Val Loss: 0.0679\n",
      "Epoch 67/300 - Train Loss: 0.0541, Val Loss: 0.0690\n",
      "Epoch 68/300 - Train Loss: 0.0534, Val Loss: 0.0686\n",
      "Epoch 69/300 - Train Loss: 0.0510, Val Loss: 0.0702\n",
      "Epoch 70/300 - Train Loss: 0.0520, Val Loss: 0.0712\n",
      "Epoch 71/300 - Train Loss: 0.0523, Val Loss: 0.0710\n",
      "Epoch 72/300 - Train Loss: 0.0521, Val Loss: 0.0732\n",
      "Epoch 73/300 - Train Loss: 0.0527, Val Loss: 0.0784\n",
      "Epoch 74/300 - Train Loss: 0.0510, Val Loss: 0.0748\n",
      "Epoch 75/300 - Train Loss: 0.0495, Val Loss: 0.0738\n",
      "Epoch 76/300 - Train Loss: 0.0497, Val Loss: 0.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 17:00:25,940] Trial 267 finished with value: 0.9720786545659642 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.14915994301557217, 'learning_rate': 6.70466005203829e-05, 'batch_size': 32, 'weight_decay': 6.785141349256642e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300 - Train Loss: 0.0506, Val Loss: 0.0718\n",
      "Early stopping at epoch 77\n",
      "Macro F1 Score: 0.9721, Macro Precision: 0.9773, Macro Recall: 0.9670\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 269\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1552646845655932, LR=7.157088113311516e-05, BS=32, WD=5.4324402073539356e-05\n",
      "Epoch 1/300 - Train Loss: 0.2952, Val Loss: 0.1662\n",
      "Epoch 2/300 - Train Loss: 0.1353, Val Loss: 0.0941\n",
      "Epoch 3/300 - Train Loss: 0.1004, Val Loss: 0.0759\n",
      "Epoch 4/300 - Train Loss: 0.0980, Val Loss: 0.0748\n",
      "Epoch 5/300 - Train Loss: 0.0928, Val Loss: 0.0778\n",
      "Epoch 6/300 - Train Loss: 0.0881, Val Loss: 0.0752\n",
      "Epoch 7/300 - Train Loss: 0.0875, Val Loss: 0.0763\n",
      "Epoch 8/300 - Train Loss: 0.0861, Val Loss: 0.0719\n",
      "Epoch 9/300 - Train Loss: 0.0840, Val Loss: 0.0720\n",
      "Epoch 10/300 - Train Loss: 0.0827, Val Loss: 0.0767\n",
      "Epoch 11/300 - Train Loss: 0.0820, Val Loss: 0.0732\n",
      "Epoch 12/300 - Train Loss: 0.0810, Val Loss: 0.0759\n",
      "Epoch 13/300 - Train Loss: 0.0802, Val Loss: 0.0736\n",
      "Epoch 14/300 - Train Loss: 0.0772, Val Loss: 0.0693\n",
      "Epoch 15/300 - Train Loss: 0.0791, Val Loss: 0.0655\n",
      "Epoch 16/300 - Train Loss: 0.0776, Val Loss: 0.0711\n",
      "Epoch 17/300 - Train Loss: 0.0761, Val Loss: 0.0775\n",
      "Epoch 18/300 - Train Loss: 0.0753, Val Loss: 0.0689\n",
      "Epoch 19/300 - Train Loss: 0.0748, Val Loss: 0.0730\n",
      "Epoch 20/300 - Train Loss: 0.0738, Val Loss: 0.0692\n",
      "Epoch 21/300 - Train Loss: 0.0736, Val Loss: 0.0689\n",
      "Epoch 22/300 - Train Loss: 0.0726, Val Loss: 0.0697\n",
      "Epoch 23/300 - Train Loss: 0.0736, Val Loss: 0.0709\n",
      "Epoch 24/300 - Train Loss: 0.0701, Val Loss: 0.0721\n",
      "Epoch 25/300 - Train Loss: 0.0701, Val Loss: 0.0771\n",
      "Epoch 26/300 - Train Loss: 0.0717, Val Loss: 0.0649\n",
      "Epoch 27/300 - Train Loss: 0.0703, Val Loss: 0.0660\n",
      "Epoch 28/300 - Train Loss: 0.0689, Val Loss: 0.0657\n",
      "Epoch 29/300 - Train Loss: 0.0688, Val Loss: 0.0784\n",
      "Epoch 30/300 - Train Loss: 0.0691, Val Loss: 0.0673\n",
      "Epoch 31/300 - Train Loss: 0.0690, Val Loss: 0.0676\n",
      "Epoch 32/300 - Train Loss: 0.0662, Val Loss: 0.0749\n",
      "Epoch 33/300 - Train Loss: 0.0671, Val Loss: 0.0673\n",
      "Epoch 34/300 - Train Loss: 0.0657, Val Loss: 0.0724\n",
      "Epoch 35/300 - Train Loss: 0.0651, Val Loss: 0.0697\n",
      "Epoch 36/300 - Train Loss: 0.0651, Val Loss: 0.0732\n",
      "Epoch 37/300 - Train Loss: 0.0654, Val Loss: 0.0710\n",
      "Epoch 38/300 - Train Loss: 0.0623, Val Loss: 0.0725\n",
      "Epoch 39/300 - Train Loss: 0.0637, Val Loss: 0.0696\n",
      "Epoch 40/300 - Train Loss: 0.0640, Val Loss: 0.0644\n",
      "Epoch 41/300 - Train Loss: 0.0641, Val Loss: 0.0661\n",
      "Epoch 42/300 - Train Loss: 0.0621, Val Loss: 0.0681\n",
      "Epoch 43/300 - Train Loss: 0.0598, Val Loss: 0.0696\n",
      "Epoch 44/300 - Train Loss: 0.0613, Val Loss: 0.0688\n",
      "Epoch 45/300 - Train Loss: 0.0621, Val Loss: 0.0718\n",
      "Epoch 46/300 - Train Loss: 0.0586, Val Loss: 0.0676\n",
      "Epoch 47/300 - Train Loss: 0.0595, Val Loss: 0.0729\n",
      "Epoch 48/300 - Train Loss: 0.0592, Val Loss: 0.0779\n",
      "Epoch 49/300 - Train Loss: 0.0593, Val Loss: 0.0689\n",
      "Epoch 50/300 - Train Loss: 0.0585, Val Loss: 0.0739\n",
      "Epoch 51/300 - Train Loss: 0.0579, Val Loss: 0.0717\n",
      "Epoch 52/300 - Train Loss: 0.0586, Val Loss: 0.0697\n",
      "Epoch 53/300 - Train Loss: 0.0584, Val Loss: 0.0688\n",
      "Epoch 54/300 - Train Loss: 0.0562, Val Loss: 0.0717\n",
      "Epoch 55/300 - Train Loss: 0.0578, Val Loss: 0.0683\n",
      "Epoch 56/300 - Train Loss: 0.0559, Val Loss: 0.0699\n",
      "Epoch 57/300 - Train Loss: 0.0565, Val Loss: 0.0652\n",
      "Epoch 58/300 - Train Loss: 0.0583, Val Loss: 0.0743\n",
      "Epoch 59/300 - Train Loss: 0.0551, Val Loss: 0.0744\n",
      "Epoch 60/300 - Train Loss: 0.0552, Val Loss: 0.0681\n",
      "Epoch 61/300 - Train Loss: 0.0567, Val Loss: 0.0663\n",
      "Epoch 62/300 - Train Loss: 0.0558, Val Loss: 0.0726\n",
      "Epoch 63/300 - Train Loss: 0.0539, Val Loss: 0.0677\n",
      "Epoch 64/300 - Train Loss: 0.0548, Val Loss: 0.0687\n",
      "Epoch 65/300 - Train Loss: 0.0524, Val Loss: 0.0671\n",
      "Epoch 66/300 - Train Loss: 0.0525, Val Loss: 0.0715\n",
      "Epoch 67/300 - Train Loss: 0.0557, Val Loss: 0.0709\n",
      "Epoch 68/300 - Train Loss: 0.0530, Val Loss: 0.0675\n",
      "Epoch 69/300 - Train Loss: 0.0505, Val Loss: 0.0666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 17:04:55,698] Trial 268 finished with value: 0.9659428776189268 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1552646845655932, 'learning_rate': 7.157088113311516e-05, 'batch_size': 32, 'weight_decay': 5.4324402073539356e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/300 - Train Loss: 0.0514, Val Loss: 0.0730\n",
      "Early stopping at epoch 70\n",
      "Macro F1 Score: 0.9659, Macro Precision: 0.9558, Macro Recall: 0.9771\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 270\n",
      "Training with F1=16, F2=32, D=8, dropout=0.134905478210587, LR=5.949074718689811e-05, BS=32, WD=4.948947325819346e-05\n",
      "Epoch 1/300 - Train Loss: 0.3044, Val Loss: 0.1967\n",
      "Epoch 2/300 - Train Loss: 0.1446, Val Loss: 0.1224\n",
      "Epoch 3/300 - Train Loss: 0.1132, Val Loss: 0.0913\n",
      "Epoch 4/300 - Train Loss: 0.1030, Val Loss: 0.0830\n",
      "Epoch 5/300 - Train Loss: 0.0994, Val Loss: 0.0819\n",
      "Epoch 6/300 - Train Loss: 0.0957, Val Loss: 0.0770\n",
      "Epoch 7/300 - Train Loss: 0.0910, Val Loss: 0.0806\n",
      "Epoch 8/300 - Train Loss: 0.0876, Val Loss: 0.0725\n",
      "Epoch 9/300 - Train Loss: 0.0876, Val Loss: 0.0813\n",
      "Epoch 10/300 - Train Loss: 0.0846, Val Loss: 0.0795\n",
      "Epoch 11/300 - Train Loss: 0.0838, Val Loss: 0.0737\n",
      "Epoch 12/300 - Train Loss: 0.0836, Val Loss: 0.0835\n",
      "Epoch 13/300 - Train Loss: 0.0812, Val Loss: 0.0792\n",
      "Epoch 14/300 - Train Loss: 0.0777, Val Loss: 0.0694\n",
      "Epoch 15/300 - Train Loss: 0.0785, Val Loss: 0.0770\n",
      "Epoch 16/300 - Train Loss: 0.0783, Val Loss: 0.0754\n",
      "Epoch 17/300 - Train Loss: 0.0756, Val Loss: 0.0681\n",
      "Epoch 18/300 - Train Loss: 0.0764, Val Loss: 0.0782\n",
      "Epoch 19/300 - Train Loss: 0.0741, Val Loss: 0.0746\n",
      "Epoch 20/300 - Train Loss: 0.0738, Val Loss: 0.0734\n",
      "Epoch 21/300 - Train Loss: 0.0739, Val Loss: 0.0678\n",
      "Epoch 22/300 - Train Loss: 0.0726, Val Loss: 0.0676\n",
      "Epoch 23/300 - Train Loss: 0.0734, Val Loss: 0.0701\n",
      "Epoch 24/300 - Train Loss: 0.0707, Val Loss: 0.0702\n",
      "Epoch 25/300 - Train Loss: 0.0706, Val Loss: 0.0722\n",
      "Epoch 26/300 - Train Loss: 0.0693, Val Loss: 0.0699\n",
      "Epoch 27/300 - Train Loss: 0.0716, Val Loss: 0.0729\n",
      "Epoch 28/300 - Train Loss: 0.0680, Val Loss: 0.0728\n",
      "Epoch 29/300 - Train Loss: 0.0668, Val Loss: 0.0686\n",
      "Epoch 30/300 - Train Loss: 0.0682, Val Loss: 0.0666\n",
      "Epoch 31/300 - Train Loss: 0.0665, Val Loss: 0.0664\n",
      "Epoch 32/300 - Train Loss: 0.0676, Val Loss: 0.0719\n",
      "Epoch 33/300 - Train Loss: 0.0666, Val Loss: 0.0714\n",
      "Epoch 34/300 - Train Loss: 0.0656, Val Loss: 0.0690\n",
      "Epoch 35/300 - Train Loss: 0.0657, Val Loss: 0.0770\n",
      "Epoch 36/300 - Train Loss: 0.0659, Val Loss: 0.0669\n",
      "Epoch 37/300 - Train Loss: 0.0659, Val Loss: 0.0786\n",
      "Epoch 38/300 - Train Loss: 0.0647, Val Loss: 0.0669\n",
      "Epoch 39/300 - Train Loss: 0.0611, Val Loss: 0.0718\n",
      "Epoch 40/300 - Train Loss: 0.0627, Val Loss: 0.0644\n",
      "Epoch 41/300 - Train Loss: 0.0646, Val Loss: 0.0678\n",
      "Epoch 42/300 - Train Loss: 0.0596, Val Loss: 0.0686\n",
      "Epoch 43/300 - Train Loss: 0.0622, Val Loss: 0.0670\n",
      "Epoch 44/300 - Train Loss: 0.0610, Val Loss: 0.0719\n",
      "Epoch 45/300 - Train Loss: 0.0613, Val Loss: 0.0750\n",
      "Epoch 46/300 - Train Loss: 0.0611, Val Loss: 0.0708\n",
      "Epoch 47/300 - Train Loss: 0.0584, Val Loss: 0.0716\n",
      "Epoch 48/300 - Train Loss: 0.0582, Val Loss: 0.0640\n",
      "Epoch 49/300 - Train Loss: 0.0585, Val Loss: 0.0686\n",
      "Epoch 50/300 - Train Loss: 0.0591, Val Loss: 0.0709\n",
      "Epoch 51/300 - Train Loss: 0.0573, Val Loss: 0.0649\n",
      "Epoch 52/300 - Train Loss: 0.0580, Val Loss: 0.0758\n",
      "Epoch 53/300 - Train Loss: 0.0563, Val Loss: 0.0707\n",
      "Epoch 54/300 - Train Loss: 0.0570, Val Loss: 0.0725\n",
      "Epoch 55/300 - Train Loss: 0.0576, Val Loss: 0.0680\n",
      "Epoch 56/300 - Train Loss: 0.0577, Val Loss: 0.0681\n",
      "Epoch 57/300 - Train Loss: 0.0543, Val Loss: 0.0679\n",
      "Epoch 58/300 - Train Loss: 0.0531, Val Loss: 0.0705\n",
      "Epoch 59/300 - Train Loss: 0.0568, Val Loss: 0.0737\n",
      "Epoch 60/300 - Train Loss: 0.0565, Val Loss: 0.0689\n",
      "Epoch 61/300 - Train Loss: 0.0535, Val Loss: 0.0662\n",
      "Epoch 62/300 - Train Loss: 0.0536, Val Loss: 0.0682\n",
      "Epoch 63/300 - Train Loss: 0.0539, Val Loss: 0.0661\n",
      "Epoch 64/300 - Train Loss: 0.0531, Val Loss: 0.0674\n",
      "Epoch 65/300 - Train Loss: 0.0548, Val Loss: 0.0751\n",
      "Epoch 66/300 - Train Loss: 0.0542, Val Loss: 0.0709\n",
      "Epoch 67/300 - Train Loss: 0.0529, Val Loss: 0.0651\n",
      "Epoch 68/300 - Train Loss: 0.0536, Val Loss: 0.0747\n",
      "Epoch 69/300 - Train Loss: 0.0505, Val Loss: 0.0665\n",
      "Epoch 70/300 - Train Loss: 0.0510, Val Loss: 0.0701\n",
      "Epoch 71/300 - Train Loss: 0.0506, Val Loss: 0.0705\n",
      "Epoch 72/300 - Train Loss: 0.0504, Val Loss: 0.0661\n",
      "Epoch 73/300 - Train Loss: 0.0516, Val Loss: 0.0716\n",
      "Epoch 74/300 - Train Loss: 0.0507, Val Loss: 0.0674\n",
      "Epoch 75/300 - Train Loss: 0.0503, Val Loss: 0.0687\n",
      "Epoch 76/300 - Train Loss: 0.0486, Val Loss: 0.0690\n",
      "Epoch 77/300 - Train Loss: 0.0505, Val Loss: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 17:09:56,629] Trial 269 finished with value: 0.9708343642906406 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.134905478210587, 'learning_rate': 5.949074718689811e-05, 'batch_size': 32, 'weight_decay': 4.948947325819346e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300 - Train Loss: 0.0494, Val Loss: 0.0709\n",
      "Early stopping at epoch 78\n",
      "Macro F1 Score: 0.9708, Macro Precision: 0.9645, Macro Recall: 0.9775\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 271\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1875843543816808, LR=5.9619845851828344e-05, BS=32, WD=4.193881466608007e-05\n",
      "Epoch 1/300 - Train Loss: 0.3142, Val Loss: 0.1455\n",
      "Epoch 2/300 - Train Loss: 0.1393, Val Loss: 0.0901\n",
      "Epoch 3/300 - Train Loss: 0.1112, Val Loss: 0.0801\n",
      "Epoch 4/300 - Train Loss: 0.1032, Val Loss: 0.0761\n",
      "Epoch 5/300 - Train Loss: 0.0947, Val Loss: 0.1058\n",
      "Epoch 6/300 - Train Loss: 0.0928, Val Loss: 0.0710\n",
      "Epoch 7/300 - Train Loss: 0.0909, Val Loss: 0.0737\n",
      "Epoch 8/300 - Train Loss: 0.0871, Val Loss: 0.0703\n",
      "Epoch 9/300 - Train Loss: 0.0851, Val Loss: 0.0824\n",
      "Epoch 10/300 - Train Loss: 0.0869, Val Loss: 0.0724\n",
      "Epoch 11/300 - Train Loss: 0.0839, Val Loss: 0.0696\n",
      "Epoch 12/300 - Train Loss: 0.0834, Val Loss: 0.0869\n",
      "Epoch 13/300 - Train Loss: 0.0823, Val Loss: 0.0710\n",
      "Epoch 14/300 - Train Loss: 0.0821, Val Loss: 0.0710\n",
      "Epoch 15/300 - Train Loss: 0.0776, Val Loss: 0.0733\n",
      "Epoch 16/300 - Train Loss: 0.0783, Val Loss: 0.0757\n",
      "Epoch 17/300 - Train Loss: 0.0775, Val Loss: 0.0689\n",
      "Epoch 18/300 - Train Loss: 0.0774, Val Loss: 0.0707\n",
      "Epoch 19/300 - Train Loss: 0.0758, Val Loss: 0.0759\n",
      "Epoch 20/300 - Train Loss: 0.0747, Val Loss: 0.0702\n",
      "Epoch 21/300 - Train Loss: 0.0742, Val Loss: 0.0694\n",
      "Epoch 22/300 - Train Loss: 0.0761, Val Loss: 0.0727\n",
      "Epoch 23/300 - Train Loss: 0.0752, Val Loss: 0.0761\n",
      "Epoch 24/300 - Train Loss: 0.0740, Val Loss: 0.0714\n",
      "Epoch 25/300 - Train Loss: 0.0732, Val Loss: 0.0719\n",
      "Epoch 26/300 - Train Loss: 0.0717, Val Loss: 0.0728\n",
      "Epoch 27/300 - Train Loss: 0.0727, Val Loss: 0.0717\n",
      "Epoch 28/300 - Train Loss: 0.0725, Val Loss: 0.0685\n",
      "Epoch 29/300 - Train Loss: 0.0708, Val Loss: 0.0711\n",
      "Epoch 30/300 - Train Loss: 0.0693, Val Loss: 0.0788\n",
      "Epoch 31/300 - Train Loss: 0.0690, Val Loss: 0.0801\n",
      "Epoch 32/300 - Train Loss: 0.0700, Val Loss: 0.0699\n",
      "Epoch 33/300 - Train Loss: 0.0700, Val Loss: 0.0746\n",
      "Epoch 34/300 - Train Loss: 0.0685, Val Loss: 0.0674\n",
      "Epoch 35/300 - Train Loss: 0.0675, Val Loss: 0.0791\n",
      "Epoch 36/300 - Train Loss: 0.0681, Val Loss: 0.0719\n",
      "Epoch 37/300 - Train Loss: 0.0662, Val Loss: 0.0701\n",
      "Epoch 38/300 - Train Loss: 0.0672, Val Loss: 0.0747\n",
      "Epoch 39/300 - Train Loss: 0.0661, Val Loss: 0.0708\n",
      "Epoch 40/300 - Train Loss: 0.0688, Val Loss: 0.0725\n",
      "Epoch 41/300 - Train Loss: 0.0633, Val Loss: 0.0725\n",
      "Epoch 42/300 - Train Loss: 0.0642, Val Loss: 0.0684\n",
      "Epoch 43/300 - Train Loss: 0.0628, Val Loss: 0.0666\n",
      "Epoch 44/300 - Train Loss: 0.0666, Val Loss: 0.0688\n",
      "Epoch 45/300 - Train Loss: 0.0643, Val Loss: 0.0800\n",
      "Epoch 46/300 - Train Loss: 0.0645, Val Loss: 0.0682\n",
      "Epoch 47/300 - Train Loss: 0.0626, Val Loss: 0.0769\n",
      "Epoch 48/300 - Train Loss: 0.0617, Val Loss: 0.0703\n",
      "Epoch 49/300 - Train Loss: 0.0624, Val Loss: 0.0712\n",
      "Epoch 50/300 - Train Loss: 0.0627, Val Loss: 0.0737\n",
      "Epoch 51/300 - Train Loss: 0.0619, Val Loss: 0.0709\n",
      "Epoch 52/300 - Train Loss: 0.0600, Val Loss: 0.0674\n",
      "Epoch 53/300 - Train Loss: 0.0594, Val Loss: 0.0687\n",
      "Epoch 54/300 - Train Loss: 0.0621, Val Loss: 0.0769\n",
      "Epoch 55/300 - Train Loss: 0.0600, Val Loss: 0.0708\n",
      "Epoch 56/300 - Train Loss: 0.0607, Val Loss: 0.0721\n",
      "Epoch 57/300 - Train Loss: 0.0614, Val Loss: 0.0714\n",
      "Epoch 58/300 - Train Loss: 0.0586, Val Loss: 0.0784\n",
      "Epoch 59/300 - Train Loss: 0.0589, Val Loss: 0.0719\n",
      "Epoch 60/300 - Train Loss: 0.0581, Val Loss: 0.0702\n",
      "Epoch 61/300 - Train Loss: 0.0579, Val Loss: 0.0662\n",
      "Epoch 62/300 - Train Loss: 0.0590, Val Loss: 0.0696\n",
      "Epoch 63/300 - Train Loss: 0.0574, Val Loss: 0.0714\n",
      "Epoch 64/300 - Train Loss: 0.0556, Val Loss: 0.0696\n",
      "Epoch 65/300 - Train Loss: 0.0574, Val Loss: 0.0754\n",
      "Epoch 66/300 - Train Loss: 0.0566, Val Loss: 0.0729\n",
      "Epoch 67/300 - Train Loss: 0.0569, Val Loss: 0.0737\n",
      "Epoch 68/300 - Train Loss: 0.0566, Val Loss: 0.0697\n",
      "Epoch 69/300 - Train Loss: 0.0558, Val Loss: 0.0749\n",
      "Epoch 70/300 - Train Loss: 0.0559, Val Loss: 0.0730\n",
      "Epoch 71/300 - Train Loss: 0.0589, Val Loss: 0.0686\n",
      "Epoch 72/300 - Train Loss: 0.0541, Val Loss: 0.0678\n",
      "Epoch 73/300 - Train Loss: 0.0531, Val Loss: 0.0776\n",
      "Epoch 74/300 - Train Loss: 0.0548, Val Loss: 0.0752\n",
      "Epoch 75/300 - Train Loss: 0.0554, Val Loss: 0.0700\n",
      "Epoch 76/300 - Train Loss: 0.0540, Val Loss: 0.0693\n",
      "Epoch 77/300 - Train Loss: 0.0542, Val Loss: 0.0741\n",
      "Epoch 78/300 - Train Loss: 0.0513, Val Loss: 0.0758\n",
      "Epoch 79/300 - Train Loss: 0.0522, Val Loss: 0.0753\n",
      "Epoch 80/300 - Train Loss: 0.0517, Val Loss: 0.0779\n",
      "Epoch 81/300 - Train Loss: 0.0505, Val Loss: 0.0743\n",
      "Epoch 82/300 - Train Loss: 0.0508, Val Loss: 0.0751\n",
      "Epoch 83/300 - Train Loss: 0.0504, Val Loss: 0.0740\n",
      "Epoch 84/300 - Train Loss: 0.0500, Val Loss: 0.0788\n",
      "Epoch 85/300 - Train Loss: 0.0529, Val Loss: 0.0732\n",
      "Epoch 86/300 - Train Loss: 0.0514, Val Loss: 0.0734\n",
      "Epoch 87/300 - Train Loss: 0.0489, Val Loss: 0.0742\n",
      "Epoch 88/300 - Train Loss: 0.0507, Val Loss: 0.0734\n",
      "Epoch 89/300 - Train Loss: 0.0491, Val Loss: 0.0779\n",
      "Epoch 90/300 - Train Loss: 0.0494, Val Loss: 0.0763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 17:15:47,695] Trial 270 finished with value: 0.9672861791578423 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1875843543816808, 'learning_rate': 5.9619845851828344e-05, 'batch_size': 32, 'weight_decay': 4.193881466608007e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/300 - Train Loss: 0.0492, Val Loss: 0.0700\n",
      "Early stopping at epoch 91\n",
      "Macro F1 Score: 0.9673, Macro Precision: 0.9636, Macro Recall: 0.9712\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 272\n",
      "Training with F1=32, F2=32, D=8, dropout=0.14316248361198972, LR=6.584911088302347e-05, BS=32, WD=5.051446545978423e-05\n",
      "Epoch 1/300 - Train Loss: 0.2689, Val Loss: 0.1187\n",
      "Epoch 2/300 - Train Loss: 0.1171, Val Loss: 0.0885\n",
      "Epoch 3/300 - Train Loss: 0.1032, Val Loss: 0.0834\n",
      "Epoch 4/300 - Train Loss: 0.0933, Val Loss: 0.0829\n",
      "Epoch 5/300 - Train Loss: 0.0928, Val Loss: 0.0748\n",
      "Epoch 6/300 - Train Loss: 0.0905, Val Loss: 0.1080\n",
      "Epoch 7/300 - Train Loss: 0.0869, Val Loss: 0.0829\n",
      "Epoch 8/300 - Train Loss: 0.0846, Val Loss: 0.0848\n",
      "Epoch 9/300 - Train Loss: 0.0849, Val Loss: 0.0748\n",
      "Epoch 10/300 - Train Loss: 0.0822, Val Loss: 0.0760\n",
      "Epoch 11/300 - Train Loss: 0.0792, Val Loss: 0.0686\n",
      "Epoch 12/300 - Train Loss: 0.0787, Val Loss: 0.0708\n",
      "Epoch 13/300 - Train Loss: 0.0789, Val Loss: 0.0712\n",
      "Epoch 14/300 - Train Loss: 0.0755, Val Loss: 0.0730\n",
      "Epoch 15/300 - Train Loss: 0.0751, Val Loss: 0.0701\n",
      "Epoch 16/300 - Train Loss: 0.0751, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.0751, Val Loss: 0.0854\n",
      "Epoch 18/300 - Train Loss: 0.0721, Val Loss: 0.0689\n",
      "Epoch 19/300 - Train Loss: 0.0707, Val Loss: 0.0735\n",
      "Epoch 20/300 - Train Loss: 0.0707, Val Loss: 0.0691\n",
      "Epoch 21/300 - Train Loss: 0.0706, Val Loss: 0.0984\n",
      "Epoch 22/300 - Train Loss: 0.0692, Val Loss: 0.0661\n",
      "Epoch 23/300 - Train Loss: 0.0696, Val Loss: 0.0699\n",
      "Epoch 24/300 - Train Loss: 0.0672, Val Loss: 0.0645\n",
      "Epoch 25/300 - Train Loss: 0.0669, Val Loss: 0.0686\n",
      "Epoch 26/300 - Train Loss: 0.0660, Val Loss: 0.0705\n",
      "Epoch 27/300 - Train Loss: 0.0654, Val Loss: 0.0696\n",
      "Epoch 28/300 - Train Loss: 0.0643, Val Loss: 0.0710\n",
      "Epoch 29/300 - Train Loss: 0.0645, Val Loss: 0.0699\n",
      "Epoch 30/300 - Train Loss: 0.0641, Val Loss: 0.0674\n",
      "Epoch 31/300 - Train Loss: 0.0644, Val Loss: 0.0723\n",
      "Epoch 32/300 - Train Loss: 0.0604, Val Loss: 0.0732\n",
      "Epoch 33/300 - Train Loss: 0.0613, Val Loss: 0.0695\n",
      "Epoch 34/300 - Train Loss: 0.0601, Val Loss: 0.0681\n",
      "Epoch 35/300 - Train Loss: 0.0626, Val Loss: 0.0653\n",
      "Epoch 36/300 - Train Loss: 0.0592, Val Loss: 0.0785\n",
      "Epoch 37/300 - Train Loss: 0.0617, Val Loss: 0.0678\n",
      "Epoch 38/300 - Train Loss: 0.0608, Val Loss: 0.0720\n",
      "Epoch 39/300 - Train Loss: 0.0580, Val Loss: 0.0720\n",
      "Epoch 40/300 - Train Loss: 0.0581, Val Loss: 0.0692\n",
      "Epoch 41/300 - Train Loss: 0.0567, Val Loss: 0.0665\n",
      "Epoch 42/300 - Train Loss: 0.0576, Val Loss: 0.0718\n",
      "Epoch 43/300 - Train Loss: 0.0560, Val Loss: 0.0682\n",
      "Epoch 44/300 - Train Loss: 0.0556, Val Loss: 0.0701\n",
      "Epoch 45/300 - Train Loss: 0.0542, Val Loss: 0.0706\n",
      "Epoch 46/300 - Train Loss: 0.0559, Val Loss: 0.0716\n",
      "Epoch 47/300 - Train Loss: 0.0542, Val Loss: 0.0720\n",
      "Epoch 48/300 - Train Loss: 0.0547, Val Loss: 0.0728\n",
      "Epoch 49/300 - Train Loss: 0.0539, Val Loss: 0.0680\n",
      "Epoch 50/300 - Train Loss: 0.0524, Val Loss: 0.0733\n",
      "Epoch 51/300 - Train Loss: 0.0540, Val Loss: 0.0689\n",
      "Epoch 52/300 - Train Loss: 0.0531, Val Loss: 0.0673\n",
      "Epoch 53/300 - Train Loss: 0.0526, Val Loss: 0.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 17:21:39,041] Trial 271 finished with value: 0.973391649031107 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.14316248361198972, 'learning_rate': 6.584911088302347e-05, 'batch_size': 32, 'weight_decay': 5.051446545978423e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300 - Train Loss: 0.0499, Val Loss: 0.0749\n",
      "Early stopping at epoch 54\n",
      "Macro F1 Score: 0.9734, Macro Precision: 0.9742, Macro Recall: 0.9726\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 273\n",
      "Training with F1=32, F2=16, D=8, dropout=0.13729052093636562, LR=4.852181766311478e-05, BS=32, WD=6.52340469155681e-05\n",
      "Epoch 1/300 - Train Loss: 0.3366, Val Loss: 0.1290\n",
      "Epoch 2/300 - Train Loss: 0.1336, Val Loss: 0.0961\n",
      "Epoch 3/300 - Train Loss: 0.1095, Val Loss: 0.0810\n",
      "Epoch 4/300 - Train Loss: 0.1023, Val Loss: 0.0951\n",
      "Epoch 5/300 - Train Loss: 0.0997, Val Loss: 0.0899\n",
      "Epoch 6/300 - Train Loss: 0.0936, Val Loss: 0.0759\n",
      "Epoch 7/300 - Train Loss: 0.0910, Val Loss: 0.0771\n",
      "Epoch 8/300 - Train Loss: 0.0897, Val Loss: 0.0697\n",
      "Epoch 9/300 - Train Loss: 0.0891, Val Loss: 0.0731\n",
      "Epoch 10/300 - Train Loss: 0.0855, Val Loss: 0.0869\n",
      "Epoch 11/300 - Train Loss: 0.0850, Val Loss: 0.0686\n",
      "Epoch 12/300 - Train Loss: 0.0813, Val Loss: 0.0672\n",
      "Epoch 13/300 - Train Loss: 0.0815, Val Loss: 0.0724\n",
      "Epoch 14/300 - Train Loss: 0.0806, Val Loss: 0.0714\n",
      "Epoch 15/300 - Train Loss: 0.0821, Val Loss: 0.0674\n",
      "Epoch 16/300 - Train Loss: 0.0807, Val Loss: 0.0721\n",
      "Epoch 17/300 - Train Loss: 0.0798, Val Loss: 0.0717\n",
      "Epoch 18/300 - Train Loss: 0.0765, Val Loss: 0.0682\n",
      "Epoch 19/300 - Train Loss: 0.0759, Val Loss: 0.0726\n",
      "Epoch 20/300 - Train Loss: 0.0765, Val Loss: 0.0844\n",
      "Epoch 21/300 - Train Loss: 0.0799, Val Loss: 0.0754\n",
      "Epoch 22/300 - Train Loss: 0.0764, Val Loss: 0.0742\n",
      "Epoch 23/300 - Train Loss: 0.0766, Val Loss: 0.0742\n",
      "Epoch 24/300 - Train Loss: 0.0741, Val Loss: 0.0718\n",
      "Epoch 25/300 - Train Loss: 0.0723, Val Loss: 0.0658\n",
      "Epoch 26/300 - Train Loss: 0.0755, Val Loss: 0.0710\n",
      "Epoch 27/300 - Train Loss: 0.0751, Val Loss: 0.0789\n",
      "Epoch 28/300 - Train Loss: 0.0739, Val Loss: 0.0790\n",
      "Epoch 29/300 - Train Loss: 0.0714, Val Loss: 0.0649\n",
      "Epoch 30/300 - Train Loss: 0.0721, Val Loss: 0.0731\n",
      "Epoch 31/300 - Train Loss: 0.0717, Val Loss: 0.0682\n",
      "Epoch 32/300 - Train Loss: 0.0709, Val Loss: 0.0655\n",
      "Epoch 33/300 - Train Loss: 0.0715, Val Loss: 0.0744\n",
      "Epoch 34/300 - Train Loss: 0.0722, Val Loss: 0.0704\n",
      "Epoch 35/300 - Train Loss: 0.0704, Val Loss: 0.0658\n",
      "Epoch 36/300 - Train Loss: 0.0703, Val Loss: 0.0685\n",
      "Epoch 37/300 - Train Loss: 0.0690, Val Loss: 0.0678\n",
      "Epoch 38/300 - Train Loss: 0.0699, Val Loss: 0.0747\n",
      "Epoch 39/300 - Train Loss: 0.0698, Val Loss: 0.0673\n",
      "Epoch 40/300 - Train Loss: 0.0680, Val Loss: 0.0688\n",
      "Epoch 41/300 - Train Loss: 0.0683, Val Loss: 0.0629\n",
      "Epoch 42/300 - Train Loss: 0.0698, Val Loss: 0.0838\n",
      "Epoch 43/300 - Train Loss: 0.0673, Val Loss: 0.0667\n",
      "Epoch 44/300 - Train Loss: 0.0668, Val Loss: 0.0728\n",
      "Epoch 45/300 - Train Loss: 0.0666, Val Loss: 0.0661\n",
      "Epoch 46/300 - Train Loss: 0.0667, Val Loss: 0.0667\n",
      "Epoch 47/300 - Train Loss: 0.0645, Val Loss: 0.0687\n",
      "Epoch 48/300 - Train Loss: 0.0659, Val Loss: 0.0681\n",
      "Epoch 49/300 - Train Loss: 0.0668, Val Loss: 0.0669\n",
      "Epoch 50/300 - Train Loss: 0.0646, Val Loss: 0.0649\n",
      "Epoch 51/300 - Train Loss: 0.0681, Val Loss: 0.0822\n",
      "Epoch 52/300 - Train Loss: 0.0636, Val Loss: 0.0695\n",
      "Epoch 53/300 - Train Loss: 0.0617, Val Loss: 0.0685\n",
      "Epoch 54/300 - Train Loss: 0.0637, Val Loss: 0.0689\n",
      "Epoch 55/300 - Train Loss: 0.0636, Val Loss: 0.0648\n",
      "Epoch 56/300 - Train Loss: 0.0620, Val Loss: 0.0640\n",
      "Epoch 57/300 - Train Loss: 0.0614, Val Loss: 0.0647\n",
      "Epoch 58/300 - Train Loss: 0.0608, Val Loss: 0.0645\n",
      "Epoch 59/300 - Train Loss: 0.0633, Val Loss: 0.0672\n",
      "Epoch 60/300 - Train Loss: 0.0630, Val Loss: 0.0654\n",
      "Epoch 61/300 - Train Loss: 0.0594, Val Loss: 0.0706\n",
      "Epoch 62/300 - Train Loss: 0.0619, Val Loss: 0.0625\n",
      "Epoch 63/300 - Train Loss: 0.0617, Val Loss: 0.0635\n",
      "Epoch 64/300 - Train Loss: 0.0621, Val Loss: 0.0654\n",
      "Epoch 65/300 - Train Loss: 0.0612, Val Loss: 0.0627\n",
      "Epoch 66/300 - Train Loss: 0.0586, Val Loss: 0.0658\n",
      "Epoch 67/300 - Train Loss: 0.0611, Val Loss: 0.0636\n",
      "Epoch 68/300 - Train Loss: 0.0601, Val Loss: 0.0674\n",
      "Epoch 69/300 - Train Loss: 0.0602, Val Loss: 0.0719\n",
      "Epoch 70/300 - Train Loss: 0.0587, Val Loss: 0.0678\n",
      "Epoch 71/300 - Train Loss: 0.0586, Val Loss: 0.0646\n",
      "Epoch 72/300 - Train Loss: 0.0613, Val Loss: 0.0649\n",
      "Epoch 73/300 - Train Loss: 0.0576, Val Loss: 0.0701\n",
      "Epoch 74/300 - Train Loss: 0.0572, Val Loss: 0.0709\n",
      "Epoch 75/300 - Train Loss: 0.0585, Val Loss: 0.0642\n",
      "Epoch 76/300 - Train Loss: 0.0574, Val Loss: 0.0707\n",
      "Epoch 77/300 - Train Loss: 0.0574, Val Loss: 0.0671\n",
      "Epoch 78/300 - Train Loss: 0.0556, Val Loss: 0.0730\n",
      "Epoch 79/300 - Train Loss: 0.0566, Val Loss: 0.0634\n",
      "Epoch 80/300 - Train Loss: 0.0601, Val Loss: 0.0642\n",
      "Epoch 81/300 - Train Loss: 0.0575, Val Loss: 0.0661\n",
      "Epoch 82/300 - Train Loss: 0.0571, Val Loss: 0.0678\n",
      "Epoch 83/300 - Train Loss: 0.0577, Val Loss: 0.0651\n",
      "Epoch 84/300 - Train Loss: 0.0566, Val Loss: 0.0690\n",
      "Epoch 85/300 - Train Loss: 0.0550, Val Loss: 0.0670\n",
      "Epoch 86/300 - Train Loss: 0.0550, Val Loss: 0.0686\n",
      "Epoch 87/300 - Train Loss: 0.0553, Val Loss: 0.0707\n",
      "Epoch 88/300 - Train Loss: 0.0553, Val Loss: 0.0659\n",
      "Epoch 89/300 - Train Loss: 0.0555, Val Loss: 0.0661\n",
      "Epoch 90/300 - Train Loss: 0.0585, Val Loss: 0.0686\n",
      "Epoch 91/300 - Train Loss: 0.0547, Val Loss: 0.0638\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 17:31:19,135] Trial 272 finished with value: 0.9698477184932658 and parameters: {'F1': 32, 'F2': 16, 'D': 8, 'dropout': 0.13729052093636562, 'learning_rate': 4.852181766311478e-05, 'batch_size': 32, 'weight_decay': 6.52340469155681e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/300 - Train Loss: 0.0540, Val Loss: 0.0652\n",
      "Early stopping at epoch 92\n",
      "Macro F1 Score: 0.9698, Macro Precision: 0.9635, Macro Recall: 0.9766\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 274\n",
      "Training with F1=32, F2=32, D=8, dropout=0.1188162767955914, LR=5.4668217929199574e-05, BS=32, WD=4.074792668894867e-05\n",
      "Epoch 1/300 - Train Loss: 0.2899, Val Loss: 0.1237\n",
      "Epoch 2/300 - Train Loss: 0.1196, Val Loss: 0.0872\n",
      "Epoch 3/300 - Train Loss: 0.0984, Val Loss: 0.0858\n",
      "Epoch 4/300 - Train Loss: 0.0941, Val Loss: 0.0857\n",
      "Epoch 5/300 - Train Loss: 0.0895, Val Loss: 0.0844\n",
      "Epoch 6/300 - Train Loss: 0.0874, Val Loss: 0.0763\n",
      "Epoch 7/300 - Train Loss: 0.0861, Val Loss: 0.0958\n",
      "Epoch 8/300 - Train Loss: 0.0844, Val Loss: 0.0760\n",
      "Epoch 9/300 - Train Loss: 0.0811, Val Loss: 0.0725\n",
      "Epoch 10/300 - Train Loss: 0.0783, Val Loss: 0.0726\n",
      "Epoch 11/300 - Train Loss: 0.0798, Val Loss: 0.0662\n",
      "Epoch 12/300 - Train Loss: 0.0798, Val Loss: 0.0706\n",
      "Epoch 13/300 - Train Loss: 0.0766, Val Loss: 0.0711\n",
      "Epoch 14/300 - Train Loss: 0.0761, Val Loss: 0.0728\n",
      "Epoch 15/300 - Train Loss: 0.0743, Val Loss: 0.0664\n",
      "Epoch 16/300 - Train Loss: 0.0735, Val Loss: 0.0810\n",
      "Epoch 17/300 - Train Loss: 0.0728, Val Loss: 0.0768\n",
      "Epoch 18/300 - Train Loss: 0.0724, Val Loss: 0.0658\n",
      "Epoch 19/300 - Train Loss: 0.0710, Val Loss: 0.0763\n",
      "Epoch 20/300 - Train Loss: 0.0695, Val Loss: 0.0694\n",
      "Epoch 21/300 - Train Loss: 0.0726, Val Loss: 0.0769\n",
      "Epoch 22/300 - Train Loss: 0.0692, Val Loss: 0.0693\n",
      "Epoch 23/300 - Train Loss: 0.0681, Val Loss: 0.0670\n",
      "Epoch 24/300 - Train Loss: 0.0687, Val Loss: 0.0766\n",
      "Epoch 25/300 - Train Loss: 0.0683, Val Loss: 0.0744\n",
      "Epoch 26/300 - Train Loss: 0.0677, Val Loss: 0.0700\n",
      "Epoch 27/300 - Train Loss: 0.0670, Val Loss: 0.0689\n",
      "Epoch 28/300 - Train Loss: 0.0654, Val Loss: 0.0680\n",
      "Epoch 29/300 - Train Loss: 0.0654, Val Loss: 0.0673\n",
      "Epoch 30/300 - Train Loss: 0.0664, Val Loss: 0.0658\n",
      "Epoch 31/300 - Train Loss: 0.0644, Val Loss: 0.0706\n",
      "Epoch 32/300 - Train Loss: 0.0619, Val Loss: 0.0668\n",
      "Epoch 33/300 - Train Loss: 0.0631, Val Loss: 0.0644\n",
      "Epoch 34/300 - Train Loss: 0.0628, Val Loss: 0.0848\n",
      "Epoch 35/300 - Train Loss: 0.0628, Val Loss: 0.0668\n",
      "Epoch 36/300 - Train Loss: 0.0599, Val Loss: 0.0729\n",
      "Epoch 37/300 - Train Loss: 0.0590, Val Loss: 0.0682\n",
      "Epoch 38/300 - Train Loss: 0.0596, Val Loss: 0.0728\n",
      "Epoch 39/300 - Train Loss: 0.0576, Val Loss: 0.0673\n",
      "Epoch 40/300 - Train Loss: 0.0595, Val Loss: 0.0678\n",
      "Epoch 41/300 - Train Loss: 0.0580, Val Loss: 0.0655\n",
      "Epoch 42/300 - Train Loss: 0.0580, Val Loss: 0.0654\n",
      "Epoch 43/300 - Train Loss: 0.0560, Val Loss: 0.0702\n",
      "Epoch 44/300 - Train Loss: 0.0564, Val Loss: 0.0652\n",
      "Epoch 45/300 - Train Loss: 0.0562, Val Loss: 0.0770\n",
      "Epoch 46/300 - Train Loss: 0.0549, Val Loss: 0.0714\n",
      "Epoch 47/300 - Train Loss: 0.0537, Val Loss: 0.0693\n",
      "Epoch 48/300 - Train Loss: 0.0541, Val Loss: 0.0668\n",
      "Epoch 49/300 - Train Loss: 0.0530, Val Loss: 0.0729\n",
      "Epoch 50/300 - Train Loss: 0.0536, Val Loss: 0.0824\n",
      "Epoch 51/300 - Train Loss: 0.0531, Val Loss: 0.0830\n",
      "Epoch 52/300 - Train Loss: 0.0539, Val Loss: 0.0757\n",
      "Epoch 53/300 - Train Loss: 0.0502, Val Loss: 0.0733\n",
      "Epoch 54/300 - Train Loss: 0.0510, Val Loss: 0.0733\n",
      "Epoch 55/300 - Train Loss: 0.0506, Val Loss: 0.0661\n",
      "Epoch 56/300 - Train Loss: 0.0503, Val Loss: 0.0692\n",
      "Epoch 57/300 - Train Loss: 0.0501, Val Loss: 0.0719\n",
      "Epoch 58/300 - Train Loss: 0.0485, Val Loss: 0.0720\n",
      "Epoch 59/300 - Train Loss: 0.0482, Val Loss: 0.0735\n",
      "Epoch 60/300 - Train Loss: 0.0507, Val Loss: 0.0685\n",
      "Epoch 61/300 - Train Loss: 0.0491, Val Loss: 0.0707\n",
      "Epoch 62/300 - Train Loss: 0.0464, Val Loss: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 17:38:09,151] Trial 273 finished with value: 0.9677848090411757 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.1188162767955914, 'learning_rate': 5.4668217929199574e-05, 'batch_size': 32, 'weight_decay': 4.074792668894867e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300 - Train Loss: 0.0480, Val Loss: 0.0701\n",
      "Early stopping at epoch 63\n",
      "Macro F1 Score: 0.9678, Macro Precision: 0.9638, Macro Recall: 0.9719\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 275\n",
      "Training with F1=32, F2=32, D=8, dropout=0.16570248104956084, LR=6.327434489381173e-05, BS=32, WD=5.846099527575734e-05\n",
      "Epoch 1/300 - Train Loss: 0.2454, Val Loss: 0.0957\n",
      "Epoch 2/300 - Train Loss: 0.1138, Val Loss: 0.0789\n",
      "Epoch 3/300 - Train Loss: 0.0980, Val Loss: 0.0751\n",
      "Epoch 4/300 - Train Loss: 0.0924, Val Loss: 0.0709\n",
      "Epoch 5/300 - Train Loss: 0.0908, Val Loss: 0.0763\n",
      "Epoch 6/300 - Train Loss: 0.0861, Val Loss: 0.0774\n",
      "Epoch 7/300 - Train Loss: 0.0833, Val Loss: 0.0672\n",
      "Epoch 8/300 - Train Loss: 0.0825, Val Loss: 0.0711\n",
      "Epoch 9/300 - Train Loss: 0.0802, Val Loss: 0.0730\n",
      "Epoch 10/300 - Train Loss: 0.0812, Val Loss: 0.0719\n",
      "Epoch 11/300 - Train Loss: 0.0770, Val Loss: 0.0667\n",
      "Epoch 12/300 - Train Loss: 0.0778, Val Loss: 0.0734\n",
      "Epoch 13/300 - Train Loss: 0.0757, Val Loss: 0.0785\n",
      "Epoch 14/300 - Train Loss: 0.0763, Val Loss: 0.0722\n",
      "Epoch 15/300 - Train Loss: 0.0743, Val Loss: 0.0845\n",
      "Epoch 16/300 - Train Loss: 0.0724, Val Loss: 0.0677\n",
      "Epoch 17/300 - Train Loss: 0.0733, Val Loss: 0.0725\n",
      "Epoch 18/300 - Train Loss: 0.0704, Val Loss: 0.0738\n",
      "Epoch 19/300 - Train Loss: 0.0706, Val Loss: 0.0684\n",
      "Epoch 20/300 - Train Loss: 0.0728, Val Loss: 0.0968\n",
      "Epoch 21/300 - Train Loss: 0.0714, Val Loss: 0.0653\n",
      "Epoch 22/300 - Train Loss: 0.0692, Val Loss: 0.0637\n",
      "Epoch 23/300 - Train Loss: 0.0676, Val Loss: 0.0722\n",
      "Epoch 24/300 - Train Loss: 0.0697, Val Loss: 0.0699\n",
      "Epoch 25/300 - Train Loss: 0.0683, Val Loss: 0.0694\n",
      "Epoch 26/300 - Train Loss: 0.0646, Val Loss: 0.0669\n",
      "Epoch 27/300 - Train Loss: 0.0651, Val Loss: 0.0665\n",
      "Epoch 28/300 - Train Loss: 0.0649, Val Loss: 0.0690\n",
      "Epoch 29/300 - Train Loss: 0.0670, Val Loss: 0.0728\n",
      "Epoch 30/300 - Train Loss: 0.0659, Val Loss: 0.0673\n",
      "Epoch 31/300 - Train Loss: 0.0654, Val Loss: 0.0964\n",
      "Epoch 32/300 - Train Loss: 0.0623, Val Loss: 0.0719\n",
      "Epoch 33/300 - Train Loss: 0.0609, Val Loss: 0.0663\n",
      "Epoch 34/300 - Train Loss: 0.0606, Val Loss: 0.0846\n",
      "Epoch 35/300 - Train Loss: 0.0615, Val Loss: 0.0720\n",
      "Epoch 36/300 - Train Loss: 0.0603, Val Loss: 0.0677\n",
      "Epoch 37/300 - Train Loss: 0.0601, Val Loss: 0.0665\n",
      "Epoch 38/300 - Train Loss: 0.0604, Val Loss: 0.0658\n",
      "Epoch 39/300 - Train Loss: 0.0588, Val Loss: 0.0678\n",
      "Epoch 40/300 - Train Loss: 0.0613, Val Loss: 0.0723\n",
      "Epoch 41/300 - Train Loss: 0.0602, Val Loss: 0.0640\n",
      "Epoch 42/300 - Train Loss: 0.0594, Val Loss: 0.0671\n",
      "Epoch 43/300 - Train Loss: 0.0593, Val Loss: 0.0685\n",
      "Epoch 44/300 - Train Loss: 0.0563, Val Loss: 0.0703\n",
      "Epoch 45/300 - Train Loss: 0.0589, Val Loss: 0.0655\n",
      "Epoch 46/300 - Train Loss: 0.0575, Val Loss: 0.0693\n",
      "Epoch 47/300 - Train Loss: 0.0552, Val Loss: 0.0736\n",
      "Epoch 48/300 - Train Loss: 0.0560, Val Loss: 0.0741\n",
      "Epoch 49/300 - Train Loss: 0.0542, Val Loss: 0.0629\n",
      "Epoch 50/300 - Train Loss: 0.0554, Val Loss: 0.0657\n",
      "Epoch 51/300 - Train Loss: 0.0549, Val Loss: 0.0685\n",
      "Epoch 52/300 - Train Loss: 0.0540, Val Loss: 0.0664\n",
      "Epoch 53/300 - Train Loss: 0.0582, Val Loss: 0.0760\n",
      "Epoch 54/300 - Train Loss: 0.0541, Val Loss: 0.0750\n",
      "Epoch 55/300 - Train Loss: 0.0510, Val Loss: 0.0640\n",
      "Epoch 56/300 - Train Loss: 0.0521, Val Loss: 0.0732\n",
      "Epoch 57/300 - Train Loss: 0.0512, Val Loss: 0.0753\n",
      "Epoch 58/300 - Train Loss: 0.0523, Val Loss: 0.0684\n",
      "Epoch 59/300 - Train Loss: 0.0534, Val Loss: 0.0675\n",
      "Epoch 60/300 - Train Loss: 0.0506, Val Loss: 0.0712\n",
      "Epoch 61/300 - Train Loss: 0.0520, Val Loss: 0.0659\n",
      "Epoch 62/300 - Train Loss: 0.0534, Val Loss: 0.0654\n",
      "Epoch 63/300 - Train Loss: 0.0495, Val Loss: 0.0727\n",
      "Epoch 64/300 - Train Loss: 0.0491, Val Loss: 0.0724\n",
      "Epoch 65/300 - Train Loss: 0.0491, Val Loss: 0.0683\n",
      "Epoch 66/300 - Train Loss: 0.0492, Val Loss: 0.0706\n",
      "Epoch 67/300 - Train Loss: 0.0471, Val Loss: 0.0681\n",
      "Epoch 68/300 - Train Loss: 0.0507, Val Loss: 0.0674\n",
      "Epoch 69/300 - Train Loss: 0.0470, Val Loss: 0.0684\n",
      "Epoch 70/300 - Train Loss: 0.0494, Val Loss: 0.0731\n",
      "Epoch 71/300 - Train Loss: 0.0463, Val Loss: 0.0683\n",
      "Epoch 72/300 - Train Loss: 0.0464, Val Loss: 0.0696\n",
      "Epoch 73/300 - Train Loss: 0.0454, Val Loss: 0.0691\n",
      "Epoch 74/300 - Train Loss: 0.0480, Val Loss: 0.0704\n",
      "Epoch 75/300 - Train Loss: 0.0459, Val Loss: 0.0686\n",
      "Epoch 76/300 - Train Loss: 0.0450, Val Loss: 0.0732\n",
      "Epoch 77/300 - Train Loss: 0.0458, Val Loss: 0.0700\n",
      "Epoch 78/300 - Train Loss: 0.0468, Val Loss: 0.0673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 17:46:43,072] Trial 274 finished with value: 0.9637096593778233 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.16570248104956084, 'learning_rate': 6.327434489381173e-05, 'batch_size': 32, 'weight_decay': 5.846099527575734e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/300 - Train Loss: 0.0454, Val Loss: 0.0759\n",
      "Early stopping at epoch 79\n",
      "Macro F1 Score: 0.9637, Macro Precision: 0.9619, Macro Recall: 0.9656\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.93      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 276\n",
      "Training with F1=32, F2=32, D=8, dropout=0.19199130650389454, LR=9.641628528555152e-05, BS=32, WD=3.3114631943270935e-05\n",
      "Epoch 1/300 - Train Loss: 0.2242, Val Loss: 0.1008\n",
      "Epoch 2/300 - Train Loss: 0.1051, Val Loss: 0.0840\n",
      "Epoch 3/300 - Train Loss: 0.0966, Val Loss: 0.0837\n",
      "Epoch 4/300 - Train Loss: 0.0903, Val Loss: 0.0820\n",
      "Epoch 5/300 - Train Loss: 0.0896, Val Loss: 0.0705\n",
      "Epoch 6/300 - Train Loss: 0.0853, Val Loss: 0.0738\n",
      "Epoch 7/300 - Train Loss: 0.0826, Val Loss: 0.0794\n",
      "Epoch 8/300 - Train Loss: 0.0818, Val Loss: 0.0680\n",
      "Epoch 9/300 - Train Loss: 0.0803, Val Loss: 0.0723\n",
      "Epoch 10/300 - Train Loss: 0.0789, Val Loss: 0.0775\n",
      "Epoch 11/300 - Train Loss: 0.0765, Val Loss: 0.0697\n",
      "Epoch 12/300 - Train Loss: 0.0748, Val Loss: 0.0659\n",
      "Epoch 13/300 - Train Loss: 0.0739, Val Loss: 0.0693\n",
      "Epoch 14/300 - Train Loss: 0.0748, Val Loss: 0.0675\n",
      "Epoch 15/300 - Train Loss: 0.0741, Val Loss: 0.0706\n",
      "Epoch 16/300 - Train Loss: 0.0719, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.0720, Val Loss: 0.0744\n",
      "Epoch 18/300 - Train Loss: 0.0704, Val Loss: 0.0729\n",
      "Epoch 19/300 - Train Loss: 0.0742, Val Loss: 0.0657\n",
      "Epoch 20/300 - Train Loss: 0.0681, Val Loss: 0.0732\n",
      "Epoch 21/300 - Train Loss: 0.0675, Val Loss: 0.0729\n",
      "Epoch 22/300 - Train Loss: 0.0670, Val Loss: 0.0742\n",
      "Epoch 23/300 - Train Loss: 0.0657, Val Loss: 0.0763\n",
      "Epoch 24/300 - Train Loss: 0.0669, Val Loss: 0.0699\n",
      "Epoch 25/300 - Train Loss: 0.0647, Val Loss: 0.0641\n",
      "Epoch 26/300 - Train Loss: 0.0634, Val Loss: 0.0687\n",
      "Epoch 27/300 - Train Loss: 0.0629, Val Loss: 0.0685\n",
      "Epoch 28/300 - Train Loss: 0.0630, Val Loss: 0.0706\n",
      "Epoch 29/300 - Train Loss: 0.0640, Val Loss: 0.0714\n",
      "Epoch 30/300 - Train Loss: 0.0655, Val Loss: 0.0743\n",
      "Epoch 31/300 - Train Loss: 0.0617, Val Loss: 0.0679\n",
      "Epoch 32/300 - Train Loss: 0.0587, Val Loss: 0.0661\n",
      "Epoch 33/300 - Train Loss: 0.0597, Val Loss: 0.0696\n",
      "Epoch 34/300 - Train Loss: 0.0584, Val Loss: 0.0736\n",
      "Epoch 35/300 - Train Loss: 0.0600, Val Loss: 0.0722\n",
      "Epoch 36/300 - Train Loss: 0.0578, Val Loss: 0.0789\n",
      "Epoch 37/300 - Train Loss: 0.0584, Val Loss: 0.0670\n",
      "Epoch 38/300 - Train Loss: 0.0571, Val Loss: 0.0744\n",
      "Epoch 39/300 - Train Loss: 0.0569, Val Loss: 0.0734\n",
      "Epoch 40/300 - Train Loss: 0.0569, Val Loss: 0.0648\n",
      "Epoch 41/300 - Train Loss: 0.0576, Val Loss: 0.0703\n",
      "Epoch 42/300 - Train Loss: 0.0545, Val Loss: 0.0681\n",
      "Epoch 43/300 - Train Loss: 0.0527, Val Loss: 0.0669\n",
      "Epoch 44/300 - Train Loss: 0.0562, Val Loss: 0.0679\n",
      "Epoch 45/300 - Train Loss: 0.0561, Val Loss: 0.0742\n",
      "Epoch 46/300 - Train Loss: 0.0531, Val Loss: 0.0680\n",
      "Epoch 47/300 - Train Loss: 0.0526, Val Loss: 0.0685\n",
      "Epoch 48/300 - Train Loss: 0.0513, Val Loss: 0.0702\n",
      "Epoch 49/300 - Train Loss: 0.0521, Val Loss: 0.0697\n",
      "Epoch 50/300 - Train Loss: 0.0516, Val Loss: 0.0709\n",
      "Epoch 51/300 - Train Loss: 0.0504, Val Loss: 0.0715\n",
      "Epoch 52/300 - Train Loss: 0.0487, Val Loss: 0.0729\n",
      "Epoch 53/300 - Train Loss: 0.0513, Val Loss: 0.0687\n",
      "Epoch 54/300 - Train Loss: 0.0481, Val Loss: 0.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 17:52:40,676] Trial 275 finished with value: 0.9672896763974274 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.19199130650389454, 'learning_rate': 9.641628528555152e-05, 'batch_size': 32, 'weight_decay': 3.3114631943270935e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300 - Train Loss: 0.0489, Val Loss: 0.0696\n",
      "Early stopping at epoch 55\n",
      "Macro F1 Score: 0.9673, Macro Precision: 0.9634, Macro Recall: 0.9714\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 277\n",
      "Training with F1=32, F2=32, D=8, dropout=0.19974176088850593, LR=8.427833688497094e-05, BS=32, WD=7.961238586363137e-05\n",
      "Epoch 1/300 - Train Loss: 0.2537, Val Loss: 0.1435\n",
      "Epoch 2/300 - Train Loss: 0.1216, Val Loss: 0.0908\n",
      "Epoch 3/300 - Train Loss: 0.1008, Val Loss: 0.0764\n",
      "Epoch 4/300 - Train Loss: 0.0962, Val Loss: 0.0864\n",
      "Epoch 5/300 - Train Loss: 0.0925, Val Loss: 0.0724\n",
      "Epoch 6/300 - Train Loss: 0.0878, Val Loss: 0.0728\n",
      "Epoch 7/300 - Train Loss: 0.0854, Val Loss: 0.0738\n",
      "Epoch 8/300 - Train Loss: 0.0840, Val Loss: 0.0912\n",
      "Epoch 9/300 - Train Loss: 0.0822, Val Loss: 0.0779\n",
      "Epoch 10/300 - Train Loss: 0.0810, Val Loss: 0.0699\n",
      "Epoch 11/300 - Train Loss: 0.0782, Val Loss: 0.0824\n",
      "Epoch 12/300 - Train Loss: 0.0768, Val Loss: 0.0714\n",
      "Epoch 13/300 - Train Loss: 0.0786, Val Loss: 0.0681\n",
      "Epoch 14/300 - Train Loss: 0.0770, Val Loss: 0.0758\n",
      "Epoch 15/300 - Train Loss: 0.0775, Val Loss: 0.0727\n",
      "Epoch 16/300 - Train Loss: 0.0770, Val Loss: 0.0724\n",
      "Epoch 17/300 - Train Loss: 0.0729, Val Loss: 0.0722\n",
      "Epoch 18/300 - Train Loss: 0.0746, Val Loss: 0.0675\n",
      "Epoch 19/300 - Train Loss: 0.0744, Val Loss: 0.0715\n",
      "Epoch 20/300 - Train Loss: 0.0720, Val Loss: 0.0651\n",
      "Epoch 21/300 - Train Loss: 0.0692, Val Loss: 0.0686\n",
      "Epoch 22/300 - Train Loss: 0.0700, Val Loss: 0.0861\n",
      "Epoch 23/300 - Train Loss: 0.0706, Val Loss: 0.0707\n",
      "Epoch 24/300 - Train Loss: 0.0705, Val Loss: 0.0626\n",
      "Epoch 25/300 - Train Loss: 0.0703, Val Loss: 0.0614\n",
      "Epoch 26/300 - Train Loss: 0.0692, Val Loss: 0.0654\n",
      "Epoch 27/300 - Train Loss: 0.0683, Val Loss: 0.0700\n",
      "Epoch 28/300 - Train Loss: 0.0673, Val Loss: 0.0662\n",
      "Epoch 29/300 - Train Loss: 0.0689, Val Loss: 0.0653\n",
      "Epoch 30/300 - Train Loss: 0.0647, Val Loss: 0.0670\n",
      "Epoch 31/300 - Train Loss: 0.0685, Val Loss: 0.0649\n",
      "Epoch 32/300 - Train Loss: 0.0635, Val Loss: 0.0734\n",
      "Epoch 33/300 - Train Loss: 0.0641, Val Loss: 0.0676\n",
      "Epoch 34/300 - Train Loss: 0.0633, Val Loss: 0.0695\n",
      "Epoch 35/300 - Train Loss: 0.0635, Val Loss: 0.0700\n",
      "Epoch 36/300 - Train Loss: 0.0614, Val Loss: 0.0669\n",
      "Epoch 37/300 - Train Loss: 0.0641, Val Loss: 0.0720\n",
      "Epoch 38/300 - Train Loss: 0.0636, Val Loss: 0.0669\n",
      "Epoch 39/300 - Train Loss: 0.0625, Val Loss: 0.0641\n",
      "Epoch 40/300 - Train Loss: 0.0604, Val Loss: 0.0674\n",
      "Epoch 41/300 - Train Loss: 0.0600, Val Loss: 0.0681\n",
      "Epoch 42/300 - Train Loss: 0.0608, Val Loss: 0.0691\n",
      "Epoch 43/300 - Train Loss: 0.0588, Val Loss: 0.0679\n",
      "Epoch 44/300 - Train Loss: 0.0579, Val Loss: 0.0685\n",
      "Epoch 45/300 - Train Loss: 0.0584, Val Loss: 0.0942\n",
      "Epoch 46/300 - Train Loss: 0.0582, Val Loss: 0.0676\n",
      "Epoch 47/300 - Train Loss: 0.0595, Val Loss: 0.0663\n",
      "Epoch 48/300 - Train Loss: 0.0562, Val Loss: 0.0637\n",
      "Epoch 49/300 - Train Loss: 0.0560, Val Loss: 0.0684\n",
      "Epoch 50/300 - Train Loss: 0.0578, Val Loss: 0.0678\n",
      "Epoch 51/300 - Train Loss: 0.0580, Val Loss: 0.0716\n",
      "Epoch 52/300 - Train Loss: 0.0548, Val Loss: 0.0639\n",
      "Epoch 53/300 - Train Loss: 0.0560, Val Loss: 0.0693\n",
      "Epoch 54/300 - Train Loss: 0.0582, Val Loss: 0.0697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 17:58:38,391] Trial 276 finished with value: 0.9738218868131318 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.19974176088850593, 'learning_rate': 8.427833688497094e-05, 'batch_size': 32, 'weight_decay': 7.961238586363137e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300 - Train Loss: 0.0558, Val Loss: 0.0689\n",
      "Early stopping at epoch 55\n",
      "Macro F1 Score: 0.9738, Macro Precision: 0.9742, Macro Recall: 0.9735\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 278\n",
      "Training with F1=32, F2=32, D=8, dropout=0.17685435209146028, LR=7.562590705853003e-05, BS=32, WD=7.379336289302619e-05\n",
      "Epoch 1/300 - Train Loss: 0.2527, Val Loss: 0.1017\n",
      "Epoch 2/300 - Train Loss: 0.1111, Val Loss: 0.0918\n",
      "Epoch 3/300 - Train Loss: 0.0984, Val Loss: 0.1008\n",
      "Epoch 4/300 - Train Loss: 0.0987, Val Loss: 0.0820\n",
      "Epoch 5/300 - Train Loss: 0.0925, Val Loss: 0.0849\n",
      "Epoch 6/300 - Train Loss: 0.0883, Val Loss: 0.0839\n",
      "Epoch 7/300 - Train Loss: 0.0861, Val Loss: 0.0752\n",
      "Epoch 8/300 - Train Loss: 0.0838, Val Loss: 0.0750\n",
      "Epoch 9/300 - Train Loss: 0.0832, Val Loss: 0.0782\n",
      "Epoch 10/300 - Train Loss: 0.0837, Val Loss: 0.0731\n",
      "Epoch 11/300 - Train Loss: 0.0799, Val Loss: 0.0720\n",
      "Epoch 12/300 - Train Loss: 0.0756, Val Loss: 0.0680\n",
      "Epoch 13/300 - Train Loss: 0.0768, Val Loss: 0.0721\n",
      "Epoch 14/300 - Train Loss: 0.0793, Val Loss: 0.0684\n",
      "Epoch 15/300 - Train Loss: 0.0774, Val Loss: 0.0763\n",
      "Epoch 16/300 - Train Loss: 0.0743, Val Loss: 0.0687\n",
      "Epoch 17/300 - Train Loss: 0.0749, Val Loss: 0.0699\n",
      "Epoch 18/300 - Train Loss: 0.0754, Val Loss: 0.0704\n",
      "Epoch 19/300 - Train Loss: 0.0738, Val Loss: 0.0660\n",
      "Epoch 20/300 - Train Loss: 0.0728, Val Loss: 0.0685\n",
      "Epoch 21/300 - Train Loss: 0.0703, Val Loss: 0.0667\n",
      "Epoch 22/300 - Train Loss: 0.0705, Val Loss: 0.0660\n",
      "Epoch 23/300 - Train Loss: 0.0708, Val Loss: 0.0687\n",
      "Epoch 24/300 - Train Loss: 0.0725, Val Loss: 0.0665\n",
      "Epoch 25/300 - Train Loss: 0.0673, Val Loss: 0.0680\n",
      "Epoch 26/300 - Train Loss: 0.0670, Val Loss: 0.0714\n",
      "Epoch 27/300 - Train Loss: 0.0673, Val Loss: 0.0643\n",
      "Epoch 28/300 - Train Loss: 0.0656, Val Loss: 0.0655\n",
      "Epoch 29/300 - Train Loss: 0.0680, Val Loss: 0.0756\n",
      "Epoch 30/300 - Train Loss: 0.0646, Val Loss: 0.0683\n",
      "Epoch 31/300 - Train Loss: 0.0658, Val Loss: 0.0685\n",
      "Epoch 32/300 - Train Loss: 0.0631, Val Loss: 0.0699\n",
      "Epoch 33/300 - Train Loss: 0.0657, Val Loss: 0.0738\n",
      "Epoch 34/300 - Train Loss: 0.0598, Val Loss: 0.0749\n",
      "Epoch 35/300 - Train Loss: 0.0612, Val Loss: 0.0678\n",
      "Epoch 36/300 - Train Loss: 0.0615, Val Loss: 0.0683\n",
      "Epoch 37/300 - Train Loss: 0.0618, Val Loss: 0.0683\n",
      "Epoch 38/300 - Train Loss: 0.0607, Val Loss: 0.0678\n",
      "Epoch 39/300 - Train Loss: 0.0593, Val Loss: 0.0677\n",
      "Epoch 40/300 - Train Loss: 0.0601, Val Loss: 0.0661\n",
      "Epoch 41/300 - Train Loss: 0.0579, Val Loss: 0.0673\n",
      "Epoch 42/300 - Train Loss: 0.0599, Val Loss: 0.0686\n",
      "Epoch 43/300 - Train Loss: 0.0570, Val Loss: 0.0642\n",
      "Epoch 44/300 - Train Loss: 0.0583, Val Loss: 0.0778\n",
      "Epoch 45/300 - Train Loss: 0.0577, Val Loss: 0.0676\n",
      "Epoch 46/300 - Train Loss: 0.0572, Val Loss: 0.0771\n",
      "Epoch 47/300 - Train Loss: 0.0580, Val Loss: 0.0697\n",
      "Epoch 48/300 - Train Loss: 0.0561, Val Loss: 0.0733\n",
      "Epoch 49/300 - Train Loss: 0.0544, Val Loss: 0.0708\n",
      "Epoch 50/300 - Train Loss: 0.0555, Val Loss: 0.0679\n",
      "Epoch 51/300 - Train Loss: 0.0548, Val Loss: 0.0712\n",
      "Epoch 52/300 - Train Loss: 0.0532, Val Loss: 0.0680\n",
      "Epoch 53/300 - Train Loss: 0.0564, Val Loss: 0.0659\n",
      "Epoch 54/300 - Train Loss: 0.0536, Val Loss: 0.0671\n",
      "Epoch 55/300 - Train Loss: 0.0533, Val Loss: 0.0655\n",
      "Epoch 56/300 - Train Loss: 0.0518, Val Loss: 0.0673\n",
      "Epoch 57/300 - Train Loss: 0.0508, Val Loss: 0.0692\n",
      "Epoch 58/300 - Train Loss: 0.0501, Val Loss: 0.0693\n",
      "Epoch 59/300 - Train Loss: 0.0515, Val Loss: 0.0712\n",
      "Epoch 60/300 - Train Loss: 0.0500, Val Loss: 0.0674\n",
      "Epoch 61/300 - Train Loss: 0.0494, Val Loss: 0.0691\n",
      "Epoch 62/300 - Train Loss: 0.0486, Val Loss: 0.0650\n",
      "Epoch 63/300 - Train Loss: 0.0476, Val Loss: 0.0739\n",
      "Epoch 64/300 - Train Loss: 0.0491, Val Loss: 0.0684\n",
      "Epoch 65/300 - Train Loss: 0.0476, Val Loss: 0.0832\n",
      "Epoch 66/300 - Train Loss: 0.0480, Val Loss: 0.0681\n",
      "Epoch 67/300 - Train Loss: 0.0481, Val Loss: 0.0711\n",
      "Epoch 68/300 - Train Loss: 0.0468, Val Loss: 0.0687\n",
      "Epoch 69/300 - Train Loss: 0.0469, Val Loss: 0.0713\n",
      "Epoch 70/300 - Train Loss: 0.0462, Val Loss: 0.0658\n",
      "Epoch 71/300 - Train Loss: 0.0456, Val Loss: 0.0701\n",
      "Epoch 72/300 - Train Loss: 0.0483, Val Loss: 0.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:06:33,135] Trial 277 finished with value: 0.9687406969958702 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.17685435209146028, 'learning_rate': 7.562590705853003e-05, 'batch_size': 32, 'weight_decay': 7.379336289302619e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/300 - Train Loss: 0.0477, Val Loss: 0.0716\n",
      "Early stopping at epoch 73\n",
      "Macro F1 Score: 0.9687, Macro Precision: 0.9720, Macro Recall: 0.9656\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 279\n",
      "Training with F1=32, F2=32, D=8, dropout=0.1399756778838307, LR=6.387820034244892e-05, BS=32, WD=8.253380099067363e-05\n",
      "Epoch 1/300 - Train Loss: 0.2468, Val Loss: 0.1215\n",
      "Epoch 2/300 - Train Loss: 0.1084, Val Loss: 0.0901\n",
      "Epoch 3/300 - Train Loss: 0.0973, Val Loss: 0.0852\n",
      "Epoch 4/300 - Train Loss: 0.0936, Val Loss: 0.0883\n",
      "Epoch 5/300 - Train Loss: 0.0887, Val Loss: 0.0760\n",
      "Epoch 6/300 - Train Loss: 0.0875, Val Loss: 0.0872\n",
      "Epoch 7/300 - Train Loss: 0.0827, Val Loss: 0.0837\n",
      "Epoch 8/300 - Train Loss: 0.0836, Val Loss: 0.0702\n",
      "Epoch 9/300 - Train Loss: 0.0825, Val Loss: 0.0740\n",
      "Epoch 10/300 - Train Loss: 0.0804, Val Loss: 0.0733\n",
      "Epoch 11/300 - Train Loss: 0.0800, Val Loss: 0.0762\n",
      "Epoch 12/300 - Train Loss: 0.0791, Val Loss: 0.0733\n",
      "Epoch 13/300 - Train Loss: 0.0766, Val Loss: 0.0770\n",
      "Epoch 14/300 - Train Loss: 0.0777, Val Loss: 0.0699\n",
      "Epoch 15/300 - Train Loss: 0.0740, Val Loss: 0.0730\n",
      "Epoch 16/300 - Train Loss: 0.0738, Val Loss: 0.0671\n",
      "Epoch 17/300 - Train Loss: 0.0710, Val Loss: 0.0699\n",
      "Epoch 18/300 - Train Loss: 0.0740, Val Loss: 0.0721\n",
      "Epoch 19/300 - Train Loss: 0.0713, Val Loss: 0.0708\n",
      "Epoch 20/300 - Train Loss: 0.0699, Val Loss: 0.0680\n",
      "Epoch 21/300 - Train Loss: 0.0693, Val Loss: 0.0688\n",
      "Epoch 22/300 - Train Loss: 0.0681, Val Loss: 0.0729\n",
      "Epoch 23/300 - Train Loss: 0.0681, Val Loss: 0.0744\n",
      "Epoch 24/300 - Train Loss: 0.0681, Val Loss: 0.0779\n",
      "Epoch 25/300 - Train Loss: 0.0677, Val Loss: 0.0702\n",
      "Epoch 26/300 - Train Loss: 0.0660, Val Loss: 0.0639\n",
      "Epoch 27/300 - Train Loss: 0.0668, Val Loss: 0.0741\n",
      "Epoch 28/300 - Train Loss: 0.0637, Val Loss: 0.0676\n",
      "Epoch 29/300 - Train Loss: 0.0645, Val Loss: 0.0659\n",
      "Epoch 30/300 - Train Loss: 0.0663, Val Loss: 0.0685\n",
      "Epoch 31/300 - Train Loss: 0.0612, Val Loss: 0.0646\n",
      "Epoch 32/300 - Train Loss: 0.0627, Val Loss: 0.0645\n",
      "Epoch 33/300 - Train Loss: 0.0623, Val Loss: 0.0697\n",
      "Epoch 34/300 - Train Loss: 0.0619, Val Loss: 0.0620\n",
      "Epoch 35/300 - Train Loss: 0.0610, Val Loss: 0.0701\n",
      "Epoch 36/300 - Train Loss: 0.0620, Val Loss: 0.0651\n",
      "Epoch 37/300 - Train Loss: 0.0626, Val Loss: 0.0648\n",
      "Epoch 38/300 - Train Loss: 0.0594, Val Loss: 0.0664\n",
      "Epoch 39/300 - Train Loss: 0.0601, Val Loss: 0.0661\n",
      "Epoch 40/300 - Train Loss: 0.0583, Val Loss: 0.0672\n",
      "Epoch 41/300 - Train Loss: 0.0583, Val Loss: 0.0627\n",
      "Epoch 42/300 - Train Loss: 0.0568, Val Loss: 0.0668\n",
      "Epoch 43/300 - Train Loss: 0.0572, Val Loss: 0.0668\n",
      "Epoch 44/300 - Train Loss: 0.0574, Val Loss: 0.0681\n",
      "Epoch 45/300 - Train Loss: 0.0573, Val Loss: 0.0632\n",
      "Epoch 46/300 - Train Loss: 0.0557, Val Loss: 0.0806\n",
      "Epoch 47/300 - Train Loss: 0.0556, Val Loss: 0.0605\n",
      "Epoch 48/300 - Train Loss: 0.0539, Val Loss: 0.0630\n",
      "Epoch 49/300 - Train Loss: 0.0537, Val Loss: 0.0634\n",
      "Epoch 50/300 - Train Loss: 0.0519, Val Loss: 0.0630\n",
      "Epoch 51/300 - Train Loss: 0.0534, Val Loss: 0.0642\n",
      "Epoch 52/300 - Train Loss: 0.0514, Val Loss: 0.0660\n",
      "Epoch 53/300 - Train Loss: 0.0510, Val Loss: 0.0719\n",
      "Epoch 54/300 - Train Loss: 0.0525, Val Loss: 0.0731\n",
      "Epoch 55/300 - Train Loss: 0.0495, Val Loss: 0.0637\n",
      "Epoch 56/300 - Train Loss: 0.0493, Val Loss: 0.0670\n",
      "Epoch 57/300 - Train Loss: 0.0493, Val Loss: 0.0741\n",
      "Epoch 58/300 - Train Loss: 0.0506, Val Loss: 0.0621\n",
      "Epoch 59/300 - Train Loss: 0.0477, Val Loss: 0.0652\n",
      "Epoch 60/300 - Train Loss: 0.0488, Val Loss: 0.0629\n",
      "Epoch 61/300 - Train Loss: 0.0471, Val Loss: 0.0689\n",
      "Epoch 62/300 - Train Loss: 0.0486, Val Loss: 0.0649\n",
      "Epoch 63/300 - Train Loss: 0.0477, Val Loss: 0.0631\n",
      "Epoch 64/300 - Train Loss: 0.0488, Val Loss: 0.0654\n",
      "Epoch 65/300 - Train Loss: 0.0455, Val Loss: 0.0665\n",
      "Epoch 66/300 - Train Loss: 0.0458, Val Loss: 0.0678\n",
      "Epoch 67/300 - Train Loss: 0.0464, Val Loss: 0.0675\n",
      "Epoch 68/300 - Train Loss: 0.0451, Val Loss: 0.0668\n",
      "Epoch 69/300 - Train Loss: 0.0464, Val Loss: 0.0653\n",
      "Epoch 70/300 - Train Loss: 0.0471, Val Loss: 0.0675\n",
      "Epoch 71/300 - Train Loss: 0.0427, Val Loss: 0.0647\n",
      "Epoch 72/300 - Train Loss: 0.0425, Val Loss: 0.0722\n",
      "Epoch 73/300 - Train Loss: 0.0429, Val Loss: 0.0725\n",
      "Epoch 74/300 - Train Loss: 0.0463, Val Loss: 0.0659\n",
      "Epoch 75/300 - Train Loss: 0.0449, Val Loss: 0.0767\n",
      "Epoch 76/300 - Train Loss: 0.0427, Val Loss: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:14:53,636] Trial 278 finished with value: 0.9711485576947547 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.1399756778838307, 'learning_rate': 6.387820034244892e-05, 'batch_size': 32, 'weight_decay': 8.253380099067363e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300 - Train Loss: 0.0439, Val Loss: 0.0669\n",
      "Early stopping at epoch 77\n",
      "Macro F1 Score: 0.9711, Macro Precision: 0.9738, Macro Recall: 0.9686\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 280\n",
      "Training with F1=32, F2=8, D=8, dropout=0.20498823715735193, LR=0.00010504338851822337, BS=32, WD=6.469242045770557e-05\n",
      "Epoch 1/300 - Train Loss: 0.3206, Val Loss: 0.1315\n",
      "Epoch 2/300 - Train Loss: 0.1198, Val Loss: 0.0848\n",
      "Epoch 3/300 - Train Loss: 0.1047, Val Loss: 0.0852\n",
      "Epoch 4/300 - Train Loss: 0.0972, Val Loss: 0.0776\n",
      "Epoch 5/300 - Train Loss: 0.0934, Val Loss: 0.0758\n",
      "Epoch 6/300 - Train Loss: 0.0889, Val Loss: 0.0751\n",
      "Epoch 7/300 - Train Loss: 0.0892, Val Loss: 0.0708\n",
      "Epoch 8/300 - Train Loss: 0.0857, Val Loss: 0.0730\n",
      "Epoch 9/300 - Train Loss: 0.0873, Val Loss: 0.0742\n",
      "Epoch 10/300 - Train Loss: 0.0832, Val Loss: 0.0664\n",
      "Epoch 11/300 - Train Loss: 0.0835, Val Loss: 0.0651\n",
      "Epoch 12/300 - Train Loss: 0.0814, Val Loss: 0.0637\n",
      "Epoch 13/300 - Train Loss: 0.0804, Val Loss: 0.0715\n",
      "Epoch 14/300 - Train Loss: 0.0814, Val Loss: 0.0658\n",
      "Epoch 15/300 - Train Loss: 0.0798, Val Loss: 0.0640\n",
      "Epoch 16/300 - Train Loss: 0.0781, Val Loss: 0.0648\n",
      "Epoch 17/300 - Train Loss: 0.0787, Val Loss: 0.0739\n",
      "Epoch 18/300 - Train Loss: 0.0778, Val Loss: 0.0706\n",
      "Epoch 19/300 - Train Loss: 0.0777, Val Loss: 0.0677\n",
      "Epoch 20/300 - Train Loss: 0.0784, Val Loss: 0.0679\n",
      "Epoch 21/300 - Train Loss: 0.0758, Val Loss: 0.0704\n",
      "Epoch 22/300 - Train Loss: 0.0749, Val Loss: 0.0736\n",
      "Epoch 23/300 - Train Loss: 0.0765, Val Loss: 0.0727\n",
      "Epoch 24/300 - Train Loss: 0.0763, Val Loss: 0.0646\n",
      "Epoch 25/300 - Train Loss: 0.0770, Val Loss: 0.0634\n",
      "Epoch 26/300 - Train Loss: 0.0722, Val Loss: 0.0639\n",
      "Epoch 27/300 - Train Loss: 0.0751, Val Loss: 0.0643\n",
      "Epoch 28/300 - Train Loss: 0.0719, Val Loss: 0.0686\n",
      "Epoch 29/300 - Train Loss: 0.0734, Val Loss: 0.0707\n",
      "Epoch 30/300 - Train Loss: 0.0724, Val Loss: 0.0713\n",
      "Epoch 31/300 - Train Loss: 0.0725, Val Loss: 0.0678\n",
      "Epoch 32/300 - Train Loss: 0.0736, Val Loss: 0.0663\n",
      "Epoch 33/300 - Train Loss: 0.0718, Val Loss: 0.0635\n",
      "Epoch 34/300 - Train Loss: 0.0719, Val Loss: 0.0664\n",
      "Epoch 35/300 - Train Loss: 0.0729, Val Loss: 0.0695\n",
      "Epoch 36/300 - Train Loss: 0.0737, Val Loss: 0.0669\n",
      "Epoch 37/300 - Train Loss: 0.0713, Val Loss: 0.0629\n",
      "Epoch 38/300 - Train Loss: 0.0710, Val Loss: 0.0664\n",
      "Epoch 39/300 - Train Loss: 0.0720, Val Loss: 0.0747\n",
      "Epoch 40/300 - Train Loss: 0.0692, Val Loss: 0.0657\n",
      "Epoch 41/300 - Train Loss: 0.0723, Val Loss: 0.0729\n",
      "Epoch 42/300 - Train Loss: 0.0720, Val Loss: 0.0702\n",
      "Epoch 43/300 - Train Loss: 0.0697, Val Loss: 0.0654\n",
      "Epoch 44/300 - Train Loss: 0.0688, Val Loss: 0.0707\n",
      "Epoch 45/300 - Train Loss: 0.0700, Val Loss: 0.0654\n",
      "Epoch 46/300 - Train Loss: 0.0695, Val Loss: 0.0640\n",
      "Epoch 47/300 - Train Loss: 0.0676, Val Loss: 0.0676\n",
      "Epoch 48/300 - Train Loss: 0.0696, Val Loss: 0.0713\n",
      "Epoch 49/300 - Train Loss: 0.0683, Val Loss: 0.0657\n",
      "Epoch 50/300 - Train Loss: 0.0690, Val Loss: 0.0708\n",
      "Epoch 51/300 - Train Loss: 0.0674, Val Loss: 0.0673\n",
      "Epoch 52/300 - Train Loss: 0.0714, Val Loss: 0.0658\n",
      "Epoch 53/300 - Train Loss: 0.0677, Val Loss: 0.0640\n",
      "Epoch 54/300 - Train Loss: 0.0675, Val Loss: 0.0686\n",
      "Epoch 55/300 - Train Loss: 0.0705, Val Loss: 0.0625\n",
      "Epoch 56/300 - Train Loss: 0.0686, Val Loss: 0.0662\n",
      "Epoch 57/300 - Train Loss: 0.0682, Val Loss: 0.0661\n",
      "Epoch 58/300 - Train Loss: 0.0682, Val Loss: 0.0641\n",
      "Epoch 59/300 - Train Loss: 0.0672, Val Loss: 0.0799\n",
      "Epoch 60/300 - Train Loss: 0.0674, Val Loss: 0.0665\n",
      "Epoch 61/300 - Train Loss: 0.0665, Val Loss: 0.0633\n",
      "Epoch 62/300 - Train Loss: 0.0670, Val Loss: 0.0636\n",
      "Epoch 63/300 - Train Loss: 0.0664, Val Loss: 0.0671\n",
      "Epoch 64/300 - Train Loss: 0.0649, Val Loss: 0.0641\n",
      "Epoch 65/300 - Train Loss: 0.0673, Val Loss: 0.0682\n",
      "Epoch 66/300 - Train Loss: 0.0656, Val Loss: 0.0670\n",
      "Epoch 67/300 - Train Loss: 0.0644, Val Loss: 0.0640\n",
      "Epoch 68/300 - Train Loss: 0.0645, Val Loss: 0.0663\n",
      "Epoch 69/300 - Train Loss: 0.0675, Val Loss: 0.0636\n",
      "Epoch 70/300 - Train Loss: 0.0651, Val Loss: 0.0677\n",
      "Epoch 71/300 - Train Loss: 0.0639, Val Loss: 0.0668\n",
      "Epoch 72/300 - Train Loss: 0.0660, Val Loss: 0.0636\n",
      "Epoch 73/300 - Train Loss: 0.0665, Val Loss: 0.0644\n",
      "Epoch 74/300 - Train Loss: 0.0626, Val Loss: 0.0651\n",
      "Epoch 75/300 - Train Loss: 0.0648, Val Loss: 0.0639\n",
      "Epoch 76/300 - Train Loss: 0.0649, Val Loss: 0.0727\n",
      "Epoch 77/300 - Train Loss: 0.0656, Val Loss: 0.0680\n",
      "Epoch 78/300 - Train Loss: 0.0661, Val Loss: 0.0648\n",
      "Epoch 79/300 - Train Loss: 0.0645, Val Loss: 0.0683\n",
      "Epoch 80/300 - Train Loss: 0.0646, Val Loss: 0.0668\n",
      "Epoch 81/300 - Train Loss: 0.0638, Val Loss: 0.0640\n",
      "Epoch 82/300 - Train Loss: 0.0630, Val Loss: 0.0698\n",
      "Epoch 83/300 - Train Loss: 0.0627, Val Loss: 0.0686\n",
      "Epoch 84/300 - Train Loss: 0.0625, Val Loss: 0.0656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:22:59,300] Trial 279 finished with value: 0.9699209209101919 and parameters: {'F1': 32, 'F2': 8, 'D': 8, 'dropout': 0.20498823715735193, 'learning_rate': 0.00010504338851822337, 'batch_size': 32, 'weight_decay': 6.469242045770557e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300 - Train Loss: 0.0612, Val Loss: 0.0724\n",
      "Early stopping at epoch 85\n",
      "Macro F1 Score: 0.9699, Macro Precision: 0.9635, Macro Recall: 0.9767\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 281\n",
      "Training with F1=32, F2=32, D=8, dropout=0.16440306475709723, LR=8.241897331163044e-05, BS=32, WD=5.406190130224825e-05\n",
      "Epoch 1/300 - Train Loss: 0.2392, Val Loss: 0.1062\n",
      "Epoch 2/300 - Train Loss: 0.1074, Val Loss: 0.0902\n",
      "Epoch 3/300 - Train Loss: 0.0963, Val Loss: 0.0760\n",
      "Epoch 4/300 - Train Loss: 0.0913, Val Loss: 0.0695\n",
      "Epoch 5/300 - Train Loss: 0.0845, Val Loss: 0.0739\n",
      "Epoch 6/300 - Train Loss: 0.0891, Val Loss: 0.0819\n",
      "Epoch 7/300 - Train Loss: 0.0845, Val Loss: 0.0662\n",
      "Epoch 8/300 - Train Loss: 0.0822, Val Loss: 0.0868\n",
      "Epoch 9/300 - Train Loss: 0.0796, Val Loss: 0.0778\n",
      "Epoch 10/300 - Train Loss: 0.0791, Val Loss: 0.0703\n",
      "Epoch 11/300 - Train Loss: 0.0771, Val Loss: 0.0755\n",
      "Epoch 12/300 - Train Loss: 0.0766, Val Loss: 0.0677\n",
      "Epoch 13/300 - Train Loss: 0.0794, Val Loss: 0.0873\n",
      "Epoch 14/300 - Train Loss: 0.0733, Val Loss: 0.0685\n",
      "Epoch 15/300 - Train Loss: 0.0771, Val Loss: 0.0644\n",
      "Epoch 16/300 - Train Loss: 0.0751, Val Loss: 0.0727\n",
      "Epoch 17/300 - Train Loss: 0.0733, Val Loss: 0.0712\n",
      "Epoch 18/300 - Train Loss: 0.0719, Val Loss: 0.0629\n",
      "Epoch 19/300 - Train Loss: 0.0693, Val Loss: 0.0955\n",
      "Epoch 20/300 - Train Loss: 0.0702, Val Loss: 0.0677\n",
      "Epoch 21/300 - Train Loss: 0.0706, Val Loss: 0.0626\n",
      "Epoch 22/300 - Train Loss: 0.0688, Val Loss: 0.0666\n",
      "Epoch 23/300 - Train Loss: 0.0683, Val Loss: 0.0624\n",
      "Epoch 24/300 - Train Loss: 0.0682, Val Loss: 0.0637\n",
      "Epoch 25/300 - Train Loss: 0.0669, Val Loss: 0.0652\n",
      "Epoch 26/300 - Train Loss: 0.0654, Val Loss: 0.0735\n",
      "Epoch 27/300 - Train Loss: 0.0656, Val Loss: 0.0736\n",
      "Epoch 28/300 - Train Loss: 0.0649, Val Loss: 0.0673\n",
      "Epoch 29/300 - Train Loss: 0.0655, Val Loss: 0.0678\n",
      "Epoch 30/300 - Train Loss: 0.0638, Val Loss: 0.0651\n",
      "Epoch 31/300 - Train Loss: 0.0638, Val Loss: 0.0655\n",
      "Epoch 32/300 - Train Loss: 0.0631, Val Loss: 0.0644\n",
      "Epoch 33/300 - Train Loss: 0.0615, Val Loss: 0.0752\n",
      "Epoch 34/300 - Train Loss: 0.0611, Val Loss: 0.0660\n",
      "Epoch 35/300 - Train Loss: 0.0623, Val Loss: 0.0628\n",
      "Epoch 36/300 - Train Loss: 0.0626, Val Loss: 0.0755\n",
      "Epoch 37/300 - Train Loss: 0.0632, Val Loss: 0.0679\n",
      "Epoch 38/300 - Train Loss: 0.0608, Val Loss: 0.0679\n",
      "Epoch 39/300 - Train Loss: 0.0595, Val Loss: 0.0671\n",
      "Epoch 40/300 - Train Loss: 0.0570, Val Loss: 0.0638\n",
      "Epoch 41/300 - Train Loss: 0.0572, Val Loss: 0.0660\n",
      "Epoch 42/300 - Train Loss: 0.0575, Val Loss: 0.0624\n",
      "Epoch 43/300 - Train Loss: 0.0571, Val Loss: 0.0630\n",
      "Epoch 44/300 - Train Loss: 0.0557, Val Loss: 0.0690\n",
      "Epoch 45/300 - Train Loss: 0.0551, Val Loss: 0.0639\n",
      "Epoch 46/300 - Train Loss: 0.0549, Val Loss: 0.0645\n",
      "Epoch 47/300 - Train Loss: 0.0545, Val Loss: 0.0648\n",
      "Epoch 48/300 - Train Loss: 0.0546, Val Loss: 0.0664\n",
      "Epoch 49/300 - Train Loss: 0.0540, Val Loss: 0.0628\n",
      "Epoch 50/300 - Train Loss: 0.0534, Val Loss: 0.0672\n",
      "Epoch 51/300 - Train Loss: 0.0516, Val Loss: 0.0721\n",
      "Epoch 52/300 - Train Loss: 0.0506, Val Loss: 0.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:28:44,155] Trial 280 finished with value: 0.9655657928863369 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.16440306475709723, 'learning_rate': 8.241897331163044e-05, 'batch_size': 32, 'weight_decay': 5.406190130224825e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300 - Train Loss: 0.0517, Val Loss: 0.0637\n",
      "Early stopping at epoch 53\n",
      "Macro F1 Score: 0.9656, Macro Precision: 0.9598, Macro Recall: 0.9718\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 282\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14982620206257946, LR=9.099555450300224e-05, BS=32, WD=4.683646340693847e-05\n",
      "Epoch 1/300 - Train Loss: 0.2819, Val Loss: 0.1174\n",
      "Epoch 2/300 - Train Loss: 0.1168, Val Loss: 0.0808\n",
      "Epoch 3/300 - Train Loss: 0.1000, Val Loss: 0.0742\n",
      "Epoch 4/300 - Train Loss: 0.0955, Val Loss: 0.0774\n",
      "Epoch 5/300 - Train Loss: 0.0939, Val Loss: 0.0731\n",
      "Epoch 6/300 - Train Loss: 0.0894, Val Loss: 0.0913\n",
      "Epoch 7/300 - Train Loss: 0.0881, Val Loss: 0.0783\n",
      "Epoch 8/300 - Train Loss: 0.0866, Val Loss: 0.0754\n",
      "Epoch 9/300 - Train Loss: 0.0877, Val Loss: 0.0724\n",
      "Epoch 10/300 - Train Loss: 0.0828, Val Loss: 0.0719\n",
      "Epoch 11/300 - Train Loss: 0.0829, Val Loss: 0.0723\n",
      "Epoch 12/300 - Train Loss: 0.0814, Val Loss: 0.0784\n",
      "Epoch 13/300 - Train Loss: 0.0821, Val Loss: 0.0729\n",
      "Epoch 14/300 - Train Loss: 0.0793, Val Loss: 0.0700\n",
      "Epoch 15/300 - Train Loss: 0.0779, Val Loss: 0.0717\n",
      "Epoch 16/300 - Train Loss: 0.0768, Val Loss: 0.0769\n",
      "Epoch 17/300 - Train Loss: 0.0774, Val Loss: 0.0763\n",
      "Epoch 18/300 - Train Loss: 0.0771, Val Loss: 0.0707\n",
      "Epoch 19/300 - Train Loss: 0.0755, Val Loss: 0.0690\n",
      "Epoch 20/300 - Train Loss: 0.0748, Val Loss: 0.0713\n",
      "Epoch 21/300 - Train Loss: 0.0760, Val Loss: 0.0674\n",
      "Epoch 22/300 - Train Loss: 0.0732, Val Loss: 0.0685\n",
      "Epoch 23/300 - Train Loss: 0.0746, Val Loss: 0.0685\n",
      "Epoch 24/300 - Train Loss: 0.0727, Val Loss: 0.0666\n",
      "Epoch 25/300 - Train Loss: 0.0729, Val Loss: 0.0670\n",
      "Epoch 26/300 - Train Loss: 0.0705, Val Loss: 0.0781\n",
      "Epoch 27/300 - Train Loss: 0.0715, Val Loss: 0.0748\n",
      "Epoch 28/300 - Train Loss: 0.0707, Val Loss: 0.0651\n",
      "Epoch 29/300 - Train Loss: 0.0700, Val Loss: 0.0663\n",
      "Epoch 30/300 - Train Loss: 0.0678, Val Loss: 0.0683\n",
      "Epoch 31/300 - Train Loss: 0.0703, Val Loss: 0.0680\n",
      "Epoch 32/300 - Train Loss: 0.0671, Val Loss: 0.0733\n",
      "Epoch 33/300 - Train Loss: 0.0676, Val Loss: 0.0664\n",
      "Epoch 34/300 - Train Loss: 0.0674, Val Loss: 0.0656\n",
      "Epoch 35/300 - Train Loss: 0.0676, Val Loss: 0.0714\n",
      "Epoch 36/300 - Train Loss: 0.0676, Val Loss: 0.0676\n",
      "Epoch 37/300 - Train Loss: 0.0667, Val Loss: 0.0722\n",
      "Epoch 38/300 - Train Loss: 0.0648, Val Loss: 0.0702\n",
      "Epoch 39/300 - Train Loss: 0.0657, Val Loss: 0.0670\n",
      "Epoch 40/300 - Train Loss: 0.0642, Val Loss: 0.0713\n",
      "Epoch 41/300 - Train Loss: 0.0638, Val Loss: 0.0684\n",
      "Epoch 42/300 - Train Loss: 0.0628, Val Loss: 0.0642\n",
      "Epoch 43/300 - Train Loss: 0.0624, Val Loss: 0.0681\n",
      "Epoch 44/300 - Train Loss: 0.0635, Val Loss: 0.0656\n",
      "Epoch 45/300 - Train Loss: 0.0629, Val Loss: 0.0666\n",
      "Epoch 46/300 - Train Loss: 0.0623, Val Loss: 0.0698\n",
      "Epoch 47/300 - Train Loss: 0.0611, Val Loss: 0.0702\n",
      "Epoch 48/300 - Train Loss: 0.0610, Val Loss: 0.0677\n",
      "Epoch 49/300 - Train Loss: 0.0610, Val Loss: 0.0675\n",
      "Epoch 50/300 - Train Loss: 0.0599, Val Loss: 0.0700\n",
      "Epoch 51/300 - Train Loss: 0.0625, Val Loss: 0.0649\n",
      "Epoch 52/300 - Train Loss: 0.0665, Val Loss: 0.0667\n",
      "Epoch 53/300 - Train Loss: 0.0591, Val Loss: 0.0693\n",
      "Epoch 54/300 - Train Loss: 0.0598, Val Loss: 0.0679\n",
      "Epoch 55/300 - Train Loss: 0.0597, Val Loss: 0.0725\n",
      "Epoch 56/300 - Train Loss: 0.0583, Val Loss: 0.0687\n",
      "Epoch 57/300 - Train Loss: 0.0585, Val Loss: 0.0661\n",
      "Epoch 58/300 - Train Loss: 0.0584, Val Loss: 0.0683\n",
      "Epoch 59/300 - Train Loss: 0.0563, Val Loss: 0.0707\n",
      "Epoch 60/300 - Train Loss: 0.0568, Val Loss: 0.0694\n",
      "Epoch 61/300 - Train Loss: 0.0580, Val Loss: 0.0693\n",
      "Epoch 62/300 - Train Loss: 0.0595, Val Loss: 0.0684\n",
      "Epoch 63/300 - Train Loss: 0.0572, Val Loss: 0.0725\n",
      "Epoch 64/300 - Train Loss: 0.0564, Val Loss: 0.0691\n",
      "Epoch 65/300 - Train Loss: 0.0546, Val Loss: 0.0751\n",
      "Epoch 66/300 - Train Loss: 0.0549, Val Loss: 0.0733\n",
      "Epoch 67/300 - Train Loss: 0.0558, Val Loss: 0.0687\n",
      "Epoch 68/300 - Train Loss: 0.0540, Val Loss: 0.0713\n",
      "Epoch 69/300 - Train Loss: 0.0568, Val Loss: 0.0725\n",
      "Epoch 70/300 - Train Loss: 0.0548, Val Loss: 0.0686\n",
      "Epoch 71/300 - Train Loss: 0.0547, Val Loss: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:32:00,987] Trial 281 finished with value: 0.9754546343148195 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14982620206257946, 'learning_rate': 9.099555450300224e-05, 'batch_size': 32, 'weight_decay': 4.683646340693847e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300 - Train Loss: 0.0548, Val Loss: 0.0747\n",
      "Early stopping at epoch 72\n",
      "Macro F1 Score: 0.9755, Macro Precision: 0.9739, Macro Recall: 0.9771\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.97      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 283\n",
      "Training with F1=32, F2=32, D=2, dropout=0.15067857645144403, LR=9.291042659915504e-05, BS=32, WD=6.0286525170505064e-05\n",
      "Epoch 1/300 - Train Loss: 0.2727, Val Loss: 0.1433\n",
      "Epoch 2/300 - Train Loss: 0.1238, Val Loss: 0.0939\n",
      "Epoch 3/300 - Train Loss: 0.1037, Val Loss: 0.0809\n",
      "Epoch 4/300 - Train Loss: 0.0968, Val Loss: 0.0728\n",
      "Epoch 5/300 - Train Loss: 0.0940, Val Loss: 0.0808\n",
      "Epoch 6/300 - Train Loss: 0.0876, Val Loss: 0.0773\n",
      "Epoch 7/300 - Train Loss: 0.0874, Val Loss: 0.0786\n",
      "Epoch 8/300 - Train Loss: 0.0853, Val Loss: 0.0709\n",
      "Epoch 9/300 - Train Loss: 0.0844, Val Loss: 0.0732\n",
      "Epoch 10/300 - Train Loss: 0.0831, Val Loss: 0.0719\n",
      "Epoch 11/300 - Train Loss: 0.0794, Val Loss: 0.0779\n",
      "Epoch 12/300 - Train Loss: 0.0797, Val Loss: 0.0711\n",
      "Epoch 13/300 - Train Loss: 0.0790, Val Loss: 0.0742\n",
      "Epoch 14/300 - Train Loss: 0.0783, Val Loss: 0.0753\n",
      "Epoch 15/300 - Train Loss: 0.0770, Val Loss: 0.0798\n",
      "Epoch 16/300 - Train Loss: 0.0792, Val Loss: 0.0757\n",
      "Epoch 17/300 - Train Loss: 0.0750, Val Loss: 0.0721\n",
      "Epoch 18/300 - Train Loss: 0.0749, Val Loss: 0.0692\n",
      "Epoch 19/300 - Train Loss: 0.0758, Val Loss: 0.0678\n",
      "Epoch 20/300 - Train Loss: 0.0758, Val Loss: 0.0742\n",
      "Epoch 21/300 - Train Loss: 0.0753, Val Loss: 0.0696\n",
      "Epoch 22/300 - Train Loss: 0.0744, Val Loss: 0.0679\n",
      "Epoch 23/300 - Train Loss: 0.0729, Val Loss: 0.0742\n",
      "Epoch 24/300 - Train Loss: 0.0733, Val Loss: 0.0712\n",
      "Epoch 25/300 - Train Loss: 0.0718, Val Loss: 0.0677\n",
      "Epoch 26/300 - Train Loss: 0.0705, Val Loss: 0.0723\n",
      "Epoch 27/300 - Train Loss: 0.0678, Val Loss: 0.0764\n",
      "Epoch 28/300 - Train Loss: 0.0696, Val Loss: 0.0681\n",
      "Epoch 29/300 - Train Loss: 0.0708, Val Loss: 0.0744\n",
      "Epoch 30/300 - Train Loss: 0.0680, Val Loss: 0.0668\n",
      "Epoch 31/300 - Train Loss: 0.0680, Val Loss: 0.0765\n",
      "Epoch 32/300 - Train Loss: 0.0681, Val Loss: 0.0708\n",
      "Epoch 33/300 - Train Loss: 0.0679, Val Loss: 0.0696\n",
      "Epoch 34/300 - Train Loss: 0.0642, Val Loss: 0.0702\n",
      "Epoch 35/300 - Train Loss: 0.0692, Val Loss: 0.0671\n",
      "Epoch 36/300 - Train Loss: 0.0679, Val Loss: 0.0696\n",
      "Epoch 37/300 - Train Loss: 0.0668, Val Loss: 0.0679\n",
      "Epoch 38/300 - Train Loss: 0.0657, Val Loss: 0.0711\n",
      "Epoch 39/300 - Train Loss: 0.0651, Val Loss: 0.0741\n",
      "Epoch 40/300 - Train Loss: 0.0668, Val Loss: 0.0681\n",
      "Epoch 41/300 - Train Loss: 0.0656, Val Loss: 0.0667\n",
      "Epoch 42/300 - Train Loss: 0.0611, Val Loss: 0.0696\n",
      "Epoch 43/300 - Train Loss: 0.0638, Val Loss: 0.0699\n",
      "Epoch 44/300 - Train Loss: 0.0637, Val Loss: 0.0671\n",
      "Epoch 45/300 - Train Loss: 0.0612, Val Loss: 0.0718\n",
      "Epoch 46/300 - Train Loss: 0.0618, Val Loss: 0.0672\n",
      "Epoch 47/300 - Train Loss: 0.0611, Val Loss: 0.0738\n",
      "Epoch 48/300 - Train Loss: 0.0629, Val Loss: 0.0708\n",
      "Epoch 49/300 - Train Loss: 0.0610, Val Loss: 0.0754\n",
      "Epoch 50/300 - Train Loss: 0.0600, Val Loss: 0.0693\n",
      "Epoch 51/300 - Train Loss: 0.0571, Val Loss: 0.0751\n",
      "Epoch 52/300 - Train Loss: 0.0604, Val Loss: 0.0755\n",
      "Epoch 53/300 - Train Loss: 0.0586, Val Loss: 0.0707\n",
      "Epoch 54/300 - Train Loss: 0.0596, Val Loss: 0.0693\n",
      "Epoch 55/300 - Train Loss: 0.0572, Val Loss: 0.0667\n",
      "Epoch 56/300 - Train Loss: 0.0569, Val Loss: 0.0699\n",
      "Epoch 57/300 - Train Loss: 0.0596, Val Loss: 0.0682\n",
      "Epoch 58/300 - Train Loss: 0.0584, Val Loss: 0.0748\n",
      "Epoch 59/300 - Train Loss: 0.0555, Val Loss: 0.0717\n",
      "Epoch 60/300 - Train Loss: 0.0596, Val Loss: 0.0689\n",
      "Epoch 61/300 - Train Loss: 0.0570, Val Loss: 0.0669\n",
      "Epoch 62/300 - Train Loss: 0.0559, Val Loss: 0.0747\n",
      "Epoch 63/300 - Train Loss: 0.0551, Val Loss: 0.0713\n",
      "Epoch 64/300 - Train Loss: 0.0564, Val Loss: 0.0725\n",
      "Epoch 65/300 - Train Loss: 0.0552, Val Loss: 0.0742\n",
      "Epoch 66/300 - Train Loss: 0.0568, Val Loss: 0.0723\n",
      "Epoch 67/300 - Train Loss: 0.0540, Val Loss: 0.0692\n",
      "Epoch 68/300 - Train Loss: 0.0553, Val Loss: 0.0707\n",
      "Epoch 69/300 - Train Loss: 0.0542, Val Loss: 0.0701\n",
      "Epoch 70/300 - Train Loss: 0.0529, Val Loss: 0.0737\n",
      "Epoch 71/300 - Train Loss: 0.0529, Val Loss: 0.0733\n",
      "Epoch 72/300 - Train Loss: 0.0510, Val Loss: 0.0723\n",
      "Epoch 73/300 - Train Loss: 0.0522, Val Loss: 0.0686\n",
      "Epoch 74/300 - Train Loss: 0.0519, Val Loss: 0.0745\n",
      "Epoch 75/300 - Train Loss: 0.0539, Val Loss: 0.0764\n",
      "Epoch 76/300 - Train Loss: 0.0522, Val Loss: 0.0780\n",
      "Epoch 77/300 - Train Loss: 0.0520, Val Loss: 0.0717\n",
      "Epoch 78/300 - Train Loss: 0.0495, Val Loss: 0.0778\n",
      "Epoch 79/300 - Train Loss: 0.0520, Val Loss: 0.0719\n",
      "Epoch 80/300 - Train Loss: 0.0495, Val Loss: 0.0721\n",
      "Epoch 81/300 - Train Loss: 0.0507, Val Loss: 0.0742\n",
      "Epoch 82/300 - Train Loss: 0.0496, Val Loss: 0.0745\n",
      "Epoch 83/300 - Train Loss: 0.0466, Val Loss: 0.0773\n",
      "Epoch 84/300 - Train Loss: 0.0481, Val Loss: 0.0749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:35:53,029] Trial 282 finished with value: 0.9742495349707377 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.15067857645144403, 'learning_rate': 9.291042659915504e-05, 'batch_size': 32, 'weight_decay': 6.0286525170505064e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300 - Train Loss: 0.0508, Val Loss: 0.0755\n",
      "Early stopping at epoch 85\n",
      "Macro F1 Score: 0.9742, Macro Precision: 0.9685, Macro Recall: 0.9805\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.98      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 284\n",
      "Training with F1=32, F2=32, D=2, dropout=0.12713548241664926, LR=9.785603322090613e-05, BS=32, WD=6.912350824331458e-05\n",
      "Epoch 1/300 - Train Loss: 0.2715, Val Loss: 0.1318\n",
      "Epoch 2/300 - Train Loss: 0.1271, Val Loss: 0.0891\n",
      "Epoch 3/300 - Train Loss: 0.1039, Val Loss: 0.0832\n",
      "Epoch 4/300 - Train Loss: 0.1003, Val Loss: 0.0746\n",
      "Epoch 5/300 - Train Loss: 0.0932, Val Loss: 0.0778\n",
      "Epoch 6/300 - Train Loss: 0.0915, Val Loss: 0.0824\n",
      "Epoch 7/300 - Train Loss: 0.0904, Val Loss: 0.0737\n",
      "Epoch 8/300 - Train Loss: 0.0878, Val Loss: 0.0845\n",
      "Epoch 9/300 - Train Loss: 0.0870, Val Loss: 0.0736\n",
      "Epoch 10/300 - Train Loss: 0.0814, Val Loss: 0.0699\n",
      "Epoch 11/300 - Train Loss: 0.0808, Val Loss: 0.0755\n",
      "Epoch 12/300 - Train Loss: 0.0788, Val Loss: 0.0733\n",
      "Epoch 13/300 - Train Loss: 0.0787, Val Loss: 0.0798\n",
      "Epoch 14/300 - Train Loss: 0.0791, Val Loss: 0.0729\n",
      "Epoch 15/300 - Train Loss: 0.0781, Val Loss: 0.0760\n",
      "Epoch 16/300 - Train Loss: 0.0743, Val Loss: 0.0753\n",
      "Epoch 17/300 - Train Loss: 0.0749, Val Loss: 0.0711\n",
      "Epoch 18/300 - Train Loss: 0.0756, Val Loss: 0.0723\n",
      "Epoch 19/300 - Train Loss: 0.0728, Val Loss: 0.0656\n",
      "Epoch 20/300 - Train Loss: 0.0764, Val Loss: 0.0891\n",
      "Epoch 21/300 - Train Loss: 0.0732, Val Loss: 0.0745\n",
      "Epoch 22/300 - Train Loss: 0.0726, Val Loss: 0.0767\n",
      "Epoch 23/300 - Train Loss: 0.0680, Val Loss: 0.0692\n",
      "Epoch 24/300 - Train Loss: 0.0701, Val Loss: 0.0729\n",
      "Epoch 25/300 - Train Loss: 0.0706, Val Loss: 0.0708\n",
      "Epoch 26/300 - Train Loss: 0.0686, Val Loss: 0.0702\n",
      "Epoch 27/300 - Train Loss: 0.0694, Val Loss: 0.0691\n",
      "Epoch 28/300 - Train Loss: 0.0668, Val Loss: 0.0706\n",
      "Epoch 29/300 - Train Loss: 0.0662, Val Loss: 0.0685\n",
      "Epoch 30/300 - Train Loss: 0.0666, Val Loss: 0.0689\n",
      "Epoch 31/300 - Train Loss: 0.0648, Val Loss: 0.0711\n",
      "Epoch 32/300 - Train Loss: 0.0668, Val Loss: 0.0663\n",
      "Epoch 33/300 - Train Loss: 0.0648, Val Loss: 0.0678\n",
      "Epoch 34/300 - Train Loss: 0.0615, Val Loss: 0.0661\n",
      "Epoch 35/300 - Train Loss: 0.0652, Val Loss: 0.0689\n",
      "Epoch 36/300 - Train Loss: 0.0626, Val Loss: 0.0732\n",
      "Epoch 37/300 - Train Loss: 0.0642, Val Loss: 0.0662\n",
      "Epoch 38/300 - Train Loss: 0.0630, Val Loss: 0.0678\n",
      "Epoch 39/300 - Train Loss: 0.0628, Val Loss: 0.0675\n",
      "Epoch 40/300 - Train Loss: 0.0616, Val Loss: 0.0682\n",
      "Epoch 41/300 - Train Loss: 0.0610, Val Loss: 0.0672\n",
      "Epoch 42/300 - Train Loss: 0.0592, Val Loss: 0.0676\n",
      "Epoch 43/300 - Train Loss: 0.0593, Val Loss: 0.0702\n",
      "Epoch 44/300 - Train Loss: 0.0597, Val Loss: 0.0734\n",
      "Epoch 45/300 - Train Loss: 0.0590, Val Loss: 0.0745\n",
      "Epoch 46/300 - Train Loss: 0.0557, Val Loss: 0.0712\n",
      "Epoch 47/300 - Train Loss: 0.0589, Val Loss: 0.0661\n",
      "Epoch 48/300 - Train Loss: 0.0585, Val Loss: 0.0796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:38:06,983] Trial 283 finished with value: 0.9752370019674133 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.12713548241664926, 'learning_rate': 9.785603322090613e-05, 'batch_size': 32, 'weight_decay': 6.912350824331458e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300 - Train Loss: 0.0584, Val Loss: 0.0773\n",
      "Early stopping at epoch 49\n",
      "Macro F1 Score: 0.9752, Macro Precision: 0.9785, Macro Recall: 0.9721\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 285\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1215257977167139, LR=0.0001020012357561303, BS=32, WD=7.696979004773314e-05\n",
      "Epoch 1/300 - Train Loss: 0.2522, Val Loss: 0.1057\n",
      "Epoch 2/300 - Train Loss: 0.1113, Val Loss: 0.0841\n",
      "Epoch 3/300 - Train Loss: 0.0984, Val Loss: 0.0756\n",
      "Epoch 4/300 - Train Loss: 0.0935, Val Loss: 0.0860\n",
      "Epoch 5/300 - Train Loss: 0.0896, Val Loss: 0.0857\n",
      "Epoch 6/300 - Train Loss: 0.0868, Val Loss: 0.0756\n",
      "Epoch 7/300 - Train Loss: 0.0832, Val Loss: 0.0754\n",
      "Epoch 8/300 - Train Loss: 0.0836, Val Loss: 0.0780\n",
      "Epoch 9/300 - Train Loss: 0.0816, Val Loss: 0.0712\n",
      "Epoch 10/300 - Train Loss: 0.0849, Val Loss: 0.0721\n",
      "Epoch 11/300 - Train Loss: 0.0801, Val Loss: 0.0738\n",
      "Epoch 12/300 - Train Loss: 0.0796, Val Loss: 0.0816\n",
      "Epoch 13/300 - Train Loss: 0.0792, Val Loss: 0.0771\n",
      "Epoch 14/300 - Train Loss: 0.0778, Val Loss: 0.0756\n",
      "Epoch 15/300 - Train Loss: 0.0758, Val Loss: 0.0763\n",
      "Epoch 16/300 - Train Loss: 0.0764, Val Loss: 0.0750\n",
      "Epoch 17/300 - Train Loss: 0.0747, Val Loss: 0.0730\n",
      "Epoch 18/300 - Train Loss: 0.0768, Val Loss: 0.0796\n",
      "Epoch 19/300 - Train Loss: 0.0742, Val Loss: 0.0692\n",
      "Epoch 20/300 - Train Loss: 0.0742, Val Loss: 0.0733\n",
      "Epoch 21/300 - Train Loss: 0.0728, Val Loss: 0.0758\n",
      "Epoch 22/300 - Train Loss: 0.0708, Val Loss: 0.0717\n",
      "Epoch 23/300 - Train Loss: 0.0702, Val Loss: 0.0749\n",
      "Epoch 24/300 - Train Loss: 0.0716, Val Loss: 0.0690\n",
      "Epoch 25/300 - Train Loss: 0.0691, Val Loss: 0.0769\n",
      "Epoch 26/300 - Train Loss: 0.0692, Val Loss: 0.0739\n",
      "Epoch 27/300 - Train Loss: 0.0687, Val Loss: 0.0753\n",
      "Epoch 28/300 - Train Loss: 0.0689, Val Loss: 0.0674\n",
      "Epoch 29/300 - Train Loss: 0.0686, Val Loss: 0.0721\n",
      "Epoch 30/300 - Train Loss: 0.0693, Val Loss: 0.0754\n",
      "Epoch 31/300 - Train Loss: 0.0652, Val Loss: 0.0740\n",
      "Epoch 32/300 - Train Loss: 0.0646, Val Loss: 0.0683\n",
      "Epoch 33/300 - Train Loss: 0.0662, Val Loss: 0.0708\n",
      "Epoch 34/300 - Train Loss: 0.0627, Val Loss: 0.0719\n",
      "Epoch 35/300 - Train Loss: 0.0650, Val Loss: 0.0698\n",
      "Epoch 36/300 - Train Loss: 0.0634, Val Loss: 0.0706\n",
      "Epoch 37/300 - Train Loss: 0.0631, Val Loss: 0.0745\n",
      "Epoch 38/300 - Train Loss: 0.0634, Val Loss: 0.0754\n",
      "Epoch 39/300 - Train Loss: 0.0613, Val Loss: 0.0742\n",
      "Epoch 40/300 - Train Loss: 0.0632, Val Loss: 0.0784\n",
      "Epoch 41/300 - Train Loss: 0.0608, Val Loss: 0.0685\n",
      "Epoch 42/300 - Train Loss: 0.0598, Val Loss: 0.0765\n",
      "Epoch 43/300 - Train Loss: 0.0607, Val Loss: 0.0762\n",
      "Epoch 44/300 - Train Loss: 0.0597, Val Loss: 0.0706\n",
      "Epoch 45/300 - Train Loss: 0.0588, Val Loss: 0.0722\n",
      "Epoch 46/300 - Train Loss: 0.0592, Val Loss: 0.0710\n",
      "Epoch 47/300 - Train Loss: 0.0607, Val Loss: 0.0705\n",
      "Epoch 48/300 - Train Loss: 0.0572, Val Loss: 0.0720\n",
      "Epoch 49/300 - Train Loss: 0.0586, Val Loss: 0.0685\n",
      "Epoch 50/300 - Train Loss: 0.0581, Val Loss: 0.0696\n",
      "Epoch 51/300 - Train Loss: 0.0565, Val Loss: 0.0694\n",
      "Epoch 52/300 - Train Loss: 0.0567, Val Loss: 0.0671\n",
      "Epoch 53/300 - Train Loss: 0.0569, Val Loss: 0.0713\n",
      "Epoch 54/300 - Train Loss: 0.0562, Val Loss: 0.0694\n",
      "Epoch 55/300 - Train Loss: 0.0547, Val Loss: 0.0689\n",
      "Epoch 56/300 - Train Loss: 0.0552, Val Loss: 0.0694\n",
      "Epoch 57/300 - Train Loss: 0.0557, Val Loss: 0.0701\n",
      "Epoch 58/300 - Train Loss: 0.0527, Val Loss: 0.0693\n",
      "Epoch 59/300 - Train Loss: 0.0534, Val Loss: 0.0725\n",
      "Epoch 60/300 - Train Loss: 0.0532, Val Loss: 0.0690\n",
      "Epoch 61/300 - Train Loss: 0.0527, Val Loss: 0.0683\n",
      "Epoch 62/300 - Train Loss: 0.0536, Val Loss: 0.0805\n",
      "Epoch 63/300 - Train Loss: 0.0535, Val Loss: 0.0670\n",
      "Epoch 64/300 - Train Loss: 0.0513, Val Loss: 0.0767\n",
      "Epoch 65/300 - Train Loss: 0.0517, Val Loss: 0.0717\n",
      "Epoch 66/300 - Train Loss: 0.0507, Val Loss: 0.0711\n",
      "Epoch 67/300 - Train Loss: 0.0509, Val Loss: 0.0698\n",
      "Epoch 68/300 - Train Loss: 0.0507, Val Loss: 0.0769\n",
      "Epoch 69/300 - Train Loss: 0.0501, Val Loss: 0.0745\n",
      "Epoch 70/300 - Train Loss: 0.0505, Val Loss: 0.0758\n",
      "Epoch 71/300 - Train Loss: 0.0499, Val Loss: 0.0704\n",
      "Epoch 72/300 - Train Loss: 0.0482, Val Loss: 0.0735\n",
      "Epoch 73/300 - Train Loss: 0.0478, Val Loss: 0.0859\n",
      "Epoch 74/300 - Train Loss: 0.0491, Val Loss: 0.0739\n",
      "Epoch 75/300 - Train Loss: 0.0478, Val Loss: 0.0704\n",
      "Epoch 76/300 - Train Loss: 0.0470, Val Loss: 0.0662\n",
      "Epoch 77/300 - Train Loss: 0.0464, Val Loss: 0.0714\n",
      "Epoch 78/300 - Train Loss: 0.0459, Val Loss: 0.0796\n",
      "Epoch 79/300 - Train Loss: 0.0458, Val Loss: 0.0703\n",
      "Epoch 80/300 - Train Loss: 0.0463, Val Loss: 0.0790\n",
      "Epoch 81/300 - Train Loss: 0.0462, Val Loss: 0.0774\n",
      "Epoch 82/300 - Train Loss: 0.0445, Val Loss: 0.0755\n",
      "Epoch 83/300 - Train Loss: 0.0455, Val Loss: 0.0747\n",
      "Epoch 84/300 - Train Loss: 0.0482, Val Loss: 0.0988\n",
      "Epoch 85/300 - Train Loss: 0.0453, Val Loss: 0.0791\n",
      "Epoch 86/300 - Train Loss: 0.0413, Val Loss: 0.0751\n",
      "Epoch 87/300 - Train Loss: 0.0426, Val Loss: 0.0830\n",
      "Epoch 88/300 - Train Loss: 0.0458, Val Loss: 0.0754\n",
      "Epoch 89/300 - Train Loss: 0.0442, Val Loss: 0.0762\n",
      "Epoch 90/300 - Train Loss: 0.0444, Val Loss: 0.0748\n",
      "Epoch 91/300 - Train Loss: 0.0435, Val Loss: 0.0773\n",
      "Epoch 92/300 - Train Loss: 0.0432, Val Loss: 0.0796\n",
      "Epoch 93/300 - Train Loss: 0.0418, Val Loss: 0.0725\n",
      "Epoch 94/300 - Train Loss: 0.0425, Val Loss: 0.0778\n",
      "Epoch 95/300 - Train Loss: 0.0418, Val Loss: 0.0741\n",
      "Epoch 96/300 - Train Loss: 0.0417, Val Loss: 0.0743\n",
      "Epoch 97/300 - Train Loss: 0.0422, Val Loss: 0.0782\n",
      "Epoch 98/300 - Train Loss: 0.0418, Val Loss: 0.0714\n",
      "Epoch 99/300 - Train Loss: 0.0421, Val Loss: 0.0780\n",
      "Epoch 100/300 - Train Loss: 0.0413, Val Loss: 0.0772\n",
      "Epoch 101/300 - Train Loss: 0.0415, Val Loss: 0.0754\n",
      "Epoch 102/300 - Train Loss: 0.0384, Val Loss: 0.0712\n",
      "Epoch 103/300 - Train Loss: 0.0436, Val Loss: 0.0722\n",
      "Epoch 104/300 - Train Loss: 0.0411, Val Loss: 0.0734\n",
      "Epoch 105/300 - Train Loss: 0.0393, Val Loss: 0.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:42:56,691] Trial 284 finished with value: 0.961822800654185 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1215257977167139, 'learning_rate': 0.0001020012357561303, 'batch_size': 32, 'weight_decay': 7.696979004773314e-05}. Best is trial 213 with value: 0.979026323031575.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 106/300 - Train Loss: 0.0410, Val Loss: 0.0773\n",
      "Early stopping at epoch 106\n",
      "Macro F1 Score: 0.9618, Macro Precision: 0.9500, Macro Recall: 0.9750\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 286\n",
      "Training with F1=32, F2=32, D=2, dropout=0.10015264740510688, LR=0.00011717033043332039, BS=32, WD=6.21593544092793e-05\n",
      "Epoch 1/300 - Train Loss: 0.2279, Val Loss: 0.0992\n",
      "Epoch 2/300 - Train Loss: 0.1033, Val Loss: 0.0885\n",
      "Epoch 3/300 - Train Loss: 0.0969, Val Loss: 0.0750\n",
      "Epoch 4/300 - Train Loss: 0.0906, Val Loss: 0.0875\n",
      "Epoch 5/300 - Train Loss: 0.0865, Val Loss: 0.0733\n",
      "Epoch 6/300 - Train Loss: 0.0882, Val Loss: 0.0794\n",
      "Epoch 7/300 - Train Loss: 0.0843, Val Loss: 0.0725\n",
      "Epoch 8/300 - Train Loss: 0.0830, Val Loss: 0.0718\n",
      "Epoch 9/300 - Train Loss: 0.0807, Val Loss: 0.0701\n",
      "Epoch 10/300 - Train Loss: 0.0811, Val Loss: 0.0739\n",
      "Epoch 11/300 - Train Loss: 0.0777, Val Loss: 0.0786\n",
      "Epoch 12/300 - Train Loss: 0.0784, Val Loss: 0.0710\n",
      "Epoch 13/300 - Train Loss: 0.0802, Val Loss: 0.0786\n",
      "Epoch 14/300 - Train Loss: 0.0752, Val Loss: 0.0730\n",
      "Epoch 15/300 - Train Loss: 0.0742, Val Loss: 0.0767\n",
      "Epoch 16/300 - Train Loss: 0.0727, Val Loss: 0.0696\n",
      "Epoch 17/300 - Train Loss: 0.0734, Val Loss: 0.0681\n",
      "Epoch 18/300 - Train Loss: 0.0763, Val Loss: 0.0734\n",
      "Epoch 19/300 - Train Loss: 0.0711, Val Loss: 0.0704\n",
      "Epoch 20/300 - Train Loss: 0.0705, Val Loss: 0.0791\n",
      "Epoch 21/300 - Train Loss: 0.0710, Val Loss: 0.0752\n",
      "Epoch 22/300 - Train Loss: 0.0740, Val Loss: 0.0692\n",
      "Epoch 23/300 - Train Loss: 0.0688, Val Loss: 0.0671\n",
      "Epoch 24/300 - Train Loss: 0.0668, Val Loss: 0.0683\n",
      "Epoch 25/300 - Train Loss: 0.0671, Val Loss: 0.0683\n",
      "Epoch 26/300 - Train Loss: 0.0742, Val Loss: 0.0638\n",
      "Epoch 27/300 - Train Loss: 0.0660, Val Loss: 0.0708\n",
      "Epoch 28/300 - Train Loss: 0.0635, Val Loss: 0.0706\n",
      "Epoch 29/300 - Train Loss: 0.0650, Val Loss: 0.0672\n",
      "Epoch 30/300 - Train Loss: 0.0633, Val Loss: 0.0689\n",
      "Epoch 31/300 - Train Loss: 0.0616, Val Loss: 0.0674\n",
      "Epoch 32/300 - Train Loss: 0.0637, Val Loss: 0.0656\n",
      "Epoch 33/300 - Train Loss: 0.0609, Val Loss: 0.0704\n",
      "Epoch 34/300 - Train Loss: 0.0596, Val Loss: 0.0678\n",
      "Epoch 35/300 - Train Loss: 0.0626, Val Loss: 0.0652\n",
      "Epoch 36/300 - Train Loss: 0.0594, Val Loss: 0.0684\n",
      "Epoch 37/300 - Train Loss: 0.0578, Val Loss: 0.0661\n",
      "Epoch 38/300 - Train Loss: 0.0589, Val Loss: 0.0680\n",
      "Epoch 39/300 - Train Loss: 0.0578, Val Loss: 0.0678\n",
      "Epoch 40/300 - Train Loss: 0.0585, Val Loss: 0.0704\n",
      "Epoch 41/300 - Train Loss: 0.0587, Val Loss: 0.0666\n",
      "Epoch 42/300 - Train Loss: 0.0575, Val Loss: 0.0646\n",
      "Epoch 43/300 - Train Loss: 0.0582, Val Loss: 0.0664\n",
      "Epoch 44/300 - Train Loss: 0.0559, Val Loss: 0.0656\n",
      "Epoch 45/300 - Train Loss: 0.0557, Val Loss: 0.0626\n",
      "Epoch 46/300 - Train Loss: 0.0553, Val Loss: 0.0696\n",
      "Epoch 47/300 - Train Loss: 0.0561, Val Loss: 0.0659\n",
      "Epoch 48/300 - Train Loss: 0.0528, Val Loss: 0.0629\n",
      "Epoch 49/300 - Train Loss: 0.0531, Val Loss: 0.0677\n",
      "Epoch 50/300 - Train Loss: 0.0518, Val Loss: 0.0656\n",
      "Epoch 51/300 - Train Loss: 0.0524, Val Loss: 0.0703\n",
      "Epoch 52/300 - Train Loss: 0.0505, Val Loss: 0.0652\n",
      "Epoch 53/300 - Train Loss: 0.0502, Val Loss: 0.0686\n",
      "Epoch 54/300 - Train Loss: 0.0497, Val Loss: 0.0656\n",
      "Epoch 55/300 - Train Loss: 0.0502, Val Loss: 0.0670\n",
      "Epoch 56/300 - Train Loss: 0.0509, Val Loss: 0.0682\n",
      "Epoch 57/300 - Train Loss: 0.0494, Val Loss: 0.0724\n",
      "Epoch 58/300 - Train Loss: 0.0470, Val Loss: 0.0662\n",
      "Epoch 59/300 - Train Loss: 0.0483, Val Loss: 0.0718\n",
      "Epoch 60/300 - Train Loss: 0.0505, Val Loss: 0.0677\n",
      "Epoch 61/300 - Train Loss: 0.0465, Val Loss: 0.0710\n",
      "Epoch 62/300 - Train Loss: 0.0469, Val Loss: 0.0648\n",
      "Epoch 63/300 - Train Loss: 0.0449, Val Loss: 0.0691\n",
      "Epoch 64/300 - Train Loss: 0.0473, Val Loss: 0.0702\n",
      "Epoch 65/300 - Train Loss: 0.0444, Val Loss: 0.0728\n",
      "Epoch 66/300 - Train Loss: 0.0467, Val Loss: 0.0720\n",
      "Epoch 67/300 - Train Loss: 0.0456, Val Loss: 0.0717\n",
      "Epoch 68/300 - Train Loss: 0.0435, Val Loss: 0.0667\n",
      "Epoch 69/300 - Train Loss: 0.0480, Val Loss: 0.0721\n",
      "Epoch 70/300 - Train Loss: 0.0447, Val Loss: 0.0713\n",
      "Epoch 71/300 - Train Loss: 0.0448, Val Loss: 0.0725\n",
      "Epoch 72/300 - Train Loss: 0.0434, Val Loss: 0.0689\n",
      "Epoch 73/300 - Train Loss: 0.0423, Val Loss: 0.0699\n",
      "Epoch 74/300 - Train Loss: 0.0429, Val Loss: 0.0738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:46:22,169] Trial 285 finished with value: 0.9804401364947632 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.10015264740510688, 'learning_rate': 0.00011717033043332039, 'batch_size': 32, 'weight_decay': 6.21593544092793e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300 - Train Loss: 0.0397, Val Loss: 0.0685\n",
      "Early stopping at epoch 75\n",
      "Macro F1 Score: 0.9804, Macro Precision: 0.9762, Macro Recall: 0.9848\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.98      0.97        61\n",
      "           2       0.99      0.98      0.99       593\n",
      "\n",
      "    accuracy                           0.99      1443\n",
      "   macro avg       0.98      0.98      0.98      1443\n",
      "weighted avg       0.99      0.99      0.99      1443\n",
      "\n",
      "\n",
      "Trial 287\n",
      "Training with F1=32, F2=32, D=2, dropout=0.11058083643878966, LR=0.00011758958194631217, BS=32, WD=6.893789042786175e-05\n",
      "Epoch 1/300 - Train Loss: 0.2426, Val Loss: 0.0986\n",
      "Epoch 2/300 - Train Loss: 0.1113, Val Loss: 0.0834\n",
      "Epoch 3/300 - Train Loss: 0.1000, Val Loss: 0.0873\n",
      "Epoch 4/300 - Train Loss: 0.0935, Val Loss: 0.0871\n",
      "Epoch 5/300 - Train Loss: 0.0912, Val Loss: 0.0745\n",
      "Epoch 6/300 - Train Loss: 0.0893, Val Loss: 0.0740\n",
      "Epoch 7/300 - Train Loss: 0.0853, Val Loss: 0.0698\n",
      "Epoch 8/300 - Train Loss: 0.0829, Val Loss: 0.0891\n",
      "Epoch 9/300 - Train Loss: 0.0806, Val Loss: 0.0692\n",
      "Epoch 10/300 - Train Loss: 0.0784, Val Loss: 0.0783\n",
      "Epoch 11/300 - Train Loss: 0.0781, Val Loss: 0.0717\n",
      "Epoch 12/300 - Train Loss: 0.0754, Val Loss: 0.0698\n",
      "Epoch 13/300 - Train Loss: 0.0745, Val Loss: 0.0674\n",
      "Epoch 14/300 - Train Loss: 0.0767, Val Loss: 0.0769\n",
      "Epoch 15/300 - Train Loss: 0.0745, Val Loss: 0.0680\n",
      "Epoch 16/300 - Train Loss: 0.0736, Val Loss: 0.0695\n",
      "Epoch 17/300 - Train Loss: 0.0709, Val Loss: 0.0727\n",
      "Epoch 18/300 - Train Loss: 0.0707, Val Loss: 0.0726\n",
      "Epoch 19/300 - Train Loss: 0.0721, Val Loss: 0.0739\n",
      "Epoch 20/300 - Train Loss: 0.0684, Val Loss: 0.0697\n",
      "Epoch 21/300 - Train Loss: 0.0693, Val Loss: 0.0726\n",
      "Epoch 22/300 - Train Loss: 0.0679, Val Loss: 0.0674\n",
      "Epoch 23/300 - Train Loss: 0.0642, Val Loss: 0.0759\n",
      "Epoch 24/300 - Train Loss: 0.0679, Val Loss: 0.0669\n",
      "Epoch 25/300 - Train Loss: 0.0655, Val Loss: 0.0691\n",
      "Epoch 26/300 - Train Loss: 0.0646, Val Loss: 0.0659\n",
      "Epoch 27/300 - Train Loss: 0.0642, Val Loss: 0.0628\n",
      "Epoch 28/300 - Train Loss: 0.0652, Val Loss: 0.0659\n",
      "Epoch 29/300 - Train Loss: 0.0631, Val Loss: 0.0687\n",
      "Epoch 30/300 - Train Loss: 0.0631, Val Loss: 0.0652\n",
      "Epoch 31/300 - Train Loss: 0.0617, Val Loss: 0.0650\n",
      "Epoch 32/300 - Train Loss: 0.0598, Val Loss: 0.0707\n",
      "Epoch 33/300 - Train Loss: 0.0609, Val Loss: 0.0695\n",
      "Epoch 34/300 - Train Loss: 0.0609, Val Loss: 0.0682\n",
      "Epoch 35/300 - Train Loss: 0.0613, Val Loss: 0.0730\n",
      "Epoch 36/300 - Train Loss: 0.0582, Val Loss: 0.0629\n",
      "Epoch 37/300 - Train Loss: 0.0575, Val Loss: 0.0684\n",
      "Epoch 38/300 - Train Loss: 0.0583, Val Loss: 0.0662\n",
      "Epoch 39/300 - Train Loss: 0.0572, Val Loss: 0.0646\n",
      "Epoch 40/300 - Train Loss: 0.0566, Val Loss: 0.0668\n",
      "Epoch 41/300 - Train Loss: 0.0593, Val Loss: 0.0656\n",
      "Epoch 42/300 - Train Loss: 0.0584, Val Loss: 0.0661\n",
      "Epoch 43/300 - Train Loss: 0.0559, Val Loss: 0.0664\n",
      "Epoch 44/300 - Train Loss: 0.0568, Val Loss: 0.0677\n",
      "Epoch 45/300 - Train Loss: 0.0545, Val Loss: 0.0699\n",
      "Epoch 46/300 - Train Loss: 0.0552, Val Loss: 0.0677\n",
      "Epoch 47/300 - Train Loss: 0.0548, Val Loss: 0.0711\n",
      "Epoch 48/300 - Train Loss: 0.0554, Val Loss: 0.0699\n",
      "Epoch 49/300 - Train Loss: 0.0541, Val Loss: 0.0666\n",
      "Epoch 50/300 - Train Loss: 0.0513, Val Loss: 0.0682\n",
      "Epoch 51/300 - Train Loss: 0.0520, Val Loss: 0.0678\n",
      "Epoch 52/300 - Train Loss: 0.0510, Val Loss: 0.0708\n",
      "Epoch 53/300 - Train Loss: 0.0523, Val Loss: 0.0706\n",
      "Epoch 54/300 - Train Loss: 0.0480, Val Loss: 0.0735\n",
      "Epoch 55/300 - Train Loss: 0.0517, Val Loss: 0.0650\n",
      "Epoch 56/300 - Train Loss: 0.0498, Val Loss: 0.0749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:48:58,141] Trial 286 finished with value: 0.9735857677162225 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.11058083643878966, 'learning_rate': 0.00011758958194631217, 'batch_size': 32, 'weight_decay': 6.893789042786175e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/300 - Train Loss: 0.0497, Val Loss: 0.0707\n",
      "Early stopping at epoch 57\n",
      "Macro F1 Score: 0.9736, Macro Precision: 0.9694, Macro Recall: 0.9780\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 288\n",
      "Training with F1=32, F2=32, D=2, dropout=0.11009004574718882, LR=0.0001210206634978174, BS=32, WD=7.016257523321093e-05\n",
      "Epoch 1/300 - Train Loss: 0.2448, Val Loss: 0.0970\n",
      "Epoch 2/300 - Train Loss: 0.1128, Val Loss: 0.0860\n",
      "Epoch 3/300 - Train Loss: 0.0973, Val Loss: 0.0768\n",
      "Epoch 4/300 - Train Loss: 0.0934, Val Loss: 0.0782\n",
      "Epoch 5/300 - Train Loss: 0.0933, Val Loss: 0.0770\n",
      "Epoch 6/300 - Train Loss: 0.0875, Val Loss: 0.0770\n",
      "Epoch 7/300 - Train Loss: 0.0849, Val Loss: 0.0694\n",
      "Epoch 8/300 - Train Loss: 0.0840, Val Loss: 0.0773\n",
      "Epoch 9/300 - Train Loss: 0.0818, Val Loss: 0.0704\n",
      "Epoch 10/300 - Train Loss: 0.0789, Val Loss: 0.0760\n",
      "Epoch 11/300 - Train Loss: 0.0795, Val Loss: 0.0827\n",
      "Epoch 12/300 - Train Loss: 0.0819, Val Loss: 0.0752\n",
      "Epoch 13/300 - Train Loss: 0.0757, Val Loss: 0.0726\n",
      "Epoch 14/300 - Train Loss: 0.0756, Val Loss: 0.0708\n",
      "Epoch 15/300 - Train Loss: 0.0747, Val Loss: 0.0775\n",
      "Epoch 16/300 - Train Loss: 0.0742, Val Loss: 0.0740\n",
      "Epoch 17/300 - Train Loss: 0.0732, Val Loss: 0.0720\n",
      "Epoch 18/300 - Train Loss: 0.0700, Val Loss: 0.0705\n",
      "Epoch 19/300 - Train Loss: 0.0719, Val Loss: 0.0835\n",
      "Epoch 20/300 - Train Loss: 0.0717, Val Loss: 0.0728\n",
      "Epoch 21/300 - Train Loss: 0.0717, Val Loss: 0.0747\n",
      "Epoch 22/300 - Train Loss: 0.0690, Val Loss: 0.0734\n",
      "Epoch 23/300 - Train Loss: 0.0700, Val Loss: 0.0739\n",
      "Epoch 24/300 - Train Loss: 0.0683, Val Loss: 0.0680\n",
      "Epoch 25/300 - Train Loss: 0.0689, Val Loss: 0.0735\n",
      "Epoch 26/300 - Train Loss: 0.0681, Val Loss: 0.0721\n",
      "Epoch 27/300 - Train Loss: 0.0666, Val Loss: 0.0694\n",
      "Epoch 28/300 - Train Loss: 0.0650, Val Loss: 0.0750\n",
      "Epoch 29/300 - Train Loss: 0.0654, Val Loss: 0.0720\n",
      "Epoch 30/300 - Train Loss: 0.0637, Val Loss: 0.0729\n",
      "Epoch 31/300 - Train Loss: 0.0683, Val Loss: 0.0701\n",
      "Epoch 32/300 - Train Loss: 0.0646, Val Loss: 0.0739\n",
      "Epoch 33/300 - Train Loss: 0.0607, Val Loss: 0.0701\n",
      "Epoch 34/300 - Train Loss: 0.0624, Val Loss: 0.0705\n",
      "Epoch 35/300 - Train Loss: 0.0618, Val Loss: 0.0712\n",
      "Epoch 36/300 - Train Loss: 0.0590, Val Loss: 0.0754\n",
      "Epoch 37/300 - Train Loss: 0.0614, Val Loss: 0.0774\n",
      "Epoch 38/300 - Train Loss: 0.0591, Val Loss: 0.0714\n",
      "Epoch 39/300 - Train Loss: 0.0593, Val Loss: 0.0786\n",
      "Epoch 40/300 - Train Loss: 0.0582, Val Loss: 0.0703\n",
      "Epoch 41/300 - Train Loss: 0.0579, Val Loss: 0.0738\n",
      "Epoch 42/300 - Train Loss: 0.0586, Val Loss: 0.0761\n",
      "Epoch 43/300 - Train Loss: 0.0591, Val Loss: 0.0791\n",
      "Epoch 44/300 - Train Loss: 0.0580, Val Loss: 0.0785\n",
      "Epoch 45/300 - Train Loss: 0.0555, Val Loss: 0.0724\n",
      "Epoch 46/300 - Train Loss: 0.0572, Val Loss: 0.0772\n",
      "Epoch 47/300 - Train Loss: 0.0577, Val Loss: 0.0690\n",
      "Epoch 48/300 - Train Loss: 0.0547, Val Loss: 0.0732\n",
      "Epoch 49/300 - Train Loss: 0.0547, Val Loss: 0.0739\n",
      "Epoch 50/300 - Train Loss: 0.0534, Val Loss: 0.0728\n",
      "Epoch 51/300 - Train Loss: 0.0530, Val Loss: 0.0700\n",
      "Epoch 52/300 - Train Loss: 0.0527, Val Loss: 0.0763\n",
      "Epoch 53/300 - Train Loss: 0.0529, Val Loss: 0.0711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:51:25,768] Trial 287 finished with value: 0.9653737390389302 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.11009004574718882, 'learning_rate': 0.0001210206634978174, 'batch_size': 32, 'weight_decay': 7.016257523321093e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300 - Train Loss: 0.0532, Val Loss: 0.0761\n",
      "Early stopping at epoch 54\n",
      "Macro F1 Score: 0.9654, Macro Precision: 0.9553, Macro Recall: 0.9764\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 289\n",
      "Training with F1=32, F2=32, D=2, dropout=0.11572281255082068, LR=9.617007384068085e-05, BS=32, WD=6.283653146284791e-05\n",
      "Epoch 1/300 - Train Loss: 0.2709, Val Loss: 0.1300\n",
      "Epoch 2/300 - Train Loss: 0.1307, Val Loss: 0.0944\n",
      "Epoch 3/300 - Train Loss: 0.1045, Val Loss: 0.0890\n",
      "Epoch 4/300 - Train Loss: 0.0979, Val Loss: 0.0742\n",
      "Epoch 5/300 - Train Loss: 0.0946, Val Loss: 0.0754\n",
      "Epoch 6/300 - Train Loss: 0.0879, Val Loss: 0.0779\n",
      "Epoch 7/300 - Train Loss: 0.0852, Val Loss: 0.0744\n",
      "Epoch 8/300 - Train Loss: 0.0869, Val Loss: 0.0925\n",
      "Epoch 9/300 - Train Loss: 0.0843, Val Loss: 0.0728\n",
      "Epoch 10/300 - Train Loss: 0.0821, Val Loss: 0.0691\n",
      "Epoch 11/300 - Train Loss: 0.0806, Val Loss: 0.0721\n",
      "Epoch 12/300 - Train Loss: 0.0802, Val Loss: 0.0705\n",
      "Epoch 13/300 - Train Loss: 0.0768, Val Loss: 0.0698\n",
      "Epoch 14/300 - Train Loss: 0.0766, Val Loss: 0.0644\n",
      "Epoch 15/300 - Train Loss: 0.0763, Val Loss: 0.0648\n",
      "Epoch 16/300 - Train Loss: 0.0775, Val Loss: 0.0677\n",
      "Epoch 17/300 - Train Loss: 0.0749, Val Loss: 0.0694\n",
      "Epoch 18/300 - Train Loss: 0.0740, Val Loss: 0.0685\n",
      "Epoch 19/300 - Train Loss: 0.0752, Val Loss: 0.0688\n",
      "Epoch 20/300 - Train Loss: 0.0734, Val Loss: 0.0672\n",
      "Epoch 21/300 - Train Loss: 0.0720, Val Loss: 0.0663\n",
      "Epoch 22/300 - Train Loss: 0.0706, Val Loss: 0.0678\n",
      "Epoch 23/300 - Train Loss: 0.0702, Val Loss: 0.0715\n",
      "Epoch 24/300 - Train Loss: 0.0699, Val Loss: 0.0696\n",
      "Epoch 25/300 - Train Loss: 0.0691, Val Loss: 0.0831\n",
      "Epoch 26/300 - Train Loss: 0.0695, Val Loss: 0.0637\n",
      "Epoch 27/300 - Train Loss: 0.0680, Val Loss: 0.0638\n",
      "Epoch 28/300 - Train Loss: 0.0690, Val Loss: 0.0654\n",
      "Epoch 29/300 - Train Loss: 0.0677, Val Loss: 0.0712\n",
      "Epoch 30/300 - Train Loss: 0.0668, Val Loss: 0.0668\n",
      "Epoch 31/300 - Train Loss: 0.0653, Val Loss: 0.0793\n",
      "Epoch 32/300 - Train Loss: 0.0653, Val Loss: 0.0713\n",
      "Epoch 33/300 - Train Loss: 0.0641, Val Loss: 0.0653\n",
      "Epoch 34/300 - Train Loss: 0.0634, Val Loss: 0.0665\n",
      "Epoch 35/300 - Train Loss: 0.0627, Val Loss: 0.0701\n",
      "Epoch 36/300 - Train Loss: 0.0629, Val Loss: 0.0715\n",
      "Epoch 37/300 - Train Loss: 0.0630, Val Loss: 0.0701\n",
      "Epoch 38/300 - Train Loss: 0.0631, Val Loss: 0.0699\n",
      "Epoch 39/300 - Train Loss: 0.0623, Val Loss: 0.0688\n",
      "Epoch 40/300 - Train Loss: 0.0632, Val Loss: 0.0671\n",
      "Epoch 41/300 - Train Loss: 0.0609, Val Loss: 0.0716\n",
      "Epoch 42/300 - Train Loss: 0.0576, Val Loss: 0.0715\n",
      "Epoch 43/300 - Train Loss: 0.0583, Val Loss: 0.0728\n",
      "Epoch 44/300 - Train Loss: 0.0607, Val Loss: 0.0645\n",
      "Epoch 45/300 - Train Loss: 0.0575, Val Loss: 0.0682\n",
      "Epoch 46/300 - Train Loss: 0.0566, Val Loss: 0.0683\n",
      "Epoch 47/300 - Train Loss: 0.0575, Val Loss: 0.0715\n",
      "Epoch 48/300 - Train Loss: 0.0562, Val Loss: 0.0687\n",
      "Epoch 49/300 - Train Loss: 0.0563, Val Loss: 0.0745\n",
      "Epoch 50/300 - Train Loss: 0.0550, Val Loss: 0.0721\n",
      "Epoch 51/300 - Train Loss: 0.0537, Val Loss: 0.0764\n",
      "Epoch 52/300 - Train Loss: 0.0546, Val Loss: 0.0704\n",
      "Epoch 53/300 - Train Loss: 0.0548, Val Loss: 0.0687\n",
      "Epoch 54/300 - Train Loss: 0.0550, Val Loss: 0.0784\n",
      "Epoch 55/300 - Train Loss: 0.0527, Val Loss: 0.0700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:53:59,069] Trial 288 finished with value: 0.9635146124441668 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.11572281255082068, 'learning_rate': 9.617007384068085e-05, 'batch_size': 32, 'weight_decay': 6.283653146284791e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300 - Train Loss: 0.0530, Val Loss: 0.0713\n",
      "Early stopping at epoch 56\n",
      "Macro F1 Score: 0.9635, Macro Precision: 0.9578, Macro Recall: 0.9697\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 290\n",
      "Training with F1=32, F2=32, D=2, dropout=0.10041979512811135, LR=0.00011266106415076785, BS=32, WD=8.80243693271445e-05\n",
      "Epoch 1/300 - Train Loss: 0.2320, Val Loss: 0.0893\n",
      "Epoch 2/300 - Train Loss: 0.1055, Val Loss: 0.0751\n",
      "Epoch 3/300 - Train Loss: 0.0958, Val Loss: 0.0755\n",
      "Epoch 4/300 - Train Loss: 0.0910, Val Loss: 0.0809\n",
      "Epoch 5/300 - Train Loss: 0.0873, Val Loss: 0.0742\n",
      "Epoch 6/300 - Train Loss: 0.0855, Val Loss: 0.0683\n",
      "Epoch 7/300 - Train Loss: 0.0844, Val Loss: 0.0746\n",
      "Epoch 8/300 - Train Loss: 0.0827, Val Loss: 0.0668\n",
      "Epoch 9/300 - Train Loss: 0.0809, Val Loss: 0.0742\n",
      "Epoch 10/300 - Train Loss: 0.0798, Val Loss: 0.0703\n",
      "Epoch 11/300 - Train Loss: 0.0785, Val Loss: 0.0667\n",
      "Epoch 12/300 - Train Loss: 0.0779, Val Loss: 0.0753\n",
      "Epoch 13/300 - Train Loss: 0.0767, Val Loss: 0.0702\n",
      "Epoch 14/300 - Train Loss: 0.0765, Val Loss: 0.0676\n",
      "Epoch 15/300 - Train Loss: 0.0747, Val Loss: 0.0690\n",
      "Epoch 16/300 - Train Loss: 0.0751, Val Loss: 0.0718\n",
      "Epoch 17/300 - Train Loss: 0.0719, Val Loss: 0.0701\n",
      "Epoch 18/300 - Train Loss: 0.0746, Val Loss: 0.0668\n",
      "Epoch 19/300 - Train Loss: 0.0700, Val Loss: 0.0705\n",
      "Epoch 20/300 - Train Loss: 0.0699, Val Loss: 0.0720\n",
      "Epoch 21/300 - Train Loss: 0.0702, Val Loss: 0.0739\n",
      "Epoch 22/300 - Train Loss: 0.0702, Val Loss: 0.0764\n",
      "Epoch 23/300 - Train Loss: 0.0685, Val Loss: 0.0713\n",
      "Epoch 24/300 - Train Loss: 0.0665, Val Loss: 0.0674\n",
      "Epoch 25/300 - Train Loss: 0.0667, Val Loss: 0.0721\n",
      "Epoch 26/300 - Train Loss: 0.0668, Val Loss: 0.0756\n",
      "Epoch 27/300 - Train Loss: 0.0660, Val Loss: 0.0729\n",
      "Epoch 28/300 - Train Loss: 0.0666, Val Loss: 0.0694\n",
      "Epoch 29/300 - Train Loss: 0.0647, Val Loss: 0.0735\n",
      "Epoch 30/300 - Train Loss: 0.0647, Val Loss: 0.0679\n",
      "Epoch 31/300 - Train Loss: 0.0624, Val Loss: 0.0708\n",
      "Epoch 32/300 - Train Loss: 0.0614, Val Loss: 0.0734\n",
      "Epoch 33/300 - Train Loss: 0.0631, Val Loss: 0.0723\n",
      "Epoch 34/300 - Train Loss: 0.0610, Val Loss: 0.0714\n",
      "Epoch 35/300 - Train Loss: 0.0613, Val Loss: 0.0719\n",
      "Epoch 36/300 - Train Loss: 0.0604, Val Loss: 0.0726\n",
      "Epoch 37/300 - Train Loss: 0.0590, Val Loss: 0.0764\n",
      "Epoch 38/300 - Train Loss: 0.0594, Val Loss: 0.0721\n",
      "Epoch 39/300 - Train Loss: 0.0588, Val Loss: 0.0723\n",
      "Epoch 40/300 - Train Loss: 0.0589, Val Loss: 0.0690\n",
      "Epoch 41/300 - Train Loss: 0.0565, Val Loss: 0.0667\n",
      "Epoch 42/300 - Train Loss: 0.0566, Val Loss: 0.0680\n",
      "Epoch 43/300 - Train Loss: 0.0553, Val Loss: 0.0724\n",
      "Epoch 44/300 - Train Loss: 0.0554, Val Loss: 0.0693\n",
      "Epoch 45/300 - Train Loss: 0.0544, Val Loss: 0.0724\n",
      "Epoch 46/300 - Train Loss: 0.0535, Val Loss: 0.0736\n",
      "Epoch 47/300 - Train Loss: 0.0524, Val Loss: 0.0802\n",
      "Epoch 48/300 - Train Loss: 0.0546, Val Loss: 0.0720\n",
      "Epoch 49/300 - Train Loss: 0.0534, Val Loss: 0.0693\n",
      "Epoch 50/300 - Train Loss: 0.0532, Val Loss: 0.0719\n",
      "Epoch 51/300 - Train Loss: 0.0531, Val Loss: 0.0703\n",
      "Epoch 52/300 - Train Loss: 0.0519, Val Loss: 0.0720\n",
      "Epoch 53/300 - Train Loss: 0.0521, Val Loss: 0.0702\n",
      "Epoch 54/300 - Train Loss: 0.0517, Val Loss: 0.0733\n",
      "Epoch 55/300 - Train Loss: 0.0515, Val Loss: 0.0742\n",
      "Epoch 56/300 - Train Loss: 0.0496, Val Loss: 0.0673\n",
      "Epoch 57/300 - Train Loss: 0.0500, Val Loss: 0.0724\n",
      "Epoch 58/300 - Train Loss: 0.0503, Val Loss: 0.0739\n",
      "Epoch 59/300 - Train Loss: 0.0483, Val Loss: 0.0734\n",
      "Epoch 60/300 - Train Loss: 0.0485, Val Loss: 0.0761\n",
      "Epoch 61/300 - Train Loss: 0.0473, Val Loss: 0.0715\n",
      "Epoch 62/300 - Train Loss: 0.0478, Val Loss: 0.0732\n",
      "Epoch 63/300 - Train Loss: 0.0465, Val Loss: 0.0752\n",
      "Epoch 64/300 - Train Loss: 0.0457, Val Loss: 0.0710\n",
      "Epoch 65/300 - Train Loss: 0.0455, Val Loss: 0.0747\n",
      "Epoch 66/300 - Train Loss: 0.0463, Val Loss: 0.0686\n",
      "Epoch 67/300 - Train Loss: 0.0455, Val Loss: 0.0718\n",
      "Epoch 68/300 - Train Loss: 0.0448, Val Loss: 0.0711\n",
      "Epoch 69/300 - Train Loss: 0.0455, Val Loss: 0.0708\n",
      "Epoch 70/300 - Train Loss: 0.0458, Val Loss: 0.0724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:57:13,079] Trial 289 finished with value: 0.9691803942568026 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.10041979512811135, 'learning_rate': 0.00011266106415076785, 'batch_size': 32, 'weight_decay': 8.80243693271445e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300 - Train Loss: 0.0460, Val Loss: 0.0785\n",
      "Early stopping at epoch 71\n",
      "Macro F1 Score: 0.9692, Macro Precision: 0.9609, Macro Recall: 0.9781\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.98      0.99       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 291\n",
      "Training with F1=32, F2=32, D=2, dropout=0.12331740720455879, LR=0.0001275577853720698, BS=32, WD=7.483241260368301e-05\n",
      "Epoch 1/300 - Train Loss: 0.2353, Val Loss: 0.1239\n",
      "Epoch 2/300 - Train Loss: 0.1088, Val Loss: 0.0810\n",
      "Epoch 3/300 - Train Loss: 0.0960, Val Loss: 0.0800\n",
      "Epoch 4/300 - Train Loss: 0.0910, Val Loss: 0.0764\n",
      "Epoch 5/300 - Train Loss: 0.0881, Val Loss: 0.0766\n",
      "Epoch 6/300 - Train Loss: 0.0860, Val Loss: 0.0838\n",
      "Epoch 7/300 - Train Loss: 0.0858, Val Loss: 0.0695\n",
      "Epoch 8/300 - Train Loss: 0.0867, Val Loss: 0.0727\n",
      "Epoch 9/300 - Train Loss: 0.0827, Val Loss: 0.0758\n",
      "Epoch 10/300 - Train Loss: 0.0832, Val Loss: 0.0783\n",
      "Epoch 11/300 - Train Loss: 0.0784, Val Loss: 0.0758\n",
      "Epoch 12/300 - Train Loss: 0.0795, Val Loss: 0.0731\n",
      "Epoch 13/300 - Train Loss: 0.0779, Val Loss: 0.0715\n",
      "Epoch 14/300 - Train Loss: 0.0782, Val Loss: 0.0679\n",
      "Epoch 15/300 - Train Loss: 0.0754, Val Loss: 0.0797\n",
      "Epoch 16/300 - Train Loss: 0.0766, Val Loss: 0.0718\n",
      "Epoch 17/300 - Train Loss: 0.0755, Val Loss: 0.0717\n",
      "Epoch 18/300 - Train Loss: 0.0726, Val Loss: 0.0768\n",
      "Epoch 19/300 - Train Loss: 0.0718, Val Loss: 0.0723\n",
      "Epoch 20/300 - Train Loss: 0.0721, Val Loss: 0.0731\n",
      "Epoch 21/300 - Train Loss: 0.0723, Val Loss: 0.0700\n",
      "Epoch 22/300 - Train Loss: 0.0711, Val Loss: 0.0685\n",
      "Epoch 23/300 - Train Loss: 0.0703, Val Loss: 0.0712\n",
      "Epoch 24/300 - Train Loss: 0.0706, Val Loss: 0.0682\n",
      "Epoch 25/300 - Train Loss: 0.0685, Val Loss: 0.0762\n",
      "Epoch 26/300 - Train Loss: 0.0696, Val Loss: 0.0754\n",
      "Epoch 27/300 - Train Loss: 0.0670, Val Loss: 0.0705\n",
      "Epoch 28/300 - Train Loss: 0.0680, Val Loss: 0.0759\n",
      "Epoch 29/300 - Train Loss: 0.0678, Val Loss: 0.0728\n",
      "Epoch 30/300 - Train Loss: 0.0667, Val Loss: 0.0692\n",
      "Epoch 31/300 - Train Loss: 0.0674, Val Loss: 0.0718\n",
      "Epoch 32/300 - Train Loss: 0.0646, Val Loss: 0.0729\n",
      "Epoch 33/300 - Train Loss: 0.0639, Val Loss: 0.0740\n",
      "Epoch 34/300 - Train Loss: 0.0655, Val Loss: 0.0707\n",
      "Epoch 35/300 - Train Loss: 0.0638, Val Loss: 0.0716\n",
      "Epoch 36/300 - Train Loss: 0.0633, Val Loss: 0.0702\n",
      "Epoch 37/300 - Train Loss: 0.0628, Val Loss: 0.0695\n",
      "Epoch 38/300 - Train Loss: 0.0601, Val Loss: 0.0700\n",
      "Epoch 39/300 - Train Loss: 0.0605, Val Loss: 0.0721\n",
      "Epoch 40/300 - Train Loss: 0.0603, Val Loss: 0.0688\n",
      "Epoch 41/300 - Train Loss: 0.0586, Val Loss: 0.0691\n",
      "Epoch 42/300 - Train Loss: 0.0590, Val Loss: 0.0717\n",
      "Epoch 43/300 - Train Loss: 0.0585, Val Loss: 0.0699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 18:59:13,450] Trial 290 finished with value: 0.9701509616451545 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.12331740720455879, 'learning_rate': 0.0001275577853720698, 'batch_size': 32, 'weight_decay': 7.483241260368301e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300 - Train Loss: 0.0585, Val Loss: 0.0696\n",
      "Early stopping at epoch 44\n",
      "Macro F1 Score: 0.9702, Macro Precision: 0.9732, Macro Recall: 0.9672\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 292\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14218024521215283, LR=9.033412852376503e-05, BS=32, WD=6.39548767161376e-05\n",
      "Epoch 1/300 - Train Loss: 0.2847, Val Loss: 0.1015\n",
      "Epoch 2/300 - Train Loss: 0.1113, Val Loss: 0.0786\n",
      "Epoch 3/300 - Train Loss: 0.0978, Val Loss: 0.0769\n",
      "Epoch 4/300 - Train Loss: 0.0933, Val Loss: 0.0807\n",
      "Epoch 5/300 - Train Loss: 0.0892, Val Loss: 0.0706\n",
      "Epoch 6/300 - Train Loss: 0.0887, Val Loss: 0.0743\n",
      "Epoch 7/300 - Train Loss: 0.0863, Val Loss: 0.0735\n",
      "Epoch 8/300 - Train Loss: 0.0848, Val Loss: 0.0748\n",
      "Epoch 9/300 - Train Loss: 0.0825, Val Loss: 0.0779\n",
      "Epoch 10/300 - Train Loss: 0.0811, Val Loss: 0.0753\n",
      "Epoch 11/300 - Train Loss: 0.0817, Val Loss: 0.0716\n",
      "Epoch 12/300 - Train Loss: 0.0786, Val Loss: 0.0685\n",
      "Epoch 13/300 - Train Loss: 0.0790, Val Loss: 0.0710\n",
      "Epoch 14/300 - Train Loss: 0.0780, Val Loss: 0.0746\n",
      "Epoch 15/300 - Train Loss: 0.0768, Val Loss: 0.0704\n",
      "Epoch 16/300 - Train Loss: 0.0771, Val Loss: 0.0757\n",
      "Epoch 17/300 - Train Loss: 0.0742, Val Loss: 0.0770\n",
      "Epoch 18/300 - Train Loss: 0.0760, Val Loss: 0.0674\n",
      "Epoch 19/300 - Train Loss: 0.0747, Val Loss: 0.0758\n",
      "Epoch 20/300 - Train Loss: 0.0733, Val Loss: 0.0866\n",
      "Epoch 21/300 - Train Loss: 0.0718, Val Loss: 0.0701\n",
      "Epoch 22/300 - Train Loss: 0.0736, Val Loss: 0.0775\n",
      "Epoch 23/300 - Train Loss: 0.0715, Val Loss: 0.0691\n",
      "Epoch 24/300 - Train Loss: 0.0731, Val Loss: 0.0711\n",
      "Epoch 25/300 - Train Loss: 0.0728, Val Loss: 0.0747\n",
      "Epoch 26/300 - Train Loss: 0.0720, Val Loss: 0.0722\n",
      "Epoch 27/300 - Train Loss: 0.0714, Val Loss: 0.0706\n",
      "Epoch 28/300 - Train Loss: 0.0713, Val Loss: 0.0718\n",
      "Epoch 29/300 - Train Loss: 0.0694, Val Loss: 0.0725\n",
      "Epoch 30/300 - Train Loss: 0.0689, Val Loss: 0.0675\n",
      "Epoch 31/300 - Train Loss: 0.0658, Val Loss: 0.0716\n",
      "Epoch 32/300 - Train Loss: 0.0674, Val Loss: 0.0769\n",
      "Epoch 33/300 - Train Loss: 0.0664, Val Loss: 0.0703\n",
      "Epoch 34/300 - Train Loss: 0.0683, Val Loss: 0.0796\n",
      "Epoch 35/300 - Train Loss: 0.0659, Val Loss: 0.0700\n",
      "Epoch 36/300 - Train Loss: 0.0681, Val Loss: 0.0708\n",
      "Epoch 37/300 - Train Loss: 0.0649, Val Loss: 0.0739\n",
      "Epoch 38/300 - Train Loss: 0.0666, Val Loss: 0.0749\n",
      "Epoch 39/300 - Train Loss: 0.0651, Val Loss: 0.0691\n",
      "Epoch 40/300 - Train Loss: 0.0641, Val Loss: 0.0758\n",
      "Epoch 41/300 - Train Loss: 0.0639, Val Loss: 0.0699\n",
      "Epoch 42/300 - Train Loss: 0.0630, Val Loss: 0.0713\n",
      "Epoch 43/300 - Train Loss: 0.0658, Val Loss: 0.0756\n",
      "Epoch 44/300 - Train Loss: 0.0634, Val Loss: 0.0819\n",
      "Epoch 45/300 - Train Loss: 0.0625, Val Loss: 0.0708\n",
      "Epoch 46/300 - Train Loss: 0.0636, Val Loss: 0.0740\n",
      "Epoch 47/300 - Train Loss: 0.0617, Val Loss: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:01:24,664] Trial 291 finished with value: 0.9657213793373794 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14218024521215283, 'learning_rate': 9.033412852376503e-05, 'batch_size': 32, 'weight_decay': 6.39548767161376e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300 - Train Loss: 0.0631, Val Loss: 0.0754\n",
      "Early stopping at epoch 48\n",
      "Macro F1 Score: 0.9657, Macro Precision: 0.9623, Macro Recall: 0.9694\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 293\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1043218788914228, LR=0.0001128764692265161, BS=32, WD=5.3013438901415424e-05\n",
      "Epoch 1/300 - Train Loss: 0.2431, Val Loss: 0.1233\n",
      "Epoch 2/300 - Train Loss: 0.1072, Val Loss: 0.0753\n",
      "Epoch 3/300 - Train Loss: 0.0960, Val Loss: 0.0711\n",
      "Epoch 4/300 - Train Loss: 0.0917, Val Loss: 0.0736\n",
      "Epoch 5/300 - Train Loss: 0.0873, Val Loss: 0.0734\n",
      "Epoch 6/300 - Train Loss: 0.0847, Val Loss: 0.0725\n",
      "Epoch 7/300 - Train Loss: 0.0854, Val Loss: 0.0729\n",
      "Epoch 8/300 - Train Loss: 0.0817, Val Loss: 0.0706\n",
      "Epoch 9/300 - Train Loss: 0.0785, Val Loss: 0.0632\n",
      "Epoch 10/300 - Train Loss: 0.0789, Val Loss: 0.0772\n",
      "Epoch 11/300 - Train Loss: 0.0790, Val Loss: 0.0699\n",
      "Epoch 12/300 - Train Loss: 0.0776, Val Loss: 0.0656\n",
      "Epoch 13/300 - Train Loss: 0.0766, Val Loss: 0.0692\n",
      "Epoch 14/300 - Train Loss: 0.0736, Val Loss: 0.0736\n",
      "Epoch 15/300 - Train Loss: 0.0758, Val Loss: 0.0740\n",
      "Epoch 16/300 - Train Loss: 0.0725, Val Loss: 0.0736\n",
      "Epoch 17/300 - Train Loss: 0.0743, Val Loss: 0.0665\n",
      "Epoch 18/300 - Train Loss: 0.0721, Val Loss: 0.0687\n",
      "Epoch 19/300 - Train Loss: 0.0694, Val Loss: 0.0739\n",
      "Epoch 20/300 - Train Loss: 0.0721, Val Loss: 0.0698\n",
      "Epoch 21/300 - Train Loss: 0.0699, Val Loss: 0.0713\n",
      "Epoch 22/300 - Train Loss: 0.0696, Val Loss: 0.0828\n",
      "Epoch 23/300 - Train Loss: 0.0689, Val Loss: 0.0786\n",
      "Epoch 24/300 - Train Loss: 0.0672, Val Loss: 0.0750\n",
      "Epoch 25/300 - Train Loss: 0.0662, Val Loss: 0.0665\n",
      "Epoch 26/300 - Train Loss: 0.0673, Val Loss: 0.0682\n",
      "Epoch 27/300 - Train Loss: 0.0680, Val Loss: 0.0746\n",
      "Epoch 28/300 - Train Loss: 0.0658, Val Loss: 0.0673\n",
      "Epoch 29/300 - Train Loss: 0.0651, Val Loss: 0.0683\n",
      "Epoch 30/300 - Train Loss: 0.0655, Val Loss: 0.0645\n",
      "Epoch 31/300 - Train Loss: 0.0605, Val Loss: 0.0704\n",
      "Epoch 32/300 - Train Loss: 0.0622, Val Loss: 0.0682\n",
      "Epoch 33/300 - Train Loss: 0.0613, Val Loss: 0.0699\n",
      "Epoch 34/300 - Train Loss: 0.0622, Val Loss: 0.1002\n",
      "Epoch 35/300 - Train Loss: 0.0622, Val Loss: 0.0745\n",
      "Epoch 36/300 - Train Loss: 0.0583, Val Loss: 0.0692\n",
      "Epoch 37/300 - Train Loss: 0.0581, Val Loss: 0.0641\n",
      "Epoch 38/300 - Train Loss: 0.0588, Val Loss: 0.0675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:03:11,267] Trial 292 finished with value: 0.9707520950157852 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1043218788914228, 'learning_rate': 0.0001128764692265161, 'batch_size': 32, 'weight_decay': 5.3013438901415424e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300 - Train Loss: 0.0604, Val Loss: 0.0703\n",
      "Early stopping at epoch 39\n",
      "Macro F1 Score: 0.9708, Macro Precision: 0.9648, Macro Recall: 0.9771\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 294\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1268019252765063, LR=9.807615349941166e-05, BS=32, WD=6.795108474689867e-05\n",
      "Epoch 1/300 - Train Loss: 0.2454, Val Loss: 0.1413\n",
      "Epoch 2/300 - Train Loss: 0.1173, Val Loss: 0.0859\n",
      "Epoch 3/300 - Train Loss: 0.1016, Val Loss: 0.0794\n",
      "Epoch 4/300 - Train Loss: 0.0947, Val Loss: 0.0827\n",
      "Epoch 5/300 - Train Loss: 0.0915, Val Loss: 0.0863\n",
      "Epoch 6/300 - Train Loss: 0.0877, Val Loss: 0.0734\n",
      "Epoch 7/300 - Train Loss: 0.0839, Val Loss: 0.0952\n",
      "Epoch 8/300 - Train Loss: 0.0846, Val Loss: 0.0760\n",
      "Epoch 9/300 - Train Loss: 0.0831, Val Loss: 0.0705\n",
      "Epoch 10/300 - Train Loss: 0.0825, Val Loss: 0.0714\n",
      "Epoch 11/300 - Train Loss: 0.0796, Val Loss: 0.0773\n",
      "Epoch 12/300 - Train Loss: 0.0805, Val Loss: 0.0761\n",
      "Epoch 13/300 - Train Loss: 0.0795, Val Loss: 0.0678\n",
      "Epoch 14/300 - Train Loss: 0.0783, Val Loss: 0.0764\n",
      "Epoch 15/300 - Train Loss: 0.0765, Val Loss: 0.0697\n",
      "Epoch 16/300 - Train Loss: 0.0760, Val Loss: 0.0686\n",
      "Epoch 17/300 - Train Loss: 0.0743, Val Loss: 0.0681\n",
      "Epoch 18/300 - Train Loss: 0.0755, Val Loss: 0.0669\n",
      "Epoch 19/300 - Train Loss: 0.0730, Val Loss: 0.0749\n",
      "Epoch 20/300 - Train Loss: 0.0724, Val Loss: 0.0809\n",
      "Epoch 21/300 - Train Loss: 0.0720, Val Loss: 0.0672\n",
      "Epoch 22/300 - Train Loss: 0.0711, Val Loss: 0.0722\n",
      "Epoch 23/300 - Train Loss: 0.0695, Val Loss: 0.0678\n",
      "Epoch 24/300 - Train Loss: 0.0716, Val Loss: 0.0693\n",
      "Epoch 25/300 - Train Loss: 0.0700, Val Loss: 0.0656\n",
      "Epoch 26/300 - Train Loss: 0.0697, Val Loss: 0.0685\n",
      "Epoch 27/300 - Train Loss: 0.0681, Val Loss: 0.0659\n",
      "Epoch 28/300 - Train Loss: 0.0683, Val Loss: 0.0702\n",
      "Epoch 29/300 - Train Loss: 0.0649, Val Loss: 0.0665\n",
      "Epoch 30/300 - Train Loss: 0.0666, Val Loss: 0.0759\n",
      "Epoch 31/300 - Train Loss: 0.0689, Val Loss: 0.0677\n",
      "Epoch 32/300 - Train Loss: 0.0664, Val Loss: 0.0677\n",
      "Epoch 33/300 - Train Loss: 0.0644, Val Loss: 0.0714\n",
      "Epoch 34/300 - Train Loss: 0.0651, Val Loss: 0.0718\n",
      "Epoch 35/300 - Train Loss: 0.0642, Val Loss: 0.0668\n",
      "Epoch 36/300 - Train Loss: 0.0638, Val Loss: 0.0717\n",
      "Epoch 37/300 - Train Loss: 0.0641, Val Loss: 0.0656\n",
      "Epoch 38/300 - Train Loss: 0.0628, Val Loss: 0.0765\n",
      "Epoch 39/300 - Train Loss: 0.0633, Val Loss: 0.0732\n",
      "Epoch 40/300 - Train Loss: 0.0622, Val Loss: 0.0733\n",
      "Epoch 41/300 - Train Loss: 0.0609, Val Loss: 0.0690\n",
      "Epoch 42/300 - Train Loss: 0.0611, Val Loss: 0.0696\n",
      "Epoch 43/300 - Train Loss: 0.0617, Val Loss: 0.0696\n",
      "Epoch 44/300 - Train Loss: 0.0594, Val Loss: 0.0702\n",
      "Epoch 45/300 - Train Loss: 0.0592, Val Loss: 0.0691\n",
      "Epoch 46/300 - Train Loss: 0.0598, Val Loss: 0.0658\n",
      "Epoch 47/300 - Train Loss: 0.0610, Val Loss: 0.0691\n",
      "Epoch 48/300 - Train Loss: 0.0603, Val Loss: 0.0688\n",
      "Epoch 49/300 - Train Loss: 0.0584, Val Loss: 0.0671\n",
      "Epoch 50/300 - Train Loss: 0.0580, Val Loss: 0.0700\n",
      "Epoch 51/300 - Train Loss: 0.0589, Val Loss: 0.0679\n",
      "Epoch 52/300 - Train Loss: 0.0596, Val Loss: 0.0686\n",
      "Epoch 53/300 - Train Loss: 0.0567, Val Loss: 0.0674\n",
      "Epoch 54/300 - Train Loss: 0.0568, Val Loss: 0.0711\n",
      "Epoch 55/300 - Train Loss: 0.0551, Val Loss: 0.0659\n",
      "Epoch 56/300 - Train Loss: 0.0573, Val Loss: 0.0681\n",
      "Epoch 57/300 - Train Loss: 0.0554, Val Loss: 0.0653\n",
      "Epoch 58/300 - Train Loss: 0.0546, Val Loss: 0.0707\n",
      "Epoch 59/300 - Train Loss: 0.0538, Val Loss: 0.0676\n",
      "Epoch 60/300 - Train Loss: 0.0573, Val Loss: 0.0697\n",
      "Epoch 61/300 - Train Loss: 0.0554, Val Loss: 0.0689\n",
      "Epoch 62/300 - Train Loss: 0.0541, Val Loss: 0.0712\n",
      "Epoch 63/300 - Train Loss: 0.0537, Val Loss: 0.0702\n",
      "Epoch 64/300 - Train Loss: 0.0532, Val Loss: 0.0665\n",
      "Epoch 65/300 - Train Loss: 0.0525, Val Loss: 0.0679\n",
      "Epoch 66/300 - Train Loss: 0.0522, Val Loss: 0.0670\n",
      "Epoch 67/300 - Train Loss: 0.0515, Val Loss: 0.0653\n",
      "Epoch 68/300 - Train Loss: 0.0503, Val Loss: 0.0702\n",
      "Epoch 69/300 - Train Loss: 0.0507, Val Loss: 0.0710\n",
      "Epoch 70/300 - Train Loss: 0.0497, Val Loss: 0.0702\n",
      "Epoch 71/300 - Train Loss: 0.0506, Val Loss: 0.0677\n",
      "Epoch 72/300 - Train Loss: 0.0499, Val Loss: 0.0724\n",
      "Epoch 73/300 - Train Loss: 0.0517, Val Loss: 0.0675\n",
      "Epoch 74/300 - Train Loss: 0.0500, Val Loss: 0.0765\n",
      "Epoch 75/300 - Train Loss: 0.0504, Val Loss: 0.0692\n",
      "Epoch 76/300 - Train Loss: 0.0503, Val Loss: 0.0699\n",
      "Epoch 77/300 - Train Loss: 0.0493, Val Loss: 0.0658\n",
      "Epoch 78/300 - Train Loss: 0.0474, Val Loss: 0.0687\n",
      "Epoch 79/300 - Train Loss: 0.0467, Val Loss: 0.0715\n",
      "Epoch 80/300 - Train Loss: 0.0476, Val Loss: 0.0733\n",
      "Epoch 81/300 - Train Loss: 0.0471, Val Loss: 0.0734\n",
      "Epoch 82/300 - Train Loss: 0.0470, Val Loss: 0.0686\n",
      "Epoch 83/300 - Train Loss: 0.0479, Val Loss: 0.0697\n",
      "Epoch 84/300 - Train Loss: 0.0465, Val Loss: 0.0790\n",
      "Epoch 85/300 - Train Loss: 0.0464, Val Loss: 0.0710\n",
      "Epoch 86/300 - Train Loss: 0.0469, Val Loss: 0.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:07:09,347] Trial 293 finished with value: 0.9700420703027294 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1268019252765063, 'learning_rate': 9.807615349941166e-05, 'batch_size': 32, 'weight_decay': 6.795108474689867e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300 - Train Loss: 0.0449, Val Loss: 0.0719\n",
      "Early stopping at epoch 87\n",
      "Macro F1 Score: 0.9700, Macro Precision: 0.9682, Macro Recall: 0.9719\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 295\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1330289385397861, LR=0.00011485643614458604, BS=32, WD=8.46132008131395e-05\n",
      "Epoch 1/300 - Train Loss: 0.2632, Val Loss: 0.1147\n",
      "Epoch 2/300 - Train Loss: 0.1182, Val Loss: 0.0836\n",
      "Epoch 3/300 - Train Loss: 0.1034, Val Loss: 0.0818\n",
      "Epoch 4/300 - Train Loss: 0.0955, Val Loss: 0.0774\n",
      "Epoch 5/300 - Train Loss: 0.0923, Val Loss: 0.0800\n",
      "Epoch 6/300 - Train Loss: 0.0936, Val Loss: 0.0799\n",
      "Epoch 7/300 - Train Loss: 0.0898, Val Loss: 0.0746\n",
      "Epoch 8/300 - Train Loss: 0.0871, Val Loss: 0.0774\n",
      "Epoch 9/300 - Train Loss: 0.0832, Val Loss: 0.0734\n",
      "Epoch 10/300 - Train Loss: 0.0829, Val Loss: 0.0719\n",
      "Epoch 11/300 - Train Loss: 0.0807, Val Loss: 0.0824\n",
      "Epoch 12/300 - Train Loss: 0.0846, Val Loss: 0.0755\n",
      "Epoch 13/300 - Train Loss: 0.0802, Val Loss: 0.0779\n",
      "Epoch 14/300 - Train Loss: 0.0771, Val Loss: 0.0984\n",
      "Epoch 15/300 - Train Loss: 0.0777, Val Loss: 0.0824\n",
      "Epoch 16/300 - Train Loss: 0.0765, Val Loss: 0.0737\n",
      "Epoch 17/300 - Train Loss: 0.0735, Val Loss: 0.0694\n",
      "Epoch 18/300 - Train Loss: 0.0763, Val Loss: 0.0672\n",
      "Epoch 19/300 - Train Loss: 0.0735, Val Loss: 0.0704\n",
      "Epoch 20/300 - Train Loss: 0.0727, Val Loss: 0.0779\n",
      "Epoch 21/300 - Train Loss: 0.0718, Val Loss: 0.0675\n",
      "Epoch 22/300 - Train Loss: 0.0716, Val Loss: 0.0715\n",
      "Epoch 23/300 - Train Loss: 0.0690, Val Loss: 0.0731\n",
      "Epoch 24/300 - Train Loss: 0.0698, Val Loss: 0.0692\n",
      "Epoch 25/300 - Train Loss: 0.0687, Val Loss: 0.0690\n",
      "Epoch 26/300 - Train Loss: 0.0694, Val Loss: 0.0788\n",
      "Epoch 27/300 - Train Loss: 0.0681, Val Loss: 0.0769\n",
      "Epoch 28/300 - Train Loss: 0.0675, Val Loss: 0.0765\n",
      "Epoch 29/300 - Train Loss: 0.0664, Val Loss: 0.0730\n",
      "Epoch 30/300 - Train Loss: 0.0657, Val Loss: 0.0706\n",
      "Epoch 31/300 - Train Loss: 0.0648, Val Loss: 0.0720\n",
      "Epoch 32/300 - Train Loss: 0.0658, Val Loss: 0.0776\n",
      "Epoch 33/300 - Train Loss: 0.0648, Val Loss: 0.0692\n",
      "Epoch 34/300 - Train Loss: 0.0637, Val Loss: 0.0753\n",
      "Epoch 35/300 - Train Loss: 0.0641, Val Loss: 0.0716\n",
      "Epoch 36/300 - Train Loss: 0.0636, Val Loss: 0.0746\n",
      "Epoch 37/300 - Train Loss: 0.0620, Val Loss: 0.0687\n",
      "Epoch 38/300 - Train Loss: 0.0607, Val Loss: 0.0692\n",
      "Epoch 39/300 - Train Loss: 0.0624, Val Loss: 0.0661\n",
      "Epoch 40/300 - Train Loss: 0.0616, Val Loss: 0.0701\n",
      "Epoch 41/300 - Train Loss: 0.0589, Val Loss: 0.0707\n",
      "Epoch 42/300 - Train Loss: 0.0607, Val Loss: 0.0748\n",
      "Epoch 43/300 - Train Loss: 0.0601, Val Loss: 0.0706\n",
      "Epoch 44/300 - Train Loss: 0.0596, Val Loss: 0.0710\n",
      "Epoch 45/300 - Train Loss: 0.0571, Val Loss: 0.0691\n",
      "Epoch 46/300 - Train Loss: 0.0560, Val Loss: 0.0707\n",
      "Epoch 47/300 - Train Loss: 0.0570, Val Loss: 0.0711\n",
      "Epoch 48/300 - Train Loss: 0.0576, Val Loss: 0.0679\n",
      "Epoch 49/300 - Train Loss: 0.0565, Val Loss: 0.0700\n",
      "Epoch 50/300 - Train Loss: 0.0578, Val Loss: 0.0682\n",
      "Epoch 51/300 - Train Loss: 0.0546, Val Loss: 0.0664\n",
      "Epoch 52/300 - Train Loss: 0.0533, Val Loss: 0.0687\n",
      "Epoch 53/300 - Train Loss: 0.0557, Val Loss: 0.0753\n",
      "Epoch 54/300 - Train Loss: 0.0555, Val Loss: 0.0693\n",
      "Epoch 55/300 - Train Loss: 0.0544, Val Loss: 0.0703\n",
      "Epoch 56/300 - Train Loss: 0.0545, Val Loss: 0.0704\n",
      "Epoch 57/300 - Train Loss: 0.0539, Val Loss: 0.0730\n",
      "Epoch 58/300 - Train Loss: 0.0529, Val Loss: 0.0740\n",
      "Epoch 59/300 - Train Loss: 0.0529, Val Loss: 0.0734\n",
      "Epoch 60/300 - Train Loss: 0.0550, Val Loss: 0.0788\n",
      "Epoch 61/300 - Train Loss: 0.0507, Val Loss: 0.0709\n",
      "Epoch 62/300 - Train Loss: 0.0509, Val Loss: 0.0714\n",
      "Epoch 63/300 - Train Loss: 0.0513, Val Loss: 0.0666\n",
      "Epoch 64/300 - Train Loss: 0.0510, Val Loss: 0.0665\n",
      "Epoch 65/300 - Train Loss: 0.0499, Val Loss: 0.0757\n",
      "Epoch 66/300 - Train Loss: 0.0512, Val Loss: 0.0676\n",
      "Epoch 67/300 - Train Loss: 0.0502, Val Loss: 0.0726\n",
      "Epoch 68/300 - Train Loss: 0.0520, Val Loss: 0.0722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:10:18,153] Trial 294 finished with value: 0.9685123295833837 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1330289385397861, 'learning_rate': 0.00011485643614458604, 'batch_size': 32, 'weight_decay': 8.46132008131395e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.0508, Val Loss: 0.0718\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9685, Macro Precision: 0.9566, Macro Recall: 0.9819\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 296\n",
      "Training with F1=32, F2=32, D=2, dropout=0.11420076003693293, LR=8.73349486258624e-05, BS=128, WD=4.730555254143442e-05\n",
      "Epoch 1/300 - Train Loss: 0.4293, Val Loss: 0.2068\n",
      "Epoch 2/300 - Train Loss: 0.1796, Val Loss: 0.1469\n",
      "Epoch 3/300 - Train Loss: 0.1359, Val Loss: 0.1132\n",
      "Epoch 4/300 - Train Loss: 0.1134, Val Loss: 0.0995\n",
      "Epoch 5/300 - Train Loss: 0.1019, Val Loss: 0.0950\n",
      "Epoch 6/300 - Train Loss: 0.0973, Val Loss: 0.0980\n",
      "Epoch 7/300 - Train Loss: 0.0913, Val Loss: 0.0886\n",
      "Epoch 8/300 - Train Loss: 0.0902, Val Loss: 0.0972\n",
      "Epoch 9/300 - Train Loss: 0.0871, Val Loss: 0.0834\n",
      "Epoch 10/300 - Train Loss: 0.0845, Val Loss: 0.0896\n",
      "Epoch 11/300 - Train Loss: 0.0821, Val Loss: 0.0825\n",
      "Epoch 12/300 - Train Loss: 0.0806, Val Loss: 0.0780\n",
      "Epoch 13/300 - Train Loss: 0.0810, Val Loss: 0.0781\n",
      "Epoch 14/300 - Train Loss: 0.0788, Val Loss: 0.0811\n",
      "Epoch 15/300 - Train Loss: 0.0776, Val Loss: 0.0859\n",
      "Epoch 16/300 - Train Loss: 0.0751, Val Loss: 0.0758\n",
      "Epoch 17/300 - Train Loss: 0.0741, Val Loss: 0.0773\n",
      "Epoch 18/300 - Train Loss: 0.0747, Val Loss: 0.0800\n",
      "Epoch 19/300 - Train Loss: 0.0716, Val Loss: 0.0788\n",
      "Epoch 20/300 - Train Loss: 0.0727, Val Loss: 0.0753\n",
      "Epoch 21/300 - Train Loss: 0.0719, Val Loss: 0.0735\n",
      "Epoch 22/300 - Train Loss: 0.0697, Val Loss: 0.0753\n",
      "Epoch 23/300 - Train Loss: 0.0695, Val Loss: 0.0774\n",
      "Epoch 24/300 - Train Loss: 0.0685, Val Loss: 0.0754\n",
      "Epoch 25/300 - Train Loss: 0.0672, Val Loss: 0.0729\n",
      "Epoch 26/300 - Train Loss: 0.0683, Val Loss: 0.0708\n",
      "Epoch 27/300 - Train Loss: 0.0675, Val Loss: 0.0743\n",
      "Epoch 28/300 - Train Loss: 0.0664, Val Loss: 0.0692\n",
      "Epoch 29/300 - Train Loss: 0.0669, Val Loss: 0.0707\n",
      "Epoch 30/300 - Train Loss: 0.0655, Val Loss: 0.0740\n",
      "Epoch 31/300 - Train Loss: 0.0641, Val Loss: 0.0731\n",
      "Epoch 32/300 - Train Loss: 0.0642, Val Loss: 0.0728\n",
      "Epoch 33/300 - Train Loss: 0.0649, Val Loss: 0.0770\n",
      "Epoch 34/300 - Train Loss: 0.0649, Val Loss: 0.0705\n",
      "Epoch 35/300 - Train Loss: 0.0629, Val Loss: 0.0718\n",
      "Epoch 36/300 - Train Loss: 0.0628, Val Loss: 0.0721\n",
      "Epoch 37/300 - Train Loss: 0.0637, Val Loss: 0.0714\n",
      "Epoch 38/300 - Train Loss: 0.0613, Val Loss: 0.0692\n",
      "Epoch 39/300 - Train Loss: 0.0621, Val Loss: 0.0659\n",
      "Epoch 40/300 - Train Loss: 0.0610, Val Loss: 0.0720\n",
      "Epoch 41/300 - Train Loss: 0.0617, Val Loss: 0.0684\n",
      "Epoch 42/300 - Train Loss: 0.0614, Val Loss: 0.0735\n",
      "Epoch 43/300 - Train Loss: 0.0603, Val Loss: 0.0719\n",
      "Epoch 44/300 - Train Loss: 0.0584, Val Loss: 0.0715\n",
      "Epoch 45/300 - Train Loss: 0.0578, Val Loss: 0.0699\n",
      "Epoch 46/300 - Train Loss: 0.0591, Val Loss: 0.0707\n",
      "Epoch 47/300 - Train Loss: 0.0592, Val Loss: 0.0718\n",
      "Epoch 48/300 - Train Loss: 0.0578, Val Loss: 0.0691\n",
      "Epoch 49/300 - Train Loss: 0.0561, Val Loss: 0.0751\n",
      "Epoch 50/300 - Train Loss: 0.0587, Val Loss: 0.0688\n",
      "Epoch 51/300 - Train Loss: 0.0577, Val Loss: 0.0672\n",
      "Epoch 52/300 - Train Loss: 0.0572, Val Loss: 0.0724\n",
      "Epoch 53/300 - Train Loss: 0.0569, Val Loss: 0.0690\n",
      "Epoch 54/300 - Train Loss: 0.0566, Val Loss: 0.0686\n",
      "Epoch 55/300 - Train Loss: 0.0580, Val Loss: 0.0714\n",
      "Epoch 56/300 - Train Loss: 0.0557, Val Loss: 0.0717\n",
      "Epoch 57/300 - Train Loss: 0.0562, Val Loss: 0.0715\n",
      "Epoch 58/300 - Train Loss: 0.0557, Val Loss: 0.0703\n",
      "Epoch 59/300 - Train Loss: 0.0545, Val Loss: 0.0718\n",
      "Epoch 60/300 - Train Loss: 0.0552, Val Loss: 0.0703\n",
      "Epoch 61/300 - Train Loss: 0.0552, Val Loss: 0.0679\n",
      "Epoch 62/300 - Train Loss: 0.0540, Val Loss: 0.0723\n",
      "Epoch 63/300 - Train Loss: 0.0534, Val Loss: 0.0731\n",
      "Epoch 64/300 - Train Loss: 0.0536, Val Loss: 0.0677\n",
      "Epoch 65/300 - Train Loss: 0.0524, Val Loss: 0.0690\n",
      "Epoch 66/300 - Train Loss: 0.0535, Val Loss: 0.0704\n",
      "Epoch 67/300 - Train Loss: 0.0519, Val Loss: 0.0714\n",
      "Epoch 68/300 - Train Loss: 0.0526, Val Loss: 0.0791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:12:48,771] Trial 295 finished with value: 0.972034416712344 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.11420076003693293, 'learning_rate': 8.73349486258624e-05, 'batch_size': 128, 'weight_decay': 4.730555254143442e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.0520, Val Loss: 0.0759\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9720, Macro Precision: 0.9678, Macro Recall: 0.9764\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.98      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 297\n",
      "Training with F1=32, F2=16, D=2, dropout=0.12909555557425417, LR=0.00010021203741542935, BS=32, WD=0.0005186778148743635\n",
      "Epoch 1/300 - Train Loss: 0.3078, Val Loss: 0.1319\n",
      "Epoch 2/300 - Train Loss: 0.1236, Val Loss: 0.1085\n",
      "Epoch 3/300 - Train Loss: 0.1039, Val Loss: 0.0811\n",
      "Epoch 4/300 - Train Loss: 0.0982, Val Loss: 0.0782\n",
      "Epoch 5/300 - Train Loss: 0.0928, Val Loss: 0.0745\n",
      "Epoch 6/300 - Train Loss: 0.0899, Val Loss: 0.0753\n",
      "Epoch 7/300 - Train Loss: 0.0877, Val Loss: 0.0697\n",
      "Epoch 8/300 - Train Loss: 0.0892, Val Loss: 0.0765\n",
      "Epoch 9/300 - Train Loss: 0.0851, Val Loss: 0.0778\n",
      "Epoch 10/300 - Train Loss: 0.0853, Val Loss: 0.0701\n",
      "Epoch 11/300 - Train Loss: 0.0822, Val Loss: 0.0758\n",
      "Epoch 12/300 - Train Loss: 0.0840, Val Loss: 0.0690\n",
      "Epoch 13/300 - Train Loss: 0.0833, Val Loss: 0.0698\n",
      "Epoch 14/300 - Train Loss: 0.0811, Val Loss: 0.0699\n",
      "Epoch 15/300 - Train Loss: 0.0790, Val Loss: 0.0723\n",
      "Epoch 16/300 - Train Loss: 0.0794, Val Loss: 0.0718\n",
      "Epoch 17/300 - Train Loss: 0.0792, Val Loss: 0.0665\n",
      "Epoch 18/300 - Train Loss: 0.0792, Val Loss: 0.0690\n",
      "Epoch 19/300 - Train Loss: 0.0777, Val Loss: 0.0765\n",
      "Epoch 20/300 - Train Loss: 0.0788, Val Loss: 0.0744\n",
      "Epoch 21/300 - Train Loss: 0.0772, Val Loss: 0.0663\n",
      "Epoch 22/300 - Train Loss: 0.0775, Val Loss: 0.0710\n",
      "Epoch 23/300 - Train Loss: 0.0763, Val Loss: 0.0735\n",
      "Epoch 24/300 - Train Loss: 0.0758, Val Loss: 0.0718\n",
      "Epoch 25/300 - Train Loss: 0.0757, Val Loss: 0.0739\n",
      "Epoch 26/300 - Train Loss: 0.0746, Val Loss: 0.0786\n",
      "Epoch 27/300 - Train Loss: 0.0759, Val Loss: 0.0663\n",
      "Epoch 28/300 - Train Loss: 0.0748, Val Loss: 0.0750\n",
      "Epoch 29/300 - Train Loss: 0.0762, Val Loss: 0.0701\n",
      "Epoch 30/300 - Train Loss: 0.0735, Val Loss: 0.0735\n",
      "Epoch 31/300 - Train Loss: 0.0735, Val Loss: 0.0725\n",
      "Epoch 32/300 - Train Loss: 0.0737, Val Loss: 0.0696\n",
      "Epoch 33/300 - Train Loss: 0.0740, Val Loss: 0.0669\n",
      "Epoch 34/300 - Train Loss: 0.0726, Val Loss: 0.0667\n",
      "Epoch 35/300 - Train Loss: 0.0735, Val Loss: 0.0778\n",
      "Epoch 36/300 - Train Loss: 0.0706, Val Loss: 0.0772\n",
      "Epoch 37/300 - Train Loss: 0.0709, Val Loss: 0.0722\n",
      "Epoch 38/300 - Train Loss: 0.0706, Val Loss: 0.0676\n",
      "Epoch 39/300 - Train Loss: 0.0701, Val Loss: 0.0718\n",
      "Epoch 40/300 - Train Loss: 0.0687, Val Loss: 0.0651\n",
      "Epoch 41/300 - Train Loss: 0.0701, Val Loss: 0.0706\n",
      "Epoch 42/300 - Train Loss: 0.0688, Val Loss: 0.0685\n",
      "Epoch 43/300 - Train Loss: 0.0697, Val Loss: 0.0688\n",
      "Epoch 44/300 - Train Loss: 0.0683, Val Loss: 0.0669\n",
      "Epoch 45/300 - Train Loss: 0.0693, Val Loss: 0.0655\n",
      "Epoch 46/300 - Train Loss: 0.0711, Val Loss: 0.0769\n",
      "Epoch 47/300 - Train Loss: 0.0672, Val Loss: 0.0688\n",
      "Epoch 48/300 - Train Loss: 0.0671, Val Loss: 0.0660\n",
      "Epoch 49/300 - Train Loss: 0.0692, Val Loss: 0.0720\n",
      "Epoch 50/300 - Train Loss: 0.0688, Val Loss: 0.0763\n",
      "Epoch 51/300 - Train Loss: 0.0679, Val Loss: 0.0725\n",
      "Epoch 52/300 - Train Loss: 0.0684, Val Loss: 0.0679\n",
      "Epoch 53/300 - Train Loss: 0.0684, Val Loss: 0.0670\n",
      "Epoch 54/300 - Train Loss: 0.0665, Val Loss: 0.0700\n",
      "Epoch 55/300 - Train Loss: 0.0703, Val Loss: 0.0684\n",
      "Epoch 56/300 - Train Loss: 0.0700, Val Loss: 0.0765\n",
      "Epoch 57/300 - Train Loss: 0.0657, Val Loss: 0.0699\n",
      "Epoch 58/300 - Train Loss: 0.0683, Val Loss: 0.0710\n",
      "Epoch 59/300 - Train Loss: 0.0637, Val Loss: 0.0683\n",
      "Epoch 60/300 - Train Loss: 0.0636, Val Loss: 0.0679\n",
      "Epoch 61/300 - Train Loss: 0.0651, Val Loss: 0.0687\n",
      "Epoch 62/300 - Train Loss: 0.0665, Val Loss: 0.0649\n",
      "Epoch 63/300 - Train Loss: 0.0671, Val Loss: 0.0733\n",
      "Epoch 64/300 - Train Loss: 0.0635, Val Loss: 0.0692\n",
      "Epoch 65/300 - Train Loss: 0.0657, Val Loss: 0.0682\n",
      "Epoch 66/300 - Train Loss: 0.0672, Val Loss: 0.0701\n",
      "Epoch 67/300 - Train Loss: 0.0655, Val Loss: 0.0715\n",
      "Epoch 68/300 - Train Loss: 0.0646, Val Loss: 0.0694\n",
      "Epoch 69/300 - Train Loss: 0.0650, Val Loss: 0.0733\n",
      "Epoch 70/300 - Train Loss: 0.0640, Val Loss: 0.0704\n",
      "Epoch 71/300 - Train Loss: 0.0649, Val Loss: 0.0686\n",
      "Epoch 72/300 - Train Loss: 0.0635, Val Loss: 0.0714\n",
      "Epoch 73/300 - Train Loss: 0.0639, Val Loss: 0.0736\n",
      "Epoch 74/300 - Train Loss: 0.0627, Val Loss: 0.0815\n",
      "Epoch 75/300 - Train Loss: 0.0638, Val Loss: 0.0718\n",
      "Epoch 76/300 - Train Loss: 0.0669, Val Loss: 0.0718\n",
      "Epoch 77/300 - Train Loss: 0.0630, Val Loss: 0.0763\n",
      "Epoch 78/300 - Train Loss: 0.0660, Val Loss: 0.0681\n",
      "Epoch 79/300 - Train Loss: 0.0642, Val Loss: 0.0692\n",
      "Epoch 80/300 - Train Loss: 0.0633, Val Loss: 0.0714\n",
      "Epoch 81/300 - Train Loss: 0.0612, Val Loss: 0.0702\n",
      "Epoch 82/300 - Train Loss: 0.0600, Val Loss: 0.0709\n",
      "Epoch 83/300 - Train Loss: 0.0628, Val Loss: 0.0762\n",
      "Epoch 84/300 - Train Loss: 0.0624, Val Loss: 0.0691\n",
      "Epoch 85/300 - Train Loss: 0.0608, Val Loss: 0.0768\n",
      "Epoch 86/300 - Train Loss: 0.0632, Val Loss: 0.0702\n",
      "Epoch 87/300 - Train Loss: 0.0613, Val Loss: 0.0678\n",
      "Epoch 88/300 - Train Loss: 0.0629, Val Loss: 0.0714\n",
      "Epoch 89/300 - Train Loss: 0.0635, Val Loss: 0.0734\n",
      "Epoch 90/300 - Train Loss: 0.0612, Val Loss: 0.0747\n",
      "Epoch 91/300 - Train Loss: 0.0600, Val Loss: 0.0737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:16:53,590] Trial 296 finished with value: 0.9721380905893523 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.12909555557425417, 'learning_rate': 0.00010021203741542935, 'batch_size': 32, 'weight_decay': 0.0005186778148743635}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/300 - Train Loss: 0.0579, Val Loss: 0.0720\n",
      "Early stopping at epoch 92\n",
      "Macro F1 Score: 0.9721, Macro Precision: 0.9779, Macro Recall: 0.9666\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 298\n",
      "Training with F1=32, F2=32, D=2, dropout=0.10164435319205697, LR=9.049300492569866e-05, BS=32, WD=5.631342468247084e-05\n",
      "Epoch 1/300 - Train Loss: 0.2674, Val Loss: 0.1166\n",
      "Epoch 2/300 - Train Loss: 0.1139, Val Loss: 0.0927\n",
      "Epoch 3/300 - Train Loss: 0.1020, Val Loss: 0.0807\n",
      "Epoch 4/300 - Train Loss: 0.0947, Val Loss: 0.0851\n",
      "Epoch 5/300 - Train Loss: 0.0927, Val Loss: 0.0790\n",
      "Epoch 6/300 - Train Loss: 0.0890, Val Loss: 0.0737\n",
      "Epoch 7/300 - Train Loss: 0.0894, Val Loss: 0.0756\n",
      "Epoch 8/300 - Train Loss: 0.0855, Val Loss: 0.0721\n",
      "Epoch 9/300 - Train Loss: 0.0866, Val Loss: 0.0857\n",
      "Epoch 10/300 - Train Loss: 0.0816, Val Loss: 0.0674\n",
      "Epoch 11/300 - Train Loss: 0.0802, Val Loss: 0.0715\n",
      "Epoch 12/300 - Train Loss: 0.0778, Val Loss: 0.0774\n",
      "Epoch 13/300 - Train Loss: 0.0783, Val Loss: 0.0714\n",
      "Epoch 14/300 - Train Loss: 0.0738, Val Loss: 0.0698\n",
      "Epoch 15/300 - Train Loss: 0.0754, Val Loss: 0.0723\n",
      "Epoch 16/300 - Train Loss: 0.0759, Val Loss: 0.0703\n",
      "Epoch 17/300 - Train Loss: 0.0722, Val Loss: 0.0690\n",
      "Epoch 18/300 - Train Loss: 0.0740, Val Loss: 0.0742\n",
      "Epoch 19/300 - Train Loss: 0.0730, Val Loss: 0.0662\n",
      "Epoch 20/300 - Train Loss: 0.0741, Val Loss: 0.0727\n",
      "Epoch 21/300 - Train Loss: 0.0700, Val Loss: 0.0749\n",
      "Epoch 22/300 - Train Loss: 0.0697, Val Loss: 0.0691\n",
      "Epoch 23/300 - Train Loss: 0.0710, Val Loss: 0.0654\n",
      "Epoch 24/300 - Train Loss: 0.0685, Val Loss: 0.0687\n",
      "Epoch 25/300 - Train Loss: 0.0669, Val Loss: 0.0664\n",
      "Epoch 26/300 - Train Loss: 0.0687, Val Loss: 0.0668\n",
      "Epoch 27/300 - Train Loss: 0.0666, Val Loss: 0.0726\n",
      "Epoch 28/300 - Train Loss: 0.0653, Val Loss: 0.0751\n",
      "Epoch 29/300 - Train Loss: 0.0660, Val Loss: 0.0658\n",
      "Epoch 30/300 - Train Loss: 0.0665, Val Loss: 0.0685\n",
      "Epoch 31/300 - Train Loss: 0.0645, Val Loss: 0.0689\n",
      "Epoch 32/300 - Train Loss: 0.0642, Val Loss: 0.0668\n",
      "Epoch 33/300 - Train Loss: 0.0624, Val Loss: 0.0663\n",
      "Epoch 34/300 - Train Loss: 0.0634, Val Loss: 0.0680\n",
      "Epoch 35/300 - Train Loss: 0.0620, Val Loss: 0.0695\n",
      "Epoch 36/300 - Train Loss: 0.0605, Val Loss: 0.0662\n",
      "Epoch 37/300 - Train Loss: 0.0623, Val Loss: 0.0676\n",
      "Epoch 38/300 - Train Loss: 0.0599, Val Loss: 0.0695\n",
      "Epoch 39/300 - Train Loss: 0.0609, Val Loss: 0.0654\n",
      "Epoch 40/300 - Train Loss: 0.0592, Val Loss: 0.0650\n",
      "Epoch 41/300 - Train Loss: 0.0593, Val Loss: 0.0695\n",
      "Epoch 42/300 - Train Loss: 0.0577, Val Loss: 0.0685\n",
      "Epoch 43/300 - Train Loss: 0.0573, Val Loss: 0.0682\n",
      "Epoch 44/300 - Train Loss: 0.0576, Val Loss: 0.0648\n",
      "Epoch 45/300 - Train Loss: 0.0573, Val Loss: 0.0652\n",
      "Epoch 46/300 - Train Loss: 0.0574, Val Loss: 0.0673\n",
      "Epoch 47/300 - Train Loss: 0.0571, Val Loss: 0.0718\n",
      "Epoch 48/300 - Train Loss: 0.0568, Val Loss: 0.0734\n",
      "Epoch 49/300 - Train Loss: 0.0546, Val Loss: 0.0713\n",
      "Epoch 50/300 - Train Loss: 0.0545, Val Loss: 0.0667\n",
      "Epoch 51/300 - Train Loss: 0.0539, Val Loss: 0.0739\n",
      "Epoch 52/300 - Train Loss: 0.0542, Val Loss: 0.0677\n",
      "Epoch 53/300 - Train Loss: 0.0543, Val Loss: 0.0700\n",
      "Epoch 54/300 - Train Loss: 0.0525, Val Loss: 0.0687\n",
      "Epoch 55/300 - Train Loss: 0.0530, Val Loss: 0.0716\n",
      "Epoch 56/300 - Train Loss: 0.0528, Val Loss: 0.0816\n",
      "Epoch 57/300 - Train Loss: 0.0523, Val Loss: 0.0695\n",
      "Epoch 58/300 - Train Loss: 0.0519, Val Loss: 0.0677\n",
      "Epoch 59/300 - Train Loss: 0.0491, Val Loss: 0.0729\n",
      "Epoch 60/300 - Train Loss: 0.0498, Val Loss: 0.0711\n",
      "Epoch 61/300 - Train Loss: 0.0489, Val Loss: 0.0730\n",
      "Epoch 62/300 - Train Loss: 0.0487, Val Loss: 0.0720\n",
      "Epoch 63/300 - Train Loss: 0.0496, Val Loss: 0.0747\n",
      "Epoch 64/300 - Train Loss: 0.0487, Val Loss: 0.0694\n",
      "Epoch 65/300 - Train Loss: 0.0485, Val Loss: 0.0719\n",
      "Epoch 66/300 - Train Loss: 0.0476, Val Loss: 0.0732\n",
      "Epoch 67/300 - Train Loss: 0.0476, Val Loss: 0.0752\n",
      "Epoch 68/300 - Train Loss: 0.0475, Val Loss: 0.0736\n",
      "Epoch 69/300 - Train Loss: 0.0450, Val Loss: 0.0705\n",
      "Epoch 70/300 - Train Loss: 0.0464, Val Loss: 0.0690\n",
      "Epoch 71/300 - Train Loss: 0.0477, Val Loss: 0.0735\n",
      "Epoch 72/300 - Train Loss: 0.0460, Val Loss: 0.0701\n",
      "Epoch 73/300 - Train Loss: 0.0448, Val Loss: 0.0711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:20:15,704] Trial 297 finished with value: 0.9648738783215703 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.10164435319205697, 'learning_rate': 9.049300492569866e-05, 'batch_size': 32, 'weight_decay': 5.631342468247084e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300 - Train Loss: 0.0429, Val Loss: 0.0769\n",
      "Early stopping at epoch 74\n",
      "Macro F1 Score: 0.9649, Macro Precision: 0.9551, Macro Recall: 0.9757\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 299\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14760327482598615, LR=0.0001299592500230486, BS=32, WD=9.252237398779632e-05\n",
      "Epoch 1/300 - Train Loss: 0.2561, Val Loss: 0.1320\n",
      "Epoch 2/300 - Train Loss: 0.1232, Val Loss: 0.0980\n",
      "Epoch 3/300 - Train Loss: 0.1003, Val Loss: 0.0816\n",
      "Epoch 4/300 - Train Loss: 0.0914, Val Loss: 0.0736\n",
      "Epoch 5/300 - Train Loss: 0.0891, Val Loss: 0.0796\n",
      "Epoch 6/300 - Train Loss: 0.0880, Val Loss: 0.0720\n",
      "Epoch 7/300 - Train Loss: 0.0850, Val Loss: 0.0704\n",
      "Epoch 8/300 - Train Loss: 0.0862, Val Loss: 0.0734\n",
      "Epoch 9/300 - Train Loss: 0.0827, Val Loss: 0.0758\n",
      "Epoch 10/300 - Train Loss: 0.0811, Val Loss: 0.0904\n",
      "Epoch 11/300 - Train Loss: 0.0798, Val Loss: 0.0718\n",
      "Epoch 12/300 - Train Loss: 0.0793, Val Loss: 0.0722\n",
      "Epoch 13/300 - Train Loss: 0.0784, Val Loss: 0.0813\n",
      "Epoch 14/300 - Train Loss: 0.0787, Val Loss: 0.0848\n",
      "Epoch 15/300 - Train Loss: 0.0770, Val Loss: 0.0705\n",
      "Epoch 16/300 - Train Loss: 0.0729, Val Loss: 0.0696\n",
      "Epoch 17/300 - Train Loss: 0.0748, Val Loss: 0.0691\n",
      "Epoch 18/300 - Train Loss: 0.0737, Val Loss: 0.0864\n",
      "Epoch 19/300 - Train Loss: 0.0729, Val Loss: 0.0684\n",
      "Epoch 20/300 - Train Loss: 0.0730, Val Loss: 0.0744\n",
      "Epoch 21/300 - Train Loss: 0.0741, Val Loss: 0.0837\n",
      "Epoch 22/300 - Train Loss: 0.0735, Val Loss: 0.0696\n",
      "Epoch 23/300 - Train Loss: 0.0725, Val Loss: 0.0703\n",
      "Epoch 24/300 - Train Loss: 0.0700, Val Loss: 0.0693\n",
      "Epoch 25/300 - Train Loss: 0.0695, Val Loss: 0.0699\n",
      "Epoch 26/300 - Train Loss: 0.0716, Val Loss: 0.0672\n",
      "Epoch 27/300 - Train Loss: 0.0682, Val Loss: 0.0741\n",
      "Epoch 28/300 - Train Loss: 0.0695, Val Loss: 0.0805\n",
      "Epoch 29/300 - Train Loss: 0.0667, Val Loss: 0.0709\n",
      "Epoch 30/300 - Train Loss: 0.0672, Val Loss: 0.0692\n",
      "Epoch 31/300 - Train Loss: 0.0666, Val Loss: 0.0707\n",
      "Epoch 32/300 - Train Loss: 0.0640, Val Loss: 0.0737\n",
      "Epoch 33/300 - Train Loss: 0.0664, Val Loss: 0.0953\n",
      "Epoch 34/300 - Train Loss: 0.0638, Val Loss: 0.0753\n",
      "Epoch 35/300 - Train Loss: 0.0630, Val Loss: 0.0720\n",
      "Epoch 36/300 - Train Loss: 0.0613, Val Loss: 0.0664\n",
      "Epoch 37/300 - Train Loss: 0.0638, Val Loss: 0.0705\n",
      "Epoch 38/300 - Train Loss: 0.0598, Val Loss: 0.0691\n",
      "Epoch 39/300 - Train Loss: 0.0618, Val Loss: 0.0789\n",
      "Epoch 40/300 - Train Loss: 0.0608, Val Loss: 0.0683\n",
      "Epoch 41/300 - Train Loss: 0.0589, Val Loss: 0.0714\n",
      "Epoch 42/300 - Train Loss: 0.0594, Val Loss: 0.0688\n",
      "Epoch 43/300 - Train Loss: 0.0588, Val Loss: 0.0759\n",
      "Epoch 44/300 - Train Loss: 0.0593, Val Loss: 0.0703\n",
      "Epoch 45/300 - Train Loss: 0.0591, Val Loss: 0.0676\n",
      "Epoch 46/300 - Train Loss: 0.0587, Val Loss: 0.0675\n",
      "Epoch 47/300 - Train Loss: 0.0569, Val Loss: 0.0719\n",
      "Epoch 48/300 - Train Loss: 0.0574, Val Loss: 0.0795\n",
      "Epoch 49/300 - Train Loss: 0.0566, Val Loss: 0.0732\n",
      "Epoch 50/300 - Train Loss: 0.0560, Val Loss: 0.0709\n",
      "Epoch 51/300 - Train Loss: 0.0558, Val Loss: 0.0744\n",
      "Epoch 52/300 - Train Loss: 0.0566, Val Loss: 0.0664\n",
      "Epoch 53/300 - Train Loss: 0.0544, Val Loss: 0.0710\n",
      "Epoch 54/300 - Train Loss: 0.0557, Val Loss: 0.0688\n",
      "Epoch 55/300 - Train Loss: 0.0544, Val Loss: 0.0747\n",
      "Epoch 56/300 - Train Loss: 0.0526, Val Loss: 0.0725\n",
      "Epoch 57/300 - Train Loss: 0.0510, Val Loss: 0.0734\n",
      "Epoch 58/300 - Train Loss: 0.0570, Val Loss: 0.0733\n",
      "Epoch 59/300 - Train Loss: 0.0526, Val Loss: 0.0727\n",
      "Epoch 60/300 - Train Loss: 0.0517, Val Loss: 0.0718\n",
      "Epoch 61/300 - Train Loss: 0.0523, Val Loss: 0.0714\n",
      "Epoch 62/300 - Train Loss: 0.0540, Val Loss: 0.0740\n",
      "Epoch 63/300 - Train Loss: 0.0532, Val Loss: 0.0716\n",
      "Epoch 64/300 - Train Loss: 0.0519, Val Loss: 0.0703\n",
      "Epoch 65/300 - Train Loss: 0.0504, Val Loss: 0.0667\n",
      "Epoch 66/300 - Train Loss: 0.0516, Val Loss: 0.0698\n",
      "Epoch 67/300 - Train Loss: 0.0533, Val Loss: 0.0736\n",
      "Epoch 68/300 - Train Loss: 0.0512, Val Loss: 0.0714\n",
      "Epoch 69/300 - Train Loss: 0.0490, Val Loss: 0.0730\n",
      "Epoch 70/300 - Train Loss: 0.0508, Val Loss: 0.0721\n",
      "Epoch 71/300 - Train Loss: 0.0480, Val Loss: 0.0782\n",
      "Epoch 72/300 - Train Loss: 0.0478, Val Loss: 0.0791\n",
      "Epoch 73/300 - Train Loss: 0.0512, Val Loss: 0.0779\n",
      "Epoch 74/300 - Train Loss: 0.0496, Val Loss: 0.0725\n",
      "Epoch 75/300 - Train Loss: 0.0477, Val Loss: 0.0703\n",
      "Epoch 76/300 - Train Loss: 0.0475, Val Loss: 0.0715\n",
      "Epoch 77/300 - Train Loss: 0.0476, Val Loss: 0.0778\n",
      "Epoch 78/300 - Train Loss: 0.0481, Val Loss: 0.0798\n",
      "Epoch 79/300 - Train Loss: 0.0481, Val Loss: 0.0735\n",
      "Epoch 80/300 - Train Loss: 0.0462, Val Loss: 0.0717\n",
      "Epoch 81/300 - Train Loss: 0.0455, Val Loss: 0.0746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:23:59,891] Trial 298 finished with value: 0.9666406933234867 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14760327482598615, 'learning_rate': 0.0001299592500230486, 'batch_size': 32, 'weight_decay': 9.252237398779632e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/300 - Train Loss: 0.0473, Val Loss: 0.0760\n",
      "Early stopping at epoch 82\n",
      "Macro F1 Score: 0.9666, Macro Precision: 0.9584, Macro Recall: 0.9756\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 300\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14589930906677895, LR=0.0002924055657969072, BS=32, WD=3.559999812748034e-05\n",
      "Epoch 1/300 - Train Loss: 0.1729, Val Loss: 0.0909\n",
      "Epoch 2/300 - Train Loss: 0.0969, Val Loss: 0.0779\n",
      "Epoch 3/300 - Train Loss: 0.0906, Val Loss: 0.0792\n",
      "Epoch 4/300 - Train Loss: 0.0873, Val Loss: 0.0686\n",
      "Epoch 5/300 - Train Loss: 0.0835, Val Loss: 0.0735\n",
      "Epoch 6/300 - Train Loss: 0.0830, Val Loss: 0.0777\n",
      "Epoch 7/300 - Train Loss: 0.0838, Val Loss: 0.0779\n",
      "Epoch 8/300 - Train Loss: 0.0823, Val Loss: 0.0773\n",
      "Epoch 9/300 - Train Loss: 0.0786, Val Loss: 0.0689\n",
      "Epoch 10/300 - Train Loss: 0.0760, Val Loss: 0.0747\n",
      "Epoch 11/300 - Train Loss: 0.0770, Val Loss: 0.0729\n",
      "Epoch 12/300 - Train Loss: 0.0753, Val Loss: 0.0644\n",
      "Epoch 13/300 - Train Loss: 0.0744, Val Loss: 0.0695\n",
      "Epoch 14/300 - Train Loss: 0.0726, Val Loss: 0.0734\n",
      "Epoch 15/300 - Train Loss: 0.0716, Val Loss: 0.0694\n",
      "Epoch 16/300 - Train Loss: 0.0716, Val Loss: 0.0670\n",
      "Epoch 17/300 - Train Loss: 0.0690, Val Loss: 0.0749\n",
      "Epoch 18/300 - Train Loss: 0.0686, Val Loss: 0.0674\n",
      "Epoch 19/300 - Train Loss: 0.0678, Val Loss: 0.0757\n",
      "Epoch 20/300 - Train Loss: 0.0665, Val Loss: 0.0648\n",
      "Epoch 21/300 - Train Loss: 0.0680, Val Loss: 0.0712\n",
      "Epoch 22/300 - Train Loss: 0.0666, Val Loss: 0.0709\n",
      "Epoch 23/300 - Train Loss: 0.0638, Val Loss: 0.0638\n",
      "Epoch 24/300 - Train Loss: 0.0636, Val Loss: 0.0751\n",
      "Epoch 25/300 - Train Loss: 0.0631, Val Loss: 0.0643\n",
      "Epoch 26/300 - Train Loss: 0.0626, Val Loss: 0.0760\n",
      "Epoch 27/300 - Train Loss: 0.0600, Val Loss: 0.0670\n",
      "Epoch 28/300 - Train Loss: 0.0621, Val Loss: 0.0627\n",
      "Epoch 29/300 - Train Loss: 0.0587, Val Loss: 0.0762\n",
      "Epoch 30/300 - Train Loss: 0.0603, Val Loss: 0.0791\n",
      "Epoch 31/300 - Train Loss: 0.0573, Val Loss: 0.0680\n",
      "Epoch 32/300 - Train Loss: 0.0576, Val Loss: 0.0690\n",
      "Epoch 33/300 - Train Loss: 0.0563, Val Loss: 0.0730\n",
      "Epoch 34/300 - Train Loss: 0.0577, Val Loss: 0.0748\n",
      "Epoch 35/300 - Train Loss: 0.0545, Val Loss: 0.0785\n",
      "Epoch 36/300 - Train Loss: 0.0550, Val Loss: 0.0702\n",
      "Epoch 37/300 - Train Loss: 0.0561, Val Loss: 0.0693\n",
      "Epoch 38/300 - Train Loss: 0.0541, Val Loss: 0.0761\n",
      "Epoch 39/300 - Train Loss: 0.0534, Val Loss: 0.0744\n",
      "Epoch 40/300 - Train Loss: 0.0508, Val Loss: 0.0725\n",
      "Epoch 41/300 - Train Loss: 0.0520, Val Loss: 0.0707\n",
      "Epoch 42/300 - Train Loss: 0.0509, Val Loss: 0.0785\n",
      "Epoch 43/300 - Train Loss: 0.0501, Val Loss: 0.0730\n",
      "Epoch 44/300 - Train Loss: 0.0491, Val Loss: 0.0736\n",
      "Epoch 45/300 - Train Loss: 0.0496, Val Loss: 0.0725\n",
      "Epoch 46/300 - Train Loss: 0.0512, Val Loss: 0.0675\n",
      "Epoch 47/300 - Train Loss: 0.0467, Val Loss: 0.0702\n",
      "Epoch 48/300 - Train Loss: 0.0463, Val Loss: 0.0745\n",
      "Epoch 49/300 - Train Loss: 0.0482, Val Loss: 0.0734\n",
      "Epoch 50/300 - Train Loss: 0.0478, Val Loss: 0.0789\n",
      "Epoch 51/300 - Train Loss: 0.0475, Val Loss: 0.0725\n",
      "Epoch 52/300 - Train Loss: 0.0460, Val Loss: 0.0766\n",
      "Epoch 53/300 - Train Loss: 0.0450, Val Loss: 0.0690\n",
      "Epoch 54/300 - Train Loss: 0.0466, Val Loss: 0.0711\n",
      "Epoch 55/300 - Train Loss: 0.0443, Val Loss: 0.0739\n",
      "Epoch 56/300 - Train Loss: 0.0436, Val Loss: 0.0777\n",
      "Epoch 57/300 - Train Loss: 0.0426, Val Loss: 0.0778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:26:38,774] Trial 299 finished with value: 0.9704097576078031 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14589930906677895, 'learning_rate': 0.0002924055657969072, 'batch_size': 32, 'weight_decay': 3.559999812748034e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300 - Train Loss: 0.0417, Val Loss: 0.0749\n",
      "Early stopping at epoch 58\n",
      "Macro F1 Score: 0.9704, Macro Precision: 0.9642, Macro Recall: 0.9770\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 301\n",
      "Training with F1=32, F2=32, D=2, dropout=0.12454484181726068, LR=0.00010714772854224446, BS=32, WD=4.2392521973083066e-05\n",
      "Epoch 1/300 - Train Loss: 0.2343, Val Loss: 0.0995\n",
      "Epoch 2/300 - Train Loss: 0.1076, Val Loss: 0.0815\n",
      "Epoch 3/300 - Train Loss: 0.0995, Val Loss: 0.0864\n",
      "Epoch 4/300 - Train Loss: 0.0948, Val Loss: 0.0845\n",
      "Epoch 5/300 - Train Loss: 0.0919, Val Loss: 0.0719\n",
      "Epoch 6/300 - Train Loss: 0.0876, Val Loss: 0.0723\n",
      "Epoch 7/300 - Train Loss: 0.0841, Val Loss: 0.0826\n",
      "Epoch 8/300 - Train Loss: 0.0816, Val Loss: 0.0706\n",
      "Epoch 9/300 - Train Loss: 0.0838, Val Loss: 0.0736\n",
      "Epoch 10/300 - Train Loss: 0.0813, Val Loss: 0.0729\n",
      "Epoch 11/300 - Train Loss: 0.0803, Val Loss: 0.0759\n",
      "Epoch 12/300 - Train Loss: 0.0797, Val Loss: 0.0712\n",
      "Epoch 13/300 - Train Loss: 0.0772, Val Loss: 0.0755\n",
      "Epoch 14/300 - Train Loss: 0.0774, Val Loss: 0.0731\n",
      "Epoch 15/300 - Train Loss: 0.0737, Val Loss: 0.0698\n",
      "Epoch 16/300 - Train Loss: 0.0768, Val Loss: 0.0660\n",
      "Epoch 17/300 - Train Loss: 0.0743, Val Loss: 0.0699\n",
      "Epoch 18/300 - Train Loss: 0.0742, Val Loss: 0.0701\n",
      "Epoch 19/300 - Train Loss: 0.0725, Val Loss: 0.0718\n",
      "Epoch 20/300 - Train Loss: 0.0722, Val Loss: 0.0686\n",
      "Epoch 21/300 - Train Loss: 0.0697, Val Loss: 0.0727\n",
      "Epoch 22/300 - Train Loss: 0.0681, Val Loss: 0.0707\n",
      "Epoch 23/300 - Train Loss: 0.0719, Val Loss: 0.0842\n",
      "Epoch 24/300 - Train Loss: 0.0681, Val Loss: 0.0649\n",
      "Epoch 25/300 - Train Loss: 0.0681, Val Loss: 0.0657\n",
      "Epoch 26/300 - Train Loss: 0.0674, Val Loss: 0.0717\n",
      "Epoch 27/300 - Train Loss: 0.0672, Val Loss: 0.0659\n",
      "Epoch 28/300 - Train Loss: 0.0679, Val Loss: 0.0701\n",
      "Epoch 29/300 - Train Loss: 0.0651, Val Loss: 0.0689\n",
      "Epoch 30/300 - Train Loss: 0.0674, Val Loss: 0.0725\n",
      "Epoch 31/300 - Train Loss: 0.0643, Val Loss: 0.0684\n",
      "Epoch 32/300 - Train Loss: 0.0633, Val Loss: 0.0724\n",
      "Epoch 33/300 - Train Loss: 0.0619, Val Loss: 0.0675\n",
      "Epoch 34/300 - Train Loss: 0.0648, Val Loss: 0.0734\n",
      "Epoch 35/300 - Train Loss: 0.0598, Val Loss: 0.0648\n",
      "Epoch 36/300 - Train Loss: 0.0624, Val Loss: 0.0686\n",
      "Epoch 37/300 - Train Loss: 0.0591, Val Loss: 0.0661\n",
      "Epoch 38/300 - Train Loss: 0.0601, Val Loss: 0.0649\n",
      "Epoch 39/300 - Train Loss: 0.0604, Val Loss: 0.0672\n",
      "Epoch 40/300 - Train Loss: 0.0619, Val Loss: 0.0689\n",
      "Epoch 41/300 - Train Loss: 0.0614, Val Loss: 0.0679\n",
      "Epoch 42/300 - Train Loss: 0.0579, Val Loss: 0.0671\n",
      "Epoch 43/300 - Train Loss: 0.0570, Val Loss: 0.0693\n",
      "Epoch 44/300 - Train Loss: 0.0564, Val Loss: 0.0655\n",
      "Epoch 45/300 - Train Loss: 0.0566, Val Loss: 0.0682\n",
      "Epoch 46/300 - Train Loss: 0.0564, Val Loss: 0.0707\n",
      "Epoch 47/300 - Train Loss: 0.0540, Val Loss: 0.0639\n",
      "Epoch 48/300 - Train Loss: 0.0566, Val Loss: 0.0668\n",
      "Epoch 49/300 - Train Loss: 0.0560, Val Loss: 0.0638\n",
      "Epoch 50/300 - Train Loss: 0.0543, Val Loss: 0.0698\n",
      "Epoch 51/300 - Train Loss: 0.0557, Val Loss: 0.0700\n",
      "Epoch 52/300 - Train Loss: 0.0530, Val Loss: 0.0698\n",
      "Epoch 53/300 - Train Loss: 0.0524, Val Loss: 0.0688\n",
      "Epoch 54/300 - Train Loss: 0.0550, Val Loss: 0.0661\n",
      "Epoch 55/300 - Train Loss: 0.0522, Val Loss: 0.0719\n",
      "Epoch 56/300 - Train Loss: 0.0529, Val Loss: 0.0687\n",
      "Epoch 57/300 - Train Loss: 0.0537, Val Loss: 0.0687\n",
      "Epoch 58/300 - Train Loss: 0.0526, Val Loss: 0.0658\n",
      "Epoch 59/300 - Train Loss: 0.0518, Val Loss: 0.0691\n",
      "Epoch 60/300 - Train Loss: 0.0516, Val Loss: 0.0650\n",
      "Epoch 61/300 - Train Loss: 0.0497, Val Loss: 0.0652\n",
      "Epoch 62/300 - Train Loss: 0.0502, Val Loss: 0.0688\n",
      "Epoch 63/300 - Train Loss: 0.0480, Val Loss: 0.0690\n",
      "Epoch 64/300 - Train Loss: 0.0486, Val Loss: 0.0740\n",
      "Epoch 65/300 - Train Loss: 0.0506, Val Loss: 0.0762\n",
      "Epoch 66/300 - Train Loss: 0.0496, Val Loss: 0.0672\n",
      "Epoch 67/300 - Train Loss: 0.0464, Val Loss: 0.0684\n",
      "Epoch 68/300 - Train Loss: 0.0466, Val Loss: 0.0804\n",
      "Epoch 69/300 - Train Loss: 0.0461, Val Loss: 0.0709\n",
      "Epoch 70/300 - Train Loss: 0.0484, Val Loss: 0.0740\n",
      "Epoch 71/300 - Train Loss: 0.0460, Val Loss: 0.0682\n",
      "Epoch 72/300 - Train Loss: 0.0452, Val Loss: 0.0726\n",
      "Epoch 73/300 - Train Loss: 0.0476, Val Loss: 0.0752\n",
      "Epoch 74/300 - Train Loss: 0.0452, Val Loss: 0.0732\n",
      "Epoch 75/300 - Train Loss: 0.0440, Val Loss: 0.0730\n",
      "Epoch 76/300 - Train Loss: 0.0449, Val Loss: 0.0662\n",
      "Epoch 77/300 - Train Loss: 0.0449, Val Loss: 0.0705\n",
      "Epoch 78/300 - Train Loss: 0.0443, Val Loss: 0.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:30:14,853] Trial 300 finished with value: 0.9778795917344382 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.12454484181726068, 'learning_rate': 0.00010714772854224446, 'batch_size': 32, 'weight_decay': 4.2392521973083066e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/300 - Train Loss: 0.0450, Val Loss: 0.0737\n",
      "Early stopping at epoch 79\n",
      "Macro F1 Score: 0.9779, Macro Precision: 0.9782, Macro Recall: 0.9775\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.97      0.97        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 302\n",
      "Training with F1=32, F2=32, D=2, dropout=0.12513945570374094, LR=0.00010001404829028386, BS=32, WD=4.08720357983213e-05\n",
      "Epoch 1/300 - Train Loss: 0.2819, Val Loss: 0.1228\n",
      "Epoch 2/300 - Train Loss: 0.1222, Val Loss: 0.0872\n",
      "Epoch 3/300 - Train Loss: 0.1020, Val Loss: 0.0797\n",
      "Epoch 4/300 - Train Loss: 0.0965, Val Loss: 0.0820\n",
      "Epoch 5/300 - Train Loss: 0.0959, Val Loss: 0.0747\n",
      "Epoch 6/300 - Train Loss: 0.0920, Val Loss: 0.0849\n",
      "Epoch 7/300 - Train Loss: 0.0864, Val Loss: 0.0719\n",
      "Epoch 8/300 - Train Loss: 0.0843, Val Loss: 0.0744\n",
      "Epoch 9/300 - Train Loss: 0.0845, Val Loss: 0.0775\n",
      "Epoch 10/300 - Train Loss: 0.0835, Val Loss: 0.0768\n",
      "Epoch 11/300 - Train Loss: 0.0829, Val Loss: 0.0730\n",
      "Epoch 12/300 - Train Loss: 0.0815, Val Loss: 0.0720\n",
      "Epoch 13/300 - Train Loss: 0.0802, Val Loss: 0.0736\n",
      "Epoch 14/300 - Train Loss: 0.0796, Val Loss: 0.0771\n",
      "Epoch 15/300 - Train Loss: 0.0771, Val Loss: 0.0721\n",
      "Epoch 16/300 - Train Loss: 0.0772, Val Loss: 0.0731\n",
      "Epoch 17/300 - Train Loss: 0.0758, Val Loss: 0.0706\n",
      "Epoch 18/300 - Train Loss: 0.0752, Val Loss: 0.0699\n",
      "Epoch 19/300 - Train Loss: 0.0754, Val Loss: 0.0682\n",
      "Epoch 20/300 - Train Loss: 0.0745, Val Loss: 0.0650\n",
      "Epoch 21/300 - Train Loss: 0.0733, Val Loss: 0.0651\n",
      "Epoch 22/300 - Train Loss: 0.0738, Val Loss: 0.0667\n",
      "Epoch 23/300 - Train Loss: 0.0727, Val Loss: 0.0703\n",
      "Epoch 24/300 - Train Loss: 0.0709, Val Loss: 0.0652\n",
      "Epoch 25/300 - Train Loss: 0.0732, Val Loss: 0.0659\n",
      "Epoch 26/300 - Train Loss: 0.0710, Val Loss: 0.0665\n",
      "Epoch 27/300 - Train Loss: 0.0703, Val Loss: 0.0696\n",
      "Epoch 28/300 - Train Loss: 0.0690, Val Loss: 0.0667\n",
      "Epoch 29/300 - Train Loss: 0.0700, Val Loss: 0.0693\n",
      "Epoch 30/300 - Train Loss: 0.0678, Val Loss: 0.0656\n",
      "Epoch 31/300 - Train Loss: 0.0647, Val Loss: 0.0722\n",
      "Epoch 32/300 - Train Loss: 0.0650, Val Loss: 0.0675\n",
      "Epoch 33/300 - Train Loss: 0.0663, Val Loss: 0.0668\n",
      "Epoch 34/300 - Train Loss: 0.0646, Val Loss: 0.0678\n",
      "Epoch 35/300 - Train Loss: 0.0649, Val Loss: 0.0697\n",
      "Epoch 36/300 - Train Loss: 0.0636, Val Loss: 0.0645\n",
      "Epoch 37/300 - Train Loss: 0.0645, Val Loss: 0.0753\n",
      "Epoch 38/300 - Train Loss: 0.0633, Val Loss: 0.0655\n",
      "Epoch 39/300 - Train Loss: 0.0632, Val Loss: 0.0670\n",
      "Epoch 40/300 - Train Loss: 0.0627, Val Loss: 0.0666\n",
      "Epoch 41/300 - Train Loss: 0.0613, Val Loss: 0.0680\n",
      "Epoch 42/300 - Train Loss: 0.0604, Val Loss: 0.0849\n",
      "Epoch 43/300 - Train Loss: 0.0613, Val Loss: 0.0690\n",
      "Epoch 44/300 - Train Loss: 0.0588, Val Loss: 0.0657\n",
      "Epoch 45/300 - Train Loss: 0.0595, Val Loss: 0.0686\n",
      "Epoch 46/300 - Train Loss: 0.0599, Val Loss: 0.0652\n",
      "Epoch 47/300 - Train Loss: 0.0577, Val Loss: 0.0665\n",
      "Epoch 48/300 - Train Loss: 0.0574, Val Loss: 0.0678\n",
      "Epoch 49/300 - Train Loss: 0.0575, Val Loss: 0.0660\n",
      "Epoch 50/300 - Train Loss: 0.0590, Val Loss: 0.0668\n",
      "Epoch 51/300 - Train Loss: 0.0567, Val Loss: 0.0691\n",
      "Epoch 52/300 - Train Loss: 0.0570, Val Loss: 0.0680\n",
      "Epoch 53/300 - Train Loss: 0.0538, Val Loss: 0.0707\n",
      "Epoch 54/300 - Train Loss: 0.0551, Val Loss: 0.0655\n",
      "Epoch 55/300 - Train Loss: 0.0562, Val Loss: 0.0805\n",
      "Epoch 56/300 - Train Loss: 0.0567, Val Loss: 0.0700\n",
      "Epoch 57/300 - Train Loss: 0.0545, Val Loss: 0.0716\n",
      "Epoch 58/300 - Train Loss: 0.0542, Val Loss: 0.0709\n",
      "Epoch 59/300 - Train Loss: 0.0540, Val Loss: 0.0790\n",
      "Epoch 60/300 - Train Loss: 0.0544, Val Loss: 0.0682\n",
      "Epoch 61/300 - Train Loss: 0.0510, Val Loss: 0.0706\n",
      "Epoch 62/300 - Train Loss: 0.0517, Val Loss: 0.0692\n",
      "Epoch 63/300 - Train Loss: 0.0524, Val Loss: 0.0687\n",
      "Epoch 64/300 - Train Loss: 0.0491, Val Loss: 0.0651\n",
      "Epoch 65/300 - Train Loss: 0.0513, Val Loss: 0.0679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:33:14,966] Trial 301 finished with value: 0.9685449111108841 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.12513945570374094, 'learning_rate': 0.00010001404829028386, 'batch_size': 32, 'weight_decay': 4.08720357983213e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/300 - Train Loss: 0.0508, Val Loss: 0.0710\n",
      "Early stopping at epoch 66\n",
      "Macro F1 Score: 0.9685, Macro Precision: 0.9602, Macro Recall: 0.9775\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 303\n",
      "Training with F1=32, F2=32, D=2, dropout=0.12517530944458008, LR=0.00010922761679214682, BS=32, WD=4.7506345463289705e-05\n",
      "Epoch 1/300 - Train Loss: 0.2423, Val Loss: 0.0985\n",
      "Epoch 2/300 - Train Loss: 0.1059, Val Loss: 0.1018\n",
      "Epoch 3/300 - Train Loss: 0.0952, Val Loss: 0.0779\n",
      "Epoch 4/300 - Train Loss: 0.0904, Val Loss: 0.0856\n",
      "Epoch 5/300 - Train Loss: 0.0880, Val Loss: 0.0899\n",
      "Epoch 6/300 - Train Loss: 0.0864, Val Loss: 0.0812\n",
      "Epoch 7/300 - Train Loss: 0.0847, Val Loss: 0.0716\n",
      "Epoch 8/300 - Train Loss: 0.0826, Val Loss: 0.0779\n",
      "Epoch 9/300 - Train Loss: 0.0822, Val Loss: 0.0740\n",
      "Epoch 10/300 - Train Loss: 0.0794, Val Loss: 0.0683\n",
      "Epoch 11/300 - Train Loss: 0.0791, Val Loss: 0.0682\n",
      "Epoch 12/300 - Train Loss: 0.0783, Val Loss: 0.0760\n",
      "Epoch 13/300 - Train Loss: 0.0778, Val Loss: 0.0689\n",
      "Epoch 14/300 - Train Loss: 0.0762, Val Loss: 0.0717\n",
      "Epoch 15/300 - Train Loss: 0.0759, Val Loss: 0.0716\n",
      "Epoch 16/300 - Train Loss: 0.0741, Val Loss: 0.0732\n",
      "Epoch 17/300 - Train Loss: 0.0758, Val Loss: 0.0700\n",
      "Epoch 18/300 - Train Loss: 0.0714, Val Loss: 0.0728\n",
      "Epoch 19/300 - Train Loss: 0.0722, Val Loss: 0.0748\n",
      "Epoch 20/300 - Train Loss: 0.0722, Val Loss: 0.0697\n",
      "Epoch 21/300 - Train Loss: 0.0708, Val Loss: 0.0692\n",
      "Epoch 22/300 - Train Loss: 0.0681, Val Loss: 0.0754\n",
      "Epoch 23/300 - Train Loss: 0.0681, Val Loss: 0.0788\n",
      "Epoch 24/300 - Train Loss: 0.0676, Val Loss: 0.0690\n",
      "Epoch 25/300 - Train Loss: 0.0708, Val Loss: 0.0700\n",
      "Epoch 26/300 - Train Loss: 0.0668, Val Loss: 0.0730\n",
      "Epoch 27/300 - Train Loss: 0.0652, Val Loss: 0.0789\n",
      "Epoch 28/300 - Train Loss: 0.0675, Val Loss: 0.0703\n",
      "Epoch 29/300 - Train Loss: 0.0647, Val Loss: 0.0763\n",
      "Epoch 30/300 - Train Loss: 0.0632, Val Loss: 0.0724\n",
      "Epoch 31/300 - Train Loss: 0.0625, Val Loss: 0.0684\n",
      "Epoch 32/300 - Train Loss: 0.0629, Val Loss: 0.0668\n",
      "Epoch 33/300 - Train Loss: 0.0633, Val Loss: 0.0673\n",
      "Epoch 34/300 - Train Loss: 0.0597, Val Loss: 0.0728\n",
      "Epoch 35/300 - Train Loss: 0.0608, Val Loss: 0.0688\n",
      "Epoch 36/300 - Train Loss: 0.0612, Val Loss: 0.0715\n",
      "Epoch 37/300 - Train Loss: 0.0597, Val Loss: 0.0700\n",
      "Epoch 38/300 - Train Loss: 0.0600, Val Loss: 0.0680\n",
      "Epoch 39/300 - Train Loss: 0.0575, Val Loss: 0.0712\n",
      "Epoch 40/300 - Train Loss: 0.0572, Val Loss: 0.0677\n",
      "Epoch 41/300 - Train Loss: 0.0577, Val Loss: 0.0728\n",
      "Epoch 42/300 - Train Loss: 0.0580, Val Loss: 0.0686\n",
      "Epoch 43/300 - Train Loss: 0.0575, Val Loss: 0.0688\n",
      "Epoch 44/300 - Train Loss: 0.0589, Val Loss: 0.0723\n",
      "Epoch 45/300 - Train Loss: 0.0565, Val Loss: 0.0682\n",
      "Epoch 46/300 - Train Loss: 0.0569, Val Loss: 0.0746\n",
      "Epoch 47/300 - Train Loss: 0.0547, Val Loss: 0.0734\n",
      "Epoch 48/300 - Train Loss: 0.0545, Val Loss: 0.0724\n",
      "Epoch 49/300 - Train Loss: 0.0545, Val Loss: 0.0692\n",
      "Epoch 50/300 - Train Loss: 0.0530, Val Loss: 0.0712\n",
      "Epoch 51/300 - Train Loss: 0.0551, Val Loss: 0.0692\n",
      "Epoch 52/300 - Train Loss: 0.0529, Val Loss: 0.0697\n",
      "Epoch 53/300 - Train Loss: 0.0529, Val Loss: 0.0722\n",
      "Epoch 54/300 - Train Loss: 0.0524, Val Loss: 0.0718\n",
      "Epoch 55/300 - Train Loss: 0.0523, Val Loss: 0.0764\n",
      "Epoch 56/300 - Train Loss: 0.0498, Val Loss: 0.0721\n",
      "Epoch 57/300 - Train Loss: 0.0498, Val Loss: 0.0691\n",
      "Epoch 58/300 - Train Loss: 0.0502, Val Loss: 0.0709\n",
      "Epoch 59/300 - Train Loss: 0.0500, Val Loss: 0.0723\n",
      "Epoch 60/300 - Train Loss: 0.0500, Val Loss: 0.0692\n",
      "Epoch 61/300 - Train Loss: 0.0509, Val Loss: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:36:04,279] Trial 302 finished with value: 0.9682797147897824 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.12517530944458008, 'learning_rate': 0.00010922761679214682, 'batch_size': 32, 'weight_decay': 4.7506345463289705e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/300 - Train Loss: 0.0486, Val Loss: 0.0683\n",
      "Early stopping at epoch 62\n",
      "Macro F1 Score: 0.9683, Macro Precision: 0.9643, Macro Recall: 0.9725\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 304\n",
      "Training with F1=32, F2=8, D=2, dropout=0.11372905143279963, LR=9.166495631759613e-05, BS=32, WD=3.753387309552919e-05\n",
      "Epoch 1/300 - Train Loss: 0.3778, Val Loss: 0.1532\n",
      "Epoch 2/300 - Train Loss: 0.1367, Val Loss: 0.0985\n",
      "Epoch 3/300 - Train Loss: 0.1120, Val Loss: 0.0798\n",
      "Epoch 4/300 - Train Loss: 0.1059, Val Loss: 0.0799\n",
      "Epoch 5/300 - Train Loss: 0.1000, Val Loss: 0.0779\n",
      "Epoch 6/300 - Train Loss: 0.0958, Val Loss: 0.0749\n",
      "Epoch 7/300 - Train Loss: 0.0938, Val Loss: 0.0908\n",
      "Epoch 8/300 - Train Loss: 0.0917, Val Loss: 0.0745\n",
      "Epoch 9/300 - Train Loss: 0.0898, Val Loss: 0.0727\n",
      "Epoch 10/300 - Train Loss: 0.0862, Val Loss: 0.0774\n",
      "Epoch 11/300 - Train Loss: 0.0871, Val Loss: 0.0705\n",
      "Epoch 12/300 - Train Loss: 0.0862, Val Loss: 0.0811\n",
      "Epoch 13/300 - Train Loss: 0.0852, Val Loss: 0.0726\n",
      "Epoch 14/300 - Train Loss: 0.0840, Val Loss: 0.0716\n",
      "Epoch 15/300 - Train Loss: 0.0836, Val Loss: 0.0669\n",
      "Epoch 16/300 - Train Loss: 0.0818, Val Loss: 0.0694\n",
      "Epoch 17/300 - Train Loss: 0.0818, Val Loss: 0.0697\n",
      "Epoch 18/300 - Train Loss: 0.0799, Val Loss: 0.0743\n",
      "Epoch 19/300 - Train Loss: 0.0814, Val Loss: 0.0703\n",
      "Epoch 20/300 - Train Loss: 0.0795, Val Loss: 0.0692\n",
      "Epoch 21/300 - Train Loss: 0.0817, Val Loss: 0.0704\n",
      "Epoch 22/300 - Train Loss: 0.0822, Val Loss: 0.0744\n",
      "Epoch 23/300 - Train Loss: 0.0783, Val Loss: 0.0732\n",
      "Epoch 24/300 - Train Loss: 0.0784, Val Loss: 0.0698\n",
      "Epoch 25/300 - Train Loss: 0.0785, Val Loss: 0.0705\n",
      "Epoch 26/300 - Train Loss: 0.0770, Val Loss: 0.0654\n",
      "Epoch 27/300 - Train Loss: 0.0765, Val Loss: 0.0684\n",
      "Epoch 28/300 - Train Loss: 0.0778, Val Loss: 0.0769\n",
      "Epoch 29/300 - Train Loss: 0.0765, Val Loss: 0.0685\n",
      "Epoch 30/300 - Train Loss: 0.0750, Val Loss: 0.0654\n",
      "Epoch 31/300 - Train Loss: 0.0744, Val Loss: 0.0725\n",
      "Epoch 32/300 - Train Loss: 0.0757, Val Loss: 0.0713\n",
      "Epoch 33/300 - Train Loss: 0.0761, Val Loss: 0.0665\n",
      "Epoch 34/300 - Train Loss: 0.0740, Val Loss: 0.0700\n",
      "Epoch 35/300 - Train Loss: 0.0745, Val Loss: 0.0659\n",
      "Epoch 36/300 - Train Loss: 0.0765, Val Loss: 0.0671\n",
      "Epoch 37/300 - Train Loss: 0.0728, Val Loss: 0.0709\n",
      "Epoch 38/300 - Train Loss: 0.0767, Val Loss: 0.0724\n",
      "Epoch 39/300 - Train Loss: 0.0749, Val Loss: 0.0661\n",
      "Epoch 40/300 - Train Loss: 0.0728, Val Loss: 0.0694\n",
      "Epoch 41/300 - Train Loss: 0.0751, Val Loss: 0.0659\n",
      "Epoch 42/300 - Train Loss: 0.0725, Val Loss: 0.0682\n",
      "Epoch 43/300 - Train Loss: 0.0735, Val Loss: 0.0671\n",
      "Epoch 44/300 - Train Loss: 0.0731, Val Loss: 0.0707\n",
      "Epoch 45/300 - Train Loss: 0.0727, Val Loss: 0.0644\n",
      "Epoch 46/300 - Train Loss: 0.0715, Val Loss: 0.0699\n",
      "Epoch 47/300 - Train Loss: 0.0725, Val Loss: 0.0700\n",
      "Epoch 48/300 - Train Loss: 0.0718, Val Loss: 0.0688\n",
      "Epoch 49/300 - Train Loss: 0.0691, Val Loss: 0.0652\n",
      "Epoch 50/300 - Train Loss: 0.0720, Val Loss: 0.0665\n",
      "Epoch 51/300 - Train Loss: 0.0726, Val Loss: 0.0694\n",
      "Epoch 52/300 - Train Loss: 0.0702, Val Loss: 0.0655\n",
      "Epoch 53/300 - Train Loss: 0.0706, Val Loss: 0.0690\n",
      "Epoch 54/300 - Train Loss: 0.0695, Val Loss: 0.0647\n",
      "Epoch 55/300 - Train Loss: 0.0702, Val Loss: 0.0673\n",
      "Epoch 56/300 - Train Loss: 0.0693, Val Loss: 0.0646\n",
      "Epoch 57/300 - Train Loss: 0.0682, Val Loss: 0.0671\n",
      "Epoch 58/300 - Train Loss: 0.0703, Val Loss: 0.0630\n",
      "Epoch 59/300 - Train Loss: 0.0691, Val Loss: 0.0658\n",
      "Epoch 60/300 - Train Loss: 0.0683, Val Loss: 0.0635\n",
      "Epoch 61/300 - Train Loss: 0.0692, Val Loss: 0.0658\n",
      "Epoch 62/300 - Train Loss: 0.0689, Val Loss: 0.0657\n",
      "Epoch 63/300 - Train Loss: 0.0690, Val Loss: 0.0642\n",
      "Epoch 64/300 - Train Loss: 0.0676, Val Loss: 0.0679\n",
      "Epoch 65/300 - Train Loss: 0.0682, Val Loss: 0.0622\n",
      "Epoch 66/300 - Train Loss: 0.0673, Val Loss: 0.0673\n",
      "Epoch 67/300 - Train Loss: 0.0668, Val Loss: 0.0629\n",
      "Epoch 68/300 - Train Loss: 0.0665, Val Loss: 0.0689\n",
      "Epoch 69/300 - Train Loss: 0.0671, Val Loss: 0.0672\n",
      "Epoch 70/300 - Train Loss: 0.0666, Val Loss: 0.0680\n",
      "Epoch 71/300 - Train Loss: 0.0685, Val Loss: 0.0690\n",
      "Epoch 72/300 - Train Loss: 0.0662, Val Loss: 0.0655\n",
      "Epoch 73/300 - Train Loss: 0.0639, Val Loss: 0.0732\n",
      "Epoch 74/300 - Train Loss: 0.0667, Val Loss: 0.0665\n",
      "Epoch 75/300 - Train Loss: 0.0663, Val Loss: 0.0661\n",
      "Epoch 76/300 - Train Loss: 0.0668, Val Loss: 0.0666\n",
      "Epoch 77/300 - Train Loss: 0.0665, Val Loss: 0.0656\n",
      "Epoch 78/300 - Train Loss: 0.0665, Val Loss: 0.0660\n",
      "Epoch 79/300 - Train Loss: 0.0649, Val Loss: 0.0655\n",
      "Epoch 80/300 - Train Loss: 0.0648, Val Loss: 0.0636\n",
      "Epoch 81/300 - Train Loss: 0.0647, Val Loss: 0.0613\n",
      "Epoch 82/300 - Train Loss: 0.0638, Val Loss: 0.0670\n",
      "Epoch 83/300 - Train Loss: 0.0645, Val Loss: 0.0626\n",
      "Epoch 84/300 - Train Loss: 0.0661, Val Loss: 0.0681\n",
      "Epoch 85/300 - Train Loss: 0.0673, Val Loss: 0.0643\n",
      "Epoch 86/300 - Train Loss: 0.0634, Val Loss: 0.0631\n",
      "Epoch 87/300 - Train Loss: 0.0660, Val Loss: 0.0690\n",
      "Epoch 88/300 - Train Loss: 0.0639, Val Loss: 0.0621\n",
      "Epoch 89/300 - Train Loss: 0.0614, Val Loss: 0.0641\n",
      "Epoch 90/300 - Train Loss: 0.0646, Val Loss: 0.0614\n",
      "Epoch 91/300 - Train Loss: 0.0650, Val Loss: 0.0671\n",
      "Epoch 92/300 - Train Loss: 0.0626, Val Loss: 0.0656\n",
      "Epoch 93/300 - Train Loss: 0.0614, Val Loss: 0.0634\n",
      "Epoch 94/300 - Train Loss: 0.0646, Val Loss: 0.0646\n",
      "Epoch 95/300 - Train Loss: 0.0622, Val Loss: 0.0656\n",
      "Epoch 96/300 - Train Loss: 0.0650, Val Loss: 0.0635\n",
      "Epoch 97/300 - Train Loss: 0.0629, Val Loss: 0.0662\n",
      "Epoch 98/300 - Train Loss: 0.0609, Val Loss: 0.0636\n",
      "Epoch 99/300 - Train Loss: 0.0639, Val Loss: 0.0626\n",
      "Epoch 100/300 - Train Loss: 0.0626, Val Loss: 0.0657\n",
      "Epoch 101/300 - Train Loss: 0.0612, Val Loss: 0.0639\n",
      "Epoch 102/300 - Train Loss: 0.0629, Val Loss: 0.0648\n",
      "Epoch 103/300 - Train Loss: 0.0589, Val Loss: 0.0630\n",
      "Epoch 104/300 - Train Loss: 0.0603, Val Loss: 0.0632\n",
      "Epoch 105/300 - Train Loss: 0.0623, Val Loss: 0.0652\n",
      "Epoch 106/300 - Train Loss: 0.0608, Val Loss: 0.0654\n",
      "Epoch 107/300 - Train Loss: 0.0626, Val Loss: 0.0659\n",
      "Epoch 108/300 - Train Loss: 0.0622, Val Loss: 0.0640\n",
      "Epoch 109/300 - Train Loss: 0.0613, Val Loss: 0.0651\n",
      "Epoch 110/300 - Train Loss: 0.0604, Val Loss: 0.0662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:40:40,932] Trial 303 finished with value: 0.9705263811648907 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.11372905143279963, 'learning_rate': 9.166495631759613e-05, 'batch_size': 32, 'weight_decay': 3.753387309552919e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300 - Train Loss: 0.0588, Val Loss: 0.0647\n",
      "Early stopping at epoch 111\n",
      "Macro F1 Score: 0.9705, Macro Precision: 0.9691, Macro Recall: 0.9721\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 305\n",
      "Training with F1=32, F2=32, D=2, dropout=0.13579111655309323, LR=8.448667454405495e-05, BS=32, WD=2.770992400018602e-05\n",
      "Epoch 1/300 - Train Loss: 0.2968, Val Loss: 0.1224\n",
      "Epoch 2/300 - Train Loss: 0.1222, Val Loss: 0.0838\n",
      "Epoch 3/300 - Train Loss: 0.1048, Val Loss: 0.0781\n",
      "Epoch 4/300 - Train Loss: 0.0970, Val Loss: 0.0742\n",
      "Epoch 5/300 - Train Loss: 0.0924, Val Loss: 0.0741\n",
      "Epoch 6/300 - Train Loss: 0.0870, Val Loss: 0.0732\n",
      "Epoch 7/300 - Train Loss: 0.0870, Val Loss: 0.0711\n",
      "Epoch 8/300 - Train Loss: 0.0873, Val Loss: 0.0778\n",
      "Epoch 9/300 - Train Loss: 0.0836, Val Loss: 0.0713\n",
      "Epoch 10/300 - Train Loss: 0.0831, Val Loss: 0.0726\n",
      "Epoch 11/300 - Train Loss: 0.0808, Val Loss: 0.0703\n",
      "Epoch 12/300 - Train Loss: 0.0807, Val Loss: 0.0709\n",
      "Epoch 13/300 - Train Loss: 0.0798, Val Loss: 0.0715\n",
      "Epoch 14/300 - Train Loss: 0.0783, Val Loss: 0.0700\n",
      "Epoch 15/300 - Train Loss: 0.0780, Val Loss: 0.0783\n",
      "Epoch 16/300 - Train Loss: 0.0757, Val Loss: 0.0770\n",
      "Epoch 17/300 - Train Loss: 0.0772, Val Loss: 0.0664\n",
      "Epoch 18/300 - Train Loss: 0.0769, Val Loss: 0.0724\n",
      "Epoch 19/300 - Train Loss: 0.0735, Val Loss: 0.0657\n",
      "Epoch 20/300 - Train Loss: 0.0730, Val Loss: 0.0780\n",
      "Epoch 21/300 - Train Loss: 0.0749, Val Loss: 0.0681\n",
      "Epoch 22/300 - Train Loss: 0.0717, Val Loss: 0.0673\n",
      "Epoch 23/300 - Train Loss: 0.0719, Val Loss: 0.0697\n",
      "Epoch 24/300 - Train Loss: 0.0713, Val Loss: 0.0711\n",
      "Epoch 25/300 - Train Loss: 0.0707, Val Loss: 0.0678\n",
      "Epoch 26/300 - Train Loss: 0.0761, Val Loss: 0.0792\n",
      "Epoch 27/300 - Train Loss: 0.0706, Val Loss: 0.0685\n",
      "Epoch 28/300 - Train Loss: 0.0701, Val Loss: 0.0698\n",
      "Epoch 29/300 - Train Loss: 0.0698, Val Loss: 0.0643\n",
      "Epoch 30/300 - Train Loss: 0.0690, Val Loss: 0.0646\n",
      "Epoch 31/300 - Train Loss: 0.0710, Val Loss: 0.0698\n",
      "Epoch 32/300 - Train Loss: 0.0675, Val Loss: 0.0689\n",
      "Epoch 33/300 - Train Loss: 0.0671, Val Loss: 0.0680\n",
      "Epoch 34/300 - Train Loss: 0.0668, Val Loss: 0.0673\n",
      "Epoch 35/300 - Train Loss: 0.0656, Val Loss: 0.0670\n",
      "Epoch 36/300 - Train Loss: 0.0653, Val Loss: 0.0664\n",
      "Epoch 37/300 - Train Loss: 0.0652, Val Loss: 0.0653\n",
      "Epoch 38/300 - Train Loss: 0.0666, Val Loss: 0.0732\n",
      "Epoch 39/300 - Train Loss: 0.0636, Val Loss: 0.0641\n",
      "Epoch 40/300 - Train Loss: 0.0626, Val Loss: 0.0714\n",
      "Epoch 41/300 - Train Loss: 0.0644, Val Loss: 0.0672\n",
      "Epoch 42/300 - Train Loss: 0.0625, Val Loss: 0.0654\n",
      "Epoch 43/300 - Train Loss: 0.0630, Val Loss: 0.0671\n",
      "Epoch 44/300 - Train Loss: 0.0629, Val Loss: 0.0683\n",
      "Epoch 45/300 - Train Loss: 0.0597, Val Loss: 0.0670\n",
      "Epoch 46/300 - Train Loss: 0.0616, Val Loss: 0.0670\n",
      "Epoch 47/300 - Train Loss: 0.0622, Val Loss: 0.0651\n",
      "Epoch 48/300 - Train Loss: 0.0626, Val Loss: 0.0683\n",
      "Epoch 49/300 - Train Loss: 0.0601, Val Loss: 0.0675\n",
      "Epoch 50/300 - Train Loss: 0.0599, Val Loss: 0.0694\n",
      "Epoch 51/300 - Train Loss: 0.0605, Val Loss: 0.0682\n",
      "Epoch 52/300 - Train Loss: 0.0603, Val Loss: 0.0717\n",
      "Epoch 53/300 - Train Loss: 0.0579, Val Loss: 0.0691\n",
      "Epoch 54/300 - Train Loss: 0.0585, Val Loss: 0.0673\n",
      "Epoch 55/300 - Train Loss: 0.0590, Val Loss: 0.0671\n",
      "Epoch 56/300 - Train Loss: 0.0576, Val Loss: 0.0668\n",
      "Epoch 57/300 - Train Loss: 0.0562, Val Loss: 0.0680\n",
      "Epoch 58/300 - Train Loss: 0.0574, Val Loss: 0.0648\n",
      "Epoch 59/300 - Train Loss: 0.0577, Val Loss: 0.0745\n",
      "Epoch 60/300 - Train Loss: 0.0544, Val Loss: 0.0643\n",
      "Epoch 61/300 - Train Loss: 0.0553, Val Loss: 0.0663\n",
      "Epoch 62/300 - Train Loss: 0.0574, Val Loss: 0.0690\n",
      "Epoch 63/300 - Train Loss: 0.0541, Val Loss: 0.0675\n",
      "Epoch 64/300 - Train Loss: 0.0555, Val Loss: 0.0637\n",
      "Epoch 65/300 - Train Loss: 0.0543, Val Loss: 0.0641\n",
      "Epoch 66/300 - Train Loss: 0.0526, Val Loss: 0.0670\n",
      "Epoch 67/300 - Train Loss: 0.0528, Val Loss: 0.0721\n",
      "Epoch 68/300 - Train Loss: 0.0533, Val Loss: 0.0703\n",
      "Epoch 69/300 - Train Loss: 0.0542, Val Loss: 0.0636\n",
      "Epoch 70/300 - Train Loss: 0.0532, Val Loss: 0.0743\n",
      "Epoch 71/300 - Train Loss: 0.0518, Val Loss: 0.0681\n",
      "Epoch 72/300 - Train Loss: 0.0531, Val Loss: 0.0674\n",
      "Epoch 73/300 - Train Loss: 0.0512, Val Loss: 0.0666\n",
      "Epoch 74/300 - Train Loss: 0.0493, Val Loss: 0.0708\n",
      "Epoch 75/300 - Train Loss: 0.0506, Val Loss: 0.0650\n",
      "Epoch 76/300 - Train Loss: 0.0504, Val Loss: 0.0703\n",
      "Epoch 77/300 - Train Loss: 0.0502, Val Loss: 0.0656\n",
      "Epoch 78/300 - Train Loss: 0.0515, Val Loss: 0.0747\n",
      "Epoch 79/300 - Train Loss: 0.0498, Val Loss: 0.0709\n",
      "Epoch 80/300 - Train Loss: 0.0505, Val Loss: 0.0700\n",
      "Epoch 81/300 - Train Loss: 0.0523, Val Loss: 0.0712\n",
      "Epoch 82/300 - Train Loss: 0.0531, Val Loss: 0.0730\n",
      "Epoch 83/300 - Train Loss: 0.0493, Val Loss: 0.0681\n",
      "Epoch 84/300 - Train Loss: 0.0492, Val Loss: 0.0704\n",
      "Epoch 85/300 - Train Loss: 0.0467, Val Loss: 0.0684\n",
      "Epoch 86/300 - Train Loss: 0.0494, Val Loss: 0.0692\n",
      "Epoch 87/300 - Train Loss: 0.0472, Val Loss: 0.0679\n",
      "Epoch 88/300 - Train Loss: 0.0476, Val Loss: 0.0688\n",
      "Epoch 89/300 - Train Loss: 0.0470, Val Loss: 0.0748\n",
      "Epoch 90/300 - Train Loss: 0.0499, Val Loss: 0.0692\n",
      "Epoch 91/300 - Train Loss: 0.0458, Val Loss: 0.0703\n",
      "Epoch 92/300 - Train Loss: 0.0451, Val Loss: 0.0702\n",
      "Epoch 93/300 - Train Loss: 0.0467, Val Loss: 0.0837\n",
      "Epoch 94/300 - Train Loss: 0.0454, Val Loss: 0.0719\n",
      "Epoch 95/300 - Train Loss: 0.0468, Val Loss: 0.0789\n",
      "Epoch 96/300 - Train Loss: 0.0435, Val Loss: 0.0729\n",
      "Epoch 97/300 - Train Loss: 0.0446, Val Loss: 0.0697\n",
      "Epoch 98/300 - Train Loss: 0.0441, Val Loss: 0.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:45:11,330] Trial 304 finished with value: 0.970630407740534 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.13579111655309323, 'learning_rate': 8.448667454405495e-05, 'batch_size': 32, 'weight_decay': 2.770992400018602e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/300 - Train Loss: 0.0413, Val Loss: 0.0683\n",
      "Early stopping at epoch 99\n",
      "Macro F1 Score: 0.9706, Macro Precision: 0.9601, Macro Recall: 0.9822\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 306\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14733968621551358, LR=0.00010458139312160606, BS=32, WD=2.2304915456044804e-05\n",
      "Epoch 1/300 - Train Loss: 0.2650, Val Loss: 0.1119\n",
      "Epoch 2/300 - Train Loss: 0.1140, Val Loss: 0.0814\n",
      "Epoch 3/300 - Train Loss: 0.0990, Val Loss: 0.0846\n",
      "Epoch 4/300 - Train Loss: 0.0999, Val Loss: 0.0785\n",
      "Epoch 5/300 - Train Loss: 0.0929, Val Loss: 0.0740\n",
      "Epoch 6/300 - Train Loss: 0.0901, Val Loss: 0.0766\n",
      "Epoch 7/300 - Train Loss: 0.0885, Val Loss: 0.0750\n",
      "Epoch 8/300 - Train Loss: 0.0882, Val Loss: 0.0785\n",
      "Epoch 9/300 - Train Loss: 0.0847, Val Loss: 0.0686\n",
      "Epoch 10/300 - Train Loss: 0.0811, Val Loss: 0.0744\n",
      "Epoch 11/300 - Train Loss: 0.0819, Val Loss: 0.0728\n",
      "Epoch 12/300 - Train Loss: 0.0813, Val Loss: 0.0761\n",
      "Epoch 13/300 - Train Loss: 0.0814, Val Loss: 0.0751\n",
      "Epoch 14/300 - Train Loss: 0.0781, Val Loss: 0.0701\n",
      "Epoch 15/300 - Train Loss: 0.0759, Val Loss: 0.0711\n",
      "Epoch 16/300 - Train Loss: 0.0777, Val Loss: 0.0719\n",
      "Epoch 17/300 - Train Loss: 0.0758, Val Loss: 0.0746\n",
      "Epoch 18/300 - Train Loss: 0.0726, Val Loss: 0.0728\n",
      "Epoch 19/300 - Train Loss: 0.0719, Val Loss: 0.0723\n",
      "Epoch 20/300 - Train Loss: 0.0735, Val Loss: 0.0767\n",
      "Epoch 21/300 - Train Loss: 0.0733, Val Loss: 0.0712\n",
      "Epoch 22/300 - Train Loss: 0.0713, Val Loss: 0.0658\n",
      "Epoch 23/300 - Train Loss: 0.0717, Val Loss: 0.0735\n",
      "Epoch 24/300 - Train Loss: 0.0703, Val Loss: 0.0680\n",
      "Epoch 25/300 - Train Loss: 0.0734, Val Loss: 0.0702\n",
      "Epoch 26/300 - Train Loss: 0.0698, Val Loss: 0.0774\n",
      "Epoch 27/300 - Train Loss: 0.0715, Val Loss: 0.0704\n",
      "Epoch 28/300 - Train Loss: 0.0673, Val Loss: 0.0675\n",
      "Epoch 29/300 - Train Loss: 0.0683, Val Loss: 0.0680\n",
      "Epoch 30/300 - Train Loss: 0.0680, Val Loss: 0.0675\n",
      "Epoch 31/300 - Train Loss: 0.0678, Val Loss: 0.0645\n",
      "Epoch 32/300 - Train Loss: 0.0636, Val Loss: 0.0691\n",
      "Epoch 33/300 - Train Loss: 0.0650, Val Loss: 0.0671\n",
      "Epoch 34/300 - Train Loss: 0.0668, Val Loss: 0.0689\n",
      "Epoch 35/300 - Train Loss: 0.0636, Val Loss: 0.0687\n",
      "Epoch 36/300 - Train Loss: 0.0619, Val Loss: 0.0704\n",
      "Epoch 37/300 - Train Loss: 0.0629, Val Loss: 0.0705\n",
      "Epoch 38/300 - Train Loss: 0.0620, Val Loss: 0.0717\n",
      "Epoch 39/300 - Train Loss: 0.0610, Val Loss: 0.0662\n",
      "Epoch 40/300 - Train Loss: 0.0613, Val Loss: 0.0657\n",
      "Epoch 41/300 - Train Loss: 0.0612, Val Loss: 0.0668\n",
      "Epoch 42/300 - Train Loss: 0.0610, Val Loss: 0.0653\n",
      "Epoch 43/300 - Train Loss: 0.0604, Val Loss: 0.0740\n",
      "Epoch 44/300 - Train Loss: 0.0598, Val Loss: 0.0677\n",
      "Epoch 45/300 - Train Loss: 0.0592, Val Loss: 0.0710\n",
      "Epoch 46/300 - Train Loss: 0.0582, Val Loss: 0.0807\n",
      "Epoch 47/300 - Train Loss: 0.0576, Val Loss: 0.0645\n",
      "Epoch 48/300 - Train Loss: 0.0584, Val Loss: 0.0728\n",
      "Epoch 49/300 - Train Loss: 0.0568, Val Loss: 0.0664\n",
      "Epoch 50/300 - Train Loss: 0.0563, Val Loss: 0.0659\n",
      "Epoch 51/300 - Train Loss: 0.0552, Val Loss: 0.0748\n",
      "Epoch 52/300 - Train Loss: 0.0561, Val Loss: 0.0658\n",
      "Epoch 53/300 - Train Loss: 0.0543, Val Loss: 0.0642\n",
      "Epoch 54/300 - Train Loss: 0.0564, Val Loss: 0.0718\n",
      "Epoch 55/300 - Train Loss: 0.0559, Val Loss: 0.0690\n",
      "Epoch 56/300 - Train Loss: 0.0558, Val Loss: 0.0714\n",
      "Epoch 57/300 - Train Loss: 0.0548, Val Loss: 0.0627\n",
      "Epoch 58/300 - Train Loss: 0.0515, Val Loss: 0.0685\n",
      "Epoch 59/300 - Train Loss: 0.0527, Val Loss: 0.0718\n",
      "Epoch 60/300 - Train Loss: 0.0529, Val Loss: 0.0676\n",
      "Epoch 61/300 - Train Loss: 0.0517, Val Loss: 0.0697\n",
      "Epoch 62/300 - Train Loss: 0.0527, Val Loss: 0.0663\n",
      "Epoch 63/300 - Train Loss: 0.0508, Val Loss: 0.0656\n",
      "Epoch 64/300 - Train Loss: 0.0545, Val Loss: 0.0673\n",
      "Epoch 65/300 - Train Loss: 0.0529, Val Loss: 0.0669\n",
      "Epoch 66/300 - Train Loss: 0.0511, Val Loss: 0.0690\n",
      "Epoch 67/300 - Train Loss: 0.0533, Val Loss: 0.0661\n",
      "Epoch 68/300 - Train Loss: 0.0495, Val Loss: 0.0706\n",
      "Epoch 69/300 - Train Loss: 0.0484, Val Loss: 0.0666\n",
      "Epoch 70/300 - Train Loss: 0.0508, Val Loss: 0.0727\n",
      "Epoch 71/300 - Train Loss: 0.0497, Val Loss: 0.0696\n",
      "Epoch 72/300 - Train Loss: 0.0525, Val Loss: 0.0687\n",
      "Epoch 73/300 - Train Loss: 0.0491, Val Loss: 0.0733\n",
      "Epoch 74/300 - Train Loss: 0.0478, Val Loss: 0.0706\n",
      "Epoch 75/300 - Train Loss: 0.0467, Val Loss: 0.0705\n",
      "Epoch 76/300 - Train Loss: 0.0481, Val Loss: 0.0730\n",
      "Epoch 77/300 - Train Loss: 0.0494, Val Loss: 0.0775\n",
      "Epoch 78/300 - Train Loss: 0.0503, Val Loss: 0.0764\n",
      "Epoch 79/300 - Train Loss: 0.0469, Val Loss: 0.0729\n",
      "Epoch 80/300 - Train Loss: 0.0464, Val Loss: 0.0696\n",
      "Epoch 81/300 - Train Loss: 0.0440, Val Loss: 0.0708\n",
      "Epoch 82/300 - Train Loss: 0.0492, Val Loss: 0.0733\n",
      "Epoch 83/300 - Train Loss: 0.0465, Val Loss: 0.0755\n",
      "Epoch 84/300 - Train Loss: 0.0455, Val Loss: 0.0697\n",
      "Epoch 85/300 - Train Loss: 0.0451, Val Loss: 0.0670\n",
      "Epoch 86/300 - Train Loss: 0.0455, Val Loss: 0.0710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:49:08,854] Trial 305 finished with value: 0.9617773543184903 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14733968621551358, 'learning_rate': 0.00010458139312160606, 'batch_size': 32, 'weight_decay': 2.2304915456044804e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300 - Train Loss: 0.0452, Val Loss: 0.0705\n",
      "Early stopping at epoch 87\n",
      "Macro F1 Score: 0.9618, Macro Precision: 0.9577, Macro Recall: 0.9660\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.90      0.93      0.92        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 307\n",
      "Training with F1=32, F2=32, D=2, dropout=0.13791927363061882, LR=8.154961888625659e-05, BS=32, WD=4.3365171828302443e-05\n",
      "Epoch 1/300 - Train Loss: 0.3061, Val Loss: 0.1572\n",
      "Epoch 2/300 - Train Loss: 0.1345, Val Loss: 0.0933\n",
      "Epoch 3/300 - Train Loss: 0.1081, Val Loss: 0.0858\n",
      "Epoch 4/300 - Train Loss: 0.0987, Val Loss: 0.0833\n",
      "Epoch 5/300 - Train Loss: 0.0948, Val Loss: 0.0800\n",
      "Epoch 6/300 - Train Loss: 0.0928, Val Loss: 0.0752\n",
      "Epoch 7/300 - Train Loss: 0.0900, Val Loss: 0.0704\n",
      "Epoch 8/300 - Train Loss: 0.0862, Val Loss: 0.0757\n",
      "Epoch 9/300 - Train Loss: 0.0874, Val Loss: 0.0690\n",
      "Epoch 10/300 - Train Loss: 0.0866, Val Loss: 0.0874\n",
      "Epoch 11/300 - Train Loss: 0.0843, Val Loss: 0.0727\n",
      "Epoch 12/300 - Train Loss: 0.0825, Val Loss: 0.0772\n",
      "Epoch 13/300 - Train Loss: 0.0824, Val Loss: 0.0765\n",
      "Epoch 14/300 - Train Loss: 0.0798, Val Loss: 0.0773\n",
      "Epoch 15/300 - Train Loss: 0.0804, Val Loss: 0.0695\n",
      "Epoch 16/300 - Train Loss: 0.0789, Val Loss: 0.0739\n",
      "Epoch 17/300 - Train Loss: 0.0764, Val Loss: 0.0718\n",
      "Epoch 18/300 - Train Loss: 0.0788, Val Loss: 0.0698\n",
      "Epoch 19/300 - Train Loss: 0.0758, Val Loss: 0.0748\n",
      "Epoch 20/300 - Train Loss: 0.0781, Val Loss: 0.0786\n",
      "Epoch 21/300 - Train Loss: 0.0751, Val Loss: 0.0696\n",
      "Epoch 22/300 - Train Loss: 0.0774, Val Loss: 0.0732\n",
      "Epoch 23/300 - Train Loss: 0.0752, Val Loss: 0.0787\n",
      "Epoch 24/300 - Train Loss: 0.0740, Val Loss: 0.0738\n",
      "Epoch 25/300 - Train Loss: 0.0734, Val Loss: 0.0737\n",
      "Epoch 26/300 - Train Loss: 0.0718, Val Loss: 0.0702\n",
      "Epoch 27/300 - Train Loss: 0.0753, Val Loss: 0.0756\n",
      "Epoch 28/300 - Train Loss: 0.0728, Val Loss: 0.0753\n",
      "Epoch 29/300 - Train Loss: 0.0722, Val Loss: 0.0763\n",
      "Epoch 30/300 - Train Loss: 0.0720, Val Loss: 0.0712\n",
      "Epoch 31/300 - Train Loss: 0.0717, Val Loss: 0.0921\n",
      "Epoch 32/300 - Train Loss: 0.0700, Val Loss: 0.0723\n",
      "Epoch 33/300 - Train Loss: 0.0704, Val Loss: 0.0753\n",
      "Epoch 34/300 - Train Loss: 0.0705, Val Loss: 0.0803\n",
      "Epoch 35/300 - Train Loss: 0.0691, Val Loss: 0.0709\n",
      "Epoch 36/300 - Train Loss: 0.0660, Val Loss: 0.0791\n",
      "Epoch 37/300 - Train Loss: 0.0668, Val Loss: 0.0767\n",
      "Epoch 38/300 - Train Loss: 0.0653, Val Loss: 0.0739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:50:55,448] Trial 306 finished with value: 0.9613379483278294 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.13791927363061882, 'learning_rate': 8.154961888625659e-05, 'batch_size': 32, 'weight_decay': 4.3365171828302443e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300 - Train Loss: 0.0665, Val Loss: 0.0713\n",
      "Early stopping at epoch 39\n",
      "Macro F1 Score: 0.9613, Macro Precision: 0.9537, Macro Recall: 0.9697\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 308\n",
      "Training with F1=32, F2=32, D=2, dropout=0.15348728356820204, LR=9.13200472105867e-05, BS=32, WD=3.23841660423455e-05\n",
      "Epoch 1/300 - Train Loss: 0.2823, Val Loss: 0.1170\n",
      "Epoch 2/300 - Train Loss: 0.1218, Val Loss: 0.0825\n",
      "Epoch 3/300 - Train Loss: 0.0991, Val Loss: 0.0827\n",
      "Epoch 4/300 - Train Loss: 0.0908, Val Loss: 0.0766\n",
      "Epoch 5/300 - Train Loss: 0.0921, Val Loss: 0.0738\n",
      "Epoch 6/300 - Train Loss: 0.0870, Val Loss: 0.0790\n",
      "Epoch 7/300 - Train Loss: 0.0853, Val Loss: 0.0742\n",
      "Epoch 8/300 - Train Loss: 0.0852, Val Loss: 0.0737\n",
      "Epoch 9/300 - Train Loss: 0.0831, Val Loss: 0.0699\n",
      "Epoch 10/300 - Train Loss: 0.0826, Val Loss: 0.0907\n",
      "Epoch 11/300 - Train Loss: 0.0814, Val Loss: 0.0906\n",
      "Epoch 12/300 - Train Loss: 0.0807, Val Loss: 0.0715\n",
      "Epoch 13/300 - Train Loss: 0.0779, Val Loss: 0.0748\n",
      "Epoch 14/300 - Train Loss: 0.0798, Val Loss: 0.0951\n",
      "Epoch 15/300 - Train Loss: 0.0758, Val Loss: 0.0709\n",
      "Epoch 16/300 - Train Loss: 0.0766, Val Loss: 0.0714\n",
      "Epoch 17/300 - Train Loss: 0.0771, Val Loss: 0.0702\n",
      "Epoch 18/300 - Train Loss: 0.0759, Val Loss: 0.0755\n",
      "Epoch 19/300 - Train Loss: 0.0745, Val Loss: 0.0708\n",
      "Epoch 20/300 - Train Loss: 0.0752, Val Loss: 0.0666\n",
      "Epoch 21/300 - Train Loss: 0.0728, Val Loss: 0.0738\n",
      "Epoch 22/300 - Train Loss: 0.0752, Val Loss: 0.0713\n",
      "Epoch 23/300 - Train Loss: 0.0722, Val Loss: 0.0717\n",
      "Epoch 24/300 - Train Loss: 0.0711, Val Loss: 0.0668\n",
      "Epoch 25/300 - Train Loss: 0.0704, Val Loss: 0.0653\n",
      "Epoch 26/300 - Train Loss: 0.0700, Val Loss: 0.0726\n",
      "Epoch 27/300 - Train Loss: 0.0717, Val Loss: 0.0734\n",
      "Epoch 28/300 - Train Loss: 0.0714, Val Loss: 0.0673\n",
      "Epoch 29/300 - Train Loss: 0.0684, Val Loss: 0.0692\n",
      "Epoch 30/300 - Train Loss: 0.0674, Val Loss: 0.0673\n",
      "Epoch 31/300 - Train Loss: 0.0664, Val Loss: 0.0720\n",
      "Epoch 32/300 - Train Loss: 0.0691, Val Loss: 0.0742\n",
      "Epoch 33/300 - Train Loss: 0.0682, Val Loss: 0.0833\n",
      "Epoch 34/300 - Train Loss: 0.0670, Val Loss: 0.0679\n",
      "Epoch 35/300 - Train Loss: 0.0674, Val Loss: 0.0676\n",
      "Epoch 36/300 - Train Loss: 0.0665, Val Loss: 0.0678\n",
      "Epoch 37/300 - Train Loss: 0.0658, Val Loss: 0.0667\n",
      "Epoch 38/300 - Train Loss: 0.0635, Val Loss: 0.0733\n",
      "Epoch 39/300 - Train Loss: 0.0646, Val Loss: 0.0729\n",
      "Epoch 40/300 - Train Loss: 0.0641, Val Loss: 0.0665\n",
      "Epoch 41/300 - Train Loss: 0.0617, Val Loss: 0.0681\n",
      "Epoch 42/300 - Train Loss: 0.0625, Val Loss: 0.0657\n",
      "Epoch 43/300 - Train Loss: 0.0613, Val Loss: 0.0683\n",
      "Epoch 44/300 - Train Loss: 0.0631, Val Loss: 0.0664\n",
      "Epoch 45/300 - Train Loss: 0.0596, Val Loss: 0.0680\n",
      "Epoch 46/300 - Train Loss: 0.0611, Val Loss: 0.0663\n",
      "Epoch 47/300 - Train Loss: 0.0638, Val Loss: 0.0734\n",
      "Epoch 48/300 - Train Loss: 0.0611, Val Loss: 0.0663\n",
      "Epoch 49/300 - Train Loss: 0.0604, Val Loss: 0.0685\n",
      "Epoch 50/300 - Train Loss: 0.0593, Val Loss: 0.0689\n",
      "Epoch 51/300 - Train Loss: 0.0581, Val Loss: 0.0685\n",
      "Epoch 52/300 - Train Loss: 0.0578, Val Loss: 0.0689\n",
      "Epoch 53/300 - Train Loss: 0.0576, Val Loss: 0.0732\n",
      "Epoch 54/300 - Train Loss: 0.0576, Val Loss: 0.0664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:53:25,835] Trial 307 finished with value: 0.9703392083758224 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.15348728356820204, 'learning_rate': 9.13200472105867e-05, 'batch_size': 32, 'weight_decay': 3.23841660423455e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300 - Train Loss: 0.0574, Val Loss: 0.0703\n",
      "Early stopping at epoch 55\n",
      "Macro F1 Score: 0.9703, Macro Precision: 0.9641, Macro Recall: 0.9770\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 309\n",
      "Training with F1=4, F2=32, D=2, dropout=0.11461341957884973, LR=9.675290423715654e-05, BS=32, WD=5.419856635652262e-05\n",
      "Epoch 1/300 - Train Loss: 0.3773, Val Loss: 0.1741\n",
      "Epoch 2/300 - Train Loss: 0.1719, Val Loss: 0.1271\n",
      "Epoch 3/300 - Train Loss: 0.1423, Val Loss: 0.1067\n",
      "Epoch 4/300 - Train Loss: 0.1330, Val Loss: 0.1026\n",
      "Epoch 5/300 - Train Loss: 0.1245, Val Loss: 0.0934\n",
      "Epoch 6/300 - Train Loss: 0.1190, Val Loss: 0.0912\n",
      "Epoch 7/300 - Train Loss: 0.1162, Val Loss: 0.0909\n",
      "Epoch 8/300 - Train Loss: 0.1086, Val Loss: 0.0841\n",
      "Epoch 9/300 - Train Loss: 0.1069, Val Loss: 0.0861\n",
      "Epoch 10/300 - Train Loss: 0.1027, Val Loss: 0.0850\n",
      "Epoch 11/300 - Train Loss: 0.1040, Val Loss: 0.0837\n",
      "Epoch 12/300 - Train Loss: 0.1023, Val Loss: 0.0821\n",
      "Epoch 13/300 - Train Loss: 0.1001, Val Loss: 0.0844\n",
      "Epoch 14/300 - Train Loss: 0.0982, Val Loss: 0.0898\n",
      "Epoch 15/300 - Train Loss: 0.0969, Val Loss: 0.0846\n",
      "Epoch 16/300 - Train Loss: 0.0988, Val Loss: 0.0833\n",
      "Epoch 17/300 - Train Loss: 0.0949, Val Loss: 0.0827\n",
      "Epoch 18/300 - Train Loss: 0.0954, Val Loss: 0.0787\n",
      "Epoch 19/300 - Train Loss: 0.0939, Val Loss: 0.0846\n",
      "Epoch 20/300 - Train Loss: 0.0927, Val Loss: 0.0804\n",
      "Epoch 21/300 - Train Loss: 0.0948, Val Loss: 0.0804\n",
      "Epoch 22/300 - Train Loss: 0.0904, Val Loss: 0.0821\n",
      "Epoch 23/300 - Train Loss: 0.0927, Val Loss: 0.0771\n",
      "Epoch 24/300 - Train Loss: 0.0910, Val Loss: 0.0813\n",
      "Epoch 25/300 - Train Loss: 0.0886, Val Loss: 0.0831\n",
      "Epoch 26/300 - Train Loss: 0.0898, Val Loss: 0.0855\n",
      "Epoch 27/300 - Train Loss: 0.0908, Val Loss: 0.0802\n",
      "Epoch 28/300 - Train Loss: 0.0883, Val Loss: 0.0772\n",
      "Epoch 29/300 - Train Loss: 0.0890, Val Loss: 0.0787\n",
      "Epoch 30/300 - Train Loss: 0.0883, Val Loss: 0.0773\n",
      "Epoch 31/300 - Train Loss: 0.0864, Val Loss: 0.0782\n",
      "Epoch 32/300 - Train Loss: 0.0858, Val Loss: 0.0788\n",
      "Epoch 33/300 - Train Loss: 0.0876, Val Loss: 0.0721\n",
      "Epoch 34/300 - Train Loss: 0.0866, Val Loss: 0.0781\n",
      "Epoch 35/300 - Train Loss: 0.0849, Val Loss: 0.0779\n",
      "Epoch 36/300 - Train Loss: 0.0857, Val Loss: 0.0755\n",
      "Epoch 37/300 - Train Loss: 0.0848, Val Loss: 0.0743\n",
      "Epoch 38/300 - Train Loss: 0.0846, Val Loss: 0.0806\n",
      "Epoch 39/300 - Train Loss: 0.0829, Val Loss: 0.0742\n",
      "Epoch 40/300 - Train Loss: 0.0824, Val Loss: 0.0759\n",
      "Epoch 41/300 - Train Loss: 0.0856, Val Loss: 0.0727\n",
      "Epoch 42/300 - Train Loss: 0.0851, Val Loss: 0.0786\n",
      "Epoch 43/300 - Train Loss: 0.0835, Val Loss: 0.0717\n",
      "Epoch 44/300 - Train Loss: 0.0835, Val Loss: 0.0812\n",
      "Epoch 45/300 - Train Loss: 0.0843, Val Loss: 0.0709\n",
      "Epoch 46/300 - Train Loss: 0.0819, Val Loss: 0.0736\n",
      "Epoch 47/300 - Train Loss: 0.0831, Val Loss: 0.0848\n",
      "Epoch 48/300 - Train Loss: 0.0840, Val Loss: 0.0737\n",
      "Epoch 49/300 - Train Loss: 0.0805, Val Loss: 0.0718\n",
      "Epoch 50/300 - Train Loss: 0.0804, Val Loss: 0.0759\n",
      "Epoch 51/300 - Train Loss: 0.0831, Val Loss: 0.0746\n",
      "Epoch 52/300 - Train Loss: 0.0832, Val Loss: 0.0773\n",
      "Epoch 53/300 - Train Loss: 0.0804, Val Loss: 0.0721\n",
      "Epoch 54/300 - Train Loss: 0.0816, Val Loss: 0.0745\n",
      "Epoch 55/300 - Train Loss: 0.0798, Val Loss: 0.0763\n",
      "Epoch 56/300 - Train Loss: 0.0809, Val Loss: 0.0747\n",
      "Epoch 57/300 - Train Loss: 0.0821, Val Loss: 0.0698\n",
      "Epoch 58/300 - Train Loss: 0.0782, Val Loss: 0.0697\n",
      "Epoch 59/300 - Train Loss: 0.0826, Val Loss: 0.0753\n",
      "Epoch 60/300 - Train Loss: 0.0782, Val Loss: 0.0712\n",
      "Epoch 61/300 - Train Loss: 0.0776, Val Loss: 0.0720\n",
      "Epoch 62/300 - Train Loss: 0.0801, Val Loss: 0.0754\n",
      "Epoch 63/300 - Train Loss: 0.0793, Val Loss: 0.0823\n",
      "Epoch 64/300 - Train Loss: 0.0773, Val Loss: 0.0728\n",
      "Epoch 65/300 - Train Loss: 0.0783, Val Loss: 0.0759\n",
      "Epoch 66/300 - Train Loss: 0.0774, Val Loss: 0.0781\n",
      "Epoch 67/300 - Train Loss: 0.0786, Val Loss: 0.0770\n",
      "Epoch 68/300 - Train Loss: 0.0785, Val Loss: 0.0752\n",
      "Epoch 69/300 - Train Loss: 0.0781, Val Loss: 0.0723\n",
      "Epoch 70/300 - Train Loss: 0.0760, Val Loss: 0.0745\n",
      "Epoch 71/300 - Train Loss: 0.0769, Val Loss: 0.0736\n",
      "Epoch 72/300 - Train Loss: 0.0771, Val Loss: 0.0721\n",
      "Epoch 73/300 - Train Loss: 0.0779, Val Loss: 0.0721\n",
      "Epoch 74/300 - Train Loss: 0.0773, Val Loss: 0.0782\n",
      "Epoch 75/300 - Train Loss: 0.0763, Val Loss: 0.0735\n",
      "Epoch 76/300 - Train Loss: 0.0770, Val Loss: 0.0707\n",
      "Epoch 77/300 - Train Loss: 0.0788, Val Loss: 0.0718\n",
      "Epoch 78/300 - Train Loss: 0.0764, Val Loss: 0.0767\n",
      "Epoch 79/300 - Train Loss: 0.0760, Val Loss: 0.0757\n",
      "Epoch 80/300 - Train Loss: 0.0756, Val Loss: 0.0704\n",
      "Epoch 81/300 - Train Loss: 0.0764, Val Loss: 0.0736\n",
      "Epoch 82/300 - Train Loss: 0.0748, Val Loss: 0.0786\n",
      "Epoch 83/300 - Train Loss: 0.0737, Val Loss: 0.0760\n",
      "Epoch 84/300 - Train Loss: 0.0748, Val Loss: 0.0797\n",
      "Epoch 85/300 - Train Loss: 0.0728, Val Loss: 0.0732\n",
      "Epoch 86/300 - Train Loss: 0.0741, Val Loss: 0.0715\n",
      "Epoch 87/300 - Train Loss: 0.0737, Val Loss: 0.0738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:56:10,926] Trial 308 finished with value: 0.9645812958242864 and parameters: {'F1': 4, 'F2': 32, 'D': 2, 'dropout': 0.11461341957884973, 'learning_rate': 9.675290423715654e-05, 'batch_size': 32, 'weight_decay': 5.419856635652262e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/300 - Train Loss: 0.0752, Val Loss: 0.0758\n",
      "Early stopping at epoch 88\n",
      "Macro F1 Score: 0.9646, Macro Precision: 0.9586, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 310\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1560550205223393, LR=0.00011453290721780344, BS=32, WD=7.480343731592311e-05\n",
      "Epoch 1/300 - Train Loss: 0.2612, Val Loss: 0.1262\n",
      "Epoch 2/300 - Train Loss: 0.1148, Val Loss: 0.1124\n",
      "Epoch 3/300 - Train Loss: 0.1031, Val Loss: 0.0775\n",
      "Epoch 4/300 - Train Loss: 0.0965, Val Loss: 0.0830\n",
      "Epoch 5/300 - Train Loss: 0.0926, Val Loss: 0.0815\n",
      "Epoch 6/300 - Train Loss: 0.0878, Val Loss: 0.0746\n",
      "Epoch 7/300 - Train Loss: 0.0878, Val Loss: 0.0800\n",
      "Epoch 8/300 - Train Loss: 0.0830, Val Loss: 0.0728\n",
      "Epoch 9/300 - Train Loss: 0.0826, Val Loss: 0.0728\n",
      "Epoch 10/300 - Train Loss: 0.0833, Val Loss: 0.0739\n",
      "Epoch 11/300 - Train Loss: 0.0813, Val Loss: 0.0765\n",
      "Epoch 12/300 - Train Loss: 0.0806, Val Loss: 0.0791\n",
      "Epoch 13/300 - Train Loss: 0.0792, Val Loss: 0.0712\n",
      "Epoch 14/300 - Train Loss: 0.0775, Val Loss: 0.0679\n",
      "Epoch 15/300 - Train Loss: 0.0766, Val Loss: 0.0722\n",
      "Epoch 16/300 - Train Loss: 0.0793, Val Loss: 0.0770\n",
      "Epoch 17/300 - Train Loss: 0.0749, Val Loss: 0.0663\n",
      "Epoch 18/300 - Train Loss: 0.0769, Val Loss: 0.0727\n",
      "Epoch 19/300 - Train Loss: 0.0741, Val Loss: 0.0657\n",
      "Epoch 20/300 - Train Loss: 0.0727, Val Loss: 0.0765\n",
      "Epoch 21/300 - Train Loss: 0.0713, Val Loss: 0.0775\n",
      "Epoch 22/300 - Train Loss: 0.0722, Val Loss: 0.0736\n",
      "Epoch 23/300 - Train Loss: 0.0700, Val Loss: 0.0729\n",
      "Epoch 24/300 - Train Loss: 0.0712, Val Loss: 0.0717\n",
      "Epoch 25/300 - Train Loss: 0.0685, Val Loss: 0.0712\n",
      "Epoch 26/300 - Train Loss: 0.0708, Val Loss: 0.0709\n",
      "Epoch 27/300 - Train Loss: 0.0704, Val Loss: 0.0730\n",
      "Epoch 28/300 - Train Loss: 0.0669, Val Loss: 0.0735\n",
      "Epoch 29/300 - Train Loss: 0.0666, Val Loss: 0.0733\n",
      "Epoch 30/300 - Train Loss: 0.0669, Val Loss: 0.0706\n",
      "Epoch 31/300 - Train Loss: 0.0673, Val Loss: 0.0703\n",
      "Epoch 32/300 - Train Loss: 0.0656, Val Loss: 0.0742\n",
      "Epoch 33/300 - Train Loss: 0.0648, Val Loss: 0.0724\n",
      "Epoch 34/300 - Train Loss: 0.0650, Val Loss: 0.0706\n",
      "Epoch 35/300 - Train Loss: 0.0650, Val Loss: 0.0693\n",
      "Epoch 36/300 - Train Loss: 0.0662, Val Loss: 0.0705\n",
      "Epoch 37/300 - Train Loss: 0.0640, Val Loss: 0.0762\n",
      "Epoch 38/300 - Train Loss: 0.0640, Val Loss: 0.0717\n",
      "Epoch 39/300 - Train Loss: 0.0631, Val Loss: 0.0684\n",
      "Epoch 40/300 - Train Loss: 0.0613, Val Loss: 0.0687\n",
      "Epoch 41/300 - Train Loss: 0.0610, Val Loss: 0.0683\n",
      "Epoch 42/300 - Train Loss: 0.0611, Val Loss: 0.0754\n",
      "Epoch 43/300 - Train Loss: 0.0595, Val Loss: 0.0712\n",
      "Epoch 44/300 - Train Loss: 0.0590, Val Loss: 0.0670\n",
      "Epoch 45/300 - Train Loss: 0.0589, Val Loss: 0.0700\n",
      "Epoch 46/300 - Train Loss: 0.0569, Val Loss: 0.0746\n",
      "Epoch 47/300 - Train Loss: 0.0583, Val Loss: 0.0701\n",
      "Epoch 48/300 - Train Loss: 0.0586, Val Loss: 0.0733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 19:58:25,024] Trial 309 finished with value: 0.967363834924003 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1560550205223393, 'learning_rate': 0.00011453290721780344, 'batch_size': 32, 'weight_decay': 7.480343731592311e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300 - Train Loss: 0.0593, Val Loss: 0.0689\n",
      "Early stopping at epoch 49\n",
      "Macro F1 Score: 0.9674, Macro Precision: 0.9680, Macro Recall: 0.9668\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 311\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1249796648020785, LR=4.597807662593724e-05, BS=32, WD=4.660748611793595e-05\n",
      "Epoch 1/300 - Train Loss: 0.3774, Val Loss: 0.1623\n",
      "Epoch 2/300 - Train Loss: 0.1508, Val Loss: 0.1055\n",
      "Epoch 3/300 - Train Loss: 0.1113, Val Loss: 0.0818\n",
      "Epoch 4/300 - Train Loss: 0.1019, Val Loss: 0.0861\n",
      "Epoch 5/300 - Train Loss: 0.0981, Val Loss: 0.0817\n",
      "Epoch 6/300 - Train Loss: 0.0964, Val Loss: 0.0820\n",
      "Epoch 7/300 - Train Loss: 0.0898, Val Loss: 0.0782\n",
      "Epoch 8/300 - Train Loss: 0.0905, Val Loss: 0.0797\n",
      "Epoch 9/300 - Train Loss: 0.0889, Val Loss: 0.0762\n",
      "Epoch 10/300 - Train Loss: 0.0867, Val Loss: 0.0759\n",
      "Epoch 11/300 - Train Loss: 0.0856, Val Loss: 0.0752\n",
      "Epoch 12/300 - Train Loss: 0.0876, Val Loss: 0.0789\n",
      "Epoch 13/300 - Train Loss: 0.0843, Val Loss: 0.0740\n",
      "Epoch 14/300 - Train Loss: 0.0817, Val Loss: 0.0734\n",
      "Epoch 15/300 - Train Loss: 0.0835, Val Loss: 0.0780\n",
      "Epoch 16/300 - Train Loss: 0.0823, Val Loss: 0.0735\n",
      "Epoch 17/300 - Train Loss: 0.0794, Val Loss: 0.0760\n",
      "Epoch 18/300 - Train Loss: 0.0796, Val Loss: 0.0748\n",
      "Epoch 19/300 - Train Loss: 0.0806, Val Loss: 0.0727\n",
      "Epoch 20/300 - Train Loss: 0.0794, Val Loss: 0.0695\n",
      "Epoch 21/300 - Train Loss: 0.0777, Val Loss: 0.0747\n",
      "Epoch 22/300 - Train Loss: 0.0754, Val Loss: 0.0707\n",
      "Epoch 23/300 - Train Loss: 0.0772, Val Loss: 0.0743\n",
      "Epoch 24/300 - Train Loss: 0.0746, Val Loss: 0.0729\n",
      "Epoch 25/300 - Train Loss: 0.0763, Val Loss: 0.0755\n",
      "Epoch 26/300 - Train Loss: 0.0759, Val Loss: 0.0704\n",
      "Epoch 27/300 - Train Loss: 0.0750, Val Loss: 0.0680\n",
      "Epoch 28/300 - Train Loss: 0.0748, Val Loss: 0.0718\n",
      "Epoch 29/300 - Train Loss: 0.0744, Val Loss: 0.0725\n",
      "Epoch 30/300 - Train Loss: 0.0746, Val Loss: 0.0724\n",
      "Epoch 31/300 - Train Loss: 0.0724, Val Loss: 0.0727\n",
      "Epoch 32/300 - Train Loss: 0.0736, Val Loss: 0.0764\n",
      "Epoch 33/300 - Train Loss: 0.0740, Val Loss: 0.0761\n",
      "Epoch 34/300 - Train Loss: 0.0714, Val Loss: 0.0701\n",
      "Epoch 35/300 - Train Loss: 0.0699, Val Loss: 0.0701\n",
      "Epoch 36/300 - Train Loss: 0.0689, Val Loss: 0.0700\n",
      "Epoch 37/300 - Train Loss: 0.0738, Val Loss: 0.0764\n",
      "Epoch 38/300 - Train Loss: 0.0687, Val Loss: 0.0686\n",
      "Epoch 39/300 - Train Loss: 0.0701, Val Loss: 0.0687\n",
      "Epoch 40/300 - Train Loss: 0.0689, Val Loss: 0.0698\n",
      "Epoch 41/300 - Train Loss: 0.0686, Val Loss: 0.0753\n",
      "Epoch 42/300 - Train Loss: 0.0682, Val Loss: 0.0698\n",
      "Epoch 43/300 - Train Loss: 0.0677, Val Loss: 0.0703\n",
      "Epoch 44/300 - Train Loss: 0.0698, Val Loss: 0.0667\n",
      "Epoch 45/300 - Train Loss: 0.0670, Val Loss: 0.0699\n",
      "Epoch 46/300 - Train Loss: 0.0659, Val Loss: 0.0658\n",
      "Epoch 47/300 - Train Loss: 0.0675, Val Loss: 0.0688\n",
      "Epoch 48/300 - Train Loss: 0.0660, Val Loss: 0.0689\n",
      "Epoch 49/300 - Train Loss: 0.0675, Val Loss: 0.0694\n",
      "Epoch 50/300 - Train Loss: 0.0680, Val Loss: 0.0697\n",
      "Epoch 51/300 - Train Loss: 0.0671, Val Loss: 0.0675\n",
      "Epoch 52/300 - Train Loss: 0.0655, Val Loss: 0.0716\n",
      "Epoch 53/300 - Train Loss: 0.0646, Val Loss: 0.0720\n",
      "Epoch 54/300 - Train Loss: 0.0649, Val Loss: 0.0679\n",
      "Epoch 55/300 - Train Loss: 0.0627, Val Loss: 0.0687\n",
      "Epoch 56/300 - Train Loss: 0.0629, Val Loss: 0.0739\n",
      "Epoch 57/300 - Train Loss: 0.0628, Val Loss: 0.0689\n",
      "Epoch 58/300 - Train Loss: 0.0624, Val Loss: 0.0695\n",
      "Epoch 59/300 - Train Loss: 0.0629, Val Loss: 0.0693\n",
      "Epoch 60/300 - Train Loss: 0.0628, Val Loss: 0.0702\n",
      "Epoch 61/300 - Train Loss: 0.0631, Val Loss: 0.0728\n",
      "Epoch 62/300 - Train Loss: 0.0610, Val Loss: 0.0767\n",
      "Epoch 63/300 - Train Loss: 0.0622, Val Loss: 0.0708\n",
      "Epoch 64/300 - Train Loss: 0.0613, Val Loss: 0.0746\n",
      "Epoch 65/300 - Train Loss: 0.0627, Val Loss: 0.0724\n",
      "Epoch 66/300 - Train Loss: 0.0608, Val Loss: 0.0680\n",
      "Epoch 67/300 - Train Loss: 0.0605, Val Loss: 0.0694\n",
      "Epoch 68/300 - Train Loss: 0.0661, Val Loss: 0.0683\n",
      "Epoch 69/300 - Train Loss: 0.0597, Val Loss: 0.0760\n",
      "Epoch 70/300 - Train Loss: 0.0604, Val Loss: 0.0697\n",
      "Epoch 71/300 - Train Loss: 0.0573, Val Loss: 0.0689\n",
      "Epoch 72/300 - Train Loss: 0.0605, Val Loss: 0.0755\n",
      "Epoch 73/300 - Train Loss: 0.0584, Val Loss: 0.0741\n",
      "Epoch 74/300 - Train Loss: 0.0592, Val Loss: 0.0774\n",
      "Epoch 75/300 - Train Loss: 0.0588, Val Loss: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:01:52,780] Trial 310 finished with value: 0.9698277311300566 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1249796648020785, 'learning_rate': 4.597807662593724e-05, 'batch_size': 32, 'weight_decay': 4.660748611793595e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300 - Train Loss: 0.0582, Val Loss: 0.0734\n",
      "Early stopping at epoch 76\n",
      "Macro F1 Score: 0.9698, Macro Precision: 0.9643, Macro Recall: 0.9759\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 312\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1357451184479775, LR=8.093651373016335e-05, BS=32, WD=6.060843538192978e-05\n",
      "Epoch 1/300 - Train Loss: 0.2970, Val Loss: 0.1344\n",
      "Epoch 2/300 - Train Loss: 0.1285, Val Loss: 0.0860\n",
      "Epoch 3/300 - Train Loss: 0.1053, Val Loss: 0.0804\n",
      "Epoch 4/300 - Train Loss: 0.0963, Val Loss: 0.0848\n",
      "Epoch 5/300 - Train Loss: 0.0936, Val Loss: 0.0783\n",
      "Epoch 6/300 - Train Loss: 0.0885, Val Loss: 0.0970\n",
      "Epoch 7/300 - Train Loss: 0.0893, Val Loss: 0.0806\n",
      "Epoch 8/300 - Train Loss: 0.0868, Val Loss: 0.0720\n",
      "Epoch 9/300 - Train Loss: 0.0844, Val Loss: 0.0697\n",
      "Epoch 10/300 - Train Loss: 0.0826, Val Loss: 0.0868\n",
      "Epoch 11/300 - Train Loss: 0.0809, Val Loss: 0.0708\n",
      "Epoch 12/300 - Train Loss: 0.0805, Val Loss: 0.0785\n",
      "Epoch 13/300 - Train Loss: 0.0802, Val Loss: 0.0770\n",
      "Epoch 14/300 - Train Loss: 0.0813, Val Loss: 0.0676\n",
      "Epoch 15/300 - Train Loss: 0.0793, Val Loss: 0.0742\n",
      "Epoch 16/300 - Train Loss: 0.0775, Val Loss: 0.0739\n",
      "Epoch 17/300 - Train Loss: 0.0745, Val Loss: 0.0693\n",
      "Epoch 18/300 - Train Loss: 0.0788, Val Loss: 0.0774\n",
      "Epoch 19/300 - Train Loss: 0.0756, Val Loss: 0.0732\n",
      "Epoch 20/300 - Train Loss: 0.0731, Val Loss: 0.0702\n",
      "Epoch 21/300 - Train Loss: 0.0746, Val Loss: 0.0743\n",
      "Epoch 22/300 - Train Loss: 0.0738, Val Loss: 0.0754\n",
      "Epoch 23/300 - Train Loss: 0.0723, Val Loss: 0.0719\n",
      "Epoch 24/300 - Train Loss: 0.0733, Val Loss: 0.0681\n",
      "Epoch 25/300 - Train Loss: 0.0696, Val Loss: 0.0733\n",
      "Epoch 26/300 - Train Loss: 0.0718, Val Loss: 0.0698\n",
      "Epoch 27/300 - Train Loss: 0.0706, Val Loss: 0.0687\n",
      "Epoch 28/300 - Train Loss: 0.0695, Val Loss: 0.0672\n",
      "Epoch 29/300 - Train Loss: 0.0695, Val Loss: 0.0675\n",
      "Epoch 30/300 - Train Loss: 0.0672, Val Loss: 0.0701\n",
      "Epoch 31/300 - Train Loss: 0.0680, Val Loss: 0.0695\n",
      "Epoch 32/300 - Train Loss: 0.0678, Val Loss: 0.0731\n",
      "Epoch 33/300 - Train Loss: 0.0665, Val Loss: 0.0682\n",
      "Epoch 34/300 - Train Loss: 0.0673, Val Loss: 0.0721\n",
      "Epoch 35/300 - Train Loss: 0.0673, Val Loss: 0.0686\n",
      "Epoch 36/300 - Train Loss: 0.0643, Val Loss: 0.0745\n",
      "Epoch 37/300 - Train Loss: 0.0663, Val Loss: 0.0687\n",
      "Epoch 38/300 - Train Loss: 0.0684, Val Loss: 0.0695\n",
      "Epoch 39/300 - Train Loss: 0.0669, Val Loss: 0.0685\n",
      "Epoch 40/300 - Train Loss: 0.0643, Val Loss: 0.0699\n",
      "Epoch 41/300 - Train Loss: 0.0631, Val Loss: 0.0710\n",
      "Epoch 42/300 - Train Loss: 0.0628, Val Loss: 0.0710\n",
      "Epoch 43/300 - Train Loss: 0.0646, Val Loss: 0.0681\n",
      "Epoch 44/300 - Train Loss: 0.0633, Val Loss: 0.0706\n",
      "Epoch 45/300 - Train Loss: 0.0625, Val Loss: 0.0698\n",
      "Epoch 46/300 - Train Loss: 0.0606, Val Loss: 0.0733\n",
      "Epoch 47/300 - Train Loss: 0.0596, Val Loss: 0.0701\n",
      "Epoch 48/300 - Train Loss: 0.0599, Val Loss: 0.0679\n",
      "Epoch 49/300 - Train Loss: 0.0615, Val Loss: 0.0642\n",
      "Epoch 50/300 - Train Loss: 0.0605, Val Loss: 0.0708\n",
      "Epoch 51/300 - Train Loss: 0.0615, Val Loss: 0.0751\n",
      "Epoch 52/300 - Train Loss: 0.0590, Val Loss: 0.0693\n",
      "Epoch 53/300 - Train Loss: 0.0593, Val Loss: 0.0707\n",
      "Epoch 54/300 - Train Loss: 0.0578, Val Loss: 0.0756\n",
      "Epoch 55/300 - Train Loss: 0.0585, Val Loss: 0.0707\n",
      "Epoch 56/300 - Train Loss: 0.0606, Val Loss: 0.0699\n",
      "Epoch 57/300 - Train Loss: 0.0580, Val Loss: 0.0737\n",
      "Epoch 58/300 - Train Loss: 0.0565, Val Loss: 0.0746\n",
      "Epoch 59/300 - Train Loss: 0.0574, Val Loss: 0.0743\n",
      "Epoch 60/300 - Train Loss: 0.0576, Val Loss: 0.0749\n",
      "Epoch 61/300 - Train Loss: 0.0544, Val Loss: 0.0702\n",
      "Epoch 62/300 - Train Loss: 0.0558, Val Loss: 0.0683\n",
      "Epoch 63/300 - Train Loss: 0.0553, Val Loss: 0.0690\n",
      "Epoch 64/300 - Train Loss: 0.0571, Val Loss: 0.0729\n",
      "Epoch 65/300 - Train Loss: 0.0554, Val Loss: 0.0682\n",
      "Epoch 66/300 - Train Loss: 0.0533, Val Loss: 0.0709\n",
      "Epoch 67/300 - Train Loss: 0.0544, Val Loss: 0.0689\n",
      "Epoch 68/300 - Train Loss: 0.0543, Val Loss: 0.0751\n",
      "Epoch 69/300 - Train Loss: 0.0520, Val Loss: 0.0701\n",
      "Epoch 70/300 - Train Loss: 0.0542, Val Loss: 0.0721\n",
      "Epoch 71/300 - Train Loss: 0.0538, Val Loss: 0.0724\n",
      "Epoch 72/300 - Train Loss: 0.0525, Val Loss: 0.0741\n",
      "Epoch 73/300 - Train Loss: 0.0527, Val Loss: 0.0716\n",
      "Epoch 74/300 - Train Loss: 0.0516, Val Loss: 0.0719\n",
      "Epoch 75/300 - Train Loss: 0.0508, Val Loss: 0.0736\n",
      "Epoch 76/300 - Train Loss: 0.0551, Val Loss: 0.0721\n",
      "Epoch 77/300 - Train Loss: 0.0529, Val Loss: 0.0696\n",
      "Epoch 78/300 - Train Loss: 0.0516, Val Loss: 0.0693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:05:28,856] Trial 311 finished with value: 0.9751771599847131 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1357451184479775, 'learning_rate': 8.093651373016335e-05, 'batch_size': 32, 'weight_decay': 6.060843538192978e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/300 - Train Loss: 0.0541, Val Loss: 0.0816\n",
      "Early stopping at epoch 79\n",
      "Macro F1 Score: 0.9752, Macro Precision: 0.9689, Macro Recall: 0.9819\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.98      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 313\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1793295240509513, LR=7.633790123734709e-05, BS=32, WD=5.9355264217928444e-05\n",
      "Epoch 1/300 - Train Loss: 0.3003, Val Loss: 0.1475\n",
      "Epoch 2/300 - Train Loss: 0.1384, Val Loss: 0.0941\n",
      "Epoch 3/300 - Train Loss: 0.1126, Val Loss: 0.0903\n",
      "Epoch 4/300 - Train Loss: 0.1017, Val Loss: 0.0856\n",
      "Epoch 5/300 - Train Loss: 0.0959, Val Loss: 0.0824\n",
      "Epoch 6/300 - Train Loss: 0.0945, Val Loss: 0.0855\n",
      "Epoch 7/300 - Train Loss: 0.0920, Val Loss: 0.0778\n",
      "Epoch 8/300 - Train Loss: 0.0915, Val Loss: 0.0750\n",
      "Epoch 9/300 - Train Loss: 0.0882, Val Loss: 0.0727\n",
      "Epoch 10/300 - Train Loss: 0.0839, Val Loss: 0.0768\n",
      "Epoch 11/300 - Train Loss: 0.0844, Val Loss: 0.0735\n",
      "Epoch 12/300 - Train Loss: 0.0823, Val Loss: 0.0834\n",
      "Epoch 13/300 - Train Loss: 0.0830, Val Loss: 0.0719\n",
      "Epoch 14/300 - Train Loss: 0.0818, Val Loss: 0.0768\n",
      "Epoch 15/300 - Train Loss: 0.0796, Val Loss: 0.0781\n",
      "Epoch 16/300 - Train Loss: 0.0798, Val Loss: 0.0766\n",
      "Epoch 17/300 - Train Loss: 0.0790, Val Loss: 0.0795\n",
      "Epoch 18/300 - Train Loss: 0.0786, Val Loss: 0.0783\n",
      "Epoch 19/300 - Train Loss: 0.0760, Val Loss: 0.0721\n",
      "Epoch 20/300 - Train Loss: 0.0767, Val Loss: 0.0723\n",
      "Epoch 21/300 - Train Loss: 0.0779, Val Loss: 0.0752\n",
      "Epoch 22/300 - Train Loss: 0.0751, Val Loss: 0.0718\n",
      "Epoch 23/300 - Train Loss: 0.0758, Val Loss: 0.0682\n",
      "Epoch 24/300 - Train Loss: 0.0749, Val Loss: 0.0677\n",
      "Epoch 25/300 - Train Loss: 0.0751, Val Loss: 0.0730\n",
      "Epoch 26/300 - Train Loss: 0.0734, Val Loss: 0.0679\n",
      "Epoch 27/300 - Train Loss: 0.0728, Val Loss: 0.0707\n",
      "Epoch 28/300 - Train Loss: 0.0746, Val Loss: 0.0669\n",
      "Epoch 29/300 - Train Loss: 0.0734, Val Loss: 0.0710\n",
      "Epoch 30/300 - Train Loss: 0.0711, Val Loss: 0.0699\n",
      "Epoch 31/300 - Train Loss: 0.0700, Val Loss: 0.0705\n",
      "Epoch 32/300 - Train Loss: 0.0700, Val Loss: 0.0696\n",
      "Epoch 33/300 - Train Loss: 0.0712, Val Loss: 0.0716\n",
      "Epoch 34/300 - Train Loss: 0.0704, Val Loss: 0.0667\n",
      "Epoch 35/300 - Train Loss: 0.0713, Val Loss: 0.0698\n",
      "Epoch 36/300 - Train Loss: 0.0697, Val Loss: 0.0655\n",
      "Epoch 37/300 - Train Loss: 0.0686, Val Loss: 0.0673\n",
      "Epoch 38/300 - Train Loss: 0.0678, Val Loss: 0.0681\n",
      "Epoch 39/300 - Train Loss: 0.0697, Val Loss: 0.0712\n",
      "Epoch 40/300 - Train Loss: 0.0688, Val Loss: 0.0689\n",
      "Epoch 41/300 - Train Loss: 0.0681, Val Loss: 0.0657\n",
      "Epoch 42/300 - Train Loss: 0.0666, Val Loss: 0.0690\n",
      "Epoch 43/300 - Train Loss: 0.0662, Val Loss: 0.0709\n",
      "Epoch 44/300 - Train Loss: 0.0681, Val Loss: 0.0709\n",
      "Epoch 45/300 - Train Loss: 0.0667, Val Loss: 0.0702\n",
      "Epoch 46/300 - Train Loss: 0.0658, Val Loss: 0.0679\n",
      "Epoch 47/300 - Train Loss: 0.0657, Val Loss: 0.0666\n",
      "Epoch 48/300 - Train Loss: 0.0646, Val Loss: 0.0674\n",
      "Epoch 49/300 - Train Loss: 0.0656, Val Loss: 0.0675\n",
      "Epoch 50/300 - Train Loss: 0.0640, Val Loss: 0.0729\n",
      "Epoch 51/300 - Train Loss: 0.0643, Val Loss: 0.0691\n",
      "Epoch 52/300 - Train Loss: 0.0662, Val Loss: 0.0672\n",
      "Epoch 53/300 - Train Loss: 0.0624, Val Loss: 0.0706\n",
      "Epoch 54/300 - Train Loss: 0.0641, Val Loss: 0.0683\n",
      "Epoch 55/300 - Train Loss: 0.0636, Val Loss: 0.0678\n",
      "Epoch 56/300 - Train Loss: 0.0643, Val Loss: 0.0677\n",
      "Epoch 57/300 - Train Loss: 0.0613, Val Loss: 0.0642\n",
      "Epoch 58/300 - Train Loss: 0.0617, Val Loss: 0.0691\n",
      "Epoch 59/300 - Train Loss: 0.0618, Val Loss: 0.0681\n",
      "Epoch 60/300 - Train Loss: 0.0610, Val Loss: 0.0749\n",
      "Epoch 61/300 - Train Loss: 0.0616, Val Loss: 0.0669\n",
      "Epoch 62/300 - Train Loss: 0.0618, Val Loss: 0.0689\n",
      "Epoch 63/300 - Train Loss: 0.0606, Val Loss: 0.0679\n",
      "Epoch 64/300 - Train Loss: 0.0591, Val Loss: 0.0678\n",
      "Epoch 65/300 - Train Loss: 0.0601, Val Loss: 0.0663\n",
      "Epoch 66/300 - Train Loss: 0.0584, Val Loss: 0.0686\n",
      "Epoch 67/300 - Train Loss: 0.0588, Val Loss: 0.0719\n",
      "Epoch 68/300 - Train Loss: 0.0584, Val Loss: 0.0677\n",
      "Epoch 69/300 - Train Loss: 0.0580, Val Loss: 0.0711\n",
      "Epoch 70/300 - Train Loss: 0.0585, Val Loss: 0.0711\n",
      "Epoch 71/300 - Train Loss: 0.0581, Val Loss: 0.0745\n",
      "Epoch 72/300 - Train Loss: 0.0566, Val Loss: 0.0675\n",
      "Epoch 73/300 - Train Loss: 0.0564, Val Loss: 0.0705\n",
      "Epoch 74/300 - Train Loss: 0.0563, Val Loss: 0.0657\n",
      "Epoch 75/300 - Train Loss: 0.0547, Val Loss: 0.0707\n",
      "Epoch 76/300 - Train Loss: 0.0549, Val Loss: 0.0665\n",
      "Epoch 77/300 - Train Loss: 0.0557, Val Loss: 0.0731\n",
      "Epoch 78/300 - Train Loss: 0.0579, Val Loss: 0.0640\n",
      "Epoch 79/300 - Train Loss: 0.0556, Val Loss: 0.0693\n",
      "Epoch 80/300 - Train Loss: 0.0569, Val Loss: 0.0679\n",
      "Epoch 81/300 - Train Loss: 0.0558, Val Loss: 0.0703\n",
      "Epoch 82/300 - Train Loss: 0.0523, Val Loss: 0.0705\n",
      "Epoch 83/300 - Train Loss: 0.0565, Val Loss: 0.0697\n",
      "Epoch 84/300 - Train Loss: 0.0528, Val Loss: 0.0685\n",
      "Epoch 85/300 - Train Loss: 0.0522, Val Loss: 0.0685\n",
      "Epoch 86/300 - Train Loss: 0.0560, Val Loss: 0.0693\n",
      "Epoch 87/300 - Train Loss: 0.0535, Val Loss: 0.0735\n",
      "Epoch 88/300 - Train Loss: 0.0534, Val Loss: 0.0763\n",
      "Epoch 89/300 - Train Loss: 0.0531, Val Loss: 0.0689\n",
      "Epoch 90/300 - Train Loss: 0.0524, Val Loss: 0.0720\n",
      "Epoch 91/300 - Train Loss: 0.0514, Val Loss: 0.0695\n",
      "Epoch 92/300 - Train Loss: 0.0554, Val Loss: 0.0724\n",
      "Epoch 93/300 - Train Loss: 0.0529, Val Loss: 0.0715\n",
      "Epoch 94/300 - Train Loss: 0.0521, Val Loss: 0.0708\n",
      "Epoch 95/300 - Train Loss: 0.0509, Val Loss: 0.0671\n",
      "Epoch 96/300 - Train Loss: 0.0529, Val Loss: 0.0683\n",
      "Epoch 97/300 - Train Loss: 0.0503, Val Loss: 0.0709\n",
      "Epoch 98/300 - Train Loss: 0.0532, Val Loss: 0.0718\n",
      "Epoch 99/300 - Train Loss: 0.0513, Val Loss: 0.0719\n",
      "Epoch 100/300 - Train Loss: 0.0489, Val Loss: 0.0693\n",
      "Epoch 101/300 - Train Loss: 0.0512, Val Loss: 0.0688\n",
      "Epoch 102/300 - Train Loss: 0.0516, Val Loss: 0.0725\n",
      "Epoch 103/300 - Train Loss: 0.0521, Val Loss: 0.0742\n",
      "Epoch 104/300 - Train Loss: 0.0498, Val Loss: 0.0703\n",
      "Epoch 105/300 - Train Loss: 0.0520, Val Loss: 0.0668\n",
      "Epoch 106/300 - Train Loss: 0.0500, Val Loss: 0.0697\n",
      "Epoch 107/300 - Train Loss: 0.0508, Val Loss: 0.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:10:24,326] Trial 312 finished with value: 0.9689394889598298 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1793295240509513, 'learning_rate': 7.633790123734709e-05, 'batch_size': 32, 'weight_decay': 5.9355264217928444e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 108/300 - Train Loss: 0.0518, Val Loss: 0.0698\n",
      "Early stopping at epoch 108\n",
      "Macro F1 Score: 0.9689, Macro Precision: 0.9567, Macro Recall: 0.9826\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 314\n",
      "Training with F1=32, F2=32, D=2, dropout=0.10204797319085446, LR=0.00010426369522068034, BS=32, WD=5.2717924359193365e-05\n",
      "Epoch 1/300 - Train Loss: 0.2475, Val Loss: 0.1006\n",
      "Epoch 2/300 - Train Loss: 0.1092, Val Loss: 0.0825\n",
      "Epoch 3/300 - Train Loss: 0.0976, Val Loss: 0.0750\n",
      "Epoch 4/300 - Train Loss: 0.0932, Val Loss: 0.0696\n",
      "Epoch 5/300 - Train Loss: 0.0874, Val Loss: 0.0719\n",
      "Epoch 6/300 - Train Loss: 0.0883, Val Loss: 0.0807\n",
      "Epoch 7/300 - Train Loss: 0.0816, Val Loss: 0.0871\n",
      "Epoch 8/300 - Train Loss: 0.0833, Val Loss: 0.0713\n",
      "Epoch 9/300 - Train Loss: 0.0814, Val Loss: 0.0733\n",
      "Epoch 10/300 - Train Loss: 0.0802, Val Loss: 0.0728\n",
      "Epoch 11/300 - Train Loss: 0.0772, Val Loss: 0.0728\n",
      "Epoch 12/300 - Train Loss: 0.0772, Val Loss: 0.0679\n",
      "Epoch 13/300 - Train Loss: 0.0769, Val Loss: 0.0693\n",
      "Epoch 14/300 - Train Loss: 0.0785, Val Loss: 0.0733\n",
      "Epoch 15/300 - Train Loss: 0.0764, Val Loss: 0.0728\n",
      "Epoch 16/300 - Train Loss: 0.0727, Val Loss: 0.0786\n",
      "Epoch 17/300 - Train Loss: 0.0730, Val Loss: 0.0696\n",
      "Epoch 18/300 - Train Loss: 0.0735, Val Loss: 0.0920\n",
      "Epoch 19/300 - Train Loss: 0.0749, Val Loss: 0.0828\n",
      "Epoch 20/300 - Train Loss: 0.0720, Val Loss: 0.0734\n",
      "Epoch 21/300 - Train Loss: 0.0718, Val Loss: 0.0730\n",
      "Epoch 22/300 - Train Loss: 0.0709, Val Loss: 0.0659\n",
      "Epoch 23/300 - Train Loss: 0.0698, Val Loss: 0.0634\n",
      "Epoch 24/300 - Train Loss: 0.0698, Val Loss: 0.0661\n",
      "Epoch 25/300 - Train Loss: 0.0679, Val Loss: 0.0695\n",
      "Epoch 26/300 - Train Loss: 0.0674, Val Loss: 0.0703\n",
      "Epoch 27/300 - Train Loss: 0.0656, Val Loss: 0.0645\n",
      "Epoch 28/300 - Train Loss: 0.0673, Val Loss: 0.0716\n",
      "Epoch 29/300 - Train Loss: 0.0670, Val Loss: 0.0690\n",
      "Epoch 30/300 - Train Loss: 0.0655, Val Loss: 0.0660\n",
      "Epoch 31/300 - Train Loss: 0.0651, Val Loss: 0.0692\n",
      "Epoch 32/300 - Train Loss: 0.0634, Val Loss: 0.0674\n",
      "Epoch 33/300 - Train Loss: 0.0638, Val Loss: 0.0689\n",
      "Epoch 34/300 - Train Loss: 0.0624, Val Loss: 0.0694\n",
      "Epoch 35/300 - Train Loss: 0.0641, Val Loss: 0.0681\n",
      "Epoch 36/300 - Train Loss: 0.0624, Val Loss: 0.0634\n",
      "Epoch 37/300 - Train Loss: 0.0613, Val Loss: 0.0628\n",
      "Epoch 38/300 - Train Loss: 0.0607, Val Loss: 0.0635\n",
      "Epoch 39/300 - Train Loss: 0.0579, Val Loss: 0.0686\n",
      "Epoch 40/300 - Train Loss: 0.0584, Val Loss: 0.0692\n",
      "Epoch 41/300 - Train Loss: 0.0583, Val Loss: 0.0645\n",
      "Epoch 42/300 - Train Loss: 0.0600, Val Loss: 0.0719\n",
      "Epoch 43/300 - Train Loss: 0.0584, Val Loss: 0.0658\n",
      "Epoch 44/300 - Train Loss: 0.0551, Val Loss: 0.0678\n",
      "Epoch 45/300 - Train Loss: 0.0562, Val Loss: 0.0641\n",
      "Epoch 46/300 - Train Loss: 0.0554, Val Loss: 0.0646\n",
      "Epoch 47/300 - Train Loss: 0.0542, Val Loss: 0.0669\n",
      "Epoch 48/300 - Train Loss: 0.0545, Val Loss: 0.0664\n",
      "Epoch 49/300 - Train Loss: 0.0542, Val Loss: 0.0732\n",
      "Epoch 50/300 - Train Loss: 0.0536, Val Loss: 0.0660\n",
      "Epoch 51/300 - Train Loss: 0.0522, Val Loss: 0.0668\n",
      "Epoch 52/300 - Train Loss: 0.0525, Val Loss: 0.0676\n",
      "Epoch 53/300 - Train Loss: 0.0518, Val Loss: 0.0677\n",
      "Epoch 54/300 - Train Loss: 0.0519, Val Loss: 0.0733\n",
      "Epoch 55/300 - Train Loss: 0.0506, Val Loss: 0.0657\n",
      "Epoch 56/300 - Train Loss: 0.0518, Val Loss: 0.0684\n",
      "Epoch 57/300 - Train Loss: 0.0520, Val Loss: 0.0697\n",
      "Epoch 58/300 - Train Loss: 0.0486, Val Loss: 0.0670\n",
      "Epoch 59/300 - Train Loss: 0.0524, Val Loss: 0.0659\n",
      "Epoch 60/300 - Train Loss: 0.0495, Val Loss: 0.0647\n",
      "Epoch 61/300 - Train Loss: 0.0498, Val Loss: 0.0672\n",
      "Epoch 62/300 - Train Loss: 0.0500, Val Loss: 0.0687\n",
      "Epoch 63/300 - Train Loss: 0.0489, Val Loss: 0.0704\n",
      "Epoch 64/300 - Train Loss: 0.0482, Val Loss: 0.0669\n",
      "Epoch 65/300 - Train Loss: 0.0463, Val Loss: 0.0642\n",
      "Epoch 66/300 - Train Loss: 0.0478, Val Loss: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:13:27,734] Trial 313 finished with value: 0.9724047220642381 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.10204797319085446, 'learning_rate': 0.00010426369522068034, 'batch_size': 32, 'weight_decay': 5.2717924359193365e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/300 - Train Loss: 0.0477, Val Loss: 0.0730\n",
      "Early stopping at epoch 67\n",
      "Macro F1 Score: 0.9724, Macro Precision: 0.9732, Macro Recall: 0.9717\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 315\n",
      "Training with F1=32, F2=32, D=2, dropout=0.15725111871609126, LR=7.994985566052584e-05, BS=32, WD=3.625883193520168e-05\n",
      "Epoch 1/300 - Train Loss: 0.2857, Val Loss: 0.1221\n",
      "Epoch 2/300 - Train Loss: 0.1176, Val Loss: 0.0920\n",
      "Epoch 3/300 - Train Loss: 0.1002, Val Loss: 0.0840\n",
      "Epoch 4/300 - Train Loss: 0.0951, Val Loss: 0.0948\n",
      "Epoch 5/300 - Train Loss: 0.0937, Val Loss: 0.0783\n",
      "Epoch 6/300 - Train Loss: 0.0922, Val Loss: 0.0981\n",
      "Epoch 7/300 - Train Loss: 0.0886, Val Loss: 0.0836\n",
      "Epoch 8/300 - Train Loss: 0.0859, Val Loss: 0.0823\n",
      "Epoch 9/300 - Train Loss: 0.0837, Val Loss: 0.0739\n",
      "Epoch 10/300 - Train Loss: 0.0823, Val Loss: 0.0948\n",
      "Epoch 11/300 - Train Loss: 0.0845, Val Loss: 0.0718\n",
      "Epoch 12/300 - Train Loss: 0.0831, Val Loss: 0.0760\n",
      "Epoch 13/300 - Train Loss: 0.0796, Val Loss: 0.0738\n",
      "Epoch 14/300 - Train Loss: 0.0800, Val Loss: 0.0699\n",
      "Epoch 15/300 - Train Loss: 0.0801, Val Loss: 0.0790\n",
      "Epoch 16/300 - Train Loss: 0.0794, Val Loss: 0.0738\n",
      "Epoch 17/300 - Train Loss: 0.0799, Val Loss: 0.0797\n",
      "Epoch 18/300 - Train Loss: 0.0770, Val Loss: 0.0745\n",
      "Epoch 19/300 - Train Loss: 0.0783, Val Loss: 0.0710\n",
      "Epoch 20/300 - Train Loss: 0.0753, Val Loss: 0.0752\n",
      "Epoch 21/300 - Train Loss: 0.0755, Val Loss: 0.0749\n",
      "Epoch 22/300 - Train Loss: 0.0752, Val Loss: 0.0800\n",
      "Epoch 23/300 - Train Loss: 0.0743, Val Loss: 0.0700\n",
      "Epoch 24/300 - Train Loss: 0.0730, Val Loss: 0.0709\n",
      "Epoch 25/300 - Train Loss: 0.0722, Val Loss: 0.0703\n",
      "Epoch 26/300 - Train Loss: 0.0728, Val Loss: 0.0703\n",
      "Epoch 27/300 - Train Loss: 0.0712, Val Loss: 0.0714\n",
      "Epoch 28/300 - Train Loss: 0.0699, Val Loss: 0.0690\n",
      "Epoch 29/300 - Train Loss: 0.0727, Val Loss: 0.0700\n",
      "Epoch 30/300 - Train Loss: 0.0710, Val Loss: 0.0731\n",
      "Epoch 31/300 - Train Loss: 0.0688, Val Loss: 0.0705\n",
      "Epoch 32/300 - Train Loss: 0.0697, Val Loss: 0.0677\n",
      "Epoch 33/300 - Train Loss: 0.0687, Val Loss: 0.0675\n",
      "Epoch 34/300 - Train Loss: 0.0680, Val Loss: 0.0718\n",
      "Epoch 35/300 - Train Loss: 0.0690, Val Loss: 0.0907\n",
      "Epoch 36/300 - Train Loss: 0.0673, Val Loss: 0.0674\n",
      "Epoch 37/300 - Train Loss: 0.0667, Val Loss: 0.0764\n",
      "Epoch 38/300 - Train Loss: 0.0646, Val Loss: 0.0774\n",
      "Epoch 39/300 - Train Loss: 0.0661, Val Loss: 0.0681\n",
      "Epoch 40/300 - Train Loss: 0.0677, Val Loss: 0.0775\n",
      "Epoch 41/300 - Train Loss: 0.0666, Val Loss: 0.0681\n",
      "Epoch 42/300 - Train Loss: 0.0659, Val Loss: 0.0670\n",
      "Epoch 43/300 - Train Loss: 0.0650, Val Loss: 0.0693\n",
      "Epoch 44/300 - Train Loss: 0.0635, Val Loss: 0.0732\n",
      "Epoch 45/300 - Train Loss: 0.0631, Val Loss: 0.0663\n",
      "Epoch 46/300 - Train Loss: 0.0625, Val Loss: 0.0722\n",
      "Epoch 47/300 - Train Loss: 0.0644, Val Loss: 0.0706\n",
      "Epoch 48/300 - Train Loss: 0.0620, Val Loss: 0.0718\n",
      "Epoch 49/300 - Train Loss: 0.0618, Val Loss: 0.0667\n",
      "Epoch 50/300 - Train Loss: 0.0608, Val Loss: 0.0716\n",
      "Epoch 51/300 - Train Loss: 0.0599, Val Loss: 0.0664\n",
      "Epoch 52/300 - Train Loss: 0.0600, Val Loss: 0.0672\n",
      "Epoch 53/300 - Train Loss: 0.0585, Val Loss: 0.0701\n",
      "Epoch 54/300 - Train Loss: 0.0598, Val Loss: 0.0707\n",
      "Epoch 55/300 - Train Loss: 0.0608, Val Loss: 0.0674\n",
      "Epoch 56/300 - Train Loss: 0.0582, Val Loss: 0.0691\n",
      "Epoch 57/300 - Train Loss: 0.0588, Val Loss: 0.0705\n",
      "Epoch 58/300 - Train Loss: 0.0578, Val Loss: 0.0747\n",
      "Epoch 59/300 - Train Loss: 0.0582, Val Loss: 0.0688\n",
      "Epoch 60/300 - Train Loss: 0.0586, Val Loss: 0.0678\n",
      "Epoch 61/300 - Train Loss: 0.0566, Val Loss: 0.0698\n",
      "Epoch 62/300 - Train Loss: 0.0583, Val Loss: 0.0718\n",
      "Epoch 63/300 - Train Loss: 0.0572, Val Loss: 0.0695\n",
      "Epoch 64/300 - Train Loss: 0.0573, Val Loss: 0.0704\n",
      "Epoch 65/300 - Train Loss: 0.0533, Val Loss: 0.0726\n",
      "Epoch 66/300 - Train Loss: 0.0548, Val Loss: 0.0690\n",
      "Epoch 67/300 - Train Loss: 0.0554, Val Loss: 0.0702\n",
      "Epoch 68/300 - Train Loss: 0.0530, Val Loss: 0.0714\n",
      "Epoch 69/300 - Train Loss: 0.0541, Val Loss: 0.0699\n",
      "Epoch 70/300 - Train Loss: 0.0561, Val Loss: 0.0672\n",
      "Epoch 71/300 - Train Loss: 0.0537, Val Loss: 0.0722\n",
      "Epoch 72/300 - Train Loss: 0.0534, Val Loss: 0.0694\n",
      "Epoch 73/300 - Train Loss: 0.0535, Val Loss: 0.0738\n",
      "Epoch 74/300 - Train Loss: 0.0536, Val Loss: 0.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:16:53,514] Trial 314 finished with value: 0.9759464353654325 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.15725111871609126, 'learning_rate': 7.994985566052584e-05, 'batch_size': 32, 'weight_decay': 3.625883193520168e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300 - Train Loss: 0.0519, Val Loss: 0.0674\n",
      "Early stopping at epoch 75\n",
      "Macro F1 Score: 0.9759, Macro Precision: 0.9745, Macro Recall: 0.9775\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.97      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 316\n",
      "Training with F1=32, F2=32, D=2, dropout=0.15713196612463867, LR=8.264546072545963e-05, BS=128, WD=3.95635815608704e-05\n",
      "Epoch 1/300 - Train Loss: 0.4669, Val Loss: 0.2229\n",
      "Epoch 2/300 - Train Loss: 0.1836, Val Loss: 0.1377\n",
      "Epoch 3/300 - Train Loss: 0.1268, Val Loss: 0.1049\n",
      "Epoch 4/300 - Train Loss: 0.1043, Val Loss: 0.1077\n",
      "Epoch 5/300 - Train Loss: 0.0961, Val Loss: 0.0888\n",
      "Epoch 6/300 - Train Loss: 0.0919, Val Loss: 0.0845\n",
      "Epoch 7/300 - Train Loss: 0.0887, Val Loss: 0.0807\n",
      "Epoch 8/300 - Train Loss: 0.0855, Val Loss: 0.0816\n",
      "Epoch 9/300 - Train Loss: 0.0840, Val Loss: 0.0769\n",
      "Epoch 10/300 - Train Loss: 0.0833, Val Loss: 0.0801\n",
      "Epoch 11/300 - Train Loss: 0.0828, Val Loss: 0.0779\n",
      "Epoch 12/300 - Train Loss: 0.0801, Val Loss: 0.0783\n",
      "Epoch 13/300 - Train Loss: 0.0782, Val Loss: 0.0774\n",
      "Epoch 14/300 - Train Loss: 0.0788, Val Loss: 0.0781\n",
      "Epoch 15/300 - Train Loss: 0.0782, Val Loss: 0.0778\n",
      "Epoch 16/300 - Train Loss: 0.0771, Val Loss: 0.0744\n",
      "Epoch 17/300 - Train Loss: 0.0758, Val Loss: 0.0757\n",
      "Epoch 18/300 - Train Loss: 0.0770, Val Loss: 0.0762\n",
      "Epoch 19/300 - Train Loss: 0.0752, Val Loss: 0.0766\n",
      "Epoch 20/300 - Train Loss: 0.0746, Val Loss: 0.0769\n",
      "Epoch 21/300 - Train Loss: 0.0742, Val Loss: 0.0782\n",
      "Epoch 22/300 - Train Loss: 0.0724, Val Loss: 0.0724\n",
      "Epoch 23/300 - Train Loss: 0.0728, Val Loss: 0.0763\n",
      "Epoch 24/300 - Train Loss: 0.0722, Val Loss: 0.0774\n",
      "Epoch 25/300 - Train Loss: 0.0711, Val Loss: 0.0722\n",
      "Epoch 26/300 - Train Loss: 0.0716, Val Loss: 0.0788\n",
      "Epoch 27/300 - Train Loss: 0.0704, Val Loss: 0.0774\n",
      "Epoch 28/300 - Train Loss: 0.0697, Val Loss: 0.0722\n",
      "Epoch 29/300 - Train Loss: 0.0703, Val Loss: 0.0788\n",
      "Epoch 30/300 - Train Loss: 0.0700, Val Loss: 0.0777\n",
      "Epoch 31/300 - Train Loss: 0.0693, Val Loss: 0.0819\n",
      "Epoch 32/300 - Train Loss: 0.0688, Val Loss: 0.0758\n",
      "Epoch 33/300 - Train Loss: 0.0691, Val Loss: 0.0744\n",
      "Epoch 34/300 - Train Loss: 0.0698, Val Loss: 0.0770\n",
      "Epoch 35/300 - Train Loss: 0.0678, Val Loss: 0.0755\n",
      "Epoch 36/300 - Train Loss: 0.0677, Val Loss: 0.0724\n",
      "Epoch 37/300 - Train Loss: 0.0664, Val Loss: 0.0777\n",
      "Epoch 38/300 - Train Loss: 0.0665, Val Loss: 0.0765\n",
      "Epoch 39/300 - Train Loss: 0.0685, Val Loss: 0.0726\n",
      "Epoch 40/300 - Train Loss: 0.0655, Val Loss: 0.0780\n",
      "Epoch 41/300 - Train Loss: 0.0670, Val Loss: 0.0736\n",
      "Epoch 42/300 - Train Loss: 0.0656, Val Loss: 0.0751\n",
      "Epoch 43/300 - Train Loss: 0.0669, Val Loss: 0.0736\n",
      "Epoch 44/300 - Train Loss: 0.0663, Val Loss: 0.0718\n",
      "Epoch 45/300 - Train Loss: 0.0640, Val Loss: 0.0736\n",
      "Epoch 46/300 - Train Loss: 0.0639, Val Loss: 0.0723\n",
      "Epoch 47/300 - Train Loss: 0.0632, Val Loss: 0.0737\n",
      "Epoch 48/300 - Train Loss: 0.0625, Val Loss: 0.0703\n",
      "Epoch 49/300 - Train Loss: 0.0628, Val Loss: 0.0733\n",
      "Epoch 50/300 - Train Loss: 0.0639, Val Loss: 0.0731\n",
      "Epoch 51/300 - Train Loss: 0.0627, Val Loss: 0.0735\n",
      "Epoch 52/300 - Train Loss: 0.0619, Val Loss: 0.0710\n",
      "Epoch 53/300 - Train Loss: 0.0620, Val Loss: 0.0728\n",
      "Epoch 54/300 - Train Loss: 0.0621, Val Loss: 0.0734\n",
      "Epoch 55/300 - Train Loss: 0.0616, Val Loss: 0.0768\n",
      "Epoch 56/300 - Train Loss: 0.0615, Val Loss: 0.0746\n",
      "Epoch 57/300 - Train Loss: 0.0615, Val Loss: 0.0716\n",
      "Epoch 58/300 - Train Loss: 0.0601, Val Loss: 0.0724\n",
      "Epoch 59/300 - Train Loss: 0.0598, Val Loss: 0.0745\n",
      "Epoch 60/300 - Train Loss: 0.0611, Val Loss: 0.0750\n",
      "Epoch 61/300 - Train Loss: 0.0587, Val Loss: 0.0735\n",
      "Epoch 62/300 - Train Loss: 0.0589, Val Loss: 0.0746\n",
      "Epoch 63/300 - Train Loss: 0.0596, Val Loss: 0.0721\n",
      "Epoch 64/300 - Train Loss: 0.0594, Val Loss: 0.0724\n",
      "Epoch 65/300 - Train Loss: 0.0585, Val Loss: 0.0716\n",
      "Epoch 66/300 - Train Loss: 0.0607, Val Loss: 0.0709\n",
      "Epoch 67/300 - Train Loss: 0.0598, Val Loss: 0.0710\n",
      "Epoch 68/300 - Train Loss: 0.0572, Val Loss: 0.0720\n",
      "Epoch 69/300 - Train Loss: 0.0570, Val Loss: 0.0689\n",
      "Epoch 70/300 - Train Loss: 0.0584, Val Loss: 0.0691\n",
      "Epoch 71/300 - Train Loss: 0.0572, Val Loss: 0.0721\n",
      "Epoch 72/300 - Train Loss: 0.0574, Val Loss: 0.0726\n",
      "Epoch 73/300 - Train Loss: 0.0566, Val Loss: 0.0707\n",
      "Epoch 74/300 - Train Loss: 0.0568, Val Loss: 0.0745\n",
      "Epoch 75/300 - Train Loss: 0.0570, Val Loss: 0.0719\n",
      "Epoch 76/300 - Train Loss: 0.0562, Val Loss: 0.0735\n",
      "Epoch 77/300 - Train Loss: 0.0556, Val Loss: 0.0721\n",
      "Epoch 78/300 - Train Loss: 0.0560, Val Loss: 0.0739\n",
      "Epoch 79/300 - Train Loss: 0.0548, Val Loss: 0.0723\n",
      "Epoch 80/300 - Train Loss: 0.0549, Val Loss: 0.0715\n",
      "Epoch 81/300 - Train Loss: 0.0547, Val Loss: 0.0703\n",
      "Epoch 82/300 - Train Loss: 0.0555, Val Loss: 0.0747\n",
      "Epoch 83/300 - Train Loss: 0.0530, Val Loss: 0.0737\n",
      "Epoch 84/300 - Train Loss: 0.0535, Val Loss: 0.0718\n",
      "Epoch 85/300 - Train Loss: 0.0549, Val Loss: 0.0707\n",
      "Epoch 86/300 - Train Loss: 0.0538, Val Loss: 0.0721\n",
      "Epoch 87/300 - Train Loss: 0.0524, Val Loss: 0.0717\n",
      "Epoch 88/300 - Train Loss: 0.0530, Val Loss: 0.0731\n",
      "Epoch 89/300 - Train Loss: 0.0533, Val Loss: 0.0702\n",
      "Epoch 90/300 - Train Loss: 0.0530, Val Loss: 0.0705\n",
      "Epoch 91/300 - Train Loss: 0.0528, Val Loss: 0.0728\n",
      "Epoch 92/300 - Train Loss: 0.0517, Val Loss: 0.0698\n",
      "Epoch 93/300 - Train Loss: 0.0527, Val Loss: 0.0701\n",
      "Epoch 94/300 - Train Loss: 0.0510, Val Loss: 0.0704\n",
      "Epoch 95/300 - Train Loss: 0.0523, Val Loss: 0.0709\n",
      "Epoch 96/300 - Train Loss: 0.0498, Val Loss: 0.0695\n",
      "Epoch 97/300 - Train Loss: 0.0503, Val Loss: 0.0712\n",
      "Epoch 98/300 - Train Loss: 0.0500, Val Loss: 0.0713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:20:29,907] Trial 315 finished with value: 0.9663680329833628 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.15713196612463867, 'learning_rate': 8.264546072545963e-05, 'batch_size': 128, 'weight_decay': 3.95635815608704e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/300 - Train Loss: 0.0500, Val Loss: 0.0744\n",
      "Early stopping at epoch 99\n",
      "Macro F1 Score: 0.9664, Macro Precision: 0.9560, Macro Recall: 0.9777\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 317\n",
      "Training with F1=32, F2=32, D=2, dropout=0.17266895076271846, LR=9.302580774161236e-05, BS=32, WD=3.426994805095979e-05\n",
      "Epoch 1/300 - Train Loss: 0.2654, Val Loss: 0.1220\n",
      "Epoch 2/300 - Train Loss: 0.1255, Val Loss: 0.0876\n",
      "Epoch 3/300 - Train Loss: 0.1052, Val Loss: 0.0802\n",
      "Epoch 4/300 - Train Loss: 0.0997, Val Loss: 0.0822\n",
      "Epoch 5/300 - Train Loss: 0.0943, Val Loss: 0.0781\n",
      "Epoch 6/300 - Train Loss: 0.0941, Val Loss: 0.0746\n",
      "Epoch 7/300 - Train Loss: 0.0892, Val Loss: 0.0725\n",
      "Epoch 8/300 - Train Loss: 0.0879, Val Loss: 0.0779\n",
      "Epoch 9/300 - Train Loss: 0.0860, Val Loss: 0.0735\n",
      "Epoch 10/300 - Train Loss: 0.0850, Val Loss: 0.0985\n",
      "Epoch 11/300 - Train Loss: 0.0834, Val Loss: 0.0740\n",
      "Epoch 12/300 - Train Loss: 0.0807, Val Loss: 0.0804\n",
      "Epoch 13/300 - Train Loss: 0.0808, Val Loss: 0.0720\n",
      "Epoch 14/300 - Train Loss: 0.0802, Val Loss: 0.0735\n",
      "Epoch 15/300 - Train Loss: 0.0814, Val Loss: 0.0684\n",
      "Epoch 16/300 - Train Loss: 0.0779, Val Loss: 0.0705\n",
      "Epoch 17/300 - Train Loss: 0.0760, Val Loss: 0.0702\n",
      "Epoch 18/300 - Train Loss: 0.0766, Val Loss: 0.0745\n",
      "Epoch 19/300 - Train Loss: 0.0753, Val Loss: 0.0740\n",
      "Epoch 20/300 - Train Loss: 0.0765, Val Loss: 0.0750\n",
      "Epoch 21/300 - Train Loss: 0.0746, Val Loss: 0.0711\n",
      "Epoch 22/300 - Train Loss: 0.0752, Val Loss: 0.0716\n",
      "Epoch 23/300 - Train Loss: 0.0741, Val Loss: 0.0698\n",
      "Epoch 24/300 - Train Loss: 0.0726, Val Loss: 0.0699\n",
      "Epoch 25/300 - Train Loss: 0.0728, Val Loss: 0.0683\n",
      "Epoch 26/300 - Train Loss: 0.0727, Val Loss: 0.0696\n",
      "Epoch 27/300 - Train Loss: 0.0724, Val Loss: 0.0733\n",
      "Epoch 28/300 - Train Loss: 0.0707, Val Loss: 0.0753\n",
      "Epoch 29/300 - Train Loss: 0.0709, Val Loss: 0.0697\n",
      "Epoch 30/300 - Train Loss: 0.0701, Val Loss: 0.0672\n",
      "Epoch 31/300 - Train Loss: 0.0710, Val Loss: 0.0702\n",
      "Epoch 32/300 - Train Loss: 0.0697, Val Loss: 0.0691\n",
      "Epoch 33/300 - Train Loss: 0.0687, Val Loss: 0.0733\n",
      "Epoch 34/300 - Train Loss: 0.0695, Val Loss: 0.0691\n",
      "Epoch 35/300 - Train Loss: 0.0682, Val Loss: 0.0692\n",
      "Epoch 36/300 - Train Loss: 0.0686, Val Loss: 0.0780\n",
      "Epoch 37/300 - Train Loss: 0.0686, Val Loss: 0.0688\n",
      "Epoch 38/300 - Train Loss: 0.0657, Val Loss: 0.0686\n",
      "Epoch 39/300 - Train Loss: 0.0673, Val Loss: 0.0716\n",
      "Epoch 40/300 - Train Loss: 0.0675, Val Loss: 0.0681\n",
      "Epoch 41/300 - Train Loss: 0.0645, Val Loss: 0.0689\n",
      "Epoch 42/300 - Train Loss: 0.0663, Val Loss: 0.0761\n",
      "Epoch 43/300 - Train Loss: 0.0664, Val Loss: 0.0690\n",
      "Epoch 44/300 - Train Loss: 0.0652, Val Loss: 0.0664\n",
      "Epoch 45/300 - Train Loss: 0.0632, Val Loss: 0.0707\n",
      "Epoch 46/300 - Train Loss: 0.0637, Val Loss: 0.0673\n",
      "Epoch 47/300 - Train Loss: 0.0630, Val Loss: 0.0708\n",
      "Epoch 48/300 - Train Loss: 0.0625, Val Loss: 0.0704\n",
      "Epoch 49/300 - Train Loss: 0.0621, Val Loss: 0.0737\n",
      "Epoch 50/300 - Train Loss: 0.0594, Val Loss: 0.0697\n",
      "Epoch 51/300 - Train Loss: 0.0631, Val Loss: 0.0709\n",
      "Epoch 52/300 - Train Loss: 0.0603, Val Loss: 0.0693\n",
      "Epoch 53/300 - Train Loss: 0.0610, Val Loss: 0.0718\n",
      "Epoch 54/300 - Train Loss: 0.0627, Val Loss: 0.0835\n",
      "Epoch 55/300 - Train Loss: 0.0598, Val Loss: 0.0699\n",
      "Epoch 56/300 - Train Loss: 0.0617, Val Loss: 0.0715\n",
      "Epoch 57/300 - Train Loss: 0.0598, Val Loss: 0.0677\n",
      "Epoch 58/300 - Train Loss: 0.0586, Val Loss: 0.0700\n",
      "Epoch 59/300 - Train Loss: 0.0585, Val Loss: 0.0729\n",
      "Epoch 60/300 - Train Loss: 0.0579, Val Loss: 0.0667\n",
      "Epoch 61/300 - Train Loss: 0.0600, Val Loss: 0.0671\n",
      "Epoch 62/300 - Train Loss: 0.0570, Val Loss: 0.0719\n",
      "Epoch 63/300 - Train Loss: 0.0561, Val Loss: 0.0693\n",
      "Epoch 64/300 - Train Loss: 0.0564, Val Loss: 0.0722\n",
      "Epoch 65/300 - Train Loss: 0.0558, Val Loss: 0.0770\n",
      "Epoch 66/300 - Train Loss: 0.0566, Val Loss: 0.0710\n",
      "Epoch 67/300 - Train Loss: 0.0554, Val Loss: 0.0698\n",
      "Epoch 68/300 - Train Loss: 0.0603, Val Loss: 0.0733\n",
      "Epoch 69/300 - Train Loss: 0.0564, Val Loss: 0.0700\n",
      "Epoch 70/300 - Train Loss: 0.0572, Val Loss: 0.0694\n",
      "Epoch 71/300 - Train Loss: 0.0529, Val Loss: 0.0744\n",
      "Epoch 72/300 - Train Loss: 0.0541, Val Loss: 0.0720\n",
      "Epoch 73/300 - Train Loss: 0.0522, Val Loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:23:52,459] Trial 316 finished with value: 0.9673678278880425 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.17266895076271846, 'learning_rate': 9.302580774161236e-05, 'batch_size': 32, 'weight_decay': 3.426994805095979e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300 - Train Loss: 0.0537, Val Loss: 0.0723\n",
      "Early stopping at epoch 74\n",
      "Macro F1 Score: 0.9674, Macro Precision: 0.9679, Macro Recall: 0.9669\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 318\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14618724046340592, LR=7.943374655324247e-05, BS=32, WD=2.946242432842296e-05\n",
      "Epoch 1/300 - Train Loss: 0.2976, Val Loss: 0.1357\n",
      "Epoch 2/300 - Train Loss: 0.1306, Val Loss: 0.0882\n",
      "Epoch 3/300 - Train Loss: 0.1050, Val Loss: 0.0849\n",
      "Epoch 4/300 - Train Loss: 0.0976, Val Loss: 0.0790\n",
      "Epoch 5/300 - Train Loss: 0.0957, Val Loss: 0.1000\n",
      "Epoch 6/300 - Train Loss: 0.0937, Val Loss: 0.0776\n",
      "Epoch 7/300 - Train Loss: 0.0895, Val Loss: 0.0746\n",
      "Epoch 8/300 - Train Loss: 0.0857, Val Loss: 0.0712\n",
      "Epoch 9/300 - Train Loss: 0.0844, Val Loss: 0.0818\n",
      "Epoch 10/300 - Train Loss: 0.0835, Val Loss: 0.0690\n",
      "Epoch 11/300 - Train Loss: 0.0825, Val Loss: 0.0716\n",
      "Epoch 12/300 - Train Loss: 0.0803, Val Loss: 0.0755\n",
      "Epoch 13/300 - Train Loss: 0.0804, Val Loss: 0.0759\n",
      "Epoch 14/300 - Train Loss: 0.0811, Val Loss: 0.0732\n",
      "Epoch 15/300 - Train Loss: 0.0782, Val Loss: 0.0770\n",
      "Epoch 16/300 - Train Loss: 0.0808, Val Loss: 0.0709\n",
      "Epoch 17/300 - Train Loss: 0.0781, Val Loss: 0.0738\n",
      "Epoch 18/300 - Train Loss: 0.0778, Val Loss: 0.0719\n",
      "Epoch 19/300 - Train Loss: 0.0753, Val Loss: 0.0692\n",
      "Epoch 20/300 - Train Loss: 0.0784, Val Loss: 0.0760\n",
      "Epoch 21/300 - Train Loss: 0.0759, Val Loss: 0.0749\n",
      "Epoch 22/300 - Train Loss: 0.0748, Val Loss: 0.0752\n",
      "Epoch 23/300 - Train Loss: 0.0754, Val Loss: 0.0735\n",
      "Epoch 24/300 - Train Loss: 0.0740, Val Loss: 0.0704\n",
      "Epoch 25/300 - Train Loss: 0.0699, Val Loss: 0.0697\n",
      "Epoch 26/300 - Train Loss: 0.0707, Val Loss: 0.0711\n",
      "Epoch 27/300 - Train Loss: 0.0691, Val Loss: 0.0786\n",
      "Epoch 28/300 - Train Loss: 0.0709, Val Loss: 0.0757\n",
      "Epoch 29/300 - Train Loss: 0.0705, Val Loss: 0.0673\n",
      "Epoch 30/300 - Train Loss: 0.0721, Val Loss: 0.0758\n",
      "Epoch 31/300 - Train Loss: 0.0697, Val Loss: 0.0670\n",
      "Epoch 32/300 - Train Loss: 0.0674, Val Loss: 0.0731\n",
      "Epoch 33/300 - Train Loss: 0.0677, Val Loss: 0.0750\n",
      "Epoch 34/300 - Train Loss: 0.0667, Val Loss: 0.0691\n",
      "Epoch 35/300 - Train Loss: 0.0664, Val Loss: 0.0673\n",
      "Epoch 36/300 - Train Loss: 0.0658, Val Loss: 0.0701\n",
      "Epoch 37/300 - Train Loss: 0.0677, Val Loss: 0.0709\n",
      "Epoch 38/300 - Train Loss: 0.0654, Val Loss: 0.0741\n",
      "Epoch 39/300 - Train Loss: 0.0665, Val Loss: 0.0651\n",
      "Epoch 40/300 - Train Loss: 0.0648, Val Loss: 0.0687\n",
      "Epoch 41/300 - Train Loss: 0.0664, Val Loss: 0.0678\n",
      "Epoch 42/300 - Train Loss: 0.0637, Val Loss: 0.0688\n",
      "Epoch 43/300 - Train Loss: 0.0642, Val Loss: 0.0681\n",
      "Epoch 44/300 - Train Loss: 0.0614, Val Loss: 0.0655\n",
      "Epoch 45/300 - Train Loss: 0.0645, Val Loss: 0.0697\n",
      "Epoch 46/300 - Train Loss: 0.0627, Val Loss: 0.0701\n",
      "Epoch 47/300 - Train Loss: 0.0616, Val Loss: 0.0728\n",
      "Epoch 48/300 - Train Loss: 0.0627, Val Loss: 0.0711\n",
      "Epoch 49/300 - Train Loss: 0.0602, Val Loss: 0.0711\n",
      "Epoch 50/300 - Train Loss: 0.0619, Val Loss: 0.0653\n",
      "Epoch 51/300 - Train Loss: 0.0606, Val Loss: 0.0678\n",
      "Epoch 52/300 - Train Loss: 0.0595, Val Loss: 0.0671\n",
      "Epoch 53/300 - Train Loss: 0.0598, Val Loss: 0.0699\n",
      "Epoch 54/300 - Train Loss: 0.0592, Val Loss: 0.0702\n",
      "Epoch 55/300 - Train Loss: 0.0605, Val Loss: 0.0692\n",
      "Epoch 56/300 - Train Loss: 0.0594, Val Loss: 0.0684\n",
      "Epoch 57/300 - Train Loss: 0.0594, Val Loss: 0.0690\n",
      "Epoch 58/300 - Train Loss: 0.0569, Val Loss: 0.0727\n",
      "Epoch 59/300 - Train Loss: 0.0567, Val Loss: 0.0703\n",
      "Epoch 60/300 - Train Loss: 0.0578, Val Loss: 0.0690\n",
      "Epoch 61/300 - Train Loss: 0.0563, Val Loss: 0.0677\n",
      "Epoch 62/300 - Train Loss: 0.0578, Val Loss: 0.0723\n",
      "Epoch 63/300 - Train Loss: 0.0575, Val Loss: 0.0659\n",
      "Epoch 64/300 - Train Loss: 0.0570, Val Loss: 0.0690\n",
      "Epoch 65/300 - Train Loss: 0.0553, Val Loss: 0.0740\n",
      "Epoch 66/300 - Train Loss: 0.0544, Val Loss: 0.0714\n",
      "Epoch 67/300 - Train Loss: 0.0530, Val Loss: 0.0702\n",
      "Epoch 68/300 - Train Loss: 0.0540, Val Loss: 0.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:27:01,320] Trial 317 finished with value: 0.9738148073948499 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14618724046340592, 'learning_rate': 7.943374655324247e-05, 'batch_size': 32, 'weight_decay': 2.946242432842296e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.0528, Val Loss: 0.0686\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9738, Macro Precision: 0.9745, Macro Recall: 0.9732\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 319\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14382779848804905, LR=7.288473078929506e-05, BS=32, WD=2.7376718097866716e-05\n",
      "Epoch 1/300 - Train Loss: 0.3259, Val Loss: 0.1542\n",
      "Epoch 2/300 - Train Loss: 0.1456, Val Loss: 0.1054\n",
      "Epoch 3/300 - Train Loss: 0.1170, Val Loss: 0.0869\n",
      "Epoch 4/300 - Train Loss: 0.1074, Val Loss: 0.0785\n",
      "Epoch 5/300 - Train Loss: 0.0998, Val Loss: 0.0821\n",
      "Epoch 6/300 - Train Loss: 0.0963, Val Loss: 0.0756\n",
      "Epoch 7/300 - Train Loss: 0.0928, Val Loss: 0.0766\n",
      "Epoch 8/300 - Train Loss: 0.0891, Val Loss: 0.0743\n",
      "Epoch 9/300 - Train Loss: 0.0883, Val Loss: 0.0739\n",
      "Epoch 10/300 - Train Loss: 0.0860, Val Loss: 0.0727\n",
      "Epoch 11/300 - Train Loss: 0.0852, Val Loss: 0.0717\n",
      "Epoch 12/300 - Train Loss: 0.0827, Val Loss: 0.0730\n",
      "Epoch 13/300 - Train Loss: 0.0825, Val Loss: 0.0727\n",
      "Epoch 14/300 - Train Loss: 0.0818, Val Loss: 0.0749\n",
      "Epoch 15/300 - Train Loss: 0.0822, Val Loss: 0.0695\n",
      "Epoch 16/300 - Train Loss: 0.0806, Val Loss: 0.0714\n",
      "Epoch 17/300 - Train Loss: 0.0801, Val Loss: 0.0761\n",
      "Epoch 18/300 - Train Loss: 0.0771, Val Loss: 0.0696\n",
      "Epoch 19/300 - Train Loss: 0.0778, Val Loss: 0.0746\n",
      "Epoch 20/300 - Train Loss: 0.0762, Val Loss: 0.0740\n",
      "Epoch 21/300 - Train Loss: 0.0762, Val Loss: 0.0665\n",
      "Epoch 22/300 - Train Loss: 0.0743, Val Loss: 0.0730\n",
      "Epoch 23/300 - Train Loss: 0.0745, Val Loss: 0.0709\n",
      "Epoch 24/300 - Train Loss: 0.0765, Val Loss: 0.0672\n",
      "Epoch 25/300 - Train Loss: 0.0752, Val Loss: 0.0696\n",
      "Epoch 26/300 - Train Loss: 0.0729, Val Loss: 0.0705\n",
      "Epoch 27/300 - Train Loss: 0.0719, Val Loss: 0.0677\n",
      "Epoch 28/300 - Train Loss: 0.0732, Val Loss: 0.0703\n",
      "Epoch 29/300 - Train Loss: 0.0733, Val Loss: 0.0698\n",
      "Epoch 30/300 - Train Loss: 0.0711, Val Loss: 0.0738\n",
      "Epoch 31/300 - Train Loss: 0.0717, Val Loss: 0.0696\n",
      "Epoch 32/300 - Train Loss: 0.0723, Val Loss: 0.0671\n",
      "Epoch 33/300 - Train Loss: 0.0704, Val Loss: 0.0666\n",
      "Epoch 34/300 - Train Loss: 0.0712, Val Loss: 0.0668\n",
      "Epoch 35/300 - Train Loss: 0.0680, Val Loss: 0.0671\n",
      "Epoch 36/300 - Train Loss: 0.0698, Val Loss: 0.0736\n",
      "Epoch 37/300 - Train Loss: 0.0676, Val Loss: 0.0690\n",
      "Epoch 38/300 - Train Loss: 0.0673, Val Loss: 0.0686\n",
      "Epoch 39/300 - Train Loss: 0.0679, Val Loss: 0.0694\n",
      "Epoch 40/300 - Train Loss: 0.0680, Val Loss: 0.0717\n",
      "Epoch 41/300 - Train Loss: 0.0670, Val Loss: 0.0726\n",
      "Epoch 42/300 - Train Loss: 0.0660, Val Loss: 0.0704\n",
      "Epoch 43/300 - Train Loss: 0.0659, Val Loss: 0.0688\n",
      "Epoch 44/300 - Train Loss: 0.0639, Val Loss: 0.0677\n",
      "Epoch 45/300 - Train Loss: 0.0644, Val Loss: 0.0684\n",
      "Epoch 46/300 - Train Loss: 0.0668, Val Loss: 0.0742\n",
      "Epoch 47/300 - Train Loss: 0.0646, Val Loss: 0.0768\n",
      "Epoch 48/300 - Train Loss: 0.0649, Val Loss: 0.0712\n",
      "Epoch 49/300 - Train Loss: 0.0638, Val Loss: 0.0713\n",
      "Epoch 50/300 - Train Loss: 0.0643, Val Loss: 0.0707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:29:20,934] Trial 318 finished with value: 0.9710183898152257 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14382779848804905, 'learning_rate': 7.288473078929506e-05, 'batch_size': 32, 'weight_decay': 2.7376718097866716e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300 - Train Loss: 0.0638, Val Loss: 0.0672\n",
      "Early stopping at epoch 51\n",
      "Macro F1 Score: 0.9710, Macro Precision: 0.9697, Macro Recall: 0.9725\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 320\n",
      "Training with F1=32, F2=16, D=2, dropout=0.13250101614680135, LR=8.001812132560771e-05, BS=32, WD=3.0018425173074034e-05\n",
      "Epoch 1/300 - Train Loss: 0.3142, Val Loss: 0.1231\n",
      "Epoch 2/300 - Train Loss: 0.1272, Val Loss: 0.1074\n",
      "Epoch 3/300 - Train Loss: 0.1071, Val Loss: 0.0851\n",
      "Epoch 4/300 - Train Loss: 0.0985, Val Loss: 0.0860\n",
      "Epoch 5/300 - Train Loss: 0.0979, Val Loss: 0.0743\n",
      "Epoch 6/300 - Train Loss: 0.0900, Val Loss: 0.0734\n",
      "Epoch 7/300 - Train Loss: 0.0883, Val Loss: 0.0747\n",
      "Epoch 8/300 - Train Loss: 0.0857, Val Loss: 0.0788\n",
      "Epoch 9/300 - Train Loss: 0.0857, Val Loss: 0.0791\n",
      "Epoch 10/300 - Train Loss: 0.0845, Val Loss: 0.0793\n",
      "Epoch 11/300 - Train Loss: 0.0833, Val Loss: 0.0748\n",
      "Epoch 12/300 - Train Loss: 0.0847, Val Loss: 0.0749\n",
      "Epoch 13/300 - Train Loss: 0.0822, Val Loss: 0.0737\n",
      "Epoch 14/300 - Train Loss: 0.0820, Val Loss: 0.0691\n",
      "Epoch 15/300 - Train Loss: 0.0819, Val Loss: 0.0851\n",
      "Epoch 16/300 - Train Loss: 0.0835, Val Loss: 0.0784\n",
      "Epoch 17/300 - Train Loss: 0.0806, Val Loss: 0.0724\n",
      "Epoch 18/300 - Train Loss: 0.0782, Val Loss: 0.0772\n",
      "Epoch 19/300 - Train Loss: 0.0790, Val Loss: 0.0789\n",
      "Epoch 20/300 - Train Loss: 0.0822, Val Loss: 0.0728\n",
      "Epoch 21/300 - Train Loss: 0.0804, Val Loss: 0.0821\n",
      "Epoch 22/300 - Train Loss: 0.0796, Val Loss: 0.0714\n",
      "Epoch 23/300 - Train Loss: 0.0756, Val Loss: 0.0740\n",
      "Epoch 24/300 - Train Loss: 0.0765, Val Loss: 0.0742\n",
      "Epoch 25/300 - Train Loss: 0.0793, Val Loss: 0.0701\n",
      "Epoch 26/300 - Train Loss: 0.0756, Val Loss: 0.0773\n",
      "Epoch 27/300 - Train Loss: 0.0744, Val Loss: 0.0787\n",
      "Epoch 28/300 - Train Loss: 0.0726, Val Loss: 0.0703\n",
      "Epoch 29/300 - Train Loss: 0.0754, Val Loss: 0.0756\n",
      "Epoch 30/300 - Train Loss: 0.0745, Val Loss: 0.0727\n",
      "Epoch 31/300 - Train Loss: 0.0725, Val Loss: 0.0749\n",
      "Epoch 32/300 - Train Loss: 0.0739, Val Loss: 0.0745\n",
      "Epoch 33/300 - Train Loss: 0.0738, Val Loss: 0.0749\n",
      "Epoch 34/300 - Train Loss: 0.0718, Val Loss: 0.0755\n",
      "Epoch 35/300 - Train Loss: 0.0732, Val Loss: 0.0733\n",
      "Epoch 36/300 - Train Loss: 0.0709, Val Loss: 0.0705\n",
      "Epoch 37/300 - Train Loss: 0.0731, Val Loss: 0.0702\n",
      "Epoch 38/300 - Train Loss: 0.0722, Val Loss: 0.0701\n",
      "Epoch 39/300 - Train Loss: 0.0714, Val Loss: 0.0772\n",
      "Epoch 40/300 - Train Loss: 0.0711, Val Loss: 0.0730\n",
      "Epoch 41/300 - Train Loss: 0.0719, Val Loss: 0.0733\n",
      "Epoch 42/300 - Train Loss: 0.0690, Val Loss: 0.0695\n",
      "Epoch 43/300 - Train Loss: 0.0700, Val Loss: 0.0741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:31:18,098] Trial 319 finished with value: 0.9640927111139699 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.13250101614680135, 'learning_rate': 8.001812132560771e-05, 'batch_size': 32, 'weight_decay': 3.0018425173074034e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300 - Train Loss: 0.0691, Val Loss: 0.0744\n",
      "Early stopping at epoch 44\n",
      "Macro F1 Score: 0.9641, Macro Precision: 0.9579, Macro Recall: 0.9707\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 321\n",
      "Training with F1=32, F2=32, D=2, dropout=0.15383994457968214, LR=8.748514585353282e-05, BS=32, WD=2.4378146511717542e-05\n",
      "Epoch 1/300 - Train Loss: 0.2601, Val Loss: 0.1155\n",
      "Epoch 2/300 - Train Loss: 0.1172, Val Loss: 0.0889\n",
      "Epoch 3/300 - Train Loss: 0.1065, Val Loss: 0.0933\n",
      "Epoch 4/300 - Train Loss: 0.0976, Val Loss: 0.0847\n",
      "Epoch 5/300 - Train Loss: 0.0942, Val Loss: 0.0752\n",
      "Epoch 6/300 - Train Loss: 0.0940, Val Loss: 0.0810\n",
      "Epoch 7/300 - Train Loss: 0.0903, Val Loss: 0.0790\n",
      "Epoch 8/300 - Train Loss: 0.0873, Val Loss: 0.0758\n",
      "Epoch 9/300 - Train Loss: 0.0880, Val Loss: 0.0747\n",
      "Epoch 10/300 - Train Loss: 0.0858, Val Loss: 0.0701\n",
      "Epoch 11/300 - Train Loss: 0.0856, Val Loss: 0.0742\n",
      "Epoch 12/300 - Train Loss: 0.0833, Val Loss: 0.0706\n",
      "Epoch 13/300 - Train Loss: 0.0840, Val Loss: 0.0777\n",
      "Epoch 14/300 - Train Loss: 0.0807, Val Loss: 0.0703\n",
      "Epoch 15/300 - Train Loss: 0.0798, Val Loss: 0.0702\n",
      "Epoch 16/300 - Train Loss: 0.0797, Val Loss: 0.0701\n",
      "Epoch 17/300 - Train Loss: 0.0811, Val Loss: 0.0735\n",
      "Epoch 18/300 - Train Loss: 0.0784, Val Loss: 0.0746\n",
      "Epoch 19/300 - Train Loss: 0.0779, Val Loss: 0.0718\n",
      "Epoch 20/300 - Train Loss: 0.0767, Val Loss: 0.0710\n",
      "Epoch 21/300 - Train Loss: 0.0741, Val Loss: 0.0703\n",
      "Epoch 22/300 - Train Loss: 0.0739, Val Loss: 0.0667\n",
      "Epoch 23/300 - Train Loss: 0.0728, Val Loss: 0.0690\n",
      "Epoch 24/300 - Train Loss: 0.0751, Val Loss: 0.0680\n",
      "Epoch 25/300 - Train Loss: 0.0734, Val Loss: 0.0685\n",
      "Epoch 26/300 - Train Loss: 0.0731, Val Loss: 0.0678\n",
      "Epoch 27/300 - Train Loss: 0.0717, Val Loss: 0.0690\n",
      "Epoch 28/300 - Train Loss: 0.0716, Val Loss: 0.0690\n",
      "Epoch 29/300 - Train Loss: 0.0701, Val Loss: 0.0704\n",
      "Epoch 30/300 - Train Loss: 0.0689, Val Loss: 0.0700\n",
      "Epoch 31/300 - Train Loss: 0.0706, Val Loss: 0.0695\n",
      "Epoch 32/300 - Train Loss: 0.0696, Val Loss: 0.0659\n",
      "Epoch 33/300 - Train Loss: 0.0687, Val Loss: 0.0742\n",
      "Epoch 34/300 - Train Loss: 0.0690, Val Loss: 0.0667\n",
      "Epoch 35/300 - Train Loss: 0.0660, Val Loss: 0.0654\n",
      "Epoch 36/300 - Train Loss: 0.0665, Val Loss: 0.0680\n",
      "Epoch 37/300 - Train Loss: 0.0657, Val Loss: 0.0696\n",
      "Epoch 38/300 - Train Loss: 0.0657, Val Loss: 0.0710\n",
      "Epoch 39/300 - Train Loss: 0.0663, Val Loss: 0.0693\n",
      "Epoch 40/300 - Train Loss: 0.0651, Val Loss: 0.0720\n",
      "Epoch 41/300 - Train Loss: 0.0633, Val Loss: 0.0711\n",
      "Epoch 42/300 - Train Loss: 0.0636, Val Loss: 0.0682\n",
      "Epoch 43/300 - Train Loss: 0.0648, Val Loss: 0.0686\n",
      "Epoch 44/300 - Train Loss: 0.0639, Val Loss: 0.0753\n",
      "Epoch 45/300 - Train Loss: 0.0623, Val Loss: 0.0687\n",
      "Epoch 46/300 - Train Loss: 0.0616, Val Loss: 0.0709\n",
      "Epoch 47/300 - Train Loss: 0.0610, Val Loss: 0.0693\n",
      "Epoch 48/300 - Train Loss: 0.0609, Val Loss: 0.0686\n",
      "Epoch 49/300 - Train Loss: 0.0612, Val Loss: 0.0681\n",
      "Epoch 50/300 - Train Loss: 0.0589, Val Loss: 0.0652\n",
      "Epoch 51/300 - Train Loss: 0.0599, Val Loss: 0.0699\n",
      "Epoch 52/300 - Train Loss: 0.0602, Val Loss: 0.0706\n",
      "Epoch 53/300 - Train Loss: 0.0602, Val Loss: 0.0680\n",
      "Epoch 54/300 - Train Loss: 0.0593, Val Loss: 0.0672\n",
      "Epoch 55/300 - Train Loss: 0.0592, Val Loss: 0.0648\n",
      "Epoch 56/300 - Train Loss: 0.0591, Val Loss: 0.0680\n",
      "Epoch 57/300 - Train Loss: 0.0575, Val Loss: 0.0690\n",
      "Epoch 58/300 - Train Loss: 0.0571, Val Loss: 0.0723\n",
      "Epoch 59/300 - Train Loss: 0.0568, Val Loss: 0.0719\n",
      "Epoch 60/300 - Train Loss: 0.0572, Val Loss: 0.0710\n",
      "Epoch 61/300 - Train Loss: 0.0544, Val Loss: 0.0654\n",
      "Epoch 62/300 - Train Loss: 0.0536, Val Loss: 0.0670\n",
      "Epoch 63/300 - Train Loss: 0.0556, Val Loss: 0.0685\n",
      "Epoch 64/300 - Train Loss: 0.0537, Val Loss: 0.0682\n",
      "Epoch 65/300 - Train Loss: 0.0556, Val Loss: 0.0694\n",
      "Epoch 66/300 - Train Loss: 0.0531, Val Loss: 0.0722\n",
      "Epoch 67/300 - Train Loss: 0.0536, Val Loss: 0.0705\n",
      "Epoch 68/300 - Train Loss: 0.0533, Val Loss: 0.0669\n",
      "Epoch 69/300 - Train Loss: 0.0540, Val Loss: 0.0701\n",
      "Epoch 70/300 - Train Loss: 0.0537, Val Loss: 0.0743\n",
      "Epoch 71/300 - Train Loss: 0.0521, Val Loss: 0.0779\n",
      "Epoch 72/300 - Train Loss: 0.0533, Val Loss: 0.0678\n",
      "Epoch 73/300 - Train Loss: 0.0512, Val Loss: 0.0739\n",
      "Epoch 74/300 - Train Loss: 0.0530, Val Loss: 0.0725\n",
      "Epoch 75/300 - Train Loss: 0.0530, Val Loss: 0.0708\n",
      "Epoch 76/300 - Train Loss: 0.0549, Val Loss: 0.0715\n",
      "Epoch 77/300 - Train Loss: 0.0506, Val Loss: 0.0709\n",
      "Epoch 78/300 - Train Loss: 0.0490, Val Loss: 0.0692\n",
      "Epoch 79/300 - Train Loss: 0.0496, Val Loss: 0.0719\n",
      "Epoch 80/300 - Train Loss: 0.0510, Val Loss: 0.0771\n",
      "Epoch 81/300 - Train Loss: 0.0495, Val Loss: 0.0671\n",
      "Epoch 82/300 - Train Loss: 0.0480, Val Loss: 0.0787\n",
      "Epoch 83/300 - Train Loss: 0.0511, Val Loss: 0.0687\n",
      "Epoch 84/300 - Train Loss: 0.0494, Val Loss: 0.0671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:35:10,645] Trial 320 finished with value: 0.9640702867056584 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.15383994457968214, 'learning_rate': 8.748514585353282e-05, 'batch_size': 32, 'weight_decay': 2.4378146511717542e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300 - Train Loss: 0.0484, Val Loss: 0.0742\n",
      "Early stopping at epoch 85\n",
      "Macro F1 Score: 0.9641, Macro Precision: 0.9619, Macro Recall: 0.9663\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.92      0.93      0.93        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 322\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1645877812853414, LR=6.89380163671664e-05, BS=32, WD=1.6639921032701185e-05\n",
      "Epoch 1/300 - Train Loss: 0.3129, Val Loss: 0.1290\n",
      "Epoch 2/300 - Train Loss: 0.1284, Val Loss: 0.0841\n",
      "Epoch 3/300 - Train Loss: 0.1079, Val Loss: 0.0795\n",
      "Epoch 4/300 - Train Loss: 0.0997, Val Loss: 0.0750\n",
      "Epoch 5/300 - Train Loss: 0.0971, Val Loss: 0.0744\n",
      "Epoch 6/300 - Train Loss: 0.0942, Val Loss: 0.0730\n",
      "Epoch 7/300 - Train Loss: 0.0904, Val Loss: 0.0737\n",
      "Epoch 8/300 - Train Loss: 0.0878, Val Loss: 0.0739\n",
      "Epoch 9/300 - Train Loss: 0.0863, Val Loss: 0.0728\n",
      "Epoch 10/300 - Train Loss: 0.0852, Val Loss: 0.0752\n",
      "Epoch 11/300 - Train Loss: 0.0837, Val Loss: 0.0693\n",
      "Epoch 12/300 - Train Loss: 0.0829, Val Loss: 0.0746\n",
      "Epoch 13/300 - Train Loss: 0.0802, Val Loss: 0.0691\n",
      "Epoch 14/300 - Train Loss: 0.0812, Val Loss: 0.0752\n",
      "Epoch 15/300 - Train Loss: 0.0805, Val Loss: 0.0752\n",
      "Epoch 16/300 - Train Loss: 0.0800, Val Loss: 0.0718\n",
      "Epoch 17/300 - Train Loss: 0.0799, Val Loss: 0.0841\n",
      "Epoch 18/300 - Train Loss: 0.0783, Val Loss: 0.0730\n",
      "Epoch 19/300 - Train Loss: 0.0770, Val Loss: 0.0716\n",
      "Epoch 20/300 - Train Loss: 0.0766, Val Loss: 0.0685\n",
      "Epoch 21/300 - Train Loss: 0.0775, Val Loss: 0.0691\n",
      "Epoch 22/300 - Train Loss: 0.0793, Val Loss: 0.0698\n",
      "Epoch 23/300 - Train Loss: 0.0747, Val Loss: 0.0687\n",
      "Epoch 24/300 - Train Loss: 0.0744, Val Loss: 0.0683\n",
      "Epoch 25/300 - Train Loss: 0.0756, Val Loss: 0.0694\n",
      "Epoch 26/300 - Train Loss: 0.0728, Val Loss: 0.0700\n",
      "Epoch 27/300 - Train Loss: 0.0742, Val Loss: 0.0731\n",
      "Epoch 28/300 - Train Loss: 0.0727, Val Loss: 0.0744\n",
      "Epoch 29/300 - Train Loss: 0.0724, Val Loss: 0.0681\n",
      "Epoch 30/300 - Train Loss: 0.0747, Val Loss: 0.0784\n",
      "Epoch 31/300 - Train Loss: 0.0735, Val Loss: 0.0730\n",
      "Epoch 32/300 - Train Loss: 0.0718, Val Loss: 0.0671\n",
      "Epoch 33/300 - Train Loss: 0.0705, Val Loss: 0.0700\n",
      "Epoch 34/300 - Train Loss: 0.0700, Val Loss: 0.0704\n",
      "Epoch 35/300 - Train Loss: 0.0681, Val Loss: 0.0707\n",
      "Epoch 36/300 - Train Loss: 0.0707, Val Loss: 0.0697\n",
      "Epoch 37/300 - Train Loss: 0.0693, Val Loss: 0.0709\n",
      "Epoch 38/300 - Train Loss: 0.0690, Val Loss: 0.0698\n",
      "Epoch 39/300 - Train Loss: 0.0681, Val Loss: 0.0669\n",
      "Epoch 40/300 - Train Loss: 0.0670, Val Loss: 0.0667\n",
      "Epoch 41/300 - Train Loss: 0.0655, Val Loss: 0.0689\n",
      "Epoch 42/300 - Train Loss: 0.0664, Val Loss: 0.0714\n",
      "Epoch 43/300 - Train Loss: 0.0689, Val Loss: 0.0830\n",
      "Epoch 44/300 - Train Loss: 0.0643, Val Loss: 0.0667\n",
      "Epoch 45/300 - Train Loss: 0.0643, Val Loss: 0.0703\n",
      "Epoch 46/300 - Train Loss: 0.0629, Val Loss: 0.0698\n",
      "Epoch 47/300 - Train Loss: 0.0649, Val Loss: 0.0660\n",
      "Epoch 48/300 - Train Loss: 0.0647, Val Loss: 0.0689\n",
      "Epoch 49/300 - Train Loss: 0.0633, Val Loss: 0.0716\n",
      "Epoch 50/300 - Train Loss: 0.0639, Val Loss: 0.0649\n",
      "Epoch 51/300 - Train Loss: 0.0645, Val Loss: 0.0668\n",
      "Epoch 52/300 - Train Loss: 0.0652, Val Loss: 0.0675\n",
      "Epoch 53/300 - Train Loss: 0.0614, Val Loss: 0.0681\n",
      "Epoch 54/300 - Train Loss: 0.0654, Val Loss: 0.0699\n",
      "Epoch 55/300 - Train Loss: 0.0619, Val Loss: 0.0675\n",
      "Epoch 56/300 - Train Loss: 0.0618, Val Loss: 0.0660\n",
      "Epoch 57/300 - Train Loss: 0.0598, Val Loss: 0.0684\n",
      "Epoch 58/300 - Train Loss: 0.0601, Val Loss: 0.0683\n",
      "Epoch 59/300 - Train Loss: 0.0602, Val Loss: 0.0723\n",
      "Epoch 60/300 - Train Loss: 0.0591, Val Loss: 0.0747\n",
      "Epoch 61/300 - Train Loss: 0.0594, Val Loss: 0.0745\n",
      "Epoch 62/300 - Train Loss: 0.0606, Val Loss: 0.0677\n",
      "Epoch 63/300 - Train Loss: 0.0588, Val Loss: 0.0675\n",
      "Epoch 64/300 - Train Loss: 0.0585, Val Loss: 0.0660\n",
      "Epoch 65/300 - Train Loss: 0.0580, Val Loss: 0.0703\n",
      "Epoch 66/300 - Train Loss: 0.0577, Val Loss: 0.0696\n",
      "Epoch 67/300 - Train Loss: 0.0553, Val Loss: 0.0721\n",
      "Epoch 68/300 - Train Loss: 0.0571, Val Loss: 0.0719\n",
      "Epoch 69/300 - Train Loss: 0.0561, Val Loss: 0.0686\n",
      "Epoch 70/300 - Train Loss: 0.0554, Val Loss: 0.0694\n",
      "Epoch 71/300 - Train Loss: 0.0563, Val Loss: 0.0765\n",
      "Epoch 72/300 - Train Loss: 0.0545, Val Loss: 0.0659\n",
      "Epoch 73/300 - Train Loss: 0.0552, Val Loss: 0.0710\n",
      "Epoch 74/300 - Train Loss: 0.0535, Val Loss: 0.0675\n",
      "Epoch 75/300 - Train Loss: 0.0552, Val Loss: 0.0698\n",
      "Epoch 76/300 - Train Loss: 0.0552, Val Loss: 0.0680\n",
      "Epoch 77/300 - Train Loss: 0.0537, Val Loss: 0.0670\n",
      "Epoch 78/300 - Train Loss: 0.0535, Val Loss: 0.0683\n",
      "Epoch 79/300 - Train Loss: 0.0539, Val Loss: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:38:49,508] Trial 321 finished with value: 0.9733325236486522 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1645877812853414, 'learning_rate': 6.89380163671664e-05, 'batch_size': 32, 'weight_decay': 1.6639921032701185e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300 - Train Loss: 0.0538, Val Loss: 0.0686\n",
      "Early stopping at epoch 80\n",
      "Macro F1 Score: 0.9733, Macro Precision: 0.9652, Macro Recall: 0.9822\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.98      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 323\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14307831862832143, LR=7.635235567641373e-05, BS=32, WD=4.5029806999503794e-05\n",
      "Epoch 1/300 - Train Loss: 0.2898, Val Loss: 0.1362\n",
      "Epoch 2/300 - Train Loss: 0.1255, Val Loss: 0.0892\n",
      "Epoch 3/300 - Train Loss: 0.1040, Val Loss: 0.0923\n",
      "Epoch 4/300 - Train Loss: 0.0972, Val Loss: 0.0845\n",
      "Epoch 5/300 - Train Loss: 0.0937, Val Loss: 0.0796\n",
      "Epoch 6/300 - Train Loss: 0.0890, Val Loss: 0.0802\n",
      "Epoch 7/300 - Train Loss: 0.0885, Val Loss: 0.0747\n",
      "Epoch 8/300 - Train Loss: 0.0855, Val Loss: 0.0747\n",
      "Epoch 9/300 - Train Loss: 0.0854, Val Loss: 0.0714\n",
      "Epoch 10/300 - Train Loss: 0.0840, Val Loss: 0.0695\n",
      "Epoch 11/300 - Train Loss: 0.0822, Val Loss: 0.0767\n",
      "Epoch 12/300 - Train Loss: 0.0821, Val Loss: 0.0738\n",
      "Epoch 13/300 - Train Loss: 0.0812, Val Loss: 0.0701\n",
      "Epoch 14/300 - Train Loss: 0.0803, Val Loss: 0.0679\n",
      "Epoch 15/300 - Train Loss: 0.0796, Val Loss: 0.0728\n",
      "Epoch 16/300 - Train Loss: 0.0798, Val Loss: 0.0670\n",
      "Epoch 17/300 - Train Loss: 0.0774, Val Loss: 0.0686\n",
      "Epoch 18/300 - Train Loss: 0.0785, Val Loss: 0.0803\n",
      "Epoch 19/300 - Train Loss: 0.0775, Val Loss: 0.0705\n",
      "Epoch 20/300 - Train Loss: 0.0758, Val Loss: 0.0706\n",
      "Epoch 21/300 - Train Loss: 0.0763, Val Loss: 0.0743\n",
      "Epoch 22/300 - Train Loss: 0.0744, Val Loss: 0.0681\n",
      "Epoch 23/300 - Train Loss: 0.0747, Val Loss: 0.0704\n",
      "Epoch 24/300 - Train Loss: 0.0735, Val Loss: 0.0717\n",
      "Epoch 25/300 - Train Loss: 0.0715, Val Loss: 0.0711\n",
      "Epoch 26/300 - Train Loss: 0.0742, Val Loss: 0.0669\n",
      "Epoch 27/300 - Train Loss: 0.0720, Val Loss: 0.0711\n",
      "Epoch 28/300 - Train Loss: 0.0721, Val Loss: 0.0697\n",
      "Epoch 29/300 - Train Loss: 0.0717, Val Loss: 0.0693\n",
      "Epoch 30/300 - Train Loss: 0.0721, Val Loss: 0.0736\n",
      "Epoch 31/300 - Train Loss: 0.0717, Val Loss: 0.0853\n",
      "Epoch 32/300 - Train Loss: 0.0718, Val Loss: 0.0693\n",
      "Epoch 33/300 - Train Loss: 0.0678, Val Loss: 0.0696\n",
      "Epoch 34/300 - Train Loss: 0.0675, Val Loss: 0.0696\n",
      "Epoch 35/300 - Train Loss: 0.0679, Val Loss: 0.0649\n",
      "Epoch 36/300 - Train Loss: 0.0652, Val Loss: 0.0780\n",
      "Epoch 37/300 - Train Loss: 0.0673, Val Loss: 0.0675\n",
      "Epoch 38/300 - Train Loss: 0.0659, Val Loss: 0.0661\n",
      "Epoch 39/300 - Train Loss: 0.0659, Val Loss: 0.0663\n",
      "Epoch 40/300 - Train Loss: 0.0650, Val Loss: 0.0663\n",
      "Epoch 41/300 - Train Loss: 0.0665, Val Loss: 0.0677\n",
      "Epoch 42/300 - Train Loss: 0.0666, Val Loss: 0.0679\n",
      "Epoch 43/300 - Train Loss: 0.0628, Val Loss: 0.0723\n",
      "Epoch 44/300 - Train Loss: 0.0655, Val Loss: 0.0678\n",
      "Epoch 45/300 - Train Loss: 0.0627, Val Loss: 0.0691\n",
      "Epoch 46/300 - Train Loss: 0.0638, Val Loss: 0.0698\n",
      "Epoch 47/300 - Train Loss: 0.0629, Val Loss: 0.0672\n",
      "Epoch 48/300 - Train Loss: 0.0622, Val Loss: 0.0733\n",
      "Epoch 49/300 - Train Loss: 0.0623, Val Loss: 0.0666\n",
      "Epoch 50/300 - Train Loss: 0.0618, Val Loss: 0.0663\n",
      "Epoch 51/300 - Train Loss: 0.0628, Val Loss: 0.0666\n",
      "Epoch 52/300 - Train Loss: 0.0600, Val Loss: 0.0669\n",
      "Epoch 53/300 - Train Loss: 0.0587, Val Loss: 0.0669\n",
      "Epoch 54/300 - Train Loss: 0.0594, Val Loss: 0.0670\n",
      "Epoch 55/300 - Train Loss: 0.0594, Val Loss: 0.0699\n",
      "Epoch 56/300 - Train Loss: 0.0595, Val Loss: 0.0694\n",
      "Epoch 57/300 - Train Loss: 0.0582, Val Loss: 0.0677\n",
      "Epoch 58/300 - Train Loss: 0.0578, Val Loss: 0.0711\n",
      "Epoch 59/300 - Train Loss: 0.0593, Val Loss: 0.0696\n",
      "Epoch 60/300 - Train Loss: 0.0575, Val Loss: 0.0693\n",
      "Epoch 61/300 - Train Loss: 0.0564, Val Loss: 0.0684\n",
      "Epoch 62/300 - Train Loss: 0.0574, Val Loss: 0.0714\n",
      "Epoch 63/300 - Train Loss: 0.0556, Val Loss: 0.0674\n",
      "Epoch 64/300 - Train Loss: 0.0568, Val Loss: 0.0687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:41:47,057] Trial 322 finished with value: 0.9637281698817969 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14307831862832143, 'learning_rate': 7.635235567641373e-05, 'batch_size': 32, 'weight_decay': 4.5029806999503794e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300 - Train Loss: 0.0551, Val Loss: 0.0673\n",
      "Early stopping at epoch 65\n",
      "Macro F1 Score: 0.9637, Macro Precision: 0.9519, Macro Recall: 0.9770\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 324\n",
      "Training with F1=32, F2=32, D=2, dropout=0.11883471335906712, LR=8.041644435746215e-05, BS=32, WD=3.742600167371167e-05\n",
      "Epoch 1/300 - Train Loss: 0.2839, Val Loss: 0.1317\n",
      "Epoch 2/300 - Train Loss: 0.1252, Val Loss: 0.0849\n",
      "Epoch 3/300 - Train Loss: 0.1048, Val Loss: 0.0807\n",
      "Epoch 4/300 - Train Loss: 0.0931, Val Loss: 0.0755\n",
      "Epoch 5/300 - Train Loss: 0.0912, Val Loss: 0.0738\n",
      "Epoch 6/300 - Train Loss: 0.0885, Val Loss: 0.0727\n",
      "Epoch 7/300 - Train Loss: 0.0885, Val Loss: 0.0765\n",
      "Epoch 8/300 - Train Loss: 0.0848, Val Loss: 0.0731\n",
      "Epoch 9/300 - Train Loss: 0.0829, Val Loss: 0.0723\n",
      "Epoch 10/300 - Train Loss: 0.0823, Val Loss: 0.0729\n",
      "Epoch 11/300 - Train Loss: 0.0818, Val Loss: 0.0709\n",
      "Epoch 12/300 - Train Loss: 0.0821, Val Loss: 0.0751\n",
      "Epoch 13/300 - Train Loss: 0.0798, Val Loss: 0.0686\n",
      "Epoch 14/300 - Train Loss: 0.0808, Val Loss: 0.0704\n",
      "Epoch 15/300 - Train Loss: 0.0801, Val Loss: 0.0803\n",
      "Epoch 16/300 - Train Loss: 0.0777, Val Loss: 0.0806\n",
      "Epoch 17/300 - Train Loss: 0.0759, Val Loss: 0.0661\n",
      "Epoch 18/300 - Train Loss: 0.0746, Val Loss: 0.0705\n",
      "Epoch 19/300 - Train Loss: 0.0745, Val Loss: 0.0702\n",
      "Epoch 20/300 - Train Loss: 0.0729, Val Loss: 0.0673\n",
      "Epoch 21/300 - Train Loss: 0.0734, Val Loss: 0.0729\n",
      "Epoch 22/300 - Train Loss: 0.0736, Val Loss: 0.0758\n",
      "Epoch 23/300 - Train Loss: 0.0718, Val Loss: 0.0728\n",
      "Epoch 24/300 - Train Loss: 0.0691, Val Loss: 0.0700\n",
      "Epoch 25/300 - Train Loss: 0.0716, Val Loss: 0.0656\n",
      "Epoch 26/300 - Train Loss: 0.0699, Val Loss: 0.0716\n",
      "Epoch 27/300 - Train Loss: 0.0713, Val Loss: 0.0685\n",
      "Epoch 28/300 - Train Loss: 0.0688, Val Loss: 0.0706\n",
      "Epoch 29/300 - Train Loss: 0.0690, Val Loss: 0.0695\n",
      "Epoch 30/300 - Train Loss: 0.0694, Val Loss: 0.0665\n",
      "Epoch 31/300 - Train Loss: 0.0683, Val Loss: 0.0770\n",
      "Epoch 32/300 - Train Loss: 0.0671, Val Loss: 0.0693\n",
      "Epoch 33/300 - Train Loss: 0.0665, Val Loss: 0.0702\n",
      "Epoch 34/300 - Train Loss: 0.0640, Val Loss: 0.0737\n",
      "Epoch 35/300 - Train Loss: 0.0650, Val Loss: 0.0730\n",
      "Epoch 36/300 - Train Loss: 0.0659, Val Loss: 0.0730\n",
      "Epoch 37/300 - Train Loss: 0.0642, Val Loss: 0.0780\n",
      "Epoch 38/300 - Train Loss: 0.0635, Val Loss: 0.0721\n",
      "Epoch 39/300 - Train Loss: 0.0639, Val Loss: 0.0684\n",
      "Epoch 40/300 - Train Loss: 0.0627, Val Loss: 0.0670\n",
      "Epoch 41/300 - Train Loss: 0.0601, Val Loss: 0.0752\n",
      "Epoch 42/300 - Train Loss: 0.0644, Val Loss: 0.0756\n",
      "Epoch 43/300 - Train Loss: 0.0615, Val Loss: 0.0736\n",
      "Epoch 44/300 - Train Loss: 0.0591, Val Loss: 0.0703\n",
      "Epoch 45/300 - Train Loss: 0.0596, Val Loss: 0.0705\n",
      "Epoch 46/300 - Train Loss: 0.0580, Val Loss: 0.0695\n",
      "Epoch 47/300 - Train Loss: 0.0587, Val Loss: 0.0653\n",
      "Epoch 48/300 - Train Loss: 0.0583, Val Loss: 0.0712\n",
      "Epoch 49/300 - Train Loss: 0.0579, Val Loss: 0.0702\n",
      "Epoch 50/300 - Train Loss: 0.0574, Val Loss: 0.0679\n",
      "Epoch 51/300 - Train Loss: 0.0586, Val Loss: 0.0684\n",
      "Epoch 52/300 - Train Loss: 0.0560, Val Loss: 0.0724\n",
      "Epoch 53/300 - Train Loss: 0.0567, Val Loss: 0.0703\n",
      "Epoch 54/300 - Train Loss: 0.0585, Val Loss: 0.0706\n",
      "Epoch 55/300 - Train Loss: 0.0544, Val Loss: 0.0714\n",
      "Epoch 56/300 - Train Loss: 0.0575, Val Loss: 0.0703\n",
      "Epoch 57/300 - Train Loss: 0.0545, Val Loss: 0.0695\n",
      "Epoch 58/300 - Train Loss: 0.0573, Val Loss: 0.0685\n",
      "Epoch 59/300 - Train Loss: 0.0536, Val Loss: 0.0755\n",
      "Epoch 60/300 - Train Loss: 0.0541, Val Loss: 0.0720\n",
      "Epoch 61/300 - Train Loss: 0.0527, Val Loss: 0.0701\n",
      "Epoch 62/300 - Train Loss: 0.0532, Val Loss: 0.0710\n",
      "Epoch 63/300 - Train Loss: 0.0529, Val Loss: 0.0705\n",
      "Epoch 64/300 - Train Loss: 0.0498, Val Loss: 0.0721\n",
      "Epoch 65/300 - Train Loss: 0.0530, Val Loss: 0.0686\n",
      "Epoch 66/300 - Train Loss: 0.0522, Val Loss: 0.0706\n",
      "Epoch 67/300 - Train Loss: 0.0520, Val Loss: 0.0718\n",
      "Epoch 68/300 - Train Loss: 0.0512, Val Loss: 0.0675\n",
      "Epoch 69/300 - Train Loss: 0.0525, Val Loss: 0.0753\n",
      "Epoch 70/300 - Train Loss: 0.0489, Val Loss: 0.0746\n",
      "Epoch 71/300 - Train Loss: 0.0492, Val Loss: 0.0781\n",
      "Epoch 72/300 - Train Loss: 0.0534, Val Loss: 0.0706\n",
      "Epoch 73/300 - Train Loss: 0.0496, Val Loss: 0.0736\n",
      "Epoch 74/300 - Train Loss: 0.0480, Val Loss: 0.0733\n",
      "Epoch 75/300 - Train Loss: 0.0488, Val Loss: 0.0787\n",
      "Epoch 76/300 - Train Loss: 0.0469, Val Loss: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:45:17,339] Trial 323 finished with value: 0.9724971495616224 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.11883471335906712, 'learning_rate': 8.041644435746215e-05, 'batch_size': 32, 'weight_decay': 3.742600167371167e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300 - Train Loss: 0.0476, Val Loss: 0.0749\n",
      "Early stopping at epoch 77\n",
      "Macro F1 Score: 0.9725, Macro Precision: 0.9641, Macro Recall: 0.9816\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.92      0.98      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 325\n",
      "Training with F1=32, F2=32, D=2, dropout=0.13594951741900677, LR=9.505791193944617e-05, BS=32, WD=3.074574816832677e-05\n",
      "Epoch 1/300 - Train Loss: 0.2640, Val Loss: 0.1096\n",
      "Epoch 2/300 - Train Loss: 0.1167, Val Loss: 0.0833\n",
      "Epoch 3/300 - Train Loss: 0.1042, Val Loss: 0.0775\n",
      "Epoch 4/300 - Train Loss: 0.0988, Val Loss: 0.0790\n",
      "Epoch 5/300 - Train Loss: 0.0934, Val Loss: 0.0740\n",
      "Epoch 6/300 - Train Loss: 0.0925, Val Loss: 0.0778\n",
      "Epoch 7/300 - Train Loss: 0.0883, Val Loss: 0.0725\n",
      "Epoch 8/300 - Train Loss: 0.0868, Val Loss: 0.0742\n",
      "Epoch 9/300 - Train Loss: 0.0846, Val Loss: 0.0772\n",
      "Epoch 10/300 - Train Loss: 0.0830, Val Loss: 0.0764\n",
      "Epoch 11/300 - Train Loss: 0.0812, Val Loss: 0.0737\n",
      "Epoch 12/300 - Train Loss: 0.0792, Val Loss: 0.0744\n",
      "Epoch 13/300 - Train Loss: 0.0799, Val Loss: 0.0695\n",
      "Epoch 14/300 - Train Loss: 0.0792, Val Loss: 0.0735\n",
      "Epoch 15/300 - Train Loss: 0.0801, Val Loss: 0.0723\n",
      "Epoch 16/300 - Train Loss: 0.0762, Val Loss: 0.0735\n",
      "Epoch 17/300 - Train Loss: 0.0786, Val Loss: 0.0769\n",
      "Epoch 18/300 - Train Loss: 0.0760, Val Loss: 0.0800\n",
      "Epoch 19/300 - Train Loss: 0.0760, Val Loss: 0.0681\n",
      "Epoch 20/300 - Train Loss: 0.0739, Val Loss: 0.0665\n",
      "Epoch 21/300 - Train Loss: 0.0734, Val Loss: 0.0698\n",
      "Epoch 22/300 - Train Loss: 0.0767, Val Loss: 0.0717\n",
      "Epoch 23/300 - Train Loss: 0.0721, Val Loss: 0.0686\n",
      "Epoch 24/300 - Train Loss: 0.0697, Val Loss: 0.0678\n",
      "Epoch 25/300 - Train Loss: 0.0692, Val Loss: 0.0689\n",
      "Epoch 26/300 - Train Loss: 0.0682, Val Loss: 0.0671\n",
      "Epoch 27/300 - Train Loss: 0.0686, Val Loss: 0.0672\n",
      "Epoch 28/300 - Train Loss: 0.0674, Val Loss: 0.0659\n",
      "Epoch 29/300 - Train Loss: 0.0669, Val Loss: 0.0735\n",
      "Epoch 30/300 - Train Loss: 0.0679, Val Loss: 0.0739\n",
      "Epoch 31/300 - Train Loss: 0.0643, Val Loss: 0.0679\n",
      "Epoch 32/300 - Train Loss: 0.0685, Val Loss: 0.0665\n",
      "Epoch 33/300 - Train Loss: 0.0657, Val Loss: 0.0675\n",
      "Epoch 34/300 - Train Loss: 0.0664, Val Loss: 0.0635\n",
      "Epoch 35/300 - Train Loss: 0.0634, Val Loss: 0.0671\n",
      "Epoch 36/300 - Train Loss: 0.0620, Val Loss: 0.0675\n",
      "Epoch 37/300 - Train Loss: 0.0596, Val Loss: 0.0671\n",
      "Epoch 38/300 - Train Loss: 0.0621, Val Loss: 0.0631\n",
      "Epoch 39/300 - Train Loss: 0.0620, Val Loss: 0.0743\n",
      "Epoch 40/300 - Train Loss: 0.0613, Val Loss: 0.0706\n",
      "Epoch 41/300 - Train Loss: 0.0604, Val Loss: 0.0624\n",
      "Epoch 42/300 - Train Loss: 0.0612, Val Loss: 0.0687\n",
      "Epoch 43/300 - Train Loss: 0.0581, Val Loss: 0.0690\n",
      "Epoch 44/300 - Train Loss: 0.0582, Val Loss: 0.0672\n",
      "Epoch 45/300 - Train Loss: 0.0567, Val Loss: 0.0673\n",
      "Epoch 46/300 - Train Loss: 0.0603, Val Loss: 0.0743\n",
      "Epoch 47/300 - Train Loss: 0.0580, Val Loss: 0.0650\n",
      "Epoch 48/300 - Train Loss: 0.0557, Val Loss: 0.0711\n",
      "Epoch 49/300 - Train Loss: 0.0560, Val Loss: 0.0656\n",
      "Epoch 50/300 - Train Loss: 0.0574, Val Loss: 0.0651\n",
      "Epoch 51/300 - Train Loss: 0.0582, Val Loss: 0.0620\n",
      "Epoch 52/300 - Train Loss: 0.0543, Val Loss: 0.0634\n",
      "Epoch 53/300 - Train Loss: 0.0543, Val Loss: 0.0692\n",
      "Epoch 54/300 - Train Loss: 0.0550, Val Loss: 0.0712\n",
      "Epoch 55/300 - Train Loss: 0.0543, Val Loss: 0.0639\n",
      "Epoch 56/300 - Train Loss: 0.0537, Val Loss: 0.0630\n",
      "Epoch 57/300 - Train Loss: 0.0537, Val Loss: 0.0664\n",
      "Epoch 58/300 - Train Loss: 0.0538, Val Loss: 0.0670\n",
      "Epoch 59/300 - Train Loss: 0.0534, Val Loss: 0.0649\n",
      "Epoch 60/300 - Train Loss: 0.0525, Val Loss: 0.0703\n",
      "Epoch 61/300 - Train Loss: 0.0526, Val Loss: 0.0674\n",
      "Epoch 62/300 - Train Loss: 0.0506, Val Loss: 0.0655\n",
      "Epoch 63/300 - Train Loss: 0.0506, Val Loss: 0.0669\n",
      "Epoch 64/300 - Train Loss: 0.0509, Val Loss: 0.0654\n",
      "Epoch 65/300 - Train Loss: 0.0508, Val Loss: 0.0650\n",
      "Epoch 66/300 - Train Loss: 0.0503, Val Loss: 0.0683\n",
      "Epoch 67/300 - Train Loss: 0.0514, Val Loss: 0.0759\n",
      "Epoch 68/300 - Train Loss: 0.0525, Val Loss: 0.0668\n",
      "Epoch 69/300 - Train Loss: 0.0505, Val Loss: 0.0658\n",
      "Epoch 70/300 - Train Loss: 0.0509, Val Loss: 0.0661\n",
      "Epoch 71/300 - Train Loss: 0.0497, Val Loss: 0.0687\n",
      "Epoch 72/300 - Train Loss: 0.0473, Val Loss: 0.0673\n",
      "Epoch 73/300 - Train Loss: 0.0494, Val Loss: 0.0694\n",
      "Epoch 74/300 - Train Loss: 0.0486, Val Loss: 0.0669\n",
      "Epoch 75/300 - Train Loss: 0.0474, Val Loss: 0.0654\n",
      "Epoch 76/300 - Train Loss: 0.0459, Val Loss: 0.0679\n",
      "Epoch 77/300 - Train Loss: 0.0472, Val Loss: 0.0663\n",
      "Epoch 78/300 - Train Loss: 0.0480, Val Loss: 0.0661\n",
      "Epoch 79/300 - Train Loss: 0.0472, Val Loss: 0.0676\n",
      "Epoch 80/300 - Train Loss: 0.0459, Val Loss: 0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:48:58,629] Trial 324 finished with value: 0.973899899535177 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.13594951741900677, 'learning_rate': 9.505791193944617e-05, 'batch_size': 32, 'weight_decay': 3.074574816832677e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300 - Train Loss: 0.0473, Val Loss: 0.0658\n",
      "Early stopping at epoch 81\n",
      "Macro F1 Score: 0.9739, Macro Precision: 0.9658, Macro Recall: 0.9827\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.98      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 326\n",
      "Training with F1=32, F2=32, D=2, dropout=0.12844180212549705, LR=9.461462081052395e-05, BS=32, WD=2.1129251480504966e-05\n",
      "Epoch 1/300 - Train Loss: 0.2732, Val Loss: 0.1154\n",
      "Epoch 2/300 - Train Loss: 0.1164, Val Loss: 0.0890\n",
      "Epoch 3/300 - Train Loss: 0.1019, Val Loss: 0.0776\n",
      "Epoch 4/300 - Train Loss: 0.0966, Val Loss: 0.0801\n",
      "Epoch 5/300 - Train Loss: 0.0913, Val Loss: 0.0765\n",
      "Epoch 6/300 - Train Loss: 0.0913, Val Loss: 0.0789\n",
      "Epoch 7/300 - Train Loss: 0.0892, Val Loss: 0.0747\n",
      "Epoch 8/300 - Train Loss: 0.0873, Val Loss: 0.0735\n",
      "Epoch 9/300 - Train Loss: 0.0859, Val Loss: 0.0807\n",
      "Epoch 10/300 - Train Loss: 0.0852, Val Loss: 0.0769\n",
      "Epoch 11/300 - Train Loss: 0.0828, Val Loss: 0.0721\n",
      "Epoch 12/300 - Train Loss: 0.0832, Val Loss: 0.0724\n",
      "Epoch 13/300 - Train Loss: 0.0817, Val Loss: 0.0768\n",
      "Epoch 14/300 - Train Loss: 0.0806, Val Loss: 0.0762\n",
      "Epoch 15/300 - Train Loss: 0.0780, Val Loss: 0.0705\n",
      "Epoch 16/300 - Train Loss: 0.0798, Val Loss: 0.0708\n",
      "Epoch 17/300 - Train Loss: 0.0766, Val Loss: 0.0702\n",
      "Epoch 18/300 - Train Loss: 0.0755, Val Loss: 0.0676\n",
      "Epoch 19/300 - Train Loss: 0.0748, Val Loss: 0.0683\n",
      "Epoch 20/300 - Train Loss: 0.0739, Val Loss: 0.0670\n",
      "Epoch 21/300 - Train Loss: 0.0724, Val Loss: 0.0689\n",
      "Epoch 22/300 - Train Loss: 0.0725, Val Loss: 0.0774\n",
      "Epoch 23/300 - Train Loss: 0.0727, Val Loss: 0.0654\n",
      "Epoch 24/300 - Train Loss: 0.0711, Val Loss: 0.0719\n",
      "Epoch 25/300 - Train Loss: 0.0692, Val Loss: 0.0663\n",
      "Epoch 26/300 - Train Loss: 0.0703, Val Loss: 0.0654\n",
      "Epoch 27/300 - Train Loss: 0.0692, Val Loss: 0.0681\n",
      "Epoch 28/300 - Train Loss: 0.0687, Val Loss: 0.0663\n",
      "Epoch 29/300 - Train Loss: 0.0678, Val Loss: 0.0701\n",
      "Epoch 30/300 - Train Loss: 0.0650, Val Loss: 0.0688\n",
      "Epoch 31/300 - Train Loss: 0.0672, Val Loss: 0.0660\n",
      "Epoch 32/300 - Train Loss: 0.0667, Val Loss: 0.0675\n",
      "Epoch 33/300 - Train Loss: 0.0660, Val Loss: 0.0650\n",
      "Epoch 34/300 - Train Loss: 0.0658, Val Loss: 0.0737\n",
      "Epoch 35/300 - Train Loss: 0.0655, Val Loss: 0.0693\n",
      "Epoch 36/300 - Train Loss: 0.0616, Val Loss: 0.0700\n",
      "Epoch 37/300 - Train Loss: 0.0622, Val Loss: 0.0670\n",
      "Epoch 38/300 - Train Loss: 0.0635, Val Loss: 0.0792\n",
      "Epoch 39/300 - Train Loss: 0.0624, Val Loss: 0.0723\n",
      "Epoch 40/300 - Train Loss: 0.0615, Val Loss: 0.0686\n",
      "Epoch 41/300 - Train Loss: 0.0601, Val Loss: 0.0708\n",
      "Epoch 42/300 - Train Loss: 0.0611, Val Loss: 0.0733\n",
      "Epoch 43/300 - Train Loss: 0.0595, Val Loss: 0.0728\n",
      "Epoch 44/300 - Train Loss: 0.0594, Val Loss: 0.0742\n",
      "Epoch 45/300 - Train Loss: 0.0590, Val Loss: 0.0721\n",
      "Epoch 46/300 - Train Loss: 0.0594, Val Loss: 0.0708\n",
      "Epoch 47/300 - Train Loss: 0.0598, Val Loss: 0.0710\n",
      "Epoch 48/300 - Train Loss: 0.0561, Val Loss: 0.0685\n",
      "Epoch 49/300 - Train Loss: 0.0583, Val Loss: 0.0653\n",
      "Epoch 50/300 - Train Loss: 0.0574, Val Loss: 0.0701\n",
      "Epoch 51/300 - Train Loss: 0.0564, Val Loss: 0.0664\n",
      "Epoch 52/300 - Train Loss: 0.0561, Val Loss: 0.0680\n",
      "Epoch 53/300 - Train Loss: 0.0561, Val Loss: 0.0699\n",
      "Epoch 54/300 - Train Loss: 0.0573, Val Loss: 0.0665\n",
      "Epoch 55/300 - Train Loss: 0.0558, Val Loss: 0.0705\n",
      "Epoch 56/300 - Train Loss: 0.0543, Val Loss: 0.0695\n",
      "Epoch 57/300 - Train Loss: 0.0530, Val Loss: 0.0690\n",
      "Epoch 58/300 - Train Loss: 0.0531, Val Loss: 0.0688\n",
      "Epoch 59/300 - Train Loss: 0.0536, Val Loss: 0.0682\n",
      "Epoch 60/300 - Train Loss: 0.0514, Val Loss: 0.0692\n",
      "Epoch 61/300 - Train Loss: 0.0555, Val Loss: 0.0672\n",
      "Epoch 62/300 - Train Loss: 0.0520, Val Loss: 0.0683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:51:50,655] Trial 325 finished with value: 0.9716109021533699 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.12844180212549705, 'learning_rate': 9.461462081052395e-05, 'batch_size': 32, 'weight_decay': 2.1129251480504966e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300 - Train Loss: 0.0550, Val Loss: 0.0678\n",
      "Early stopping at epoch 63\n",
      "Macro F1 Score: 0.9716, Macro Precision: 0.9614, Macro Recall: 0.9829\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 327\n",
      "Training with F1=32, F2=32, D=2, dropout=0.11332315565729216, LR=0.00010379569307557132, BS=32, WD=3.264460345017326e-05\n",
      "Epoch 1/300 - Train Loss: 0.2663, Val Loss: 0.1189\n",
      "Epoch 2/300 - Train Loss: 0.1179, Val Loss: 0.0829\n",
      "Epoch 3/300 - Train Loss: 0.1008, Val Loss: 0.0785\n",
      "Epoch 4/300 - Train Loss: 0.0951, Val Loss: 0.0748\n",
      "Epoch 5/300 - Train Loss: 0.0905, Val Loss: 0.0824\n",
      "Epoch 6/300 - Train Loss: 0.0895, Val Loss: 0.0742\n",
      "Epoch 7/300 - Train Loss: 0.0861, Val Loss: 0.0712\n",
      "Epoch 8/300 - Train Loss: 0.0866, Val Loss: 0.0767\n",
      "Epoch 9/300 - Train Loss: 0.0827, Val Loss: 0.0751\n",
      "Epoch 10/300 - Train Loss: 0.0829, Val Loss: 0.0735\n",
      "Epoch 11/300 - Train Loss: 0.0803, Val Loss: 0.0750\n",
      "Epoch 12/300 - Train Loss: 0.0787, Val Loss: 0.0717\n",
      "Epoch 13/300 - Train Loss: 0.0774, Val Loss: 0.0742\n",
      "Epoch 14/300 - Train Loss: 0.0754, Val Loss: 0.0789\n",
      "Epoch 15/300 - Train Loss: 0.0763, Val Loss: 0.0767\n",
      "Epoch 16/300 - Train Loss: 0.0750, Val Loss: 0.0729\n",
      "Epoch 17/300 - Train Loss: 0.0720, Val Loss: 0.0712\n",
      "Epoch 18/300 - Train Loss: 0.0721, Val Loss: 0.0751\n",
      "Epoch 19/300 - Train Loss: 0.0716, Val Loss: 0.0727\n",
      "Epoch 20/300 - Train Loss: 0.0741, Val Loss: 0.0759\n",
      "Epoch 21/300 - Train Loss: 0.0693, Val Loss: 0.0741\n",
      "Epoch 22/300 - Train Loss: 0.0696, Val Loss: 0.0703\n",
      "Epoch 23/300 - Train Loss: 0.0680, Val Loss: 0.0713\n",
      "Epoch 24/300 - Train Loss: 0.0692, Val Loss: 0.0703\n",
      "Epoch 25/300 - Train Loss: 0.0678, Val Loss: 0.0779\n",
      "Epoch 26/300 - Train Loss: 0.0655, Val Loss: 0.0672\n",
      "Epoch 27/300 - Train Loss: 0.0681, Val Loss: 0.0719\n",
      "Epoch 28/300 - Train Loss: 0.0653, Val Loss: 0.0699\n",
      "Epoch 29/300 - Train Loss: 0.0650, Val Loss: 0.0719\n",
      "Epoch 30/300 - Train Loss: 0.0644, Val Loss: 0.0680\n",
      "Epoch 31/300 - Train Loss: 0.0639, Val Loss: 0.0786\n",
      "Epoch 32/300 - Train Loss: 0.0637, Val Loss: 0.0725\n",
      "Epoch 33/300 - Train Loss: 0.0629, Val Loss: 0.0733\n",
      "Epoch 34/300 - Train Loss: 0.0625, Val Loss: 0.0717\n",
      "Epoch 35/300 - Train Loss: 0.0616, Val Loss: 0.0747\n",
      "Epoch 36/300 - Train Loss: 0.0604, Val Loss: 0.0683\n",
      "Epoch 37/300 - Train Loss: 0.0611, Val Loss: 0.0653\n",
      "Epoch 38/300 - Train Loss: 0.0621, Val Loss: 0.0751\n",
      "Epoch 39/300 - Train Loss: 0.0601, Val Loss: 0.0812\n",
      "Epoch 40/300 - Train Loss: 0.0587, Val Loss: 0.0714\n",
      "Epoch 41/300 - Train Loss: 0.0581, Val Loss: 0.0735\n",
      "Epoch 42/300 - Train Loss: 0.0584, Val Loss: 0.0688\n",
      "Epoch 43/300 - Train Loss: 0.0574, Val Loss: 0.0665\n",
      "Epoch 44/300 - Train Loss: 0.0553, Val Loss: 0.0688\n",
      "Epoch 45/300 - Train Loss: 0.0588, Val Loss: 0.0660\n",
      "Epoch 46/300 - Train Loss: 0.0551, Val Loss: 0.0711\n",
      "Epoch 47/300 - Train Loss: 0.0551, Val Loss: 0.0707\n",
      "Epoch 48/300 - Train Loss: 0.0554, Val Loss: 0.0706\n",
      "Epoch 49/300 - Train Loss: 0.0563, Val Loss: 0.0715\n",
      "Epoch 50/300 - Train Loss: 0.0533, Val Loss: 0.0730\n",
      "Epoch 51/300 - Train Loss: 0.0558, Val Loss: 0.0721\n",
      "Epoch 52/300 - Train Loss: 0.0560, Val Loss: 0.0790\n",
      "Epoch 53/300 - Train Loss: 0.0523, Val Loss: 0.0706\n",
      "Epoch 54/300 - Train Loss: 0.0519, Val Loss: 0.0713\n",
      "Epoch 55/300 - Train Loss: 0.0540, Val Loss: 0.0700\n",
      "Epoch 56/300 - Train Loss: 0.0506, Val Loss: 0.0723\n",
      "Epoch 57/300 - Train Loss: 0.0514, Val Loss: 0.0717\n",
      "Epoch 58/300 - Train Loss: 0.0527, Val Loss: 0.0709\n",
      "Epoch 59/300 - Train Loss: 0.0502, Val Loss: 0.0711\n",
      "Epoch 60/300 - Train Loss: 0.0490, Val Loss: 0.0691\n",
      "Epoch 61/300 - Train Loss: 0.0483, Val Loss: 0.0713\n",
      "Epoch 62/300 - Train Loss: 0.0515, Val Loss: 0.0783\n",
      "Epoch 63/300 - Train Loss: 0.0517, Val Loss: 0.0709\n",
      "Epoch 64/300 - Train Loss: 0.0476, Val Loss: 0.0721\n",
      "Epoch 65/300 - Train Loss: 0.0481, Val Loss: 0.0725\n",
      "Epoch 66/300 - Train Loss: 0.0478, Val Loss: 0.0736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:54:53,718] Trial 326 finished with value: 0.9655010309573072 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.11332315565729216, 'learning_rate': 0.00010379569307557132, 'batch_size': 32, 'weight_decay': 3.264460345017326e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/300 - Train Loss: 0.0521, Val Loss: 0.0724\n",
      "Early stopping at epoch 67\n",
      "Macro F1 Score: 0.9655, Macro Precision: 0.9593, Macro Recall: 0.9721\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 328\n",
      "Training with F1=32, F2=32, D=2, dropout=0.163307013324207, LR=9.085700819277894e-05, BS=32, WD=3.6000993951948275e-05\n",
      "Epoch 1/300 - Train Loss: 0.2711, Val Loss: 0.1205\n",
      "Epoch 2/300 - Train Loss: 0.1193, Val Loss: 0.0943\n",
      "Epoch 3/300 - Train Loss: 0.1099, Val Loss: 0.0788\n",
      "Epoch 4/300 - Train Loss: 0.1006, Val Loss: 0.0880\n",
      "Epoch 5/300 - Train Loss: 0.0994, Val Loss: 0.0753\n",
      "Epoch 6/300 - Train Loss: 0.0922, Val Loss: 0.0779\n",
      "Epoch 7/300 - Train Loss: 0.0879, Val Loss: 0.0852\n",
      "Epoch 8/300 - Train Loss: 0.0877, Val Loss: 0.0704\n",
      "Epoch 9/300 - Train Loss: 0.0874, Val Loss: 0.0699\n",
      "Epoch 10/300 - Train Loss: 0.0856, Val Loss: 0.0727\n",
      "Epoch 11/300 - Train Loss: 0.0849, Val Loss: 0.0778\n",
      "Epoch 12/300 - Train Loss: 0.0835, Val Loss: 0.0733\n",
      "Epoch 13/300 - Train Loss: 0.0812, Val Loss: 0.0706\n",
      "Epoch 14/300 - Train Loss: 0.0808, Val Loss: 0.0754\n",
      "Epoch 15/300 - Train Loss: 0.0807, Val Loss: 0.0709\n",
      "Epoch 16/300 - Train Loss: 0.0763, Val Loss: 0.0701\n",
      "Epoch 17/300 - Train Loss: 0.0766, Val Loss: 0.0688\n",
      "Epoch 18/300 - Train Loss: 0.0769, Val Loss: 0.0800\n",
      "Epoch 19/300 - Train Loss: 0.0752, Val Loss: 0.0696\n",
      "Epoch 20/300 - Train Loss: 0.0767, Val Loss: 0.0725\n",
      "Epoch 21/300 - Train Loss: 0.0752, Val Loss: 0.0711\n",
      "Epoch 22/300 - Train Loss: 0.0738, Val Loss: 0.0719\n",
      "Epoch 23/300 - Train Loss: 0.0743, Val Loss: 0.0768\n",
      "Epoch 24/300 - Train Loss: 0.0745, Val Loss: 0.0765\n",
      "Epoch 25/300 - Train Loss: 0.0741, Val Loss: 0.0777\n",
      "Epoch 26/300 - Train Loss: 0.0720, Val Loss: 0.0713\n",
      "Epoch 27/300 - Train Loss: 0.0716, Val Loss: 0.0743\n",
      "Epoch 28/300 - Train Loss: 0.0714, Val Loss: 0.0738\n",
      "Epoch 29/300 - Train Loss: 0.0724, Val Loss: 0.0786\n",
      "Epoch 30/300 - Train Loss: 0.0692, Val Loss: 0.0730\n",
      "Epoch 31/300 - Train Loss: 0.0711, Val Loss: 0.0681\n",
      "Epoch 32/300 - Train Loss: 0.0679, Val Loss: 0.0689\n",
      "Epoch 33/300 - Train Loss: 0.0700, Val Loss: 0.0725\n",
      "Epoch 34/300 - Train Loss: 0.0685, Val Loss: 0.0708\n",
      "Epoch 35/300 - Train Loss: 0.0695, Val Loss: 0.0731\n",
      "Epoch 36/300 - Train Loss: 0.0672, Val Loss: 0.0760\n",
      "Epoch 37/300 - Train Loss: 0.0655, Val Loss: 0.0739\n",
      "Epoch 38/300 - Train Loss: 0.0681, Val Loss: 0.0716\n",
      "Epoch 39/300 - Train Loss: 0.0653, Val Loss: 0.0710\n",
      "Epoch 40/300 - Train Loss: 0.0667, Val Loss: 0.0720\n",
      "Epoch 41/300 - Train Loss: 0.0637, Val Loss: 0.0804\n",
      "Epoch 42/300 - Train Loss: 0.0659, Val Loss: 0.0735\n",
      "Epoch 43/300 - Train Loss: 0.0638, Val Loss: 0.0699\n",
      "Epoch 44/300 - Train Loss: 0.0614, Val Loss: 0.0694\n",
      "Epoch 45/300 - Train Loss: 0.0626, Val Loss: 0.0738\n",
      "Epoch 46/300 - Train Loss: 0.0640, Val Loss: 0.0700\n",
      "Epoch 47/300 - Train Loss: 0.0617, Val Loss: 0.0743\n",
      "Epoch 48/300 - Train Loss: 0.0609, Val Loss: 0.0765\n",
      "Epoch 49/300 - Train Loss: 0.0598, Val Loss: 0.0719\n",
      "Epoch 50/300 - Train Loss: 0.0623, Val Loss: 0.0712\n",
      "Epoch 51/300 - Train Loss: 0.0607, Val Loss: 0.0736\n",
      "Epoch 52/300 - Train Loss: 0.0630, Val Loss: 0.0748\n",
      "Epoch 53/300 - Train Loss: 0.0632, Val Loss: 0.0693\n",
      "Epoch 54/300 - Train Loss: 0.0625, Val Loss: 0.0792\n",
      "Epoch 55/300 - Train Loss: 0.0597, Val Loss: 0.0678\n",
      "Epoch 56/300 - Train Loss: 0.0582, Val Loss: 0.0720\n",
      "Epoch 57/300 - Train Loss: 0.0595, Val Loss: 0.0685\n",
      "Epoch 58/300 - Train Loss: 0.0605, Val Loss: 0.0729\n",
      "Epoch 59/300 - Train Loss: 0.0575, Val Loss: 0.0734\n",
      "Epoch 60/300 - Train Loss: 0.0562, Val Loss: 0.0742\n",
      "Epoch 61/300 - Train Loss: 0.0572, Val Loss: 0.0696\n",
      "Epoch 62/300 - Train Loss: 0.0574, Val Loss: 0.0688\n",
      "Epoch 63/300 - Train Loss: 0.0615, Val Loss: 0.0743\n",
      "Epoch 64/300 - Train Loss: 0.0573, Val Loss: 0.0724\n",
      "Epoch 65/300 - Train Loss: 0.0559, Val Loss: 0.0713\n",
      "Epoch 66/300 - Train Loss: 0.0574, Val Loss: 0.1047\n",
      "Epoch 67/300 - Train Loss: 0.0552, Val Loss: 0.0704\n",
      "Epoch 68/300 - Train Loss: 0.0552, Val Loss: 0.0720\n",
      "Epoch 69/300 - Train Loss: 0.0569, Val Loss: 0.0690\n",
      "Epoch 70/300 - Train Loss: 0.0567, Val Loss: 0.0735\n",
      "Epoch 71/300 - Train Loss: 0.0562, Val Loss: 0.0722\n",
      "Epoch 72/300 - Train Loss: 0.0543, Val Loss: 0.0717\n",
      "Epoch 73/300 - Train Loss: 0.0540, Val Loss: 0.0711\n",
      "Epoch 74/300 - Train Loss: 0.0520, Val Loss: 0.0719\n",
      "Epoch 75/300 - Train Loss: 0.0524, Val Loss: 0.0765\n",
      "Epoch 76/300 - Train Loss: 0.0516, Val Loss: 0.0689\n",
      "Epoch 77/300 - Train Loss: 0.0537, Val Loss: 0.0750\n",
      "Epoch 78/300 - Train Loss: 0.0537, Val Loss: 0.0734\n",
      "Epoch 79/300 - Train Loss: 0.0520, Val Loss: 0.0734\n",
      "Epoch 80/300 - Train Loss: 0.0518, Val Loss: 0.0707\n",
      "Epoch 81/300 - Train Loss: 0.0529, Val Loss: 0.0706\n",
      "Epoch 82/300 - Train Loss: 0.0517, Val Loss: 0.0719\n",
      "Epoch 83/300 - Train Loss: 0.0534, Val Loss: 0.0781\n",
      "Epoch 84/300 - Train Loss: 0.0521, Val Loss: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 20:58:46,011] Trial 327 finished with value: 0.9645780723171725 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.163307013324207, 'learning_rate': 9.085700819277894e-05, 'batch_size': 32, 'weight_decay': 3.6000993951948275e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300 - Train Loss: 0.0504, Val Loss: 0.0772\n",
      "Early stopping at epoch 85\n",
      "Macro F1 Score: 0.9646, Macro Precision: 0.9588, Macro Recall: 0.9708\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 329\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1522442723102698, LR=0.00010951521591963147, BS=32, WD=2.9241936414946886e-05\n",
      "Epoch 1/300 - Train Loss: 0.3471, Val Loss: 0.1519\n",
      "Epoch 2/300 - Train Loss: 0.1414, Val Loss: 0.1045\n",
      "Epoch 3/300 - Train Loss: 0.1139, Val Loss: 0.1000\n",
      "Epoch 4/300 - Train Loss: 0.1080, Val Loss: 0.0820\n",
      "Epoch 5/300 - Train Loss: 0.0989, Val Loss: 0.0821\n",
      "Epoch 6/300 - Train Loss: 0.0984, Val Loss: 0.0855\n",
      "Epoch 7/300 - Train Loss: 0.0947, Val Loss: 0.0755\n",
      "Epoch 8/300 - Train Loss: 0.0916, Val Loss: 0.0768\n",
      "Epoch 9/300 - Train Loss: 0.0904, Val Loss: 0.0740\n",
      "Epoch 10/300 - Train Loss: 0.0907, Val Loss: 0.0754\n",
      "Epoch 11/300 - Train Loss: 0.0898, Val Loss: 0.0731\n",
      "Epoch 12/300 - Train Loss: 0.0851, Val Loss: 0.0802\n",
      "Epoch 13/300 - Train Loss: 0.0873, Val Loss: 0.0717\n",
      "Epoch 14/300 - Train Loss: 0.0872, Val Loss: 0.0719\n",
      "Epoch 15/300 - Train Loss: 0.0861, Val Loss: 0.0722\n",
      "Epoch 16/300 - Train Loss: 0.0834, Val Loss: 0.0690\n",
      "Epoch 17/300 - Train Loss: 0.0829, Val Loss: 0.0707\n",
      "Epoch 18/300 - Train Loss: 0.0822, Val Loss: 0.0744\n",
      "Epoch 19/300 - Train Loss: 0.0828, Val Loss: 0.0735\n",
      "Epoch 20/300 - Train Loss: 0.0810, Val Loss: 0.0740\n",
      "Epoch 21/300 - Train Loss: 0.0820, Val Loss: 0.0797\n",
      "Epoch 22/300 - Train Loss: 0.0801, Val Loss: 0.0691\n",
      "Epoch 23/300 - Train Loss: 0.0790, Val Loss: 0.0692\n",
      "Epoch 24/300 - Train Loss: 0.0793, Val Loss: 0.0664\n",
      "Epoch 25/300 - Train Loss: 0.0786, Val Loss: 0.0728\n",
      "Epoch 26/300 - Train Loss: 0.0794, Val Loss: 0.0740\n",
      "Epoch 27/300 - Train Loss: 0.0781, Val Loss: 0.0676\n",
      "Epoch 28/300 - Train Loss: 0.0771, Val Loss: 0.0753\n",
      "Epoch 29/300 - Train Loss: 0.0781, Val Loss: 0.0716\n",
      "Epoch 30/300 - Train Loss: 0.0790, Val Loss: 0.0696\n",
      "Epoch 31/300 - Train Loss: 0.0771, Val Loss: 0.0664\n",
      "Epoch 32/300 - Train Loss: 0.0779, Val Loss: 0.0707\n",
      "Epoch 33/300 - Train Loss: 0.0792, Val Loss: 0.0700\n",
      "Epoch 34/300 - Train Loss: 0.0741, Val Loss: 0.0676\n",
      "Epoch 35/300 - Train Loss: 0.0778, Val Loss: 0.0695\n",
      "Epoch 36/300 - Train Loss: 0.0759, Val Loss: 0.0673\n",
      "Epoch 37/300 - Train Loss: 0.0751, Val Loss: 0.0666\n",
      "Epoch 38/300 - Train Loss: 0.0771, Val Loss: 0.0662\n",
      "Epoch 39/300 - Train Loss: 0.0752, Val Loss: 0.0692\n",
      "Epoch 40/300 - Train Loss: 0.0743, Val Loss: 0.0700\n",
      "Epoch 41/300 - Train Loss: 0.0761, Val Loss: 0.0687\n",
      "Epoch 42/300 - Train Loss: 0.0738, Val Loss: 0.0678\n",
      "Epoch 43/300 - Train Loss: 0.0723, Val Loss: 0.0640\n",
      "Epoch 44/300 - Train Loss: 0.0750, Val Loss: 0.0649\n",
      "Epoch 45/300 - Train Loss: 0.0765, Val Loss: 0.0682\n",
      "Epoch 46/300 - Train Loss: 0.0736, Val Loss: 0.0685\n",
      "Epoch 47/300 - Train Loss: 0.0743, Val Loss: 0.0639\n",
      "Epoch 48/300 - Train Loss: 0.0749, Val Loss: 0.0710\n",
      "Epoch 49/300 - Train Loss: 0.0748, Val Loss: 0.0688\n",
      "Epoch 50/300 - Train Loss: 0.0729, Val Loss: 0.0746\n",
      "Epoch 51/300 - Train Loss: 0.0729, Val Loss: 0.0670\n",
      "Epoch 52/300 - Train Loss: 0.0708, Val Loss: 0.0640\n",
      "Epoch 53/300 - Train Loss: 0.0715, Val Loss: 0.0647\n",
      "Epoch 54/300 - Train Loss: 0.0701, Val Loss: 0.0661\n",
      "Epoch 55/300 - Train Loss: 0.0712, Val Loss: 0.0658\n",
      "Epoch 56/300 - Train Loss: 0.0713, Val Loss: 0.0646\n",
      "Epoch 57/300 - Train Loss: 0.0722, Val Loss: 0.0658\n",
      "Epoch 58/300 - Train Loss: 0.0704, Val Loss: 0.0653\n",
      "Epoch 59/300 - Train Loss: 0.0705, Val Loss: 0.0686\n",
      "Epoch 60/300 - Train Loss: 0.0700, Val Loss: 0.0725\n",
      "Epoch 61/300 - Train Loss: 0.0698, Val Loss: 0.0681\n",
      "Epoch 62/300 - Train Loss: 0.0694, Val Loss: 0.0674\n",
      "Epoch 63/300 - Train Loss: 0.0705, Val Loss: 0.0634\n",
      "Epoch 64/300 - Train Loss: 0.0694, Val Loss: 0.0643\n",
      "Epoch 65/300 - Train Loss: 0.0686, Val Loss: 0.0651\n",
      "Epoch 66/300 - Train Loss: 0.0695, Val Loss: 0.0653\n",
      "Epoch 67/300 - Train Loss: 0.0690, Val Loss: 0.0713\n",
      "Epoch 68/300 - Train Loss: 0.0682, Val Loss: 0.0646\n",
      "Epoch 69/300 - Train Loss: 0.0697, Val Loss: 0.0649\n",
      "Epoch 70/300 - Train Loss: 0.0685, Val Loss: 0.0672\n",
      "Epoch 71/300 - Train Loss: 0.0665, Val Loss: 0.0680\n",
      "Epoch 72/300 - Train Loss: 0.0710, Val Loss: 0.0631\n",
      "Epoch 73/300 - Train Loss: 0.0675, Val Loss: 0.0656\n",
      "Epoch 74/300 - Train Loss: 0.0679, Val Loss: 0.0676\n",
      "Epoch 75/300 - Train Loss: 0.0677, Val Loss: 0.0639\n",
      "Epoch 76/300 - Train Loss: 0.0666, Val Loss: 0.0640\n",
      "Epoch 77/300 - Train Loss: 0.0672, Val Loss: 0.0642\n",
      "Epoch 78/300 - Train Loss: 0.0669, Val Loss: 0.0648\n",
      "Epoch 79/300 - Train Loss: 0.0669, Val Loss: 0.0654\n",
      "Epoch 80/300 - Train Loss: 0.0658, Val Loss: 0.0634\n",
      "Epoch 81/300 - Train Loss: 0.0654, Val Loss: 0.0603\n",
      "Epoch 82/300 - Train Loss: 0.0660, Val Loss: 0.0642\n",
      "Epoch 83/300 - Train Loss: 0.0664, Val Loss: 0.0644\n",
      "Epoch 84/300 - Train Loss: 0.0677, Val Loss: 0.0604\n",
      "Epoch 85/300 - Train Loss: 0.0657, Val Loss: 0.0635\n",
      "Epoch 86/300 - Train Loss: 0.0668, Val Loss: 0.0670\n",
      "Epoch 87/300 - Train Loss: 0.0664, Val Loss: 0.0626\n",
      "Epoch 88/300 - Train Loss: 0.0649, Val Loss: 0.0619\n",
      "Epoch 89/300 - Train Loss: 0.0657, Val Loss: 0.0641\n",
      "Epoch 90/300 - Train Loss: 0.0629, Val Loss: 0.0627\n",
      "Epoch 91/300 - Train Loss: 0.0649, Val Loss: 0.0640\n",
      "Epoch 92/300 - Train Loss: 0.0652, Val Loss: 0.0608\n",
      "Epoch 93/300 - Train Loss: 0.0638, Val Loss: 0.0664\n",
      "Epoch 94/300 - Train Loss: 0.0656, Val Loss: 0.0673\n",
      "Epoch 95/300 - Train Loss: 0.0660, Val Loss: 0.0662\n",
      "Epoch 96/300 - Train Loss: 0.0642, Val Loss: 0.0642\n",
      "Epoch 97/300 - Train Loss: 0.0666, Val Loss: 0.0661\n",
      "Epoch 98/300 - Train Loss: 0.0641, Val Loss: 0.0663\n",
      "Epoch 99/300 - Train Loss: 0.0665, Val Loss: 0.0671\n",
      "Epoch 100/300 - Train Loss: 0.0635, Val Loss: 0.0629\n",
      "Epoch 101/300 - Train Loss: 0.0635, Val Loss: 0.0643\n",
      "Epoch 102/300 - Train Loss: 0.0639, Val Loss: 0.0615\n",
      "Epoch 103/300 - Train Loss: 0.0647, Val Loss: 0.0626\n",
      "Epoch 104/300 - Train Loss: 0.0609, Val Loss: 0.0616\n",
      "Epoch 105/300 - Train Loss: 0.0634, Val Loss: 0.0648\n",
      "Epoch 106/300 - Train Loss: 0.0632, Val Loss: 0.0622\n",
      "Epoch 107/300 - Train Loss: 0.0623, Val Loss: 0.0640\n",
      "Epoch 108/300 - Train Loss: 0.0612, Val Loss: 0.0632\n",
      "Epoch 109/300 - Train Loss: 0.0640, Val Loss: 0.0625\n",
      "Epoch 110/300 - Train Loss: 0.0647, Val Loss: 0.0644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:03:24,988] Trial 328 finished with value: 0.9678560881787925 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1522442723102698, 'learning_rate': 0.00010951521591963147, 'batch_size': 32, 'weight_decay': 2.9241936414946886e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 111/300 - Train Loss: 0.0620, Val Loss: 0.0634\n",
      "Early stopping at epoch 111\n",
      "Macro F1 Score: 0.9679, Macro Precision: 0.9640, Macro Recall: 0.9719\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 330\n",
      "Training with F1=32, F2=32, D=2, dropout=0.12461181424295843, LR=8.764852207152685e-05, BS=32, WD=2.6381499358895678e-05\n",
      "Epoch 1/300 - Train Loss: 0.2865, Val Loss: 0.1146\n",
      "Epoch 2/300 - Train Loss: 0.1174, Val Loss: 0.0862\n",
      "Epoch 3/300 - Train Loss: 0.1007, Val Loss: 0.0859\n",
      "Epoch 4/300 - Train Loss: 0.0965, Val Loss: 0.0783\n",
      "Epoch 5/300 - Train Loss: 0.0927, Val Loss: 0.0730\n",
      "Epoch 6/300 - Train Loss: 0.0869, Val Loss: 0.0858\n",
      "Epoch 7/300 - Train Loss: 0.0875, Val Loss: 0.0752\n",
      "Epoch 8/300 - Train Loss: 0.0871, Val Loss: 0.0853\n",
      "Epoch 9/300 - Train Loss: 0.0825, Val Loss: 0.0728\n",
      "Epoch 10/300 - Train Loss: 0.0818, Val Loss: 0.0715\n",
      "Epoch 11/300 - Train Loss: 0.0801, Val Loss: 0.0736\n",
      "Epoch 12/300 - Train Loss: 0.0802, Val Loss: 0.0716\n",
      "Epoch 13/300 - Train Loss: 0.0791, Val Loss: 0.0817\n",
      "Epoch 14/300 - Train Loss: 0.0812, Val Loss: 0.0808\n",
      "Epoch 15/300 - Train Loss: 0.0776, Val Loss: 0.0731\n",
      "Epoch 16/300 - Train Loss: 0.0774, Val Loss: 0.0649\n",
      "Epoch 17/300 - Train Loss: 0.0753, Val Loss: 0.0736\n",
      "Epoch 18/300 - Train Loss: 0.0754, Val Loss: 0.0683\n",
      "Epoch 19/300 - Train Loss: 0.0738, Val Loss: 0.0674\n",
      "Epoch 20/300 - Train Loss: 0.0714, Val Loss: 0.0733\n",
      "Epoch 21/300 - Train Loss: 0.0728, Val Loss: 0.0735\n",
      "Epoch 22/300 - Train Loss: 0.0727, Val Loss: 0.0703\n",
      "Epoch 23/300 - Train Loss: 0.0712, Val Loss: 0.0720\n",
      "Epoch 24/300 - Train Loss: 0.0712, Val Loss: 0.0730\n",
      "Epoch 25/300 - Train Loss: 0.0710, Val Loss: 0.0682\n",
      "Epoch 26/300 - Train Loss: 0.0698, Val Loss: 0.0766\n",
      "Epoch 27/300 - Train Loss: 0.0700, Val Loss: 0.0851\n",
      "Epoch 28/300 - Train Loss: 0.0683, Val Loss: 0.0738\n",
      "Epoch 29/300 - Train Loss: 0.0669, Val Loss: 0.0699\n",
      "Epoch 30/300 - Train Loss: 0.0671, Val Loss: 0.0730\n",
      "Epoch 31/300 - Train Loss: 0.0668, Val Loss: 0.0712\n",
      "Epoch 32/300 - Train Loss: 0.0664, Val Loss: 0.0694\n",
      "Epoch 33/300 - Train Loss: 0.0668, Val Loss: 0.0734\n",
      "Epoch 34/300 - Train Loss: 0.0644, Val Loss: 0.0671\n",
      "Epoch 35/300 - Train Loss: 0.0625, Val Loss: 0.0665\n",
      "Epoch 36/300 - Train Loss: 0.0650, Val Loss: 0.0667\n",
      "Epoch 37/300 - Train Loss: 0.0645, Val Loss: 0.0687\n",
      "Epoch 38/300 - Train Loss: 0.0647, Val Loss: 0.0687\n",
      "Epoch 39/300 - Train Loss: 0.0624, Val Loss: 0.0729\n",
      "Epoch 40/300 - Train Loss: 0.0607, Val Loss: 0.0707\n",
      "Epoch 41/300 - Train Loss: 0.0643, Val Loss: 0.0670\n",
      "Epoch 42/300 - Train Loss: 0.0633, Val Loss: 0.0702\n",
      "Epoch 43/300 - Train Loss: 0.0616, Val Loss: 0.0726\n",
      "Epoch 44/300 - Train Loss: 0.0608, Val Loss: 0.0691\n",
      "Epoch 45/300 - Train Loss: 0.0615, Val Loss: 0.0665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:05:30,987] Trial 329 finished with value: 0.9665778066201794 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.12461181424295843, 'learning_rate': 8.764852207152685e-05, 'batch_size': 32, 'weight_decay': 2.6381499358895678e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300 - Train Loss: 0.0599, Val Loss: 0.0698\n",
      "Early stopping at epoch 46\n",
      "Macro F1 Score: 0.9666, Macro Precision: 0.9768, Macro Recall: 0.9571\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.96      0.90      0.93        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 331\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1826799644394827, LR=9.389477000300498e-05, BS=32, WD=4.011672335105505e-05\n",
      "Epoch 1/300 - Train Loss: 0.2677, Val Loss: 0.1258\n",
      "Epoch 2/300 - Train Loss: 0.1214, Val Loss: 0.0838\n",
      "Epoch 3/300 - Train Loss: 0.1014, Val Loss: 0.0774\n",
      "Epoch 4/300 - Train Loss: 0.0944, Val Loss: 0.0755\n",
      "Epoch 5/300 - Train Loss: 0.0944, Val Loss: 0.0754\n",
      "Epoch 6/300 - Train Loss: 0.0899, Val Loss: 0.0786\n",
      "Epoch 7/300 - Train Loss: 0.0876, Val Loss: 0.0798\n",
      "Epoch 8/300 - Train Loss: 0.0851, Val Loss: 0.0778\n",
      "Epoch 9/300 - Train Loss: 0.0843, Val Loss: 0.0796\n",
      "Epoch 10/300 - Train Loss: 0.0835, Val Loss: 0.0750\n",
      "Epoch 11/300 - Train Loss: 0.0820, Val Loss: 0.0713\n",
      "Epoch 12/300 - Train Loss: 0.0833, Val Loss: 0.0873\n",
      "Epoch 13/300 - Train Loss: 0.0820, Val Loss: 0.0677\n",
      "Epoch 14/300 - Train Loss: 0.0799, Val Loss: 0.0676\n",
      "Epoch 15/300 - Train Loss: 0.0803, Val Loss: 0.0855\n",
      "Epoch 16/300 - Train Loss: 0.0782, Val Loss: 0.0793\n",
      "Epoch 17/300 - Train Loss: 0.0786, Val Loss: 0.0722\n",
      "Epoch 18/300 - Train Loss: 0.0770, Val Loss: 0.0719\n",
      "Epoch 19/300 - Train Loss: 0.0780, Val Loss: 0.0753\n",
      "Epoch 20/300 - Train Loss: 0.0764, Val Loss: 0.0751\n",
      "Epoch 21/300 - Train Loss: 0.0743, Val Loss: 0.0702\n",
      "Epoch 22/300 - Train Loss: 0.0735, Val Loss: 0.0774\n",
      "Epoch 23/300 - Train Loss: 0.0760, Val Loss: 0.0724\n",
      "Epoch 24/300 - Train Loss: 0.0747, Val Loss: 0.0718\n",
      "Epoch 25/300 - Train Loss: 0.0735, Val Loss: 0.0698\n",
      "Epoch 26/300 - Train Loss: 0.0730, Val Loss: 0.0689\n",
      "Epoch 27/300 - Train Loss: 0.0721, Val Loss: 0.0689\n",
      "Epoch 28/300 - Train Loss: 0.0707, Val Loss: 0.0729\n",
      "Epoch 29/300 - Train Loss: 0.0691, Val Loss: 0.0738\n",
      "Epoch 30/300 - Train Loss: 0.0727, Val Loss: 0.0707\n",
      "Epoch 31/300 - Train Loss: 0.0684, Val Loss: 0.0689\n",
      "Epoch 32/300 - Train Loss: 0.0668, Val Loss: 0.0759\n",
      "Epoch 33/300 - Train Loss: 0.0682, Val Loss: 0.0707\n",
      "Epoch 34/300 - Train Loss: 0.0698, Val Loss: 0.0691\n",
      "Epoch 35/300 - Train Loss: 0.0714, Val Loss: 0.0675\n",
      "Epoch 36/300 - Train Loss: 0.0676, Val Loss: 0.0660\n",
      "Epoch 37/300 - Train Loss: 0.0677, Val Loss: 0.0730\n",
      "Epoch 38/300 - Train Loss: 0.0663, Val Loss: 0.0651\n",
      "Epoch 39/300 - Train Loss: 0.0664, Val Loss: 0.0685\n",
      "Epoch 40/300 - Train Loss: 0.0657, Val Loss: 0.0682\n",
      "Epoch 41/300 - Train Loss: 0.0664, Val Loss: 0.0683\n",
      "Epoch 42/300 - Train Loss: 0.0644, Val Loss: 0.0682\n",
      "Epoch 43/300 - Train Loss: 0.0643, Val Loss: 0.0665\n",
      "Epoch 44/300 - Train Loss: 0.0627, Val Loss: 0.0719\n",
      "Epoch 45/300 - Train Loss: 0.0626, Val Loss: 0.0719\n",
      "Epoch 46/300 - Train Loss: 0.0645, Val Loss: 0.0645\n",
      "Epoch 47/300 - Train Loss: 0.0616, Val Loss: 0.0663\n",
      "Epoch 48/300 - Train Loss: 0.0634, Val Loss: 0.0711\n",
      "Epoch 49/300 - Train Loss: 0.0614, Val Loss: 0.0727\n",
      "Epoch 50/300 - Train Loss: 0.0627, Val Loss: 0.0684\n",
      "Epoch 51/300 - Train Loss: 0.0616, Val Loss: 0.0677\n",
      "Epoch 52/300 - Train Loss: 0.0592, Val Loss: 0.0704\n",
      "Epoch 53/300 - Train Loss: 0.0607, Val Loss: 0.0699\n",
      "Epoch 54/300 - Train Loss: 0.0574, Val Loss: 0.0706\n",
      "Epoch 55/300 - Train Loss: 0.0600, Val Loss: 0.0683\n",
      "Epoch 56/300 - Train Loss: 0.0592, Val Loss: 0.0683\n",
      "Epoch 57/300 - Train Loss: 0.0591, Val Loss: 0.0644\n",
      "Epoch 58/300 - Train Loss: 0.0572, Val Loss: 0.0679\n",
      "Epoch 59/300 - Train Loss: 0.0597, Val Loss: 0.0708\n",
      "Epoch 60/300 - Train Loss: 0.0577, Val Loss: 0.0674\n",
      "Epoch 61/300 - Train Loss: 0.0586, Val Loss: 0.0653\n",
      "Epoch 62/300 - Train Loss: 0.0576, Val Loss: 0.0677\n",
      "Epoch 63/300 - Train Loss: 0.0561, Val Loss: 0.0698\n",
      "Epoch 64/300 - Train Loss: 0.0568, Val Loss: 0.0697\n",
      "Epoch 65/300 - Train Loss: 0.0558, Val Loss: 0.0692\n",
      "Epoch 66/300 - Train Loss: 0.0561, Val Loss: 0.0663\n",
      "Epoch 67/300 - Train Loss: 0.0555, Val Loss: 0.0701\n",
      "Epoch 68/300 - Train Loss: 0.0557, Val Loss: 0.0697\n",
      "Epoch 69/300 - Train Loss: 0.0563, Val Loss: 0.0689\n",
      "Epoch 70/300 - Train Loss: 0.0551, Val Loss: 0.0697\n",
      "Epoch 71/300 - Train Loss: 0.0555, Val Loss: 0.0725\n",
      "Epoch 72/300 - Train Loss: 0.0532, Val Loss: 0.0762\n",
      "Epoch 73/300 - Train Loss: 0.0544, Val Loss: 0.0687\n",
      "Epoch 74/300 - Train Loss: 0.0571, Val Loss: 0.0683\n",
      "Epoch 75/300 - Train Loss: 0.0518, Val Loss: 0.0727\n",
      "Epoch 76/300 - Train Loss: 0.0512, Val Loss: 0.0670\n",
      "Epoch 77/300 - Train Loss: 0.0527, Val Loss: 0.0730\n",
      "Epoch 78/300 - Train Loss: 0.0521, Val Loss: 0.0699\n",
      "Epoch 79/300 - Train Loss: 0.0530, Val Loss: 0.0778\n",
      "Epoch 80/300 - Train Loss: 0.0503, Val Loss: 0.0731\n",
      "Epoch 81/300 - Train Loss: 0.0527, Val Loss: 0.0682\n",
      "Epoch 82/300 - Train Loss: 0.0512, Val Loss: 0.0759\n",
      "Epoch 83/300 - Train Loss: 0.0521, Val Loss: 0.0737\n",
      "Epoch 84/300 - Train Loss: 0.0514, Val Loss: 0.0716\n",
      "Epoch 85/300 - Train Loss: 0.0521, Val Loss: 0.0681\n",
      "Epoch 86/300 - Train Loss: 0.0484, Val Loss: 0.0711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:09:29,321] Trial 330 finished with value: 0.9691071177165803 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1826799644394827, 'learning_rate': 9.389477000300498e-05, 'batch_size': 32, 'weight_decay': 4.011672335105505e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300 - Train Loss: 0.0498, Val Loss: 0.0720\n",
      "Early stopping at epoch 87\n",
      "Macro F1 Score: 0.9691, Macro Precision: 0.9609, Macro Recall: 0.9780\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 332\n",
      "Training with F1=32, F2=32, D=2, dropout=0.13476725401834866, LR=0.0001007602097414478, BS=32, WD=6.247813261715426e-05\n",
      "Epoch 1/300 - Train Loss: 0.2599, Val Loss: 0.1045\n",
      "Epoch 2/300 - Train Loss: 0.1104, Val Loss: 0.0759\n",
      "Epoch 3/300 - Train Loss: 0.0980, Val Loss: 0.0788\n",
      "Epoch 4/300 - Train Loss: 0.0934, Val Loss: 0.0754\n",
      "Epoch 5/300 - Train Loss: 0.0904, Val Loss: 0.0851\n",
      "Epoch 6/300 - Train Loss: 0.0862, Val Loss: 0.0729\n",
      "Epoch 7/300 - Train Loss: 0.0826, Val Loss: 0.0811\n",
      "Epoch 8/300 - Train Loss: 0.0824, Val Loss: 0.0722\n",
      "Epoch 9/300 - Train Loss: 0.0827, Val Loss: 0.0775\n",
      "Epoch 10/300 - Train Loss: 0.0797, Val Loss: 0.0718\n",
      "Epoch 11/300 - Train Loss: 0.0783, Val Loss: 0.0729\n",
      "Epoch 12/300 - Train Loss: 0.0782, Val Loss: 0.0741\n",
      "Epoch 13/300 - Train Loss: 0.0760, Val Loss: 0.0690\n",
      "Epoch 14/300 - Train Loss: 0.0782, Val Loss: 0.0694\n",
      "Epoch 15/300 - Train Loss: 0.0748, Val Loss: 0.0692\n",
      "Epoch 16/300 - Train Loss: 0.0780, Val Loss: 0.0687\n",
      "Epoch 17/300 - Train Loss: 0.0741, Val Loss: 0.0723\n",
      "Epoch 18/300 - Train Loss: 0.0740, Val Loss: 0.0686\n",
      "Epoch 19/300 - Train Loss: 0.0724, Val Loss: 0.0669\n",
      "Epoch 20/300 - Train Loss: 0.0730, Val Loss: 0.0726\n",
      "Epoch 21/300 - Train Loss: 0.0720, Val Loss: 0.0696\n",
      "Epoch 22/300 - Train Loss: 0.0702, Val Loss: 0.0722\n",
      "Epoch 23/300 - Train Loss: 0.0706, Val Loss: 0.0656\n",
      "Epoch 24/300 - Train Loss: 0.0701, Val Loss: 0.0714\n",
      "Epoch 25/300 - Train Loss: 0.0688, Val Loss: 0.0682\n",
      "Epoch 26/300 - Train Loss: 0.0708, Val Loss: 0.0708\n",
      "Epoch 27/300 - Train Loss: 0.0693, Val Loss: 0.0689\n",
      "Epoch 28/300 - Train Loss: 0.0664, Val Loss: 0.0696\n",
      "Epoch 29/300 - Train Loss: 0.0671, Val Loss: 0.0760\n",
      "Epoch 30/300 - Train Loss: 0.0669, Val Loss: 0.0691\n",
      "Epoch 31/300 - Train Loss: 0.0668, Val Loss: 0.0730\n",
      "Epoch 32/300 - Train Loss: 0.0650, Val Loss: 0.0815\n",
      "Epoch 33/300 - Train Loss: 0.0658, Val Loss: 0.0800\n",
      "Epoch 34/300 - Train Loss: 0.0647, Val Loss: 0.0687\n",
      "Epoch 35/300 - Train Loss: 0.0665, Val Loss: 0.0672\n",
      "Epoch 36/300 - Train Loss: 0.0622, Val Loss: 0.0673\n",
      "Epoch 37/300 - Train Loss: 0.0638, Val Loss: 0.0676\n",
      "Epoch 38/300 - Train Loss: 0.0627, Val Loss: 0.0682\n",
      "Epoch 39/300 - Train Loss: 0.0627, Val Loss: 0.0702\n",
      "Epoch 40/300 - Train Loss: 0.0600, Val Loss: 0.0709\n",
      "Epoch 41/300 - Train Loss: 0.0615, Val Loss: 0.0685\n",
      "Epoch 42/300 - Train Loss: 0.0613, Val Loss: 0.0777\n",
      "Epoch 43/300 - Train Loss: 0.0603, Val Loss: 0.0743\n",
      "Epoch 44/300 - Train Loss: 0.0588, Val Loss: 0.0742\n",
      "Epoch 45/300 - Train Loss: 0.0596, Val Loss: 0.0716\n",
      "Epoch 46/300 - Train Loss: 0.0588, Val Loss: 0.0686\n",
      "Epoch 47/300 - Train Loss: 0.0568, Val Loss: 0.0713\n",
      "Epoch 48/300 - Train Loss: 0.0578, Val Loss: 0.0683\n",
      "Epoch 49/300 - Train Loss: 0.0584, Val Loss: 0.0705\n",
      "Epoch 50/300 - Train Loss: 0.0557, Val Loss: 0.0674\n",
      "Epoch 51/300 - Train Loss: 0.0550, Val Loss: 0.0716\n",
      "Epoch 52/300 - Train Loss: 0.0559, Val Loss: 0.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:11:54,545] Trial 331 finished with value: 0.9650764242744697 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.13476725401834866, 'learning_rate': 0.0001007602097414478, 'batch_size': 32, 'weight_decay': 6.247813261715426e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300 - Train Loss: 0.0586, Val Loss: 0.0748\n",
      "Early stopping at epoch 53\n",
      "Macro F1 Score: 0.9651, Macro Precision: 0.9590, Macro Recall: 0.9715\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 333\n",
      "Training with F1=4, F2=32, D=2, dropout=0.1726253667905684, LR=8.109845357753573e-05, BS=32, WD=4.231015573086634e-05\n",
      "Epoch 1/300 - Train Loss: 0.4542, Val Loss: 0.2338\n",
      "Epoch 2/300 - Train Loss: 0.2153, Val Loss: 0.1454\n",
      "Epoch 3/300 - Train Loss: 0.1650, Val Loss: 0.1269\n",
      "Epoch 4/300 - Train Loss: 0.1525, Val Loss: 0.1129\n",
      "Epoch 5/300 - Train Loss: 0.1421, Val Loss: 0.1099\n",
      "Epoch 6/300 - Train Loss: 0.1369, Val Loss: 0.1018\n",
      "Epoch 7/300 - Train Loss: 0.1313, Val Loss: 0.0982\n",
      "Epoch 8/300 - Train Loss: 0.1251, Val Loss: 0.0958\n",
      "Epoch 9/300 - Train Loss: 0.1225, Val Loss: 0.0914\n",
      "Epoch 10/300 - Train Loss: 0.1143, Val Loss: 0.0910\n",
      "Epoch 11/300 - Train Loss: 0.1115, Val Loss: 0.0904\n",
      "Epoch 12/300 - Train Loss: 0.1079, Val Loss: 0.0911\n",
      "Epoch 13/300 - Train Loss: 0.1059, Val Loss: 0.0863\n",
      "Epoch 14/300 - Train Loss: 0.1027, Val Loss: 0.0789\n",
      "Epoch 15/300 - Train Loss: 0.0994, Val Loss: 0.0857\n",
      "Epoch 16/300 - Train Loss: 0.0990, Val Loss: 0.0852\n",
      "Epoch 17/300 - Train Loss: 0.0988, Val Loss: 0.0811\n",
      "Epoch 18/300 - Train Loss: 0.0981, Val Loss: 0.0839\n",
      "Epoch 19/300 - Train Loss: 0.0978, Val Loss: 0.0847\n",
      "Epoch 20/300 - Train Loss: 0.0965, Val Loss: 0.0810\n",
      "Epoch 21/300 - Train Loss: 0.0977, Val Loss: 0.0858\n",
      "Epoch 22/300 - Train Loss: 0.0962, Val Loss: 0.0827\n",
      "Epoch 23/300 - Train Loss: 0.0946, Val Loss: 0.0862\n",
      "Epoch 24/300 - Train Loss: 0.0953, Val Loss: 0.0857\n",
      "Epoch 25/300 - Train Loss: 0.0939, Val Loss: 0.0806\n",
      "Epoch 26/300 - Train Loss: 0.0910, Val Loss: 0.0787\n",
      "Epoch 27/300 - Train Loss: 0.0946, Val Loss: 0.0773\n",
      "Epoch 28/300 - Train Loss: 0.0911, Val Loss: 0.0793\n",
      "Epoch 29/300 - Train Loss: 0.0936, Val Loss: 0.0821\n",
      "Epoch 30/300 - Train Loss: 0.0960, Val Loss: 0.0764\n",
      "Epoch 31/300 - Train Loss: 0.0936, Val Loss: 0.0840\n",
      "Epoch 32/300 - Train Loss: 0.0906, Val Loss: 0.0801\n",
      "Epoch 33/300 - Train Loss: 0.0918, Val Loss: 0.0763\n",
      "Epoch 34/300 - Train Loss: 0.0942, Val Loss: 0.0848\n",
      "Epoch 35/300 - Train Loss: 0.0916, Val Loss: 0.0844\n",
      "Epoch 36/300 - Train Loss: 0.0907, Val Loss: 0.0765\n",
      "Epoch 37/300 - Train Loss: 0.0911, Val Loss: 0.0781\n",
      "Epoch 38/300 - Train Loss: 0.0927, Val Loss: 0.0769\n",
      "Epoch 39/300 - Train Loss: 0.0891, Val Loss: 0.0840\n",
      "Epoch 40/300 - Train Loss: 0.0891, Val Loss: 0.0771\n",
      "Epoch 41/300 - Train Loss: 0.0899, Val Loss: 0.0848\n",
      "Epoch 42/300 - Train Loss: 0.0886, Val Loss: 0.0791\n",
      "Epoch 43/300 - Train Loss: 0.0891, Val Loss: 0.0776\n",
      "Epoch 44/300 - Train Loss: 0.0890, Val Loss: 0.0789\n",
      "Epoch 45/300 - Train Loss: 0.0883, Val Loss: 0.0829\n",
      "Epoch 46/300 - Train Loss: 0.0852, Val Loss: 0.0780\n",
      "Epoch 47/300 - Train Loss: 0.0856, Val Loss: 0.0749\n",
      "Epoch 48/300 - Train Loss: 0.0890, Val Loss: 0.0776\n",
      "Epoch 49/300 - Train Loss: 0.0863, Val Loss: 0.0728\n",
      "Epoch 50/300 - Train Loss: 0.0891, Val Loss: 0.0776\n",
      "Epoch 51/300 - Train Loss: 0.0876, Val Loss: 0.0754\n",
      "Epoch 52/300 - Train Loss: 0.0875, Val Loss: 0.0790\n",
      "Epoch 53/300 - Train Loss: 0.0883, Val Loss: 0.0769\n",
      "Epoch 54/300 - Train Loss: 0.0858, Val Loss: 0.0790\n",
      "Epoch 55/300 - Train Loss: 0.0872, Val Loss: 0.0776\n",
      "Epoch 56/300 - Train Loss: 0.0858, Val Loss: 0.0756\n",
      "Epoch 57/300 - Train Loss: 0.0853, Val Loss: 0.0797\n",
      "Epoch 58/300 - Train Loss: 0.0857, Val Loss: 0.0734\n",
      "Epoch 59/300 - Train Loss: 0.0863, Val Loss: 0.0761\n",
      "Epoch 60/300 - Train Loss: 0.0845, Val Loss: 0.0768\n",
      "Epoch 61/300 - Train Loss: 0.0861, Val Loss: 0.0749\n",
      "Epoch 62/300 - Train Loss: 0.0844, Val Loss: 0.0751\n",
      "Epoch 63/300 - Train Loss: 0.0838, Val Loss: 0.0756\n",
      "Epoch 64/300 - Train Loss: 0.0866, Val Loss: 0.0807\n",
      "Epoch 65/300 - Train Loss: 0.0837, Val Loss: 0.0796\n",
      "Epoch 66/300 - Train Loss: 0.0852, Val Loss: 0.0776\n",
      "Epoch 67/300 - Train Loss: 0.0848, Val Loss: 0.0782\n",
      "Epoch 68/300 - Train Loss: 0.0844, Val Loss: 0.0751\n",
      "Epoch 69/300 - Train Loss: 0.0846, Val Loss: 0.0780\n",
      "Epoch 70/300 - Train Loss: 0.0843, Val Loss: 0.0727\n",
      "Epoch 71/300 - Train Loss: 0.0839, Val Loss: 0.0735\n",
      "Epoch 72/300 - Train Loss: 0.0818, Val Loss: 0.0743\n",
      "Epoch 73/300 - Train Loss: 0.0827, Val Loss: 0.0719\n",
      "Epoch 74/300 - Train Loss: 0.0838, Val Loss: 0.0756\n",
      "Epoch 75/300 - Train Loss: 0.0833, Val Loss: 0.0775\n",
      "Epoch 76/300 - Train Loss: 0.0820, Val Loss: 0.0764\n",
      "Epoch 77/300 - Train Loss: 0.0867, Val Loss: 0.0845\n",
      "Epoch 78/300 - Train Loss: 0.0830, Val Loss: 0.0757\n",
      "Epoch 79/300 - Train Loss: 0.0799, Val Loss: 0.0761\n",
      "Epoch 80/300 - Train Loss: 0.0835, Val Loss: 0.0724\n",
      "Epoch 81/300 - Train Loss: 0.0810, Val Loss: 0.0740\n",
      "Epoch 82/300 - Train Loss: 0.0804, Val Loss: 0.0781\n",
      "Epoch 83/300 - Train Loss: 0.0808, Val Loss: 0.0789\n",
      "Epoch 84/300 - Train Loss: 0.0817, Val Loss: 0.0780\n",
      "Epoch 85/300 - Train Loss: 0.0822, Val Loss: 0.0764\n",
      "Epoch 86/300 - Train Loss: 0.0816, Val Loss: 0.0761\n",
      "Epoch 87/300 - Train Loss: 0.0815, Val Loss: 0.0764\n",
      "Epoch 88/300 - Train Loss: 0.0824, Val Loss: 0.0745\n",
      "Epoch 89/300 - Train Loss: 0.0792, Val Loss: 0.0780\n",
      "Epoch 90/300 - Train Loss: 0.0785, Val Loss: 0.0735\n",
      "Epoch 91/300 - Train Loss: 0.0813, Val Loss: 0.0739\n",
      "Epoch 92/300 - Train Loss: 0.0804, Val Loss: 0.0753\n",
      "Epoch 93/300 - Train Loss: 0.0802, Val Loss: 0.0783\n",
      "Epoch 94/300 - Train Loss: 0.0806, Val Loss: 0.0761\n",
      "Epoch 95/300 - Train Loss: 0.0811, Val Loss: 0.0703\n",
      "Epoch 96/300 - Train Loss: 0.0782, Val Loss: 0.0735\n",
      "Epoch 97/300 - Train Loss: 0.0805, Val Loss: 0.0767\n",
      "Epoch 98/300 - Train Loss: 0.0802, Val Loss: 0.0770\n",
      "Epoch 99/300 - Train Loss: 0.0773, Val Loss: 0.0802\n",
      "Epoch 100/300 - Train Loss: 0.0819, Val Loss: 0.0836\n",
      "Epoch 101/300 - Train Loss: 0.0812, Val Loss: 0.0755\n",
      "Epoch 102/300 - Train Loss: 0.0776, Val Loss: 0.0777\n",
      "Epoch 103/300 - Train Loss: 0.0788, Val Loss: 0.0787\n",
      "Epoch 104/300 - Train Loss: 0.0775, Val Loss: 0.0823\n",
      "Epoch 105/300 - Train Loss: 0.0788, Val Loss: 0.0719\n",
      "Epoch 106/300 - Train Loss: 0.0780, Val Loss: 0.0755\n",
      "Epoch 107/300 - Train Loss: 0.0805, Val Loss: 0.0764\n",
      "Epoch 108/300 - Train Loss: 0.0783, Val Loss: 0.0731\n",
      "Epoch 109/300 - Train Loss: 0.0799, Val Loss: 0.0738\n",
      "Epoch 110/300 - Train Loss: 0.0771, Val Loss: 0.0755\n",
      "Epoch 111/300 - Train Loss: 0.0776, Val Loss: 0.0724\n",
      "Epoch 112/300 - Train Loss: 0.0778, Val Loss: 0.0710\n",
      "Epoch 113/300 - Train Loss: 0.0783, Val Loss: 0.0753\n",
      "Epoch 114/300 - Train Loss: 0.0788, Val Loss: 0.0725\n",
      "Epoch 115/300 - Train Loss: 0.0764, Val Loss: 0.0766\n",
      "Epoch 116/300 - Train Loss: 0.0763, Val Loss: 0.0780\n",
      "Epoch 117/300 - Train Loss: 0.0790, Val Loss: 0.0746\n",
      "Epoch 118/300 - Train Loss: 0.0795, Val Loss: 0.0719\n",
      "Epoch 119/300 - Train Loss: 0.0773, Val Loss: 0.0731\n",
      "Epoch 120/300 - Train Loss: 0.0774, Val Loss: 0.0763\n",
      "Epoch 121/300 - Train Loss: 0.0777, Val Loss: 0.0743\n",
      "Epoch 122/300 - Train Loss: 0.0758, Val Loss: 0.0750\n",
      "Epoch 123/300 - Train Loss: 0.0752, Val Loss: 0.0751\n",
      "Epoch 124/300 - Train Loss: 0.0780, Val Loss: 0.0730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:15:50,914] Trial 332 finished with value: 0.9586270625016332 and parameters: {'F1': 4, 'F2': 32, 'D': 2, 'dropout': 0.1726253667905684, 'learning_rate': 8.109845357753573e-05, 'batch_size': 32, 'weight_decay': 4.231015573086634e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/300 - Train Loss: 0.0739, Val Loss: 0.0732\n",
      "Early stopping at epoch 125\n",
      "Macro F1 Score: 0.9586, Macro Precision: 0.9494, Macro Recall: 0.9690\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.88      0.95      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 334\n",
      "Training with F1=32, F2=32, D=2, dropout=0.10146532198706779, LR=0.00011674783589045458, BS=32, WD=3.13524542810889e-05\n",
      "Epoch 1/300 - Train Loss: 0.2469, Val Loss: 0.1117\n",
      "Epoch 2/300 - Train Loss: 0.1110, Val Loss: 0.0797\n",
      "Epoch 3/300 - Train Loss: 0.0999, Val Loss: 0.0773\n",
      "Epoch 4/300 - Train Loss: 0.0917, Val Loss: 0.0797\n",
      "Epoch 5/300 - Train Loss: 0.0905, Val Loss: 0.0760\n",
      "Epoch 6/300 - Train Loss: 0.0875, Val Loss: 0.0725\n",
      "Epoch 7/300 - Train Loss: 0.0868, Val Loss: 0.0683\n",
      "Epoch 8/300 - Train Loss: 0.0814, Val Loss: 0.0708\n",
      "Epoch 9/300 - Train Loss: 0.0799, Val Loss: 0.0691\n",
      "Epoch 10/300 - Train Loss: 0.0770, Val Loss: 0.0692\n",
      "Epoch 11/300 - Train Loss: 0.0784, Val Loss: 0.0779\n",
      "Epoch 12/300 - Train Loss: 0.0772, Val Loss: 0.0796\n",
      "Epoch 13/300 - Train Loss: 0.0771, Val Loss: 0.0766\n",
      "Epoch 14/300 - Train Loss: 0.0740, Val Loss: 0.0714\n",
      "Epoch 15/300 - Train Loss: 0.0730, Val Loss: 0.0732\n",
      "Epoch 16/300 - Train Loss: 0.0722, Val Loss: 0.0714\n",
      "Epoch 17/300 - Train Loss: 0.0752, Val Loss: 0.0896\n",
      "Epoch 18/300 - Train Loss: 0.0714, Val Loss: 0.0759\n",
      "Epoch 19/300 - Train Loss: 0.0699, Val Loss: 0.0733\n",
      "Epoch 20/300 - Train Loss: 0.0719, Val Loss: 0.0862\n",
      "Epoch 21/300 - Train Loss: 0.0712, Val Loss: 0.0743\n",
      "Epoch 22/300 - Train Loss: 0.0681, Val Loss: 0.0711\n",
      "Epoch 23/300 - Train Loss: 0.0686, Val Loss: 0.0707\n",
      "Epoch 24/300 - Train Loss: 0.0692, Val Loss: 0.0707\n",
      "Epoch 25/300 - Train Loss: 0.0673, Val Loss: 0.0691\n",
      "Epoch 26/300 - Train Loss: 0.0688, Val Loss: 0.0783\n",
      "Epoch 27/300 - Train Loss: 0.0662, Val Loss: 0.0728\n",
      "Epoch 28/300 - Train Loss: 0.0663, Val Loss: 0.0807\n",
      "Epoch 29/300 - Train Loss: 0.0652, Val Loss: 0.0720\n",
      "Epoch 30/300 - Train Loss: 0.0644, Val Loss: 0.0692\n",
      "Epoch 31/300 - Train Loss: 0.0626, Val Loss: 0.0677\n",
      "Epoch 32/300 - Train Loss: 0.0624, Val Loss: 0.0710\n",
      "Epoch 33/300 - Train Loss: 0.0620, Val Loss: 0.0734\n",
      "Epoch 34/300 - Train Loss: 0.0590, Val Loss: 0.0703\n",
      "Epoch 35/300 - Train Loss: 0.0587, Val Loss: 0.0726\n",
      "Epoch 36/300 - Train Loss: 0.0608, Val Loss: 0.0761\n",
      "Epoch 37/300 - Train Loss: 0.0601, Val Loss: 0.0868\n",
      "Epoch 38/300 - Train Loss: 0.0578, Val Loss: 0.0706\n",
      "Epoch 39/300 - Train Loss: 0.0600, Val Loss: 0.0664\n",
      "Epoch 40/300 - Train Loss: 0.0587, Val Loss: 0.0704\n",
      "Epoch 41/300 - Train Loss: 0.0598, Val Loss: 0.0731\n",
      "Epoch 42/300 - Train Loss: 0.0567, Val Loss: 0.0717\n",
      "Epoch 43/300 - Train Loss: 0.0549, Val Loss: 0.0709\n",
      "Epoch 44/300 - Train Loss: 0.0542, Val Loss: 0.0723\n",
      "Epoch 45/300 - Train Loss: 0.0582, Val Loss: 0.0749\n",
      "Epoch 46/300 - Train Loss: 0.0565, Val Loss: 0.0730\n",
      "Epoch 47/300 - Train Loss: 0.0533, Val Loss: 0.0760\n",
      "Epoch 48/300 - Train Loss: 0.0549, Val Loss: 0.0737\n",
      "Epoch 49/300 - Train Loss: 0.0533, Val Loss: 0.0717\n",
      "Epoch 50/300 - Train Loss: 0.0524, Val Loss: 0.0744\n",
      "Epoch 51/300 - Train Loss: 0.0508, Val Loss: 0.0739\n",
      "Epoch 52/300 - Train Loss: 0.0552, Val Loss: 0.0825\n",
      "Epoch 53/300 - Train Loss: 0.0514, Val Loss: 0.0733\n",
      "Epoch 54/300 - Train Loss: 0.0494, Val Loss: 0.0775\n",
      "Epoch 55/300 - Train Loss: 0.0521, Val Loss: 0.0857\n",
      "Epoch 56/300 - Train Loss: 0.0499, Val Loss: 0.0738\n",
      "Epoch 57/300 - Train Loss: 0.0521, Val Loss: 0.0741\n",
      "Epoch 58/300 - Train Loss: 0.0482, Val Loss: 0.0758\n",
      "Epoch 59/300 - Train Loss: 0.0487, Val Loss: 0.0724\n",
      "Epoch 60/300 - Train Loss: 0.0529, Val Loss: 0.0745\n",
      "Epoch 61/300 - Train Loss: 0.0476, Val Loss: 0.0801\n",
      "Epoch 62/300 - Train Loss: 0.0454, Val Loss: 0.0822\n",
      "Epoch 63/300 - Train Loss: 0.0480, Val Loss: 0.0739\n",
      "Epoch 64/300 - Train Loss: 0.0477, Val Loss: 0.0794\n",
      "Epoch 65/300 - Train Loss: 0.0474, Val Loss: 0.0785\n",
      "Epoch 66/300 - Train Loss: 0.0481, Val Loss: 0.0797\n",
      "Epoch 67/300 - Train Loss: 0.0447, Val Loss: 0.0752\n",
      "Epoch 68/300 - Train Loss: 0.0465, Val Loss: 0.0730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:18:59,563] Trial 333 finished with value: 0.9610038852896065 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.10146532198706779, 'learning_rate': 0.00011674783589045458, 'batch_size': 32, 'weight_decay': 3.13524542810889e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.0455, Val Loss: 0.0781\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9610, Macro Precision: 0.9570, Macro Recall: 0.9652\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.90      0.93      0.92        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 335\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1541094689581848, LR=8.459697503307877e-05, BS=32, WD=5.069809139820592e-05\n",
      "Epoch 1/300 - Train Loss: 0.2839, Val Loss: 0.1304\n",
      "Epoch 2/300 - Train Loss: 0.1260, Val Loss: 0.0922\n",
      "Epoch 3/300 - Train Loss: 0.1058, Val Loss: 0.0861\n",
      "Epoch 4/300 - Train Loss: 0.0976, Val Loss: 0.0764\n",
      "Epoch 5/300 - Train Loss: 0.0919, Val Loss: 0.0757\n",
      "Epoch 6/300 - Train Loss: 0.0904, Val Loss: 0.0748\n",
      "Epoch 7/300 - Train Loss: 0.0911, Val Loss: 0.0704\n",
      "Epoch 8/300 - Train Loss: 0.0882, Val Loss: 0.0746\n",
      "Epoch 9/300 - Train Loss: 0.0858, Val Loss: 0.0770\n",
      "Epoch 10/300 - Train Loss: 0.0823, Val Loss: 0.0682\n",
      "Epoch 11/300 - Train Loss: 0.0823, Val Loss: 0.0737\n",
      "Epoch 12/300 - Train Loss: 0.0840, Val Loss: 0.0699\n",
      "Epoch 13/300 - Train Loss: 0.0815, Val Loss: 0.0736\n",
      "Epoch 14/300 - Train Loss: 0.0784, Val Loss: 0.0688\n",
      "Epoch 15/300 - Train Loss: 0.0782, Val Loss: 0.0813\n",
      "Epoch 16/300 - Train Loss: 0.0779, Val Loss: 0.0788\n",
      "Epoch 17/300 - Train Loss: 0.0786, Val Loss: 0.0712\n",
      "Epoch 18/300 - Train Loss: 0.0759, Val Loss: 0.0688\n",
      "Epoch 19/300 - Train Loss: 0.0753, Val Loss: 0.0662\n",
      "Epoch 20/300 - Train Loss: 0.0766, Val Loss: 0.0698\n",
      "Epoch 21/300 - Train Loss: 0.0773, Val Loss: 0.0755\n",
      "Epoch 22/300 - Train Loss: 0.0749, Val Loss: 0.0747\n",
      "Epoch 23/300 - Train Loss: 0.0750, Val Loss: 0.0685\n",
      "Epoch 24/300 - Train Loss: 0.0737, Val Loss: 0.0774\n",
      "Epoch 25/300 - Train Loss: 0.0720, Val Loss: 0.0727\n",
      "Epoch 26/300 - Train Loss: 0.0718, Val Loss: 0.0717\n",
      "Epoch 27/300 - Train Loss: 0.0718, Val Loss: 0.0676\n",
      "Epoch 28/300 - Train Loss: 0.0698, Val Loss: 0.0755\n",
      "Epoch 29/300 - Train Loss: 0.0714, Val Loss: 0.0662\n",
      "Epoch 30/300 - Train Loss: 0.0708, Val Loss: 0.0661\n",
      "Epoch 31/300 - Train Loss: 0.0694, Val Loss: 0.0712\n",
      "Epoch 32/300 - Train Loss: 0.0673, Val Loss: 0.0725\n",
      "Epoch 33/300 - Train Loss: 0.0668, Val Loss: 0.0663\n",
      "Epoch 34/300 - Train Loss: 0.0678, Val Loss: 0.0719\n",
      "Epoch 35/300 - Train Loss: 0.0679, Val Loss: 0.0695\n",
      "Epoch 36/300 - Train Loss: 0.0660, Val Loss: 0.0661\n",
      "Epoch 37/300 - Train Loss: 0.0657, Val Loss: 0.0696\n",
      "Epoch 38/300 - Train Loss: 0.0669, Val Loss: 0.0673\n",
      "Epoch 39/300 - Train Loss: 0.0669, Val Loss: 0.0710\n",
      "Epoch 40/300 - Train Loss: 0.0634, Val Loss: 0.0678\n",
      "Epoch 41/300 - Train Loss: 0.0643, Val Loss: 0.0665\n",
      "Epoch 42/300 - Train Loss: 0.0629, Val Loss: 0.0737\n",
      "Epoch 43/300 - Train Loss: 0.0625, Val Loss: 0.0665\n",
      "Epoch 44/300 - Train Loss: 0.0622, Val Loss: 0.0724\n",
      "Epoch 45/300 - Train Loss: 0.0605, Val Loss: 0.0710\n",
      "Epoch 46/300 - Train Loss: 0.0619, Val Loss: 0.0717\n",
      "Epoch 47/300 - Train Loss: 0.0625, Val Loss: 0.0655\n",
      "Epoch 48/300 - Train Loss: 0.0600, Val Loss: 0.0661\n",
      "Epoch 49/300 - Train Loss: 0.0607, Val Loss: 0.0696\n",
      "Epoch 50/300 - Train Loss: 0.0603, Val Loss: 0.0662\n",
      "Epoch 51/300 - Train Loss: 0.0590, Val Loss: 0.0648\n",
      "Epoch 52/300 - Train Loss: 0.0597, Val Loss: 0.0715\n",
      "Epoch 53/300 - Train Loss: 0.0582, Val Loss: 0.0693\n",
      "Epoch 54/300 - Train Loss: 0.0597, Val Loss: 0.0661\n",
      "Epoch 55/300 - Train Loss: 0.0598, Val Loss: 0.0687\n",
      "Epoch 56/300 - Train Loss: 0.0580, Val Loss: 0.0678\n",
      "Epoch 57/300 - Train Loss: 0.0576, Val Loss: 0.0683\n",
      "Epoch 58/300 - Train Loss: 0.0566, Val Loss: 0.0744\n",
      "Epoch 59/300 - Train Loss: 0.0570, Val Loss: 0.0706\n",
      "Epoch 60/300 - Train Loss: 0.0547, Val Loss: 0.0739\n",
      "Epoch 61/300 - Train Loss: 0.0558, Val Loss: 0.0724\n",
      "Epoch 62/300 - Train Loss: 0.0558, Val Loss: 0.0674\n",
      "Epoch 63/300 - Train Loss: 0.0566, Val Loss: 0.0653\n",
      "Epoch 64/300 - Train Loss: 0.0538, Val Loss: 0.0645\n",
      "Epoch 65/300 - Train Loss: 0.0558, Val Loss: 0.0674\n",
      "Epoch 66/300 - Train Loss: 0.0539, Val Loss: 0.0705\n",
      "Epoch 67/300 - Train Loss: 0.0543, Val Loss: 0.0659\n",
      "Epoch 68/300 - Train Loss: 0.0548, Val Loss: 0.0654\n",
      "Epoch 69/300 - Train Loss: 0.0539, Val Loss: 0.0724\n",
      "Epoch 70/300 - Train Loss: 0.0543, Val Loss: 0.0696\n",
      "Epoch 71/300 - Train Loss: 0.0534, Val Loss: 0.0751\n",
      "Epoch 72/300 - Train Loss: 0.0521, Val Loss: 0.0651\n",
      "Epoch 73/300 - Train Loss: 0.0524, Val Loss: 0.0736\n",
      "Epoch 74/300 - Train Loss: 0.0503, Val Loss: 0.0690\n",
      "Epoch 75/300 - Train Loss: 0.0508, Val Loss: 0.0735\n",
      "Epoch 76/300 - Train Loss: 0.0528, Val Loss: 0.0754\n",
      "Epoch 77/300 - Train Loss: 0.0498, Val Loss: 0.0705\n",
      "Epoch 78/300 - Train Loss: 0.0504, Val Loss: 0.0683\n",
      "Epoch 79/300 - Train Loss: 0.0507, Val Loss: 0.0695\n",
      "Epoch 80/300 - Train Loss: 0.0493, Val Loss: 0.0649\n",
      "Epoch 81/300 - Train Loss: 0.0487, Val Loss: 0.0684\n",
      "Epoch 82/300 - Train Loss: 0.0494, Val Loss: 0.0701\n",
      "Epoch 83/300 - Train Loss: 0.0540, Val Loss: 0.0725\n",
      "Epoch 84/300 - Train Loss: 0.0487, Val Loss: 0.0673\n",
      "Epoch 85/300 - Train Loss: 0.0501, Val Loss: 0.0701\n",
      "Epoch 86/300 - Train Loss: 0.0488, Val Loss: 0.0707\n",
      "Epoch 87/300 - Train Loss: 0.0462, Val Loss: 0.0665\n",
      "Epoch 88/300 - Train Loss: 0.0523, Val Loss: 0.0618\n",
      "Epoch 89/300 - Train Loss: 0.0514, Val Loss: 0.0830\n",
      "Epoch 90/300 - Train Loss: 0.0495, Val Loss: 0.0663\n",
      "Epoch 91/300 - Train Loss: 0.0481, Val Loss: 0.0675\n",
      "Epoch 92/300 - Train Loss: 0.0473, Val Loss: 0.0695\n",
      "Epoch 93/300 - Train Loss: 0.0484, Val Loss: 0.0724\n",
      "Epoch 94/300 - Train Loss: 0.0470, Val Loss: 0.0681\n",
      "Epoch 95/300 - Train Loss: 0.0441, Val Loss: 0.0749\n",
      "Epoch 96/300 - Train Loss: 0.0456, Val Loss: 0.0702\n",
      "Epoch 97/300 - Train Loss: 0.0456, Val Loss: 0.0694\n",
      "Epoch 98/300 - Train Loss: 0.0439, Val Loss: 0.0745\n",
      "Epoch 99/300 - Train Loss: 0.0459, Val Loss: 0.0713\n",
      "Epoch 100/300 - Train Loss: 0.0451, Val Loss: 0.0710\n",
      "Epoch 101/300 - Train Loss: 0.0443, Val Loss: 0.0719\n",
      "Epoch 102/300 - Train Loss: 0.0457, Val Loss: 0.0737\n",
      "Epoch 103/300 - Train Loss: 0.0477, Val Loss: 0.0694\n",
      "Epoch 104/300 - Train Loss: 0.0450, Val Loss: 0.0716\n",
      "Epoch 105/300 - Train Loss: 0.0448, Val Loss: 0.0657\n",
      "Epoch 106/300 - Train Loss: 0.0451, Val Loss: 0.0730\n",
      "Epoch 107/300 - Train Loss: 0.0428, Val Loss: 0.0664\n",
      "Epoch 108/300 - Train Loss: 0.0434, Val Loss: 0.0722\n",
      "Epoch 109/300 - Train Loss: 0.0443, Val Loss: 0.0724\n",
      "Epoch 110/300 - Train Loss: 0.0435, Val Loss: 0.0704\n",
      "Epoch 111/300 - Train Loss: 0.0446, Val Loss: 0.0767\n",
      "Epoch 112/300 - Train Loss: 0.0445, Val Loss: 0.0689\n",
      "Epoch 113/300 - Train Loss: 0.0418, Val Loss: 0.0746\n",
      "Epoch 114/300 - Train Loss: 0.0429, Val Loss: 0.0735\n",
      "Epoch 115/300 - Train Loss: 0.0433, Val Loss: 0.0818\n",
      "Epoch 116/300 - Train Loss: 0.0419, Val Loss: 0.0741\n",
      "Epoch 117/300 - Train Loss: 0.0431, Val Loss: 0.0716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:24:22,391] Trial 334 finished with value: 0.9652125119330245 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1541094689581848, 'learning_rate': 8.459697503307877e-05, 'batch_size': 32, 'weight_decay': 5.069809139820592e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/300 - Train Loss: 0.0405, Val Loss: 0.0739\n",
      "Early stopping at epoch 118\n",
      "Macro F1 Score: 0.9652, Macro Precision: 0.9578, Macro Recall: 0.9735\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 336\n",
      "Training with F1=32, F2=32, D=2, dropout=0.19033988102276536, LR=9.813546095158938e-05, BS=32, WD=7.344269046837261e-05\n",
      "Epoch 1/300 - Train Loss: 0.2783, Val Loss: 0.1310\n",
      "Epoch 2/300 - Train Loss: 0.1251, Val Loss: 0.1188\n",
      "Epoch 3/300 - Train Loss: 0.1043, Val Loss: 0.0831\n",
      "Epoch 4/300 - Train Loss: 0.1001, Val Loss: 0.0784\n",
      "Epoch 5/300 - Train Loss: 0.0971, Val Loss: 0.0750\n",
      "Epoch 6/300 - Train Loss: 0.0933, Val Loss: 0.0719\n",
      "Epoch 7/300 - Train Loss: 0.0921, Val Loss: 0.0752\n",
      "Epoch 8/300 - Train Loss: 0.0893, Val Loss: 0.0750\n",
      "Epoch 9/300 - Train Loss: 0.0895, Val Loss: 0.0769\n",
      "Epoch 10/300 - Train Loss: 0.0875, Val Loss: 0.0727\n",
      "Epoch 11/300 - Train Loss: 0.0862, Val Loss: 0.0744\n",
      "Epoch 12/300 - Train Loss: 0.0823, Val Loss: 0.0746\n",
      "Epoch 13/300 - Train Loss: 0.0834, Val Loss: 0.0654\n",
      "Epoch 14/300 - Train Loss: 0.0808, Val Loss: 0.0701\n",
      "Epoch 15/300 - Train Loss: 0.0826, Val Loss: 0.0721\n",
      "Epoch 16/300 - Train Loss: 0.0801, Val Loss: 0.0853\n",
      "Epoch 17/300 - Train Loss: 0.0785, Val Loss: 0.0768\n",
      "Epoch 18/300 - Train Loss: 0.0799, Val Loss: 0.0660\n",
      "Epoch 19/300 - Train Loss: 0.0777, Val Loss: 0.0814\n",
      "Epoch 20/300 - Train Loss: 0.0784, Val Loss: 0.0681\n",
      "Epoch 21/300 - Train Loss: 0.0783, Val Loss: 0.0719\n",
      "Epoch 22/300 - Train Loss: 0.0773, Val Loss: 0.0727\n",
      "Epoch 23/300 - Train Loss: 0.0752, Val Loss: 0.0716\n",
      "Epoch 24/300 - Train Loss: 0.0746, Val Loss: 0.0671\n",
      "Epoch 25/300 - Train Loss: 0.0740, Val Loss: 0.0689\n",
      "Epoch 26/300 - Train Loss: 0.0750, Val Loss: 0.0752\n",
      "Epoch 27/300 - Train Loss: 0.0729, Val Loss: 0.0710\n",
      "Epoch 28/300 - Train Loss: 0.0733, Val Loss: 0.0669\n",
      "Epoch 29/300 - Train Loss: 0.0710, Val Loss: 0.0695\n",
      "Epoch 30/300 - Train Loss: 0.0727, Val Loss: 0.0684\n",
      "Epoch 31/300 - Train Loss: 0.0716, Val Loss: 0.0719\n",
      "Epoch 32/300 - Train Loss: 0.0714, Val Loss: 0.0741\n",
      "Epoch 33/300 - Train Loss: 0.0718, Val Loss: 0.0717\n",
      "Epoch 34/300 - Train Loss: 0.0689, Val Loss: 0.0729\n",
      "Epoch 35/300 - Train Loss: 0.0682, Val Loss: 0.0660\n",
      "Epoch 36/300 - Train Loss: 0.0708, Val Loss: 0.0721\n",
      "Epoch 37/300 - Train Loss: 0.0700, Val Loss: 0.0666\n",
      "Epoch 38/300 - Train Loss: 0.0667, Val Loss: 0.0685\n",
      "Epoch 39/300 - Train Loss: 0.0678, Val Loss: 0.0669\n",
      "Epoch 40/300 - Train Loss: 0.0676, Val Loss: 0.0658\n",
      "Epoch 41/300 - Train Loss: 0.0661, Val Loss: 0.0721\n",
      "Epoch 42/300 - Train Loss: 0.0655, Val Loss: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:26:20,277] Trial 335 finished with value: 0.9667097169888752 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.19033988102276536, 'learning_rate': 9.813546095158938e-05, 'batch_size': 32, 'weight_decay': 7.344269046837261e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/300 - Train Loss: 0.0671, Val Loss: 0.0676\n",
      "Early stopping at epoch 43\n",
      "Macro F1 Score: 0.9667, Macro Precision: 0.9633, Macro Recall: 0.9704\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 337\n",
      "Training with F1=32, F2=32, D=2, dropout=0.13422088112872504, LR=7.552775919636659e-05, BS=32, WD=5.8932060112440746e-05\n",
      "Epoch 1/300 - Train Loss: 0.3044, Val Loss: 0.1232\n",
      "Epoch 2/300 - Train Loss: 0.1180, Val Loss: 0.0893\n",
      "Epoch 3/300 - Train Loss: 0.0987, Val Loss: 0.0754\n",
      "Epoch 4/300 - Train Loss: 0.0945, Val Loss: 0.0710\n",
      "Epoch 5/300 - Train Loss: 0.0909, Val Loss: 0.0767\n",
      "Epoch 6/300 - Train Loss: 0.0906, Val Loss: 0.0695\n",
      "Epoch 7/300 - Train Loss: 0.0889, Val Loss: 0.0825\n",
      "Epoch 8/300 - Train Loss: 0.0883, Val Loss: 0.0676\n",
      "Epoch 9/300 - Train Loss: 0.0853, Val Loss: 0.0717\n",
      "Epoch 10/300 - Train Loss: 0.0834, Val Loss: 0.0716\n",
      "Epoch 11/300 - Train Loss: 0.0832, Val Loss: 0.0720\n",
      "Epoch 12/300 - Train Loss: 0.0835, Val Loss: 0.0718\n",
      "Epoch 13/300 - Train Loss: 0.0799, Val Loss: 0.0697\n",
      "Epoch 14/300 - Train Loss: 0.0787, Val Loss: 0.0721\n",
      "Epoch 15/300 - Train Loss: 0.0770, Val Loss: 0.0666\n",
      "Epoch 16/300 - Train Loss: 0.0790, Val Loss: 0.0725\n",
      "Epoch 17/300 - Train Loss: 0.0769, Val Loss: 0.0711\n",
      "Epoch 18/300 - Train Loss: 0.0779, Val Loss: 0.0712\n",
      "Epoch 19/300 - Train Loss: 0.0754, Val Loss: 0.0689\n",
      "Epoch 20/300 - Train Loss: 0.0767, Val Loss: 0.0733\n",
      "Epoch 21/300 - Train Loss: 0.0738, Val Loss: 0.0716\n",
      "Epoch 22/300 - Train Loss: 0.0747, Val Loss: 0.0708\n",
      "Epoch 23/300 - Train Loss: 0.0745, Val Loss: 0.0701\n",
      "Epoch 24/300 - Train Loss: 0.0743, Val Loss: 0.0713\n",
      "Epoch 25/300 - Train Loss: 0.0726, Val Loss: 0.0726\n",
      "Epoch 26/300 - Train Loss: 0.0714, Val Loss: 0.0697\n",
      "Epoch 27/300 - Train Loss: 0.0711, Val Loss: 0.0730\n",
      "Epoch 28/300 - Train Loss: 0.0716, Val Loss: 0.0667\n",
      "Epoch 29/300 - Train Loss: 0.0726, Val Loss: 0.0889\n",
      "Epoch 30/300 - Train Loss: 0.0703, Val Loss: 0.0688\n",
      "Epoch 31/300 - Train Loss: 0.0714, Val Loss: 0.0797\n",
      "Epoch 32/300 - Train Loss: 0.0705, Val Loss: 0.0710\n",
      "Epoch 33/300 - Train Loss: 0.0697, Val Loss: 0.0700\n",
      "Epoch 34/300 - Train Loss: 0.0688, Val Loss: 0.0682\n",
      "Epoch 35/300 - Train Loss: 0.0683, Val Loss: 0.0662\n",
      "Epoch 36/300 - Train Loss: 0.0680, Val Loss: 0.0653\n",
      "Epoch 37/300 - Train Loss: 0.0686, Val Loss: 0.0706\n",
      "Epoch 38/300 - Train Loss: 0.0672, Val Loss: 0.0726\n",
      "Epoch 39/300 - Train Loss: 0.0667, Val Loss: 0.0681\n",
      "Epoch 40/300 - Train Loss: 0.0678, Val Loss: 0.0638\n",
      "Epoch 41/300 - Train Loss: 0.0667, Val Loss: 0.0648\n",
      "Epoch 42/300 - Train Loss: 0.0629, Val Loss: 0.0662\n",
      "Epoch 43/300 - Train Loss: 0.0663, Val Loss: 0.0702\n",
      "Epoch 44/300 - Train Loss: 0.0675, Val Loss: 0.0767\n",
      "Epoch 45/300 - Train Loss: 0.0656, Val Loss: 0.0701\n",
      "Epoch 46/300 - Train Loss: 0.0634, Val Loss: 0.0704\n",
      "Epoch 47/300 - Train Loss: 0.0648, Val Loss: 0.0656\n",
      "Epoch 48/300 - Train Loss: 0.0607, Val Loss: 0.0666\n",
      "Epoch 49/300 - Train Loss: 0.0633, Val Loss: 0.0693\n",
      "Epoch 50/300 - Train Loss: 0.0616, Val Loss: 0.0671\n",
      "Epoch 51/300 - Train Loss: 0.0624, Val Loss: 0.0680\n",
      "Epoch 52/300 - Train Loss: 0.0618, Val Loss: 0.0704\n",
      "Epoch 53/300 - Train Loss: 0.0598, Val Loss: 0.0721\n",
      "Epoch 54/300 - Train Loss: 0.0606, Val Loss: 0.0649\n",
      "Epoch 55/300 - Train Loss: 0.0591, Val Loss: 0.0676\n",
      "Epoch 56/300 - Train Loss: 0.0614, Val Loss: 0.0680\n",
      "Epoch 57/300 - Train Loss: 0.0598, Val Loss: 0.0653\n",
      "Epoch 58/300 - Train Loss: 0.0602, Val Loss: 0.0662\n",
      "Epoch 59/300 - Train Loss: 0.0586, Val Loss: 0.0656\n",
      "Epoch 60/300 - Train Loss: 0.0584, Val Loss: 0.0676\n",
      "Epoch 61/300 - Train Loss: 0.0578, Val Loss: 0.0733\n",
      "Epoch 62/300 - Train Loss: 0.0579, Val Loss: 0.0659\n",
      "Epoch 63/300 - Train Loss: 0.0566, Val Loss: 0.0663\n",
      "Epoch 64/300 - Train Loss: 0.0551, Val Loss: 0.0672\n",
      "Epoch 65/300 - Train Loss: 0.0555, Val Loss: 0.0647\n",
      "Epoch 66/300 - Train Loss: 0.0572, Val Loss: 0.0673\n",
      "Epoch 67/300 - Train Loss: 0.0559, Val Loss: 0.0668\n",
      "Epoch 68/300 - Train Loss: 0.0555, Val Loss: 0.0708\n",
      "Epoch 69/300 - Train Loss: 0.0546, Val Loss: 0.0681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:29:31,753] Trial 336 finished with value: 0.9733234563037786 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.13422088112872504, 'learning_rate': 7.552775919636659e-05, 'batch_size': 32, 'weight_decay': 5.8932060112440746e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/300 - Train Loss: 0.0547, Val Loss: 0.0703\n",
      "Early stopping at epoch 70\n",
      "Macro F1 Score: 0.9733, Macro Precision: 0.9739, Macro Recall: 0.9728\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 338\n",
      "Training with F1=32, F2=32, D=2, dropout=0.11864414194979424, LR=0.00010880802588185972, BS=32, WD=4.330427213714079e-05\n",
      "Epoch 1/300 - Train Loss: 0.2553, Val Loss: 0.1328\n",
      "Epoch 2/300 - Train Loss: 0.1206, Val Loss: 0.0794\n",
      "Epoch 3/300 - Train Loss: 0.1012, Val Loss: 0.0957\n",
      "Epoch 4/300 - Train Loss: 0.0942, Val Loss: 0.0790\n",
      "Epoch 5/300 - Train Loss: 0.0893, Val Loss: 0.0817\n",
      "Epoch 6/300 - Train Loss: 0.0887, Val Loss: 0.0741\n",
      "Epoch 7/300 - Train Loss: 0.0842, Val Loss: 0.0808\n",
      "Epoch 8/300 - Train Loss: 0.0840, Val Loss: 0.0929\n",
      "Epoch 9/300 - Train Loss: 0.0836, Val Loss: 0.0870\n",
      "Epoch 10/300 - Train Loss: 0.0798, Val Loss: 0.0771\n",
      "Epoch 11/300 - Train Loss: 0.0804, Val Loss: 0.0722\n",
      "Epoch 12/300 - Train Loss: 0.0770, Val Loss: 0.0711\n",
      "Epoch 13/300 - Train Loss: 0.0781, Val Loss: 0.0722\n",
      "Epoch 14/300 - Train Loss: 0.0768, Val Loss: 0.0711\n",
      "Epoch 15/300 - Train Loss: 0.0753, Val Loss: 0.0777\n",
      "Epoch 16/300 - Train Loss: 0.0727, Val Loss: 0.0716\n",
      "Epoch 17/300 - Train Loss: 0.0736, Val Loss: 0.0754\n",
      "Epoch 18/300 - Train Loss: 0.0734, Val Loss: 0.0820\n",
      "Epoch 19/300 - Train Loss: 0.0714, Val Loss: 0.0690\n",
      "Epoch 20/300 - Train Loss: 0.0708, Val Loss: 0.0758\n",
      "Epoch 21/300 - Train Loss: 0.0715, Val Loss: 0.0780\n",
      "Epoch 22/300 - Train Loss: 0.0693, Val Loss: 0.0700\n",
      "Epoch 23/300 - Train Loss: 0.0679, Val Loss: 0.0672\n",
      "Epoch 24/300 - Train Loss: 0.0703, Val Loss: 0.0697\n",
      "Epoch 25/300 - Train Loss: 0.0688, Val Loss: 0.0798\n",
      "Epoch 26/300 - Train Loss: 0.0676, Val Loss: 0.0706\n",
      "Epoch 27/300 - Train Loss: 0.0669, Val Loss: 0.0669\n",
      "Epoch 28/300 - Train Loss: 0.0671, Val Loss: 0.0694\n",
      "Epoch 29/300 - Train Loss: 0.0685, Val Loss: 0.0681\n",
      "Epoch 30/300 - Train Loss: 0.0651, Val Loss: 0.0658\n",
      "Epoch 31/300 - Train Loss: 0.0638, Val Loss: 0.0647\n",
      "Epoch 32/300 - Train Loss: 0.0638, Val Loss: 0.0644\n",
      "Epoch 33/300 - Train Loss: 0.0640, Val Loss: 0.0713\n",
      "Epoch 34/300 - Train Loss: 0.0634, Val Loss: 0.0653\n",
      "Epoch 35/300 - Train Loss: 0.0620, Val Loss: 0.0682\n",
      "Epoch 36/300 - Train Loss: 0.0626, Val Loss: 0.0672\n",
      "Epoch 37/300 - Train Loss: 0.0644, Val Loss: 0.0644\n",
      "Epoch 38/300 - Train Loss: 0.0580, Val Loss: 0.0661\n",
      "Epoch 39/300 - Train Loss: 0.0620, Val Loss: 0.0662\n",
      "Epoch 40/300 - Train Loss: 0.0616, Val Loss: 0.0682\n",
      "Epoch 41/300 - Train Loss: 0.0595, Val Loss: 0.0690\n",
      "Epoch 42/300 - Train Loss: 0.0599, Val Loss: 0.0657\n",
      "Epoch 43/300 - Train Loss: 0.0551, Val Loss: 0.0660\n",
      "Epoch 44/300 - Train Loss: 0.0587, Val Loss: 0.0673\n",
      "Epoch 45/300 - Train Loss: 0.0565, Val Loss: 0.0725\n",
      "Epoch 46/300 - Train Loss: 0.0556, Val Loss: 0.0678\n",
      "Epoch 47/300 - Train Loss: 0.0550, Val Loss: 0.0731\n",
      "Epoch 48/300 - Train Loss: 0.0564, Val Loss: 0.0661\n",
      "Epoch 49/300 - Train Loss: 0.0549, Val Loss: 0.0676\n",
      "Epoch 50/300 - Train Loss: 0.0549, Val Loss: 0.0669\n",
      "Epoch 51/300 - Train Loss: 0.0552, Val Loss: 0.0643\n",
      "Epoch 52/300 - Train Loss: 0.0548, Val Loss: 0.0650\n",
      "Epoch 53/300 - Train Loss: 0.0531, Val Loss: 0.0689\n",
      "Epoch 54/300 - Train Loss: 0.0529, Val Loss: 0.0671\n",
      "Epoch 55/300 - Train Loss: 0.0540, Val Loss: 0.0673\n",
      "Epoch 56/300 - Train Loss: 0.0515, Val Loss: 0.0711\n",
      "Epoch 57/300 - Train Loss: 0.0524, Val Loss: 0.0704\n",
      "Epoch 58/300 - Train Loss: 0.0515, Val Loss: 0.0669\n",
      "Epoch 59/300 - Train Loss: 0.0494, Val Loss: 0.0713\n",
      "Epoch 60/300 - Train Loss: 0.0527, Val Loss: 0.0676\n",
      "Epoch 61/300 - Train Loss: 0.0504, Val Loss: 0.0687\n",
      "Epoch 62/300 - Train Loss: 0.0485, Val Loss: 0.0700\n",
      "Epoch 63/300 - Train Loss: 0.0508, Val Loss: 0.0669\n",
      "Epoch 64/300 - Train Loss: 0.0490, Val Loss: 0.0692\n",
      "Epoch 65/300 - Train Loss: 0.0484, Val Loss: 0.0698\n",
      "Epoch 66/300 - Train Loss: 0.0467, Val Loss: 0.0650\n",
      "Epoch 67/300 - Train Loss: 0.0512, Val Loss: 0.0688\n",
      "Epoch 68/300 - Train Loss: 0.0466, Val Loss: 0.0685\n",
      "Epoch 69/300 - Train Loss: 0.0471, Val Loss: 0.0747\n",
      "Epoch 70/300 - Train Loss: 0.0467, Val Loss: 0.0687\n",
      "Epoch 71/300 - Train Loss: 0.0452, Val Loss: 0.0753\n",
      "Epoch 72/300 - Train Loss: 0.0457, Val Loss: 0.0740\n",
      "Epoch 73/300 - Train Loss: 0.0460, Val Loss: 0.0742\n",
      "Epoch 74/300 - Train Loss: 0.0457, Val Loss: 0.0763\n",
      "Epoch 75/300 - Train Loss: 0.0444, Val Loss: 0.0707\n",
      "Epoch 76/300 - Train Loss: 0.0428, Val Loss: 0.0729\n",
      "Epoch 77/300 - Train Loss: 0.0454, Val Loss: 0.0735\n",
      "Epoch 78/300 - Train Loss: 0.0422, Val Loss: 0.0676\n",
      "Epoch 79/300 - Train Loss: 0.0419, Val Loss: 0.0691\n",
      "Epoch 80/300 - Train Loss: 0.0426, Val Loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:33:13,451] Trial 337 finished with value: 0.972242443428699 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.11864414194979424, 'learning_rate': 0.00010880802588185972, 'batch_size': 32, 'weight_decay': 4.330427213714079e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300 - Train Loss: 0.0444, Val Loss: 0.0729\n",
      "Early stopping at epoch 81\n",
      "Macro F1 Score: 0.9722, Macro Precision: 0.9685, Macro Recall: 0.9763\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 339\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1657311329570526, LR=8.841609581107197e-05, BS=32, WD=0.0004527498890730825\n",
      "Epoch 1/300 - Train Loss: 0.2760, Val Loss: 0.1230\n",
      "Epoch 2/300 - Train Loss: 0.1270, Val Loss: 0.0942\n",
      "Epoch 3/300 - Train Loss: 0.1057, Val Loss: 0.0830\n",
      "Epoch 4/300 - Train Loss: 0.0976, Val Loss: 0.0809\n",
      "Epoch 5/300 - Train Loss: 0.0921, Val Loss: 0.0743\n",
      "Epoch 6/300 - Train Loss: 0.0911, Val Loss: 0.0825\n",
      "Epoch 7/300 - Train Loss: 0.0900, Val Loss: 0.0735\n",
      "Epoch 8/300 - Train Loss: 0.0879, Val Loss: 0.0732\n",
      "Epoch 9/300 - Train Loss: 0.0879, Val Loss: 0.0773\n",
      "Epoch 10/300 - Train Loss: 0.0850, Val Loss: 0.0735\n",
      "Epoch 11/300 - Train Loss: 0.0817, Val Loss: 0.0711\n",
      "Epoch 12/300 - Train Loss: 0.0803, Val Loss: 0.0710\n",
      "Epoch 13/300 - Train Loss: 0.0814, Val Loss: 0.0766\n",
      "Epoch 14/300 - Train Loss: 0.0799, Val Loss: 0.0740\n",
      "Epoch 15/300 - Train Loss: 0.0810, Val Loss: 0.0693\n",
      "Epoch 16/300 - Train Loss: 0.0777, Val Loss: 0.0684\n",
      "Epoch 17/300 - Train Loss: 0.0773, Val Loss: 0.0716\n",
      "Epoch 18/300 - Train Loss: 0.0763, Val Loss: 0.0720\n",
      "Epoch 19/300 - Train Loss: 0.0773, Val Loss: 0.0721\n",
      "Epoch 20/300 - Train Loss: 0.0751, Val Loss: 0.0716\n",
      "Epoch 21/300 - Train Loss: 0.0753, Val Loss: 0.0736\n",
      "Epoch 22/300 - Train Loss: 0.0766, Val Loss: 0.0665\n",
      "Epoch 23/300 - Train Loss: 0.0760, Val Loss: 0.0702\n",
      "Epoch 24/300 - Train Loss: 0.0732, Val Loss: 0.0692\n",
      "Epoch 25/300 - Train Loss: 0.0745, Val Loss: 0.0732\n",
      "Epoch 26/300 - Train Loss: 0.0728, Val Loss: 0.0701\n",
      "Epoch 27/300 - Train Loss: 0.0727, Val Loss: 0.0893\n",
      "Epoch 28/300 - Train Loss: 0.0719, Val Loss: 0.0726\n",
      "Epoch 29/300 - Train Loss: 0.0733, Val Loss: 0.0760\n",
      "Epoch 30/300 - Train Loss: 0.0706, Val Loss: 0.0710\n",
      "Epoch 31/300 - Train Loss: 0.0694, Val Loss: 0.0749\n",
      "Epoch 32/300 - Train Loss: 0.0695, Val Loss: 0.0710\n",
      "Epoch 33/300 - Train Loss: 0.0731, Val Loss: 0.0694\n",
      "Epoch 34/300 - Train Loss: 0.0694, Val Loss: 0.0720\n",
      "Epoch 35/300 - Train Loss: 0.0712, Val Loss: 0.0724\n",
      "Epoch 36/300 - Train Loss: 0.0695, Val Loss: 0.0720\n",
      "Epoch 37/300 - Train Loss: 0.0700, Val Loss: 0.0748\n",
      "Epoch 38/300 - Train Loss: 0.0681, Val Loss: 0.0678\n",
      "Epoch 39/300 - Train Loss: 0.0678, Val Loss: 0.0709\n",
      "Epoch 40/300 - Train Loss: 0.0669, Val Loss: 0.0718\n",
      "Epoch 41/300 - Train Loss: 0.0667, Val Loss: 0.0717\n",
      "Epoch 42/300 - Train Loss: 0.0658, Val Loss: 0.0697\n",
      "Epoch 43/300 - Train Loss: 0.0665, Val Loss: 0.0673\n",
      "Epoch 44/300 - Train Loss: 0.0646, Val Loss: 0.0680\n",
      "Epoch 45/300 - Train Loss: 0.0657, Val Loss: 0.0703\n",
      "Epoch 46/300 - Train Loss: 0.0652, Val Loss: 0.0664\n",
      "Epoch 47/300 - Train Loss: 0.0665, Val Loss: 0.0726\n",
      "Epoch 48/300 - Train Loss: 0.0663, Val Loss: 0.0713\n",
      "Epoch 49/300 - Train Loss: 0.0656, Val Loss: 0.0696\n",
      "Epoch 50/300 - Train Loss: 0.0647, Val Loss: 0.0722\n",
      "Epoch 51/300 - Train Loss: 0.0665, Val Loss: 0.0712\n",
      "Epoch 52/300 - Train Loss: 0.0635, Val Loss: 0.0746\n",
      "Epoch 53/300 - Train Loss: 0.0629, Val Loss: 0.0668\n",
      "Epoch 54/300 - Train Loss: 0.0652, Val Loss: 0.0707\n",
      "Epoch 55/300 - Train Loss: 0.0666, Val Loss: 0.0679\n",
      "Epoch 56/300 - Train Loss: 0.0638, Val Loss: 0.0691\n",
      "Epoch 57/300 - Train Loss: 0.0619, Val Loss: 0.0690\n",
      "Epoch 58/300 - Train Loss: 0.0604, Val Loss: 0.0715\n",
      "Epoch 59/300 - Train Loss: 0.0634, Val Loss: 0.0709\n",
      "Epoch 60/300 - Train Loss: 0.0620, Val Loss: 0.0712\n",
      "Epoch 61/300 - Train Loss: 0.0602, Val Loss: 0.0706\n",
      "Epoch 62/300 - Train Loss: 0.0615, Val Loss: 0.0717\n",
      "Epoch 63/300 - Train Loss: 0.0605, Val Loss: 0.0679\n",
      "Epoch 64/300 - Train Loss: 0.0607, Val Loss: 0.0704\n",
      "Epoch 65/300 - Train Loss: 0.0641, Val Loss: 0.0663\n",
      "Epoch 66/300 - Train Loss: 0.0612, Val Loss: 0.0766\n",
      "Epoch 67/300 - Train Loss: 0.0594, Val Loss: 0.0665\n",
      "Epoch 68/300 - Train Loss: 0.0595, Val Loss: 0.0700\n",
      "Epoch 69/300 - Train Loss: 0.0626, Val Loss: 0.0731\n",
      "Epoch 70/300 - Train Loss: 0.0595, Val Loss: 0.0701\n",
      "Epoch 71/300 - Train Loss: 0.0602, Val Loss: 0.0684\n",
      "Epoch 72/300 - Train Loss: 0.0614, Val Loss: 0.0672\n",
      "Epoch 73/300 - Train Loss: 0.0608, Val Loss: 0.0690\n",
      "Epoch 74/300 - Train Loss: 0.0595, Val Loss: 0.0716\n",
      "Epoch 75/300 - Train Loss: 0.0622, Val Loss: 0.0694\n",
      "Epoch 76/300 - Train Loss: 0.0592, Val Loss: 0.0697\n",
      "Epoch 77/300 - Train Loss: 0.0593, Val Loss: 0.0697\n",
      "Epoch 78/300 - Train Loss: 0.0576, Val Loss: 0.0716\n",
      "Epoch 79/300 - Train Loss: 0.0600, Val Loss: 0.0768\n",
      "Epoch 80/300 - Train Loss: 0.0582, Val Loss: 0.0652\n",
      "Epoch 81/300 - Train Loss: 0.0578, Val Loss: 0.0680\n",
      "Epoch 82/300 - Train Loss: 0.0580, Val Loss: 0.0708\n",
      "Epoch 83/300 - Train Loss: 0.0591, Val Loss: 0.0687\n",
      "Epoch 84/300 - Train Loss: 0.0564, Val Loss: 0.0719\n",
      "Epoch 85/300 - Train Loss: 0.0563, Val Loss: 0.0741\n",
      "Epoch 86/300 - Train Loss: 0.0598, Val Loss: 0.0702\n",
      "Epoch 87/300 - Train Loss: 0.0559, Val Loss: 0.0744\n",
      "Epoch 88/300 - Train Loss: 0.0555, Val Loss: 0.0744\n",
      "Epoch 89/300 - Train Loss: 0.0558, Val Loss: 0.0673\n",
      "Epoch 90/300 - Train Loss: 0.0547, Val Loss: 0.0728\n",
      "Epoch 91/300 - Train Loss: 0.0560, Val Loss: 0.0674\n",
      "Epoch 92/300 - Train Loss: 0.0580, Val Loss: 0.0658\n",
      "Epoch 93/300 - Train Loss: 0.0574, Val Loss: 0.0701\n",
      "Epoch 94/300 - Train Loss: 0.0561, Val Loss: 0.0708\n",
      "Epoch 95/300 - Train Loss: 0.0559, Val Loss: 0.0750\n",
      "Epoch 96/300 - Train Loss: 0.0547, Val Loss: 0.0709\n",
      "Epoch 97/300 - Train Loss: 0.0547, Val Loss: 0.0677\n",
      "Epoch 98/300 - Train Loss: 0.0556, Val Loss: 0.0733\n",
      "Epoch 99/300 - Train Loss: 0.0566, Val Loss: 0.0677\n",
      "Epoch 100/300 - Train Loss: 0.0553, Val Loss: 0.0697\n",
      "Epoch 101/300 - Train Loss: 0.0564, Val Loss: 0.0740\n",
      "Epoch 102/300 - Train Loss: 0.0552, Val Loss: 0.0686\n",
      "Epoch 103/300 - Train Loss: 0.0550, Val Loss: 0.0722\n",
      "Epoch 104/300 - Train Loss: 0.0574, Val Loss: 0.0724\n",
      "Epoch 105/300 - Train Loss: 0.0553, Val Loss: 0.0706\n",
      "Epoch 106/300 - Train Loss: 0.0549, Val Loss: 0.0715\n",
      "Epoch 107/300 - Train Loss: 0.0535, Val Loss: 0.0714\n",
      "Epoch 108/300 - Train Loss: 0.0526, Val Loss: 0.0713\n",
      "Epoch 109/300 - Train Loss: 0.0537, Val Loss: 0.0722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:38:14,269] Trial 338 finished with value: 0.968744919301237 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1657311329570526, 'learning_rate': 8.841609581107197e-05, 'batch_size': 32, 'weight_decay': 0.0004527498890730825}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/300 - Train Loss: 0.0522, Val Loss: 0.0762\n",
      "Early stopping at epoch 110\n",
      "Macro F1 Score: 0.9687, Macro Precision: 0.9718, Macro Recall: 0.9658\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 340\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14752157937595628, LR=7.085193509957984e-05, BS=32, WD=6.606635127660775e-05\n",
      "Epoch 1/300 - Train Loss: 0.3277, Val Loss: 0.1449\n",
      "Epoch 2/300 - Train Loss: 0.1437, Val Loss: 0.1084\n",
      "Epoch 3/300 - Train Loss: 0.1157, Val Loss: 0.0987\n",
      "Epoch 4/300 - Train Loss: 0.1050, Val Loss: 0.0823\n",
      "Epoch 5/300 - Train Loss: 0.1008, Val Loss: 0.0789\n",
      "Epoch 6/300 - Train Loss: 0.0946, Val Loss: 0.0863\n",
      "Epoch 7/300 - Train Loss: 0.0917, Val Loss: 0.0807\n",
      "Epoch 8/300 - Train Loss: 0.0891, Val Loss: 0.0778\n",
      "Epoch 9/300 - Train Loss: 0.0867, Val Loss: 0.0787\n",
      "Epoch 10/300 - Train Loss: 0.0886, Val Loss: 0.0742\n",
      "Epoch 11/300 - Train Loss: 0.0848, Val Loss: 0.0697\n",
      "Epoch 12/300 - Train Loss: 0.0834, Val Loss: 0.0741\n",
      "Epoch 13/300 - Train Loss: 0.0818, Val Loss: 0.0734\n",
      "Epoch 14/300 - Train Loss: 0.0801, Val Loss: 0.0718\n",
      "Epoch 15/300 - Train Loss: 0.0786, Val Loss: 0.0717\n",
      "Epoch 16/300 - Train Loss: 0.0793, Val Loss: 0.0707\n",
      "Epoch 17/300 - Train Loss: 0.0803, Val Loss: 0.0801\n",
      "Epoch 18/300 - Train Loss: 0.0787, Val Loss: 0.0712\n",
      "Epoch 19/300 - Train Loss: 0.0762, Val Loss: 0.0802\n",
      "Epoch 20/300 - Train Loss: 0.0767, Val Loss: 0.0734\n",
      "Epoch 21/300 - Train Loss: 0.0776, Val Loss: 0.0722\n",
      "Epoch 22/300 - Train Loss: 0.0771, Val Loss: 0.0746\n",
      "Epoch 23/300 - Train Loss: 0.0742, Val Loss: 0.0743\n",
      "Epoch 24/300 - Train Loss: 0.0740, Val Loss: 0.0764\n",
      "Epoch 25/300 - Train Loss: 0.0719, Val Loss: 0.0753\n",
      "Epoch 26/300 - Train Loss: 0.0721, Val Loss: 0.0756\n",
      "Epoch 27/300 - Train Loss: 0.0731, Val Loss: 0.0719\n",
      "Epoch 28/300 - Train Loss: 0.0716, Val Loss: 0.0766\n",
      "Epoch 29/300 - Train Loss: 0.0715, Val Loss: 0.0766\n",
      "Epoch 30/300 - Train Loss: 0.0695, Val Loss: 0.0749\n",
      "Epoch 31/300 - Train Loss: 0.0688, Val Loss: 0.0743\n",
      "Epoch 32/300 - Train Loss: 0.0694, Val Loss: 0.0729\n",
      "Epoch 33/300 - Train Loss: 0.0682, Val Loss: 0.0752\n",
      "Epoch 34/300 - Train Loss: 0.0686, Val Loss: 0.0756\n",
      "Epoch 35/300 - Train Loss: 0.0712, Val Loss: 0.0703\n",
      "Epoch 36/300 - Train Loss: 0.0674, Val Loss: 0.0770\n",
      "Epoch 37/300 - Train Loss: 0.0674, Val Loss: 0.0716\n",
      "Epoch 38/300 - Train Loss: 0.0677, Val Loss: 0.0719\n",
      "Epoch 39/300 - Train Loss: 0.0646, Val Loss: 0.0720\n",
      "Epoch 40/300 - Train Loss: 0.0642, Val Loss: 0.0716\n",
      "Epoch 41/300 - Train Loss: 0.0701, Val Loss: 0.0671\n",
      "Epoch 42/300 - Train Loss: 0.0663, Val Loss: 0.0735\n",
      "Epoch 43/300 - Train Loss: 0.0653, Val Loss: 0.0746\n",
      "Epoch 44/300 - Train Loss: 0.0648, Val Loss: 0.0742\n",
      "Epoch 45/300 - Train Loss: 0.0635, Val Loss: 0.0700\n",
      "Epoch 46/300 - Train Loss: 0.0639, Val Loss: 0.0727\n",
      "Epoch 47/300 - Train Loss: 0.0628, Val Loss: 0.0733\n",
      "Epoch 48/300 - Train Loss: 0.0652, Val Loss: 0.0752\n",
      "Epoch 49/300 - Train Loss: 0.0639, Val Loss: 0.0724\n",
      "Epoch 50/300 - Train Loss: 0.0625, Val Loss: 0.0724\n",
      "Epoch 51/300 - Train Loss: 0.0659, Val Loss: 0.0792\n",
      "Epoch 52/300 - Train Loss: 0.0605, Val Loss: 0.0695\n",
      "Epoch 53/300 - Train Loss: 0.0608, Val Loss: 0.0714\n",
      "Epoch 54/300 - Train Loss: 0.0599, Val Loss: 0.0751\n",
      "Epoch 55/300 - Train Loss: 0.0592, Val Loss: 0.0707\n",
      "Epoch 56/300 - Train Loss: 0.0601, Val Loss: 0.0725\n",
      "Epoch 57/300 - Train Loss: 0.0613, Val Loss: 0.0742\n",
      "Epoch 58/300 - Train Loss: 0.0612, Val Loss: 0.0692\n",
      "Epoch 59/300 - Train Loss: 0.0584, Val Loss: 0.0755\n",
      "Epoch 60/300 - Train Loss: 0.0586, Val Loss: 0.0729\n",
      "Epoch 61/300 - Train Loss: 0.0621, Val Loss: 0.0739\n",
      "Epoch 62/300 - Train Loss: 0.0594, Val Loss: 0.0864\n",
      "Epoch 63/300 - Train Loss: 0.0607, Val Loss: 0.0812\n",
      "Epoch 64/300 - Train Loss: 0.0583, Val Loss: 0.0706\n",
      "Epoch 65/300 - Train Loss: 0.0573, Val Loss: 0.0723\n",
      "Epoch 66/300 - Train Loss: 0.0591, Val Loss: 0.0747\n",
      "Epoch 67/300 - Train Loss: 0.0583, Val Loss: 0.0716\n",
      "Epoch 68/300 - Train Loss: 0.0575, Val Loss: 0.0711\n",
      "Epoch 69/300 - Train Loss: 0.0540, Val Loss: 0.0704\n",
      "Epoch 70/300 - Train Loss: 0.0554, Val Loss: 0.0734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:41:29,062] Trial 339 finished with value: 0.9638188775712911 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14752157937595628, 'learning_rate': 7.085193509957984e-05, 'batch_size': 32, 'weight_decay': 6.606635127660775e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300 - Train Loss: 0.0579, Val Loss: 0.0717\n",
      "Early stopping at epoch 71\n",
      "Macro F1 Score: 0.9638, Macro Precision: 0.9556, Macro Recall: 0.9726\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 341\n",
      "Training with F1=32, F2=32, D=2, dropout=0.19836660117717694, LR=0.00012732043134626306, BS=128, WD=4.9482472554710035e-05\n",
      "Epoch 1/300 - Train Loss: 0.4053, Val Loss: 0.1971\n",
      "Epoch 2/300 - Train Loss: 0.1727, Val Loss: 0.1420\n",
      "Epoch 3/300 - Train Loss: 0.1334, Val Loss: 0.1259\n",
      "Epoch 4/300 - Train Loss: 0.1086, Val Loss: 0.0948\n",
      "Epoch 5/300 - Train Loss: 0.0976, Val Loss: 0.0875\n",
      "Epoch 6/300 - Train Loss: 0.0905, Val Loss: 0.0876\n",
      "Epoch 7/300 - Train Loss: 0.0892, Val Loss: 0.1016\n",
      "Epoch 8/300 - Train Loss: 0.0859, Val Loss: 0.0880\n",
      "Epoch 9/300 - Train Loss: 0.0822, Val Loss: 0.0797\n",
      "Epoch 10/300 - Train Loss: 0.0799, Val Loss: 0.0779\n",
      "Epoch 11/300 - Train Loss: 0.0788, Val Loss: 0.0782\n",
      "Epoch 12/300 - Train Loss: 0.0778, Val Loss: 0.0769\n",
      "Epoch 13/300 - Train Loss: 0.0767, Val Loss: 0.0759\n",
      "Epoch 14/300 - Train Loss: 0.0765, Val Loss: 0.0760\n",
      "Epoch 15/300 - Train Loss: 0.0752, Val Loss: 0.0779\n",
      "Epoch 16/300 - Train Loss: 0.0745, Val Loss: 0.0746\n",
      "Epoch 17/300 - Train Loss: 0.0743, Val Loss: 0.0807\n",
      "Epoch 18/300 - Train Loss: 0.0721, Val Loss: 0.0751\n",
      "Epoch 19/300 - Train Loss: 0.0715, Val Loss: 0.0762\n",
      "Epoch 20/300 - Train Loss: 0.0712, Val Loss: 0.0779\n",
      "Epoch 21/300 - Train Loss: 0.0702, Val Loss: 0.0708\n",
      "Epoch 22/300 - Train Loss: 0.0699, Val Loss: 0.0745\n",
      "Epoch 23/300 - Train Loss: 0.0711, Val Loss: 0.0751\n",
      "Epoch 24/300 - Train Loss: 0.0690, Val Loss: 0.0738\n",
      "Epoch 25/300 - Train Loss: 0.0686, Val Loss: 0.0739\n",
      "Epoch 26/300 - Train Loss: 0.0684, Val Loss: 0.0797\n",
      "Epoch 27/300 - Train Loss: 0.0683, Val Loss: 0.0778\n",
      "Epoch 28/300 - Train Loss: 0.0667, Val Loss: 0.0747\n",
      "Epoch 29/300 - Train Loss: 0.0673, Val Loss: 0.0755\n",
      "Epoch 30/300 - Train Loss: 0.0670, Val Loss: 0.0742\n",
      "Epoch 31/300 - Train Loss: 0.0669, Val Loss: 0.0760\n",
      "Epoch 32/300 - Train Loss: 0.0651, Val Loss: 0.0782\n",
      "Epoch 33/300 - Train Loss: 0.0651, Val Loss: 0.0753\n",
      "Epoch 34/300 - Train Loss: 0.0657, Val Loss: 0.0768\n",
      "Epoch 35/300 - Train Loss: 0.0646, Val Loss: 0.0742\n",
      "Epoch 36/300 - Train Loss: 0.0653, Val Loss: 0.0756\n",
      "Epoch 37/300 - Train Loss: 0.0635, Val Loss: 0.0707\n",
      "Epoch 38/300 - Train Loss: 0.0631, Val Loss: 0.0741\n",
      "Epoch 39/300 - Train Loss: 0.0633, Val Loss: 0.0734\n",
      "Epoch 40/300 - Train Loss: 0.0626, Val Loss: 0.0736\n",
      "Epoch 41/300 - Train Loss: 0.0621, Val Loss: 0.0750\n",
      "Epoch 42/300 - Train Loss: 0.0627, Val Loss: 0.0714\n",
      "Epoch 43/300 - Train Loss: 0.0606, Val Loss: 0.0715\n",
      "Epoch 44/300 - Train Loss: 0.0617, Val Loss: 0.0739\n",
      "Epoch 45/300 - Train Loss: 0.0609, Val Loss: 0.0742\n",
      "Epoch 46/300 - Train Loss: 0.0614, Val Loss: 0.0738\n",
      "Epoch 47/300 - Train Loss: 0.0603, Val Loss: 0.0731\n",
      "Epoch 48/300 - Train Loss: 0.0590, Val Loss: 0.0711\n",
      "Epoch 49/300 - Train Loss: 0.0581, Val Loss: 0.0728\n",
      "Epoch 50/300 - Train Loss: 0.0583, Val Loss: 0.0712\n",
      "Epoch 51/300 - Train Loss: 0.0587, Val Loss: 0.0736\n",
      "Epoch 52/300 - Train Loss: 0.0584, Val Loss: 0.0737\n",
      "Epoch 53/300 - Train Loss: 0.0585, Val Loss: 0.0722\n",
      "Epoch 54/300 - Train Loss: 0.0576, Val Loss: 0.0709\n",
      "Epoch 55/300 - Train Loss: 0.0581, Val Loss: 0.0760\n",
      "Epoch 56/300 - Train Loss: 0.0587, Val Loss: 0.0710\n",
      "Epoch 57/300 - Train Loss: 0.0562, Val Loss: 0.0733\n",
      "Epoch 58/300 - Train Loss: 0.0588, Val Loss: 0.0734\n",
      "Epoch 59/300 - Train Loss: 0.0559, Val Loss: 0.0752\n",
      "Epoch 60/300 - Train Loss: 0.0561, Val Loss: 0.0742\n",
      "Epoch 61/300 - Train Loss: 0.0583, Val Loss: 0.0737\n",
      "Epoch 62/300 - Train Loss: 0.0569, Val Loss: 0.0728\n",
      "Epoch 63/300 - Train Loss: 0.0561, Val Loss: 0.0714\n",
      "Epoch 64/300 - Train Loss: 0.0552, Val Loss: 0.0746\n",
      "Epoch 65/300 - Train Loss: 0.0547, Val Loss: 0.0740\n",
      "Epoch 66/300 - Train Loss: 0.0538, Val Loss: 0.0761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:43:55,456] Trial 340 finished with value: 0.961834041790475 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.19836660117717694, 'learning_rate': 0.00012732043134626306, 'batch_size': 128, 'weight_decay': 4.9482472554710035e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/300 - Train Loss: 0.0554, Val Loss: 0.0777\n",
      "Early stopping at epoch 67\n",
      "Macro F1 Score: 0.9618, Macro Precision: 0.9541, Macro Recall: 0.9702\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 342\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14411927933590535, LR=9.796665515905153e-05, BS=32, WD=3.555061963673996e-05\n",
      "Epoch 1/300 - Train Loss: 0.2440, Val Loss: 0.1199\n",
      "Epoch 2/300 - Train Loss: 0.1098, Val Loss: 0.0836\n",
      "Epoch 3/300 - Train Loss: 0.1012, Val Loss: 0.0783\n",
      "Epoch 4/300 - Train Loss: 0.0946, Val Loss: 0.0828\n",
      "Epoch 5/300 - Train Loss: 0.0928, Val Loss: 0.0754\n",
      "Epoch 6/300 - Train Loss: 0.0892, Val Loss: 0.0726\n",
      "Epoch 7/300 - Train Loss: 0.0872, Val Loss: 0.0752\n",
      "Epoch 8/300 - Train Loss: 0.0859, Val Loss: 0.0731\n",
      "Epoch 9/300 - Train Loss: 0.0829, Val Loss: 0.0827\n",
      "Epoch 10/300 - Train Loss: 0.0828, Val Loss: 0.0766\n",
      "Epoch 11/300 - Train Loss: 0.0801, Val Loss: 0.0728\n",
      "Epoch 12/300 - Train Loss: 0.0835, Val Loss: 0.0939\n",
      "Epoch 13/300 - Train Loss: 0.0799, Val Loss: 0.0777\n",
      "Epoch 14/300 - Train Loss: 0.0807, Val Loss: 0.0751\n",
      "Epoch 15/300 - Train Loss: 0.0790, Val Loss: 0.0697\n",
      "Epoch 16/300 - Train Loss: 0.0765, Val Loss: 0.0694\n",
      "Epoch 17/300 - Train Loss: 0.0760, Val Loss: 0.0715\n",
      "Epoch 18/300 - Train Loss: 0.0741, Val Loss: 0.0729\n",
      "Epoch 19/300 - Train Loss: 0.0746, Val Loss: 0.0811\n",
      "Epoch 20/300 - Train Loss: 0.0747, Val Loss: 0.0736\n",
      "Epoch 21/300 - Train Loss: 0.0749, Val Loss: 0.0702\n",
      "Epoch 22/300 - Train Loss: 0.0712, Val Loss: 0.0695\n",
      "Epoch 23/300 - Train Loss: 0.0747, Val Loss: 0.0804\n",
      "Epoch 24/300 - Train Loss: 0.0717, Val Loss: 0.0724\n",
      "Epoch 25/300 - Train Loss: 0.0710, Val Loss: 0.0768\n",
      "Epoch 26/300 - Train Loss: 0.0715, Val Loss: 0.0721\n",
      "Epoch 27/300 - Train Loss: 0.0721, Val Loss: 0.0703\n",
      "Epoch 28/300 - Train Loss: 0.0728, Val Loss: 0.0806\n",
      "Epoch 29/300 - Train Loss: 0.0684, Val Loss: 0.0711\n",
      "Epoch 30/300 - Train Loss: 0.0690, Val Loss: 0.0693\n",
      "Epoch 31/300 - Train Loss: 0.0687, Val Loss: 0.0705\n",
      "Epoch 32/300 - Train Loss: 0.0672, Val Loss: 0.0779\n",
      "Epoch 33/300 - Train Loss: 0.0684, Val Loss: 0.0692\n",
      "Epoch 34/300 - Train Loss: 0.0690, Val Loss: 0.0704\n",
      "Epoch 35/300 - Train Loss: 0.0690, Val Loss: 0.0736\n",
      "Epoch 36/300 - Train Loss: 0.0645, Val Loss: 0.0693\n",
      "Epoch 37/300 - Train Loss: 0.0650, Val Loss: 0.0746\n",
      "Epoch 38/300 - Train Loss: 0.0663, Val Loss: 0.0741\n",
      "Epoch 39/300 - Train Loss: 0.0639, Val Loss: 0.0722\n",
      "Epoch 40/300 - Train Loss: 0.0655, Val Loss: 0.0712\n",
      "Epoch 41/300 - Train Loss: 0.0637, Val Loss: 0.0661\n",
      "Epoch 42/300 - Train Loss: 0.0639, Val Loss: 0.0707\n",
      "Epoch 43/300 - Train Loss: 0.0622, Val Loss: 0.0679\n",
      "Epoch 44/300 - Train Loss: 0.0614, Val Loss: 0.0683\n",
      "Epoch 45/300 - Train Loss: 0.0618, Val Loss: 0.0687\n",
      "Epoch 46/300 - Train Loss: 0.0600, Val Loss: 0.0727\n",
      "Epoch 47/300 - Train Loss: 0.0598, Val Loss: 0.0681\n",
      "Epoch 48/300 - Train Loss: 0.0608, Val Loss: 0.0718\n",
      "Epoch 49/300 - Train Loss: 0.0594, Val Loss: 0.0722\n",
      "Epoch 50/300 - Train Loss: 0.0581, Val Loss: 0.0754\n",
      "Epoch 51/300 - Train Loss: 0.0590, Val Loss: 0.0677\n",
      "Epoch 52/300 - Train Loss: 0.0585, Val Loss: 0.0721\n",
      "Epoch 53/300 - Train Loss: 0.0583, Val Loss: 0.0675\n",
      "Epoch 54/300 - Train Loss: 0.0589, Val Loss: 0.0689\n",
      "Epoch 55/300 - Train Loss: 0.0566, Val Loss: 0.0680\n",
      "Epoch 56/300 - Train Loss: 0.0562, Val Loss: 0.0731\n",
      "Epoch 57/300 - Train Loss: 0.0554, Val Loss: 0.0714\n",
      "Epoch 58/300 - Train Loss: 0.0567, Val Loss: 0.0708\n",
      "Epoch 59/300 - Train Loss: 0.0561, Val Loss: 0.0730\n",
      "Epoch 60/300 - Train Loss: 0.0551, Val Loss: 0.0716\n",
      "Epoch 61/300 - Train Loss: 0.0565, Val Loss: 0.0674\n",
      "Epoch 62/300 - Train Loss: 0.0541, Val Loss: 0.0705\n",
      "Epoch 63/300 - Train Loss: 0.0527, Val Loss: 0.0699\n",
      "Epoch 64/300 - Train Loss: 0.0544, Val Loss: 0.0706\n",
      "Epoch 65/300 - Train Loss: 0.0525, Val Loss: 0.0712\n",
      "Epoch 66/300 - Train Loss: 0.0538, Val Loss: 0.0711\n",
      "Epoch 67/300 - Train Loss: 0.0518, Val Loss: 0.0715\n",
      "Epoch 68/300 - Train Loss: 0.0532, Val Loss: 0.0775\n",
      "Epoch 69/300 - Train Loss: 0.0519, Val Loss: 0.0691\n",
      "Epoch 70/300 - Train Loss: 0.0514, Val Loss: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:47:09,809] Trial 341 finished with value: 0.9742668990147175 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14411927933590535, 'learning_rate': 9.796665515905153e-05, 'batch_size': 32, 'weight_decay': 3.555061963673996e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300 - Train Loss: 0.0506, Val Loss: 0.0669\n",
      "Early stopping at epoch 71\n",
      "Macro F1 Score: 0.9743, Macro Precision: 0.9769, Macro Recall: 0.9717\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 343\n",
      "Training with F1=32, F2=32, D=2, dropout=0.13892642296045501, LR=8.237624453223097e-05, BS=32, WD=3.5242946526581315e-05\n",
      "Epoch 1/300 - Train Loss: 0.2733, Val Loss: 0.1184\n",
      "Epoch 2/300 - Train Loss: 0.1214, Val Loss: 0.0924\n",
      "Epoch 3/300 - Train Loss: 0.1042, Val Loss: 0.0822\n",
      "Epoch 4/300 - Train Loss: 0.0985, Val Loss: 0.0776\n",
      "Epoch 5/300 - Train Loss: 0.0943, Val Loss: 0.0773\n",
      "Epoch 6/300 - Train Loss: 0.0937, Val Loss: 0.0764\n",
      "Epoch 7/300 - Train Loss: 0.0887, Val Loss: 0.0764\n",
      "Epoch 8/300 - Train Loss: 0.0849, Val Loss: 0.0759\n",
      "Epoch 9/300 - Train Loss: 0.0856, Val Loss: 0.0721\n",
      "Epoch 10/300 - Train Loss: 0.0827, Val Loss: 0.0771\n",
      "Epoch 11/300 - Train Loss: 0.0841, Val Loss: 0.0781\n",
      "Epoch 12/300 - Train Loss: 0.0833, Val Loss: 0.0755\n",
      "Epoch 13/300 - Train Loss: 0.0812, Val Loss: 0.0791\n",
      "Epoch 14/300 - Train Loss: 0.0797, Val Loss: 0.0716\n",
      "Epoch 15/300 - Train Loss: 0.0793, Val Loss: 0.0750\n",
      "Epoch 16/300 - Train Loss: 0.0769, Val Loss: 0.0761\n",
      "Epoch 17/300 - Train Loss: 0.0783, Val Loss: 0.0694\n",
      "Epoch 18/300 - Train Loss: 0.0769, Val Loss: 0.0708\n",
      "Epoch 19/300 - Train Loss: 0.0758, Val Loss: 0.0744\n",
      "Epoch 20/300 - Train Loss: 0.0749, Val Loss: 0.0695\n",
      "Epoch 21/300 - Train Loss: 0.0754, Val Loss: 0.0701\n",
      "Epoch 22/300 - Train Loss: 0.0729, Val Loss: 0.0693\n",
      "Epoch 23/300 - Train Loss: 0.0729, Val Loss: 0.0700\n",
      "Epoch 24/300 - Train Loss: 0.0729, Val Loss: 0.0749\n",
      "Epoch 25/300 - Train Loss: 0.0736, Val Loss: 0.0788\n",
      "Epoch 26/300 - Train Loss: 0.0723, Val Loss: 0.0697\n",
      "Epoch 27/300 - Train Loss: 0.0712, Val Loss: 0.0728\n",
      "Epoch 28/300 - Train Loss: 0.0714, Val Loss: 0.0729\n",
      "Epoch 29/300 - Train Loss: 0.0684, Val Loss: 0.0704\n",
      "Epoch 30/300 - Train Loss: 0.0697, Val Loss: 0.0704\n",
      "Epoch 31/300 - Train Loss: 0.0694, Val Loss: 0.0754\n",
      "Epoch 32/300 - Train Loss: 0.0671, Val Loss: 0.0676\n",
      "Epoch 33/300 - Train Loss: 0.0690, Val Loss: 0.0719\n",
      "Epoch 34/300 - Train Loss: 0.0674, Val Loss: 0.0721\n",
      "Epoch 35/300 - Train Loss: 0.0669, Val Loss: 0.0697\n",
      "Epoch 36/300 - Train Loss: 0.0655, Val Loss: 0.0716\n",
      "Epoch 37/300 - Train Loss: 0.0637, Val Loss: 0.0722\n",
      "Epoch 38/300 - Train Loss: 0.0638, Val Loss: 0.0709\n",
      "Epoch 39/300 - Train Loss: 0.0656, Val Loss: 0.0685\n",
      "Epoch 40/300 - Train Loss: 0.0646, Val Loss: 0.0681\n",
      "Epoch 41/300 - Train Loss: 0.0641, Val Loss: 0.0735\n",
      "Epoch 42/300 - Train Loss: 0.0620, Val Loss: 0.0721\n",
      "Epoch 43/300 - Train Loss: 0.0656, Val Loss: 0.0725\n",
      "Epoch 44/300 - Train Loss: 0.0607, Val Loss: 0.0696\n",
      "Epoch 45/300 - Train Loss: 0.0608, Val Loss: 0.0732\n",
      "Epoch 46/300 - Train Loss: 0.0612, Val Loss: 0.0759\n",
      "Epoch 47/300 - Train Loss: 0.0603, Val Loss: 0.0717\n",
      "Epoch 48/300 - Train Loss: 0.0602, Val Loss: 0.0704\n",
      "Epoch 49/300 - Train Loss: 0.0619, Val Loss: 0.0727\n",
      "Epoch 50/300 - Train Loss: 0.0620, Val Loss: 0.0704\n",
      "Epoch 51/300 - Train Loss: 0.0606, Val Loss: 0.0736\n",
      "Epoch 52/300 - Train Loss: 0.0598, Val Loss: 0.0700\n",
      "Epoch 53/300 - Train Loss: 0.0594, Val Loss: 0.0705\n",
      "Epoch 54/300 - Train Loss: 0.0576, Val Loss: 0.0747\n",
      "Epoch 55/300 - Train Loss: 0.0611, Val Loss: 0.0721\n",
      "Epoch 56/300 - Train Loss: 0.0587, Val Loss: 0.0745\n",
      "Epoch 57/300 - Train Loss: 0.0563, Val Loss: 0.0748\n",
      "Epoch 58/300 - Train Loss: 0.0584, Val Loss: 0.0714\n",
      "Epoch 59/300 - Train Loss: 0.0570, Val Loss: 0.0760\n",
      "Epoch 60/300 - Train Loss: 0.0568, Val Loss: 0.0715\n",
      "Epoch 61/300 - Train Loss: 0.0568, Val Loss: 0.0730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:49:59,137] Trial 342 finished with value: 0.9632389073274473 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.13892642296045501, 'learning_rate': 8.237624453223097e-05, 'batch_size': 32, 'weight_decay': 3.5242946526581315e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/300 - Train Loss: 0.0552, Val Loss: 0.0777\n",
      "Early stopping at epoch 62\n",
      "Macro F1 Score: 0.9632, Macro Precision: 0.9512, Macro Recall: 0.9767\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 344\n",
      "Training with F1=32, F2=16, D=2, dropout=0.1292159898829406, LR=9.315155373244094e-05, BS=32, WD=3.014088719161336e-05\n",
      "Epoch 1/300 - Train Loss: 0.3106, Val Loss: 0.1261\n",
      "Epoch 2/300 - Train Loss: 0.1250, Val Loss: 0.0909\n",
      "Epoch 3/300 - Train Loss: 0.1079, Val Loss: 0.0777\n",
      "Epoch 4/300 - Train Loss: 0.0983, Val Loss: 0.0742\n",
      "Epoch 5/300 - Train Loss: 0.0949, Val Loss: 0.0710\n",
      "Epoch 6/300 - Train Loss: 0.0912, Val Loss: 0.0781\n",
      "Epoch 7/300 - Train Loss: 0.0879, Val Loss: 0.0732\n",
      "Epoch 8/300 - Train Loss: 0.0863, Val Loss: 0.0703\n",
      "Epoch 9/300 - Train Loss: 0.0850, Val Loss: 0.0708\n",
      "Epoch 10/300 - Train Loss: 0.0850, Val Loss: 0.0759\n",
      "Epoch 11/300 - Train Loss: 0.0816, Val Loss: 0.0867\n",
      "Epoch 12/300 - Train Loss: 0.0838, Val Loss: 0.0789\n",
      "Epoch 13/300 - Train Loss: 0.0814, Val Loss: 0.0721\n",
      "Epoch 14/300 - Train Loss: 0.0818, Val Loss: 0.0796\n",
      "Epoch 15/300 - Train Loss: 0.0777, Val Loss: 0.0707\n",
      "Epoch 16/300 - Train Loss: 0.0789, Val Loss: 0.0662\n",
      "Epoch 17/300 - Train Loss: 0.0805, Val Loss: 0.0759\n",
      "Epoch 18/300 - Train Loss: 0.0777, Val Loss: 0.0683\n",
      "Epoch 19/300 - Train Loss: 0.0793, Val Loss: 0.0733\n",
      "Epoch 20/300 - Train Loss: 0.0788, Val Loss: 0.0680\n",
      "Epoch 21/300 - Train Loss: 0.0778, Val Loss: 0.0679\n",
      "Epoch 22/300 - Train Loss: 0.0744, Val Loss: 0.0660\n",
      "Epoch 23/300 - Train Loss: 0.0758, Val Loss: 0.0710\n",
      "Epoch 24/300 - Train Loss: 0.0752, Val Loss: 0.0693\n",
      "Epoch 25/300 - Train Loss: 0.0739, Val Loss: 0.0719\n",
      "Epoch 26/300 - Train Loss: 0.0726, Val Loss: 0.0676\n",
      "Epoch 27/300 - Train Loss: 0.0724, Val Loss: 0.0665\n",
      "Epoch 28/300 - Train Loss: 0.0721, Val Loss: 0.0698\n",
      "Epoch 29/300 - Train Loss: 0.0728, Val Loss: 0.0663\n",
      "Epoch 30/300 - Train Loss: 0.0715, Val Loss: 0.0699\n",
      "Epoch 31/300 - Train Loss: 0.0711, Val Loss: 0.0685\n",
      "Epoch 32/300 - Train Loss: 0.0701, Val Loss: 0.0679\n",
      "Epoch 33/300 - Train Loss: 0.0724, Val Loss: 0.0658\n",
      "Epoch 34/300 - Train Loss: 0.0707, Val Loss: 0.0684\n",
      "Epoch 35/300 - Train Loss: 0.0690, Val Loss: 0.0757\n",
      "Epoch 36/300 - Train Loss: 0.0690, Val Loss: 0.0632\n",
      "Epoch 37/300 - Train Loss: 0.0698, Val Loss: 0.0650\n",
      "Epoch 38/300 - Train Loss: 0.0672, Val Loss: 0.0730\n",
      "Epoch 39/300 - Train Loss: 0.0711, Val Loss: 0.0671\n",
      "Epoch 40/300 - Train Loss: 0.0698, Val Loss: 0.0686\n",
      "Epoch 41/300 - Train Loss: 0.0663, Val Loss: 0.0659\n",
      "Epoch 42/300 - Train Loss: 0.0673, Val Loss: 0.0702\n",
      "Epoch 43/300 - Train Loss: 0.0667, Val Loss: 0.0693\n",
      "Epoch 44/300 - Train Loss: 0.0647, Val Loss: 0.0688\n",
      "Epoch 45/300 - Train Loss: 0.0648, Val Loss: 0.0707\n",
      "Epoch 46/300 - Train Loss: 0.0668, Val Loss: 0.0662\n",
      "Epoch 47/300 - Train Loss: 0.0658, Val Loss: 0.0690\n",
      "Epoch 48/300 - Train Loss: 0.0641, Val Loss: 0.0664\n",
      "Epoch 49/300 - Train Loss: 0.0644, Val Loss: 0.0684\n",
      "Epoch 50/300 - Train Loss: 0.0656, Val Loss: 0.0680\n",
      "Epoch 51/300 - Train Loss: 0.0637, Val Loss: 0.0720\n",
      "Epoch 52/300 - Train Loss: 0.0640, Val Loss: 0.0679\n",
      "Epoch 53/300 - Train Loss: 0.0647, Val Loss: 0.0675\n",
      "Epoch 54/300 - Train Loss: 0.0638, Val Loss: 0.0732\n",
      "Epoch 55/300 - Train Loss: 0.0620, Val Loss: 0.0647\n",
      "Epoch 56/300 - Train Loss: 0.0646, Val Loss: 0.0672\n",
      "Epoch 57/300 - Train Loss: 0.0634, Val Loss: 0.0646\n",
      "Epoch 58/300 - Train Loss: 0.0622, Val Loss: 0.0668\n",
      "Epoch 59/300 - Train Loss: 0.0619, Val Loss: 0.0677\n",
      "Epoch 60/300 - Train Loss: 0.0614, Val Loss: 0.0709\n",
      "Epoch 61/300 - Train Loss: 0.0613, Val Loss: 0.0647\n",
      "Epoch 62/300 - Train Loss: 0.0621, Val Loss: 0.0747\n",
      "Epoch 63/300 - Train Loss: 0.0599, Val Loss: 0.0676\n",
      "Epoch 64/300 - Train Loss: 0.0608, Val Loss: 0.0651\n",
      "Epoch 65/300 - Train Loss: 0.0599, Val Loss: 0.0710\n",
      "Epoch 66/300 - Train Loss: 0.0600, Val Loss: 0.0622\n",
      "Epoch 67/300 - Train Loss: 0.0595, Val Loss: 0.0653\n",
      "Epoch 68/300 - Train Loss: 0.0609, Val Loss: 0.0696\n",
      "Epoch 69/300 - Train Loss: 0.0601, Val Loss: 0.0666\n",
      "Epoch 70/300 - Train Loss: 0.0600, Val Loss: 0.0660\n",
      "Epoch 71/300 - Train Loss: 0.0599, Val Loss: 0.0656\n",
      "Epoch 72/300 - Train Loss: 0.0565, Val Loss: 0.0694\n",
      "Epoch 73/300 - Train Loss: 0.0577, Val Loss: 0.0654\n",
      "Epoch 74/300 - Train Loss: 0.0589, Val Loss: 0.0665\n",
      "Epoch 75/300 - Train Loss: 0.0577, Val Loss: 0.0660\n",
      "Epoch 76/300 - Train Loss: 0.0573, Val Loss: 0.0654\n",
      "Epoch 77/300 - Train Loss: 0.0553, Val Loss: 0.0657\n",
      "Epoch 78/300 - Train Loss: 0.0578, Val Loss: 0.0689\n",
      "Epoch 79/300 - Train Loss: 0.0569, Val Loss: 0.0658\n",
      "Epoch 80/300 - Train Loss: 0.0556, Val Loss: 0.0714\n",
      "Epoch 81/300 - Train Loss: 0.0543, Val Loss: 0.0667\n",
      "Epoch 82/300 - Train Loss: 0.0565, Val Loss: 0.0669\n",
      "Epoch 83/300 - Train Loss: 0.0561, Val Loss: 0.0640\n",
      "Epoch 84/300 - Train Loss: 0.0577, Val Loss: 0.0636\n",
      "Epoch 85/300 - Train Loss: 0.0557, Val Loss: 0.0716\n",
      "Epoch 86/300 - Train Loss: 0.0566, Val Loss: 0.0627\n",
      "Epoch 87/300 - Train Loss: 0.0581, Val Loss: 0.0648\n",
      "Epoch 88/300 - Train Loss: 0.0572, Val Loss: 0.0672\n",
      "Epoch 89/300 - Train Loss: 0.0554, Val Loss: 0.0657\n",
      "Epoch 90/300 - Train Loss: 0.0571, Val Loss: 0.0705\n",
      "Epoch 91/300 - Train Loss: 0.0539, Val Loss: 0.0655\n",
      "Epoch 92/300 - Train Loss: 0.0540, Val Loss: 0.0689\n",
      "Epoch 93/300 - Train Loss: 0.0540, Val Loss: 0.0637\n",
      "Epoch 94/300 - Train Loss: 0.0532, Val Loss: 0.0681\n",
      "Epoch 95/300 - Train Loss: 0.0524, Val Loss: 0.0681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:54:14,292] Trial 343 finished with value: 0.9627400840544693 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.1292159898829406, 'learning_rate': 9.315155373244094e-05, 'batch_size': 32, 'weight_decay': 3.014088719161336e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/300 - Train Loss: 0.0552, Val Loss: 0.0715\n",
      "Early stopping at epoch 96\n",
      "Macro F1 Score: 0.9627, Macro Precision: 0.9509, Macro Recall: 0.9760\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 345\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14837358275471219, LR=0.0005700155366376475, BS=32, WD=0.0002882162393825639\n",
      "Epoch 1/300 - Train Loss: 0.1426, Val Loss: 0.1111\n",
      "Epoch 2/300 - Train Loss: 0.0999, Val Loss: 0.0866\n",
      "Epoch 3/300 - Train Loss: 0.0918, Val Loss: 0.0786\n",
      "Epoch 4/300 - Train Loss: 0.0924, Val Loss: 0.0719\n",
      "Epoch 5/300 - Train Loss: 0.0860, Val Loss: 0.0728\n",
      "Epoch 6/300 - Train Loss: 0.0839, Val Loss: 0.0757\n",
      "Epoch 7/300 - Train Loss: 0.0854, Val Loss: 0.0703\n",
      "Epoch 8/300 - Train Loss: 0.0811, Val Loss: 0.0704\n",
      "Epoch 9/300 - Train Loss: 0.0811, Val Loss: 0.0809\n",
      "Epoch 10/300 - Train Loss: 0.0817, Val Loss: 0.0826\n",
      "Epoch 11/300 - Train Loss: 0.0790, Val Loss: 0.0709\n",
      "Epoch 12/300 - Train Loss: 0.0775, Val Loss: 0.0735\n",
      "Epoch 13/300 - Train Loss: 0.0776, Val Loss: 0.0781\n",
      "Epoch 14/300 - Train Loss: 0.0790, Val Loss: 0.0749\n",
      "Epoch 15/300 - Train Loss: 0.0760, Val Loss: 0.0708\n",
      "Epoch 16/300 - Train Loss: 0.0733, Val Loss: 0.0815\n",
      "Epoch 17/300 - Train Loss: 0.0750, Val Loss: 0.0691\n",
      "Epoch 18/300 - Train Loss: 0.0740, Val Loss: 0.0726\n",
      "Epoch 19/300 - Train Loss: 0.0730, Val Loss: 0.0668\n",
      "Epoch 20/300 - Train Loss: 0.0724, Val Loss: 0.0719\n",
      "Epoch 21/300 - Train Loss: 0.0734, Val Loss: 0.0775\n",
      "Epoch 22/300 - Train Loss: 0.0718, Val Loss: 0.0786\n",
      "Epoch 23/300 - Train Loss: 0.0732, Val Loss: 0.0714\n",
      "Epoch 24/300 - Train Loss: 0.0717, Val Loss: 0.0856\n",
      "Epoch 25/300 - Train Loss: 0.0701, Val Loss: 0.0732\n",
      "Epoch 26/300 - Train Loss: 0.0704, Val Loss: 0.0738\n",
      "Epoch 27/300 - Train Loss: 0.0687, Val Loss: 0.0733\n",
      "Epoch 28/300 - Train Loss: 0.0696, Val Loss: 0.0750\n",
      "Epoch 29/300 - Train Loss: 0.0687, Val Loss: 0.0739\n",
      "Epoch 30/300 - Train Loss: 0.0688, Val Loss: 0.0715\n",
      "Epoch 31/300 - Train Loss: 0.0670, Val Loss: 0.0712\n",
      "Epoch 32/300 - Train Loss: 0.0675, Val Loss: 0.0712\n",
      "Epoch 33/300 - Train Loss: 0.0690, Val Loss: 0.0772\n",
      "Epoch 34/300 - Train Loss: 0.0672, Val Loss: 0.0724\n",
      "Epoch 35/300 - Train Loss: 0.0679, Val Loss: 0.0739\n",
      "Epoch 36/300 - Train Loss: 0.0650, Val Loss: 0.0754\n",
      "Epoch 37/300 - Train Loss: 0.0687, Val Loss: 0.0790\n",
      "Epoch 38/300 - Train Loss: 0.0688, Val Loss: 0.0835\n",
      "Epoch 39/300 - Train Loss: 0.0659, Val Loss: 0.0790\n",
      "Epoch 40/300 - Train Loss: 0.0659, Val Loss: 0.0748\n",
      "Epoch 41/300 - Train Loss: 0.0686, Val Loss: 0.0909\n",
      "Epoch 42/300 - Train Loss: 0.0660, Val Loss: 0.0723\n",
      "Epoch 43/300 - Train Loss: 0.0673, Val Loss: 0.0768\n",
      "Epoch 44/300 - Train Loss: 0.0670, Val Loss: 0.0827\n",
      "Epoch 45/300 - Train Loss: 0.0659, Val Loss: 0.0758\n",
      "Epoch 46/300 - Train Loss: 0.0643, Val Loss: 0.0748\n",
      "Epoch 47/300 - Train Loss: 0.0661, Val Loss: 0.0750\n",
      "Epoch 48/300 - Train Loss: 0.0667, Val Loss: 0.0797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:56:28,082] Trial 344 finished with value: 0.9742503388691485 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14837358275471219, 'learning_rate': 0.0005700155366376475, 'batch_size': 32, 'weight_decay': 0.0002882162393825639}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300 - Train Loss: 0.0653, Val Loss: 0.0758\n",
      "Early stopping at epoch 49\n",
      "Macro F1 Score: 0.9743, Macro Precision: 0.9775, Macro Recall: 0.9711\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 346\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1542568824706907, LR=0.0005112809593746739, BS=32, WD=0.00025460533248361045\n",
      "Epoch 1/300 - Train Loss: 0.1482, Val Loss: 0.0848\n",
      "Epoch 2/300 - Train Loss: 0.0972, Val Loss: 0.0696\n",
      "Epoch 3/300 - Train Loss: 0.0880, Val Loss: 0.0723\n",
      "Epoch 4/300 - Train Loss: 0.0909, Val Loss: 0.0729\n",
      "Epoch 5/300 - Train Loss: 0.0872, Val Loss: 0.0793\n",
      "Epoch 6/300 - Train Loss: 0.0847, Val Loss: 0.0702\n",
      "Epoch 7/300 - Train Loss: 0.0825, Val Loss: 0.0792\n",
      "Epoch 8/300 - Train Loss: 0.0831, Val Loss: 0.0713\n",
      "Epoch 9/300 - Train Loss: 0.0820, Val Loss: 0.0760\n",
      "Epoch 10/300 - Train Loss: 0.0795, Val Loss: 0.0683\n",
      "Epoch 11/300 - Train Loss: 0.0751, Val Loss: 0.0768\n",
      "Epoch 12/300 - Train Loss: 0.0743, Val Loss: 0.0703\n",
      "Epoch 13/300 - Train Loss: 0.0760, Val Loss: 0.0750\n",
      "Epoch 14/300 - Train Loss: 0.0762, Val Loss: 0.0736\n",
      "Epoch 15/300 - Train Loss: 0.0734, Val Loss: 0.0716\n",
      "Epoch 16/300 - Train Loss: 0.0744, Val Loss: 0.0762\n",
      "Epoch 17/300 - Train Loss: 0.0754, Val Loss: 0.0727\n",
      "Epoch 18/300 - Train Loss: 0.0752, Val Loss: 0.0757\n",
      "Epoch 19/300 - Train Loss: 0.0732, Val Loss: 0.0725\n",
      "Epoch 20/300 - Train Loss: 0.0702, Val Loss: 0.0800\n",
      "Epoch 21/300 - Train Loss: 0.0747, Val Loss: 0.0737\n",
      "Epoch 22/300 - Train Loss: 0.0690, Val Loss: 0.0814\n",
      "Epoch 23/300 - Train Loss: 0.0712, Val Loss: 0.0718\n",
      "Epoch 24/300 - Train Loss: 0.0698, Val Loss: 0.0780\n",
      "Epoch 25/300 - Train Loss: 0.0691, Val Loss: 0.0749\n",
      "Epoch 26/300 - Train Loss: 0.0676, Val Loss: 0.0791\n",
      "Epoch 27/300 - Train Loss: 0.0677, Val Loss: 0.0822\n",
      "Epoch 28/300 - Train Loss: 0.0680, Val Loss: 0.0720\n",
      "Epoch 29/300 - Train Loss: 0.0708, Val Loss: 0.0743\n",
      "Epoch 30/300 - Train Loss: 0.0671, Val Loss: 0.0872\n",
      "Epoch 31/300 - Train Loss: 0.0681, Val Loss: 0.0727\n",
      "Epoch 32/300 - Train Loss: 0.0680, Val Loss: 0.0779\n",
      "Epoch 33/300 - Train Loss: 0.0693, Val Loss: 0.0753\n",
      "Epoch 34/300 - Train Loss: 0.0649, Val Loss: 0.0907\n",
      "Epoch 35/300 - Train Loss: 0.0666, Val Loss: 0.0810\n",
      "Epoch 36/300 - Train Loss: 0.0670, Val Loss: 0.0799\n",
      "Epoch 37/300 - Train Loss: 0.0651, Val Loss: 0.0811\n",
      "Epoch 38/300 - Train Loss: 0.0651, Val Loss: 0.0716\n",
      "Epoch 39/300 - Train Loss: 0.0650, Val Loss: 0.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 21:58:17,577] Trial 345 finished with value: 0.9651729927153655 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1542568824706907, 'learning_rate': 0.0005112809593746739, 'batch_size': 32, 'weight_decay': 0.00025460533248361045}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/300 - Train Loss: 0.0633, Val Loss: 0.0743\n",
      "Early stopping at epoch 40\n",
      "Macro F1 Score: 0.9652, Macro Precision: 0.9754, Macro Recall: 0.9557\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.96      0.90      0.93        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 347\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14568485786021035, LR=0.0004659287230529287, BS=32, WD=0.00027118415056833053\n",
      "Epoch 1/300 - Train Loss: 0.1497, Val Loss: 0.0772\n",
      "Epoch 2/300 - Train Loss: 0.0983, Val Loss: 0.0728\n",
      "Epoch 3/300 - Train Loss: 0.0892, Val Loss: 0.0714\n",
      "Epoch 4/300 - Train Loss: 0.0860, Val Loss: 0.0699\n",
      "Epoch 5/300 - Train Loss: 0.0860, Val Loss: 0.0764\n",
      "Epoch 6/300 - Train Loss: 0.0847, Val Loss: 0.0789\n",
      "Epoch 7/300 - Train Loss: 0.0815, Val Loss: 0.0688\n",
      "Epoch 8/300 - Train Loss: 0.0813, Val Loss: 0.0737\n",
      "Epoch 9/300 - Train Loss: 0.0793, Val Loss: 0.0661\n",
      "Epoch 10/300 - Train Loss: 0.0791, Val Loss: 0.0727\n",
      "Epoch 11/300 - Train Loss: 0.0764, Val Loss: 0.0703\n",
      "Epoch 12/300 - Train Loss: 0.0764, Val Loss: 0.0697\n",
      "Epoch 13/300 - Train Loss: 0.0776, Val Loss: 0.0850\n",
      "Epoch 14/300 - Train Loss: 0.0749, Val Loss: 0.0780\n",
      "Epoch 15/300 - Train Loss: 0.0756, Val Loss: 0.0748\n",
      "Epoch 16/300 - Train Loss: 0.0739, Val Loss: 0.0687\n",
      "Epoch 17/300 - Train Loss: 0.0731, Val Loss: 0.0723\n",
      "Epoch 18/300 - Train Loss: 0.0726, Val Loss: 0.0775\n",
      "Epoch 19/300 - Train Loss: 0.0724, Val Loss: 0.0714\n",
      "Epoch 20/300 - Train Loss: 0.0718, Val Loss: 0.1187\n",
      "Epoch 21/300 - Train Loss: 0.0719, Val Loss: 0.0749\n",
      "Epoch 22/300 - Train Loss: 0.0698, Val Loss: 0.0718\n",
      "Epoch 23/300 - Train Loss: 0.0706, Val Loss: 0.0834\n",
      "Epoch 24/300 - Train Loss: 0.0722, Val Loss: 0.0771\n",
      "Epoch 25/300 - Train Loss: 0.0692, Val Loss: 0.0748\n",
      "Epoch 26/300 - Train Loss: 0.0711, Val Loss: 0.0697\n",
      "Epoch 27/300 - Train Loss: 0.0700, Val Loss: 0.0744\n",
      "Epoch 28/300 - Train Loss: 0.0674, Val Loss: 0.0690\n",
      "Epoch 29/300 - Train Loss: 0.0668, Val Loss: 0.0768\n",
      "Epoch 30/300 - Train Loss: 0.0697, Val Loss: 0.0709\n",
      "Epoch 31/300 - Train Loss: 0.0646, Val Loss: 0.0740\n",
      "Epoch 32/300 - Train Loss: 0.0663, Val Loss: 0.0746\n",
      "Epoch 33/300 - Train Loss: 0.0661, Val Loss: 0.0801\n",
      "Epoch 34/300 - Train Loss: 0.0678, Val Loss: 0.0729\n",
      "Epoch 35/300 - Train Loss: 0.0668, Val Loss: 0.0714\n",
      "Epoch 36/300 - Train Loss: 0.0660, Val Loss: 0.0753\n",
      "Epoch 37/300 - Train Loss: 0.0670, Val Loss: 0.0733\n",
      "Epoch 38/300 - Train Loss: 0.0634, Val Loss: 0.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:00:04,314] Trial 346 finished with value: 0.9686208066287086 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14568485786021035, 'learning_rate': 0.0004659287230529287, 'batch_size': 32, 'weight_decay': 0.00027118415056833053}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300 - Train Loss: 0.0621, Val Loss: 0.0762\n",
      "Early stopping at epoch 39\n",
      "Macro F1 Score: 0.9686, Macro Precision: 0.9601, Macro Recall: 0.9778\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 348\n",
      "Training with F1=32, F2=32, D=2, dropout=0.16260573928843366, LR=0.0006936469884345454, BS=32, WD=0.0003038702613784173\n",
      "Epoch 1/300 - Train Loss: 0.1461, Val Loss: 0.0740\n",
      "Epoch 2/300 - Train Loss: 0.1017, Val Loss: 0.0737\n",
      "Epoch 3/300 - Train Loss: 0.0940, Val Loss: 0.1087\n",
      "Epoch 4/300 - Train Loss: 0.0904, Val Loss: 0.0796\n",
      "Epoch 5/300 - Train Loss: 0.0868, Val Loss: 0.0887\n",
      "Epoch 6/300 - Train Loss: 0.0877, Val Loss: 0.0765\n",
      "Epoch 7/300 - Train Loss: 0.0848, Val Loss: 0.0757\n",
      "Epoch 8/300 - Train Loss: 0.0835, Val Loss: 0.0801\n",
      "Epoch 9/300 - Train Loss: 0.0851, Val Loss: 0.0653\n",
      "Epoch 10/300 - Train Loss: 0.0847, Val Loss: 0.0715\n",
      "Epoch 11/300 - Train Loss: 0.0835, Val Loss: 0.0729\n",
      "Epoch 12/300 - Train Loss: 0.0815, Val Loss: 0.0729\n",
      "Epoch 13/300 - Train Loss: 0.0798, Val Loss: 0.0680\n",
      "Epoch 14/300 - Train Loss: 0.0780, Val Loss: 0.0706\n",
      "Epoch 15/300 - Train Loss: 0.0795, Val Loss: 0.0712\n",
      "Epoch 16/300 - Train Loss: 0.0781, Val Loss: 0.0707\n",
      "Epoch 17/300 - Train Loss: 0.0781, Val Loss: 0.0826\n",
      "Epoch 18/300 - Train Loss: 0.0791, Val Loss: 0.0713\n",
      "Epoch 19/300 - Train Loss: 0.0770, Val Loss: 0.0796\n",
      "Epoch 20/300 - Train Loss: 0.0777, Val Loss: 0.0708\n",
      "Epoch 21/300 - Train Loss: 0.0782, Val Loss: 0.0818\n",
      "Epoch 22/300 - Train Loss: 0.0780, Val Loss: 0.0741\n",
      "Epoch 23/300 - Train Loss: 0.0758, Val Loss: 0.0744\n",
      "Epoch 24/300 - Train Loss: 0.0751, Val Loss: 0.0876\n",
      "Epoch 25/300 - Train Loss: 0.0757, Val Loss: 0.0731\n",
      "Epoch 26/300 - Train Loss: 0.0794, Val Loss: 0.0939\n",
      "Epoch 27/300 - Train Loss: 0.0745, Val Loss: 0.0797\n",
      "Epoch 28/300 - Train Loss: 0.0767, Val Loss: 0.0767\n",
      "Epoch 29/300 - Train Loss: 0.0758, Val Loss: 0.0757\n",
      "Epoch 30/300 - Train Loss: 0.0754, Val Loss: 0.0838\n",
      "Epoch 31/300 - Train Loss: 0.0772, Val Loss: 0.0801\n",
      "Epoch 32/300 - Train Loss: 0.0736, Val Loss: 0.0697\n",
      "Epoch 33/300 - Train Loss: 0.0751, Val Loss: 0.0791\n",
      "Epoch 34/300 - Train Loss: 0.0738, Val Loss: 0.0759\n",
      "Epoch 35/300 - Train Loss: 0.0748, Val Loss: 0.0730\n",
      "Epoch 36/300 - Train Loss: 0.0710, Val Loss: 0.0734\n",
      "Epoch 37/300 - Train Loss: 0.0734, Val Loss: 0.0732\n",
      "Epoch 38/300 - Train Loss: 0.0740, Val Loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:01:50,904] Trial 347 finished with value: 0.9643980999912031 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.16260573928843366, 'learning_rate': 0.0006936469884345454, 'batch_size': 32, 'weight_decay': 0.0003038702613784173}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300 - Train Loss: 0.0732, Val Loss: 0.0729\n",
      "Early stopping at epoch 39\n",
      "Macro F1 Score: 0.9644, Macro Precision: 0.9539, Macro Recall: 0.9758\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 349\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14300331187812323, LR=2.270831387804943e-05, BS=32, WD=0.0003694255040983627\n",
      "Epoch 1/300 - Train Loss: 0.5440, Val Loss: 0.2675\n",
      "Epoch 2/300 - Train Loss: 0.2436, Val Loss: 0.1869\n",
      "Epoch 3/300 - Train Loss: 0.1899, Val Loss: 0.1501\n",
      "Epoch 4/300 - Train Loss: 0.1623, Val Loss: 0.1292\n",
      "Epoch 5/300 - Train Loss: 0.1460, Val Loss: 0.1137\n",
      "Epoch 6/300 - Train Loss: 0.1299, Val Loss: 0.0999\n",
      "Epoch 7/300 - Train Loss: 0.1151, Val Loss: 0.0917\n",
      "Epoch 8/300 - Train Loss: 0.1113, Val Loss: 0.0845\n",
      "Epoch 9/300 - Train Loss: 0.1046, Val Loss: 0.0800\n",
      "Epoch 10/300 - Train Loss: 0.0995, Val Loss: 0.0816\n",
      "Epoch 11/300 - Train Loss: 0.1006, Val Loss: 0.0761\n",
      "Epoch 12/300 - Train Loss: 0.0961, Val Loss: 0.0775\n",
      "Epoch 13/300 - Train Loss: 0.0962, Val Loss: 0.0772\n",
      "Epoch 14/300 - Train Loss: 0.0958, Val Loss: 0.0764\n",
      "Epoch 15/300 - Train Loss: 0.0929, Val Loss: 0.0802\n",
      "Epoch 16/300 - Train Loss: 0.0919, Val Loss: 0.0746\n",
      "Epoch 17/300 - Train Loss: 0.0920, Val Loss: 0.0719\n",
      "Epoch 18/300 - Train Loss: 0.0922, Val Loss: 0.0759\n",
      "Epoch 19/300 - Train Loss: 0.0918, Val Loss: 0.0810\n",
      "Epoch 20/300 - Train Loss: 0.0901, Val Loss: 0.0771\n",
      "Epoch 21/300 - Train Loss: 0.0890, Val Loss: 0.0785\n",
      "Epoch 22/300 - Train Loss: 0.0852, Val Loss: 0.0743\n",
      "Epoch 23/300 - Train Loss: 0.0861, Val Loss: 0.0757\n",
      "Epoch 24/300 - Train Loss: 0.0851, Val Loss: 0.0734\n",
      "Epoch 25/300 - Train Loss: 0.0851, Val Loss: 0.0744\n",
      "Epoch 26/300 - Train Loss: 0.0829, Val Loss: 0.0764\n",
      "Epoch 27/300 - Train Loss: 0.0844, Val Loss: 0.0687\n",
      "Epoch 28/300 - Train Loss: 0.0826, Val Loss: 0.0722\n",
      "Epoch 29/300 - Train Loss: 0.0830, Val Loss: 0.0722\n",
      "Epoch 30/300 - Train Loss: 0.0825, Val Loss: 0.0695\n",
      "Epoch 31/300 - Train Loss: 0.0827, Val Loss: 0.0746\n",
      "Epoch 32/300 - Train Loss: 0.0810, Val Loss: 0.0790\n",
      "Epoch 33/300 - Train Loss: 0.0825, Val Loss: 0.0702\n",
      "Epoch 34/300 - Train Loss: 0.0811, Val Loss: 0.0681\n",
      "Epoch 35/300 - Train Loss: 0.0820, Val Loss: 0.0708\n",
      "Epoch 36/300 - Train Loss: 0.0802, Val Loss: 0.0697\n",
      "Epoch 37/300 - Train Loss: 0.0814, Val Loss: 0.0720\n",
      "Epoch 38/300 - Train Loss: 0.0832, Val Loss: 0.0700\n",
      "Epoch 39/300 - Train Loss: 0.0797, Val Loss: 0.0694\n",
      "Epoch 40/300 - Train Loss: 0.0779, Val Loss: 0.0727\n",
      "Epoch 41/300 - Train Loss: 0.0781, Val Loss: 0.0709\n",
      "Epoch 42/300 - Train Loss: 0.0795, Val Loss: 0.0729\n",
      "Epoch 43/300 - Train Loss: 0.0776, Val Loss: 0.0727\n",
      "Epoch 44/300 - Train Loss: 0.0765, Val Loss: 0.0738\n",
      "Epoch 45/300 - Train Loss: 0.0781, Val Loss: 0.0694\n",
      "Epoch 46/300 - Train Loss: 0.0759, Val Loss: 0.0699\n",
      "Epoch 47/300 - Train Loss: 0.0755, Val Loss: 0.0681\n",
      "Epoch 48/300 - Train Loss: 0.0751, Val Loss: 0.0688\n",
      "Epoch 49/300 - Train Loss: 0.0747, Val Loss: 0.0711\n",
      "Epoch 50/300 - Train Loss: 0.0752, Val Loss: 0.0717\n",
      "Epoch 51/300 - Train Loss: 0.0747, Val Loss: 0.0712\n",
      "Epoch 52/300 - Train Loss: 0.0753, Val Loss: 0.0698\n",
      "Epoch 53/300 - Train Loss: 0.0750, Val Loss: 0.0696\n",
      "Epoch 54/300 - Train Loss: 0.0738, Val Loss: 0.0720\n",
      "Epoch 55/300 - Train Loss: 0.0741, Val Loss: 0.0691\n",
      "Epoch 56/300 - Train Loss: 0.0741, Val Loss: 0.0728\n",
      "Epoch 57/300 - Train Loss: 0.0743, Val Loss: 0.0733\n",
      "Epoch 58/300 - Train Loss: 0.0767, Val Loss: 0.0710\n",
      "Epoch 59/300 - Train Loss: 0.0740, Val Loss: 0.0678\n",
      "Epoch 60/300 - Train Loss: 0.0739, Val Loss: 0.0680\n",
      "Epoch 61/300 - Train Loss: 0.0720, Val Loss: 0.0671\n",
      "Epoch 62/300 - Train Loss: 0.0738, Val Loss: 0.0664\n",
      "Epoch 63/300 - Train Loss: 0.0738, Val Loss: 0.0737\n",
      "Epoch 64/300 - Train Loss: 0.0732, Val Loss: 0.0696\n",
      "Epoch 65/300 - Train Loss: 0.0714, Val Loss: 0.0651\n",
      "Epoch 66/300 - Train Loss: 0.0729, Val Loss: 0.0682\n",
      "Epoch 67/300 - Train Loss: 0.0721, Val Loss: 0.0699\n",
      "Epoch 68/300 - Train Loss: 0.0747, Val Loss: 0.0677\n",
      "Epoch 69/300 - Train Loss: 0.0746, Val Loss: 0.0676\n",
      "Epoch 70/300 - Train Loss: 0.0708, Val Loss: 0.0704\n",
      "Epoch 71/300 - Train Loss: 0.0728, Val Loss: 0.0675\n",
      "Epoch 72/300 - Train Loss: 0.0676, Val Loss: 0.0652\n",
      "Epoch 73/300 - Train Loss: 0.0710, Val Loss: 0.0694\n",
      "Epoch 74/300 - Train Loss: 0.0714, Val Loss: 0.0679\n",
      "Epoch 75/300 - Train Loss: 0.0680, Val Loss: 0.0670\n",
      "Epoch 76/300 - Train Loss: 0.0695, Val Loss: 0.0700\n",
      "Epoch 77/300 - Train Loss: 0.0694, Val Loss: 0.0688\n",
      "Epoch 78/300 - Train Loss: 0.0719, Val Loss: 0.0708\n",
      "Epoch 79/300 - Train Loss: 0.0705, Val Loss: 0.0696\n",
      "Epoch 80/300 - Train Loss: 0.0697, Val Loss: 0.0720\n",
      "Epoch 81/300 - Train Loss: 0.0687, Val Loss: 0.0681\n",
      "Epoch 82/300 - Train Loss: 0.0696, Val Loss: 0.0676\n",
      "Epoch 83/300 - Train Loss: 0.0687, Val Loss: 0.0703\n",
      "Epoch 84/300 - Train Loss: 0.0694, Val Loss: 0.0681\n",
      "Epoch 85/300 - Train Loss: 0.0683, Val Loss: 0.0722\n",
      "Epoch 86/300 - Train Loss: 0.0677, Val Loss: 0.0643\n",
      "Epoch 87/300 - Train Loss: 0.0689, Val Loss: 0.0662\n",
      "Epoch 88/300 - Train Loss: 0.0681, Val Loss: 0.0676\n",
      "Epoch 89/300 - Train Loss: 0.0674, Val Loss: 0.0691\n",
      "Epoch 90/300 - Train Loss: 0.0676, Val Loss: 0.0675\n",
      "Epoch 91/300 - Train Loss: 0.0659, Val Loss: 0.0663\n",
      "Epoch 92/300 - Train Loss: 0.0667, Val Loss: 0.0661\n",
      "Epoch 93/300 - Train Loss: 0.0671, Val Loss: 0.0671\n",
      "Epoch 94/300 - Train Loss: 0.0674, Val Loss: 0.0666\n",
      "Epoch 95/300 - Train Loss: 0.0671, Val Loss: 0.0681\n",
      "Epoch 96/300 - Train Loss: 0.0661, Val Loss: 0.0664\n",
      "Epoch 97/300 - Train Loss: 0.0670, Val Loss: 0.0680\n",
      "Epoch 98/300 - Train Loss: 0.0665, Val Loss: 0.0651\n",
      "Epoch 99/300 - Train Loss: 0.0648, Val Loss: 0.0694\n",
      "Epoch 100/300 - Train Loss: 0.0693, Val Loss: 0.0655\n",
      "Epoch 101/300 - Train Loss: 0.0663, Val Loss: 0.0680\n",
      "Epoch 102/300 - Train Loss: 0.0660, Val Loss: 0.0653\n",
      "Epoch 103/300 - Train Loss: 0.0669, Val Loss: 0.0642\n",
      "Epoch 104/300 - Train Loss: 0.0666, Val Loss: 0.0662\n",
      "Epoch 105/300 - Train Loss: 0.0652, Val Loss: 0.0640\n",
      "Epoch 106/300 - Train Loss: 0.0654, Val Loss: 0.0650\n",
      "Epoch 107/300 - Train Loss: 0.0679, Val Loss: 0.0650\n",
      "Epoch 108/300 - Train Loss: 0.0664, Val Loss: 0.0700\n",
      "Epoch 109/300 - Train Loss: 0.0654, Val Loss: 0.0666\n",
      "Epoch 110/300 - Train Loss: 0.0650, Val Loss: 0.0681\n",
      "Epoch 111/300 - Train Loss: 0.0639, Val Loss: 0.0678\n",
      "Epoch 112/300 - Train Loss: 0.0628, Val Loss: 0.0680\n",
      "Epoch 113/300 - Train Loss: 0.0636, Val Loss: 0.0641\n",
      "Epoch 114/300 - Train Loss: 0.0632, Val Loss: 0.0686\n",
      "Epoch 115/300 - Train Loss: 0.0654, Val Loss: 0.0681\n",
      "Epoch 116/300 - Train Loss: 0.0642, Val Loss: 0.0645\n",
      "Epoch 117/300 - Train Loss: 0.0646, Val Loss: 0.0665\n",
      "Epoch 118/300 - Train Loss: 0.0633, Val Loss: 0.0690\n",
      "Epoch 119/300 - Train Loss: 0.0640, Val Loss: 0.0685\n",
      "Epoch 120/300 - Train Loss: 0.0613, Val Loss: 0.0667\n",
      "Epoch 121/300 - Train Loss: 0.0635, Val Loss: 0.0644\n",
      "Epoch 122/300 - Train Loss: 0.0643, Val Loss: 0.0703\n",
      "Epoch 123/300 - Train Loss: 0.0615, Val Loss: 0.0645\n",
      "Epoch 124/300 - Train Loss: 0.0642, Val Loss: 0.0688\n",
      "Epoch 125/300 - Train Loss: 0.0628, Val Loss: 0.0675\n",
      "Epoch 126/300 - Train Loss: 0.0612, Val Loss: 0.0650\n",
      "Epoch 127/300 - Train Loss: 0.0629, Val Loss: 0.0663\n",
      "Epoch 128/300 - Train Loss: 0.0608, Val Loss: 0.0673\n",
      "Epoch 129/300 - Train Loss: 0.0623, Val Loss: 0.0667\n",
      "Epoch 130/300 - Train Loss: 0.0615, Val Loss: 0.0658\n",
      "Epoch 131/300 - Train Loss: 0.0590, Val Loss: 0.0632\n",
      "Epoch 132/300 - Train Loss: 0.0637, Val Loss: 0.0659\n",
      "Epoch 133/300 - Train Loss: 0.0621, Val Loss: 0.0670\n",
      "Epoch 134/300 - Train Loss: 0.0626, Val Loss: 0.0644\n",
      "Epoch 135/300 - Train Loss: 0.0621, Val Loss: 0.0655\n",
      "Epoch 136/300 - Train Loss: 0.0609, Val Loss: 0.0658\n",
      "Epoch 137/300 - Train Loss: 0.0609, Val Loss: 0.0650\n",
      "Epoch 138/300 - Train Loss: 0.0602, Val Loss: 0.0661\n",
      "Epoch 139/300 - Train Loss: 0.0616, Val Loss: 0.0672\n",
      "Epoch 140/300 - Train Loss: 0.0619, Val Loss: 0.0654\n",
      "Epoch 141/300 - Train Loss: 0.0602, Val Loss: 0.0656\n",
      "Epoch 142/300 - Train Loss: 0.0601, Val Loss: 0.0672\n",
      "Epoch 143/300 - Train Loss: 0.0598, Val Loss: 0.0703\n",
      "Epoch 144/300 - Train Loss: 0.0597, Val Loss: 0.0655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0587, Val Loss: 0.0661\n",
      "Epoch 146/300 - Train Loss: 0.0615, Val Loss: 0.0681\n",
      "Epoch 147/300 - Train Loss: 0.0585, Val Loss: 0.0652\n",
      "Epoch 148/300 - Train Loss: 0.0619, Val Loss: 0.0681\n",
      "Epoch 149/300 - Train Loss: 0.0604, Val Loss: 0.0642\n",
      "Epoch 150/300 - Train Loss: 0.0603, Val Loss: 0.0652\n",
      "Epoch 151/300 - Train Loss: 0.0581, Val Loss: 0.0652\n",
      "Epoch 152/300 - Train Loss: 0.0579, Val Loss: 0.0641\n",
      "Epoch 153/300 - Train Loss: 0.0589, Val Loss: 0.0666\n",
      "Epoch 154/300 - Train Loss: 0.0583, Val Loss: 0.0654\n",
      "Epoch 155/300 - Train Loss: 0.0604, Val Loss: 0.0655\n",
      "Epoch 156/300 - Train Loss: 0.0605, Val Loss: 0.0683\n",
      "Epoch 157/300 - Train Loss: 0.0603, Val Loss: 0.0657\n",
      "Epoch 158/300 - Train Loss: 0.0576, Val Loss: 0.0631\n",
      "Epoch 159/300 - Train Loss: 0.0587, Val Loss: 0.0646\n",
      "Epoch 160/300 - Train Loss: 0.0582, Val Loss: 0.0651\n",
      "Epoch 161/300 - Train Loss: 0.0577, Val Loss: 0.0659\n",
      "Epoch 162/300 - Train Loss: 0.0595, Val Loss: 0.0661\n",
      "Epoch 163/300 - Train Loss: 0.0587, Val Loss: 0.0691\n",
      "Epoch 164/300 - Train Loss: 0.0595, Val Loss: 0.0693\n",
      "Epoch 165/300 - Train Loss: 0.0563, Val Loss: 0.0662\n",
      "Epoch 166/300 - Train Loss: 0.0593, Val Loss: 0.0687\n",
      "Epoch 167/300 - Train Loss: 0.0592, Val Loss: 0.0779\n",
      "Epoch 168/300 - Train Loss: 0.0594, Val Loss: 0.0691\n",
      "Epoch 169/300 - Train Loss: 0.0569, Val Loss: 0.0676\n",
      "Epoch 170/300 - Train Loss: 0.0581, Val Loss: 0.0659\n",
      "Epoch 171/300 - Train Loss: 0.0563, Val Loss: 0.0664\n",
      "Epoch 172/300 - Train Loss: 0.0574, Val Loss: 0.0651\n",
      "Epoch 173/300 - Train Loss: 0.0548, Val Loss: 0.0681\n",
      "Epoch 174/300 - Train Loss: 0.0553, Val Loss: 0.0671\n",
      "Epoch 175/300 - Train Loss: 0.0558, Val Loss: 0.0640\n",
      "Epoch 176/300 - Train Loss: 0.0555, Val Loss: 0.0672\n",
      "Epoch 177/300 - Train Loss: 0.0550, Val Loss: 0.0649\n",
      "Epoch 178/300 - Train Loss: 0.0561, Val Loss: 0.0653\n",
      "Epoch 179/300 - Train Loss: 0.0554, Val Loss: 0.0716\n",
      "Epoch 180/300 - Train Loss: 0.0557, Val Loss: 0.0655\n",
      "Epoch 181/300 - Train Loss: 0.0547, Val Loss: 0.0669\n",
      "Epoch 182/300 - Train Loss: 0.0561, Val Loss: 0.0676\n",
      "Epoch 183/300 - Train Loss: 0.0545, Val Loss: 0.0674\n",
      "Epoch 184/300 - Train Loss: 0.0553, Val Loss: 0.0687\n",
      "Epoch 185/300 - Train Loss: 0.0547, Val Loss: 0.0684\n",
      "Epoch 186/300 - Train Loss: 0.0566, Val Loss: 0.0664\n",
      "Epoch 187/300 - Train Loss: 0.0570, Val Loss: 0.0658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:10:25,382] Trial 348 finished with value: 0.9637340948758714 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14300331187812323, 'learning_rate': 2.270831387804943e-05, 'batch_size': 32, 'weight_decay': 0.0003694255040983627}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 188/300 - Train Loss: 0.0539, Val Loss: 0.0637\n",
      "Early stopping at epoch 188\n",
      "Macro F1 Score: 0.9637, Macro Precision: 0.9516, Macro Recall: 0.9773\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 350\n",
      "Training with F1=32, F2=32, D=2, dropout=0.15954603799831973, LR=0.000883918977836937, BS=32, WD=2.5978979333873665e-05\n",
      "Epoch 1/300 - Train Loss: 0.1383, Val Loss: 0.0802\n",
      "Epoch 2/300 - Train Loss: 0.0936, Val Loss: 0.0904\n",
      "Epoch 3/300 - Train Loss: 0.0931, Val Loss: 0.0964\n",
      "Epoch 4/300 - Train Loss: 0.0882, Val Loss: 0.0942\n",
      "Epoch 5/300 - Train Loss: 0.0850, Val Loss: 0.0842\n",
      "Epoch 6/300 - Train Loss: 0.0823, Val Loss: 0.0906\n",
      "Epoch 7/300 - Train Loss: 0.0835, Val Loss: 0.0740\n",
      "Epoch 8/300 - Train Loss: 0.0784, Val Loss: 0.0841\n",
      "Epoch 9/300 - Train Loss: 0.0771, Val Loss: 0.0953\n",
      "Epoch 10/300 - Train Loss: 0.0783, Val Loss: 0.0799\n",
      "Epoch 11/300 - Train Loss: 0.0759, Val Loss: 0.0761\n",
      "Epoch 12/300 - Train Loss: 0.0757, Val Loss: 0.0856\n",
      "Epoch 13/300 - Train Loss: 0.0713, Val Loss: 0.0749\n",
      "Epoch 14/300 - Train Loss: 0.0705, Val Loss: 0.0820\n",
      "Epoch 15/300 - Train Loss: 0.0712, Val Loss: 0.0784\n",
      "Epoch 16/300 - Train Loss: 0.0686, Val Loss: 0.0782\n",
      "Epoch 17/300 - Train Loss: 0.0678, Val Loss: 0.0921\n",
      "Epoch 18/300 - Train Loss: 0.0697, Val Loss: 0.0798\n",
      "Epoch 19/300 - Train Loss: 0.0661, Val Loss: 0.0870\n",
      "Epoch 20/300 - Train Loss: 0.0643, Val Loss: 0.0902\n",
      "Epoch 21/300 - Train Loss: 0.0675, Val Loss: 0.0790\n",
      "Epoch 22/300 - Train Loss: 0.0634, Val Loss: 0.0844\n",
      "Epoch 23/300 - Train Loss: 0.0609, Val Loss: 0.0769\n",
      "Epoch 24/300 - Train Loss: 0.0605, Val Loss: 0.0833\n",
      "Epoch 25/300 - Train Loss: 0.0606, Val Loss: 0.0842\n",
      "Epoch 26/300 - Train Loss: 0.0582, Val Loss: 0.0811\n",
      "Epoch 27/300 - Train Loss: 0.0594, Val Loss: 0.0777\n",
      "Epoch 28/300 - Train Loss: 0.0583, Val Loss: 0.0804\n",
      "Epoch 29/300 - Train Loss: 0.0571, Val Loss: 0.0943\n",
      "Epoch 30/300 - Train Loss: 0.0546, Val Loss: 0.0890\n",
      "Epoch 31/300 - Train Loss: 0.0579, Val Loss: 0.0961\n",
      "Epoch 32/300 - Train Loss: 0.0584, Val Loss: 0.0816\n",
      "Epoch 33/300 - Train Loss: 0.0526, Val Loss: 0.0857\n",
      "Epoch 34/300 - Train Loss: 0.0509, Val Loss: 0.0961\n",
      "Epoch 35/300 - Train Loss: 0.0553, Val Loss: 0.0899\n",
      "Epoch 36/300 - Train Loss: 0.0531, Val Loss: 0.0909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:12:06,466] Trial 349 finished with value: 0.9773661763582985 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.15954603799831973, 'learning_rate': 0.000883918977836937, 'batch_size': 32, 'weight_decay': 2.5978979333873665e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/300 - Train Loss: 0.0536, Val Loss: 0.0843\n",
      "Early stopping at epoch 37\n",
      "Macro F1 Score: 0.9774, Macro Precision: 0.9786, Macro Recall: 0.9763\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.97      0.97        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 351\n",
      "Training with F1=4, F2=8, D=2, dropout=0.16229232548580536, LR=0.000824703986535269, BS=32, WD=0.0001877987049603798\n",
      "Epoch 1/300 - Train Loss: 0.2334, Val Loss: 0.0924\n",
      "Epoch 2/300 - Train Loss: 0.1137, Val Loss: 0.0877\n",
      "Epoch 3/300 - Train Loss: 0.1069, Val Loss: 0.1103\n",
      "Epoch 4/300 - Train Loss: 0.1060, Val Loss: 0.0726\n",
      "Epoch 5/300 - Train Loss: 0.1022, Val Loss: 0.0812\n",
      "Epoch 6/300 - Train Loss: 0.1008, Val Loss: 0.0733\n",
      "Epoch 7/300 - Train Loss: 0.1001, Val Loss: 0.0698\n",
      "Epoch 8/300 - Train Loss: 0.1013, Val Loss: 0.0836\n",
      "Epoch 9/300 - Train Loss: 0.0991, Val Loss: 0.0761\n",
      "Epoch 10/300 - Train Loss: 0.0998, Val Loss: 0.0745\n",
      "Epoch 11/300 - Train Loss: 0.0989, Val Loss: 0.0761\n",
      "Epoch 12/300 - Train Loss: 0.0962, Val Loss: 0.0735\n",
      "Epoch 13/300 - Train Loss: 0.0968, Val Loss: 0.0802\n",
      "Epoch 14/300 - Train Loss: 0.0950, Val Loss: 0.0732\n",
      "Epoch 15/300 - Train Loss: 0.0951, Val Loss: 0.0762\n",
      "Epoch 16/300 - Train Loss: 0.0961, Val Loss: 0.0738\n",
      "Epoch 17/300 - Train Loss: 0.0948, Val Loss: 0.0726\n",
      "Epoch 18/300 - Train Loss: 0.0940, Val Loss: 0.0832\n",
      "Epoch 19/300 - Train Loss: 0.0927, Val Loss: 0.0692\n",
      "Epoch 20/300 - Train Loss: 0.0930, Val Loss: 0.0731\n",
      "Epoch 21/300 - Train Loss: 0.0915, Val Loss: 0.0816\n",
      "Epoch 22/300 - Train Loss: 0.0916, Val Loss: 0.0753\n",
      "Epoch 23/300 - Train Loss: 0.0952, Val Loss: 0.0716\n",
      "Epoch 24/300 - Train Loss: 0.0906, Val Loss: 0.0740\n",
      "Epoch 25/300 - Train Loss: 0.0898, Val Loss: 0.0816\n",
      "Epoch 26/300 - Train Loss: 0.0892, Val Loss: 0.0765\n",
      "Epoch 27/300 - Train Loss: 0.0935, Val Loss: 0.0832\n",
      "Epoch 28/300 - Train Loss: 0.0929, Val Loss: 0.0722\n",
      "Epoch 29/300 - Train Loss: 0.0885, Val Loss: 0.0752\n",
      "Epoch 30/300 - Train Loss: 0.0896, Val Loss: 0.0692\n",
      "Epoch 31/300 - Train Loss: 0.0888, Val Loss: 0.0739\n",
      "Epoch 32/300 - Train Loss: 0.0913, Val Loss: 0.0708\n",
      "Epoch 33/300 - Train Loss: 0.0879, Val Loss: 0.0753\n",
      "Epoch 34/300 - Train Loss: 0.0889, Val Loss: 0.0721\n",
      "Epoch 35/300 - Train Loss: 0.0908, Val Loss: 0.0772\n",
      "Epoch 36/300 - Train Loss: 0.0905, Val Loss: 0.0830\n",
      "Epoch 37/300 - Train Loss: 0.0867, Val Loss: 0.0718\n",
      "Epoch 38/300 - Train Loss: 0.0902, Val Loss: 0.0749\n",
      "Epoch 39/300 - Train Loss: 0.0876, Val Loss: 0.0675\n",
      "Epoch 40/300 - Train Loss: 0.0885, Val Loss: 0.0745\n",
      "Epoch 41/300 - Train Loss: 0.0883, Val Loss: 0.0866\n",
      "Epoch 42/300 - Train Loss: 0.0882, Val Loss: 0.0720\n",
      "Epoch 43/300 - Train Loss: 0.0869, Val Loss: 0.0752\n",
      "Epoch 44/300 - Train Loss: 0.0846, Val Loss: 0.0747\n",
      "Epoch 45/300 - Train Loss: 0.0869, Val Loss: 0.0733\n",
      "Epoch 46/300 - Train Loss: 0.0872, Val Loss: 0.0731\n",
      "Epoch 47/300 - Train Loss: 0.0858, Val Loss: 0.0714\n",
      "Epoch 48/300 - Train Loss: 0.0842, Val Loss: 0.0717\n",
      "Epoch 49/300 - Train Loss: 0.0888, Val Loss: 0.0668\n",
      "Epoch 50/300 - Train Loss: 0.0856, Val Loss: 0.0745\n",
      "Epoch 51/300 - Train Loss: 0.0873, Val Loss: 0.0667\n",
      "Epoch 52/300 - Train Loss: 0.0881, Val Loss: 0.0747\n",
      "Epoch 53/300 - Train Loss: 0.0868, Val Loss: 0.0698\n",
      "Epoch 54/300 - Train Loss: 0.0886, Val Loss: 0.0716\n",
      "Epoch 55/300 - Train Loss: 0.0848, Val Loss: 0.0695\n",
      "Epoch 56/300 - Train Loss: 0.0828, Val Loss: 0.0688\n",
      "Epoch 57/300 - Train Loss: 0.0855, Val Loss: 0.0718\n",
      "Epoch 58/300 - Train Loss: 0.0834, Val Loss: 0.0783\n",
      "Epoch 59/300 - Train Loss: 0.0860, Val Loss: 0.0720\n",
      "Epoch 60/300 - Train Loss: 0.0840, Val Loss: 0.0683\n",
      "Epoch 61/300 - Train Loss: 0.0862, Val Loss: 0.0765\n",
      "Epoch 62/300 - Train Loss: 0.0843, Val Loss: 0.0687\n",
      "Epoch 63/300 - Train Loss: 0.0813, Val Loss: 0.0678\n",
      "Epoch 64/300 - Train Loss: 0.0832, Val Loss: 0.0780\n",
      "Epoch 65/300 - Train Loss: 0.0815, Val Loss: 0.0679\n",
      "Epoch 66/300 - Train Loss: 0.0847, Val Loss: 0.0726\n",
      "Epoch 67/300 - Train Loss: 0.0839, Val Loss: 0.0738\n",
      "Epoch 68/300 - Train Loss: 0.0835, Val Loss: 0.0775\n",
      "Epoch 69/300 - Train Loss: 0.0820, Val Loss: 0.0677\n",
      "Epoch 70/300 - Train Loss: 0.0843, Val Loss: 0.0791\n",
      "Epoch 71/300 - Train Loss: 0.0810, Val Loss: 0.0687\n",
      "Epoch 72/300 - Train Loss: 0.0843, Val Loss: 0.0721\n",
      "Epoch 73/300 - Train Loss: 0.0830, Val Loss: 0.0717\n",
      "Epoch 74/300 - Train Loss: 0.0827, Val Loss: 0.0873\n",
      "Epoch 75/300 - Train Loss: 0.0841, Val Loss: 0.0737\n",
      "Epoch 76/300 - Train Loss: 0.0840, Val Loss: 0.0718\n",
      "Epoch 77/300 - Train Loss: 0.0819, Val Loss: 0.0672\n",
      "Epoch 78/300 - Train Loss: 0.0822, Val Loss: 0.0755\n",
      "Epoch 79/300 - Train Loss: 0.0812, Val Loss: 0.0700\n",
      "Epoch 80/300 - Train Loss: 0.0851, Val Loss: 0.0672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:14:39,426] Trial 350 finished with value: 0.9746112290282897 and parameters: {'F1': 4, 'F2': 8, 'D': 2, 'dropout': 0.16229232548580536, 'learning_rate': 0.000824703986535269, 'batch_size': 32, 'weight_decay': 0.0001877987049603798}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81/300 - Train Loss: 0.0828, Val Loss: 0.0725\n",
      "Early stopping at epoch 81\n",
      "Macro F1 Score: 0.9746, Macro Precision: 0.9683, Macro Recall: 0.9813\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.98      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 352\n",
      "Training with F1=32, F2=8, D=2, dropout=0.16253250919261042, LR=0.0005662511772438774, BS=32, WD=2.4119810175030112e-05\n",
      "Epoch 1/300 - Train Loss: 0.1693, Val Loss: 0.0798\n",
      "Epoch 2/300 - Train Loss: 0.0975, Val Loss: 0.0750\n",
      "Epoch 3/300 - Train Loss: 0.0934, Val Loss: 0.0829\n",
      "Epoch 4/300 - Train Loss: 0.0902, Val Loss: 0.0763\n",
      "Epoch 5/300 - Train Loss: 0.0846, Val Loss: 0.0731\n",
      "Epoch 6/300 - Train Loss: 0.0864, Val Loss: 0.0779\n",
      "Epoch 7/300 - Train Loss: 0.0817, Val Loss: 0.0767\n",
      "Epoch 8/300 - Train Loss: 0.0842, Val Loss: 0.0823\n",
      "Epoch 9/300 - Train Loss: 0.0817, Val Loss: 0.0699\n",
      "Epoch 10/300 - Train Loss: 0.0816, Val Loss: 0.0801\n",
      "Epoch 11/300 - Train Loss: 0.0799, Val Loss: 0.0750\n",
      "Epoch 12/300 - Train Loss: 0.0801, Val Loss: 0.0757\n",
      "Epoch 13/300 - Train Loss: 0.0793, Val Loss: 0.1079\n",
      "Epoch 14/300 - Train Loss: 0.0764, Val Loss: 0.0702\n",
      "Epoch 15/300 - Train Loss: 0.0789, Val Loss: 0.0691\n",
      "Epoch 16/300 - Train Loss: 0.0786, Val Loss: 0.0771\n",
      "Epoch 17/300 - Train Loss: 0.0770, Val Loss: 0.0685\n",
      "Epoch 18/300 - Train Loss: 0.0748, Val Loss: 0.0707\n",
      "Epoch 19/300 - Train Loss: 0.0768, Val Loss: 0.0707\n",
      "Epoch 20/300 - Train Loss: 0.0766, Val Loss: 0.0784\n",
      "Epoch 21/300 - Train Loss: 0.0730, Val Loss: 0.0722\n",
      "Epoch 22/300 - Train Loss: 0.0743, Val Loss: 0.0748\n",
      "Epoch 23/300 - Train Loss: 0.0749, Val Loss: 0.0880\n",
      "Epoch 24/300 - Train Loss: 0.0738, Val Loss: 0.0771\n",
      "Epoch 25/300 - Train Loss: 0.0722, Val Loss: 0.0731\n",
      "Epoch 26/300 - Train Loss: 0.0719, Val Loss: 0.0693\n",
      "Epoch 27/300 - Train Loss: 0.0710, Val Loss: 0.0691\n",
      "Epoch 28/300 - Train Loss: 0.0727, Val Loss: 0.0742\n",
      "Epoch 29/300 - Train Loss: 0.0740, Val Loss: 0.0701\n",
      "Epoch 30/300 - Train Loss: 0.0712, Val Loss: 0.0725\n",
      "Epoch 31/300 - Train Loss: 0.0718, Val Loss: 0.0786\n",
      "Epoch 32/300 - Train Loss: 0.0705, Val Loss: 0.0694\n",
      "Epoch 33/300 - Train Loss: 0.0713, Val Loss: 0.0710\n",
      "Epoch 34/300 - Train Loss: 0.0696, Val Loss: 0.0697\n",
      "Epoch 35/300 - Train Loss: 0.0670, Val Loss: 0.0755\n",
      "Epoch 36/300 - Train Loss: 0.0690, Val Loss: 0.0681\n",
      "Epoch 37/300 - Train Loss: 0.0686, Val Loss: 0.0707\n",
      "Epoch 38/300 - Train Loss: 0.0699, Val Loss: 0.0711\n",
      "Epoch 39/300 - Train Loss: 0.0717, Val Loss: 0.0708\n",
      "Epoch 40/300 - Train Loss: 0.0674, Val Loss: 0.0727\n",
      "Epoch 41/300 - Train Loss: 0.0672, Val Loss: 0.0713\n",
      "Epoch 42/300 - Train Loss: 0.0681, Val Loss: 0.0627\n",
      "Epoch 43/300 - Train Loss: 0.0672, Val Loss: 0.0727\n",
      "Epoch 44/300 - Train Loss: 0.0669, Val Loss: 0.0709\n",
      "Epoch 45/300 - Train Loss: 0.0638, Val Loss: 0.0759\n",
      "Epoch 46/300 - Train Loss: 0.0670, Val Loss: 0.0701\n",
      "Epoch 47/300 - Train Loss: 0.0641, Val Loss: 0.0733\n",
      "Epoch 48/300 - Train Loss: 0.0649, Val Loss: 0.0728\n",
      "Epoch 49/300 - Train Loss: 0.0663, Val Loss: 0.0728\n",
      "Epoch 50/300 - Train Loss: 0.0653, Val Loss: 0.0716\n",
      "Epoch 51/300 - Train Loss: 0.0643, Val Loss: 0.0783\n",
      "Epoch 52/300 - Train Loss: 0.0642, Val Loss: 0.0720\n",
      "Epoch 53/300 - Train Loss: 0.0643, Val Loss: 0.0718\n",
      "Epoch 54/300 - Train Loss: 0.0624, Val Loss: 0.0773\n",
      "Epoch 55/300 - Train Loss: 0.0608, Val Loss: 0.0692\n",
      "Epoch 56/300 - Train Loss: 0.0607, Val Loss: 0.0717\n",
      "Epoch 57/300 - Train Loss: 0.0620, Val Loss: 0.0795\n",
      "Epoch 58/300 - Train Loss: 0.0641, Val Loss: 0.0700\n",
      "Epoch 59/300 - Train Loss: 0.0599, Val Loss: 0.0705\n",
      "Epoch 60/300 - Train Loss: 0.0625, Val Loss: 0.0734\n",
      "Epoch 61/300 - Train Loss: 0.0647, Val Loss: 0.0757\n",
      "Epoch 62/300 - Train Loss: 0.0610, Val Loss: 0.0703\n",
      "Epoch 63/300 - Train Loss: 0.0653, Val Loss: 0.0690\n",
      "Epoch 64/300 - Train Loss: 0.0629, Val Loss: 0.0709\n",
      "Epoch 65/300 - Train Loss: 0.0598, Val Loss: 0.0695\n",
      "Epoch 66/300 - Train Loss: 0.0592, Val Loss: 0.0729\n",
      "Epoch 67/300 - Train Loss: 0.0587, Val Loss: 0.0728\n",
      "Epoch 68/300 - Train Loss: 0.0593, Val Loss: 0.0814\n",
      "Epoch 69/300 - Train Loss: 0.0599, Val Loss: 0.0712\n",
      "Epoch 70/300 - Train Loss: 0.0598, Val Loss: 0.0747\n",
      "Epoch 71/300 - Train Loss: 0.0595, Val Loss: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:17:38,931] Trial 351 finished with value: 0.975950012383886 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.16253250919261042, 'learning_rate': 0.0005662511772438774, 'batch_size': 32, 'weight_decay': 2.4119810175030112e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300 - Train Loss: 0.0619, Val Loss: 0.0722\n",
      "Early stopping at epoch 72\n",
      "Macro F1 Score: 0.9760, Macro Precision: 0.9743, Macro Recall: 0.9777\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.97      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 353\n",
      "Training with F1=4, F2=8, D=2, dropout=0.16585810484293237, LR=0.0008006474958294733, BS=32, WD=0.00032409696654500073\n",
      "Epoch 1/300 - Train Loss: 0.2136, Val Loss: 0.0949\n",
      "Epoch 2/300 - Train Loss: 0.1211, Val Loss: 0.0767\n",
      "Epoch 3/300 - Train Loss: 0.1122, Val Loss: 0.0788\n",
      "Epoch 4/300 - Train Loss: 0.1067, Val Loss: 0.0769\n",
      "Epoch 5/300 - Train Loss: 0.1038, Val Loss: 0.0765\n",
      "Epoch 6/300 - Train Loss: 0.1009, Val Loss: 0.0726\n",
      "Epoch 7/300 - Train Loss: 0.0978, Val Loss: 0.0777\n",
      "Epoch 8/300 - Train Loss: 0.1011, Val Loss: 0.0750\n",
      "Epoch 9/300 - Train Loss: 0.0990, Val Loss: 0.0746\n",
      "Epoch 10/300 - Train Loss: 0.1003, Val Loss: 0.0750\n",
      "Epoch 11/300 - Train Loss: 0.0996, Val Loss: 0.0779\n",
      "Epoch 12/300 - Train Loss: 0.0989, Val Loss: 0.0744\n",
      "Epoch 13/300 - Train Loss: 0.0953, Val Loss: 0.0725\n",
      "Epoch 14/300 - Train Loss: 0.0964, Val Loss: 0.0862\n",
      "Epoch 15/300 - Train Loss: 0.0965, Val Loss: 0.0711\n",
      "Epoch 16/300 - Train Loss: 0.0937, Val Loss: 0.0799\n",
      "Epoch 17/300 - Train Loss: 0.0947, Val Loss: 0.0725\n",
      "Epoch 18/300 - Train Loss: 0.0941, Val Loss: 0.0763\n",
      "Epoch 19/300 - Train Loss: 0.0968, Val Loss: 0.0735\n",
      "Epoch 20/300 - Train Loss: 0.0944, Val Loss: 0.0711\n",
      "Epoch 21/300 - Train Loss: 0.0927, Val Loss: 0.0730\n",
      "Epoch 22/300 - Train Loss: 0.0923, Val Loss: 0.0723\n",
      "Epoch 23/300 - Train Loss: 0.0922, Val Loss: 0.0716\n",
      "Epoch 24/300 - Train Loss: 0.0906, Val Loss: 0.0693\n",
      "Epoch 25/300 - Train Loss: 0.0932, Val Loss: 0.0768\n",
      "Epoch 26/300 - Train Loss: 0.0937, Val Loss: 0.0753\n",
      "Epoch 27/300 - Train Loss: 0.0917, Val Loss: 0.0734\n",
      "Epoch 28/300 - Train Loss: 0.0897, Val Loss: 0.0779\n",
      "Epoch 29/300 - Train Loss: 0.0924, Val Loss: 0.0766\n",
      "Epoch 30/300 - Train Loss: 0.0908, Val Loss: 0.0712\n",
      "Epoch 31/300 - Train Loss: 0.0905, Val Loss: 0.0735\n",
      "Epoch 32/300 - Train Loss: 0.0902, Val Loss: 0.0751\n",
      "Epoch 33/300 - Train Loss: 0.0949, Val Loss: 0.0818\n",
      "Epoch 34/300 - Train Loss: 0.0894, Val Loss: 0.0766\n",
      "Epoch 35/300 - Train Loss: 0.0898, Val Loss: 0.0696\n",
      "Epoch 36/300 - Train Loss: 0.0880, Val Loss: 0.0715\n",
      "Epoch 37/300 - Train Loss: 0.0887, Val Loss: 0.0785\n",
      "Epoch 38/300 - Train Loss: 0.0905, Val Loss: 0.0696\n",
      "Epoch 39/300 - Train Loss: 0.0900, Val Loss: 0.0684\n",
      "Epoch 40/300 - Train Loss: 0.0885, Val Loss: 0.0804\n",
      "Epoch 41/300 - Train Loss: 0.0888, Val Loss: 0.0747\n",
      "Epoch 42/300 - Train Loss: 0.0901, Val Loss: 0.0695\n",
      "Epoch 43/300 - Train Loss: 0.0886, Val Loss: 0.0733\n",
      "Epoch 44/300 - Train Loss: 0.0875, Val Loss: 0.0734\n",
      "Epoch 45/300 - Train Loss: 0.0901, Val Loss: 0.0700\n",
      "Epoch 46/300 - Train Loss: 0.0895, Val Loss: 0.0682\n",
      "Epoch 47/300 - Train Loss: 0.0894, Val Loss: 0.0759\n",
      "Epoch 48/300 - Train Loss: 0.0865, Val Loss: 0.0701\n",
      "Epoch 49/300 - Train Loss: 0.0902, Val Loss: 0.0756\n",
      "Epoch 50/300 - Train Loss: 0.0890, Val Loss: 0.0679\n",
      "Epoch 51/300 - Train Loss: 0.0857, Val Loss: 0.0742\n",
      "Epoch 52/300 - Train Loss: 0.0881, Val Loss: 0.0726\n",
      "Epoch 53/300 - Train Loss: 0.0884, Val Loss: 0.0731\n",
      "Epoch 54/300 - Train Loss: 0.0876, Val Loss: 0.0746\n",
      "Epoch 55/300 - Train Loss: 0.0873, Val Loss: 0.0707\n",
      "Epoch 56/300 - Train Loss: 0.0885, Val Loss: 0.0730\n",
      "Epoch 57/300 - Train Loss: 0.0896, Val Loss: 0.0748\n",
      "Epoch 58/300 - Train Loss: 0.0872, Val Loss: 0.0681\n",
      "Epoch 59/300 - Train Loss: 0.0887, Val Loss: 0.0721\n",
      "Epoch 60/300 - Train Loss: 0.0862, Val Loss: 0.0746\n",
      "Epoch 61/300 - Train Loss: 0.0866, Val Loss: 0.2548\n",
      "Epoch 62/300 - Train Loss: 0.0873, Val Loss: 0.0723\n",
      "Epoch 63/300 - Train Loss: 0.0887, Val Loss: 0.0751\n",
      "Epoch 64/300 - Train Loss: 0.0881, Val Loss: 0.0725\n",
      "Epoch 65/300 - Train Loss: 0.0863, Val Loss: 0.0750\n",
      "Epoch 66/300 - Train Loss: 0.0842, Val Loss: 0.0760\n",
      "Epoch 67/300 - Train Loss: 0.0873, Val Loss: 0.0724\n",
      "Epoch 68/300 - Train Loss: 0.0849, Val Loss: 0.0752\n",
      "Epoch 69/300 - Train Loss: 0.0842, Val Loss: 0.0703\n",
      "Epoch 70/300 - Train Loss: 0.0859, Val Loss: 0.0705\n",
      "Epoch 71/300 - Train Loss: 0.0876, Val Loss: 0.0738\n",
      "Epoch 72/300 - Train Loss: 0.0860, Val Loss: 0.0744\n",
      "Epoch 73/300 - Train Loss: 0.0870, Val Loss: 0.0914\n",
      "Epoch 74/300 - Train Loss: 0.0858, Val Loss: 0.0670\n",
      "Epoch 75/300 - Train Loss: 0.0861, Val Loss: 0.0707\n",
      "Epoch 76/300 - Train Loss: 0.0856, Val Loss: 0.0754\n",
      "Epoch 77/300 - Train Loss: 0.0857, Val Loss: 0.0687\n",
      "Epoch 78/300 - Train Loss: 0.0877, Val Loss: 0.0680\n",
      "Epoch 79/300 - Train Loss: 0.0845, Val Loss: 0.0716\n",
      "Epoch 80/300 - Train Loss: 0.0845, Val Loss: 0.0682\n",
      "Epoch 81/300 - Train Loss: 0.0849, Val Loss: 0.0749\n",
      "Epoch 82/300 - Train Loss: 0.0851, Val Loss: 0.0807\n",
      "Epoch 83/300 - Train Loss: 0.0864, Val Loss: 0.0777\n",
      "Epoch 84/300 - Train Loss: 0.0845, Val Loss: 0.0769\n",
      "Epoch 85/300 - Train Loss: 0.0855, Val Loss: 0.0713\n",
      "Epoch 86/300 - Train Loss: 0.0852, Val Loss: 0.0735\n",
      "Epoch 87/300 - Train Loss: 0.0848, Val Loss: 0.0739\n",
      "Epoch 88/300 - Train Loss: 0.0875, Val Loss: 0.0700\n",
      "Epoch 89/300 - Train Loss: 0.0839, Val Loss: 0.0731\n",
      "Epoch 90/300 - Train Loss: 0.0846, Val Loss: 0.0710\n",
      "Epoch 91/300 - Train Loss: 0.0841, Val Loss: 0.0731\n",
      "Epoch 92/300 - Train Loss: 0.0856, Val Loss: 0.0749\n",
      "Epoch 93/300 - Train Loss: 0.0835, Val Loss: 0.0683\n",
      "Epoch 94/300 - Train Loss: 0.0840, Val Loss: 0.0699\n",
      "Epoch 95/300 - Train Loss: 0.0837, Val Loss: 0.0838\n",
      "Epoch 96/300 - Train Loss: 0.0886, Val Loss: 0.0742\n",
      "Epoch 97/300 - Train Loss: 0.0849, Val Loss: 0.0655\n",
      "Epoch 98/300 - Train Loss: 0.0849, Val Loss: 0.0716\n",
      "Epoch 99/300 - Train Loss: 0.0826, Val Loss: 0.0737\n",
      "Epoch 100/300 - Train Loss: 0.0831, Val Loss: 0.0787\n",
      "Epoch 101/300 - Train Loss: 0.0827, Val Loss: 0.0789\n",
      "Epoch 102/300 - Train Loss: 0.0841, Val Loss: 0.0684\n",
      "Epoch 103/300 - Train Loss: 0.0820, Val Loss: 0.0659\n",
      "Epoch 104/300 - Train Loss: 0.0810, Val Loss: 0.0721\n",
      "Epoch 105/300 - Train Loss: 0.0834, Val Loss: 0.0716\n",
      "Epoch 106/300 - Train Loss: 0.0847, Val Loss: 0.0783\n",
      "Epoch 107/300 - Train Loss: 0.0880, Val Loss: 0.0717\n",
      "Epoch 108/300 - Train Loss: 0.0831, Val Loss: 0.0674\n",
      "Epoch 109/300 - Train Loss: 0.0825, Val Loss: 0.0706\n",
      "Epoch 110/300 - Train Loss: 0.0830, Val Loss: 0.0740\n",
      "Epoch 111/300 - Train Loss: 0.0843, Val Loss: 0.0683\n",
      "Epoch 112/300 - Train Loss: 0.0855, Val Loss: 0.0769\n",
      "Epoch 113/300 - Train Loss: 0.0835, Val Loss: 0.0714\n",
      "Epoch 114/300 - Train Loss: 0.0810, Val Loss: 0.0746\n",
      "Epoch 115/300 - Train Loss: 0.0832, Val Loss: 0.0747\n",
      "Epoch 116/300 - Train Loss: 0.0839, Val Loss: 0.0764\n",
      "Epoch 117/300 - Train Loss: 0.0828, Val Loss: 0.0753\n",
      "Epoch 118/300 - Train Loss: 0.0844, Val Loss: 0.0719\n",
      "Epoch 119/300 - Train Loss: 0.0819, Val Loss: 0.0736\n",
      "Epoch 120/300 - Train Loss: 0.0825, Val Loss: 0.0695\n",
      "Epoch 121/300 - Train Loss: 0.0859, Val Loss: 0.0722\n",
      "Epoch 122/300 - Train Loss: 0.0855, Val Loss: 0.0723\n",
      "Epoch 123/300 - Train Loss: 0.0841, Val Loss: 0.0761\n",
      "Epoch 124/300 - Train Loss: 0.0833, Val Loss: 0.0804\n",
      "Epoch 125/300 - Train Loss: 0.0831, Val Loss: 0.0673\n",
      "Epoch 126/300 - Train Loss: 0.0847, Val Loss: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:21:37,625] Trial 352 finished with value: 0.96754539176412 and parameters: {'F1': 4, 'F2': 8, 'D': 2, 'dropout': 0.16585810484293237, 'learning_rate': 0.0008006474958294733, 'batch_size': 32, 'weight_decay': 0.00032409696654500073}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/300 - Train Loss: 0.0793, Val Loss: 0.0773\n",
      "Early stopping at epoch 127\n",
      "Macro F1 Score: 0.9675, Macro Precision: 0.9597, Macro Recall: 0.9761\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 354\n",
      "Training with F1=4, F2=8, D=2, dropout=0.15880790413179174, LR=0.0008774892365254062, BS=32, WD=2.5811675733138536e-05\n",
      "Epoch 1/300 - Train Loss: 0.2086, Val Loss: 0.0910\n",
      "Epoch 2/300 - Train Loss: 0.1117, Val Loss: 0.0828\n",
      "Epoch 3/300 - Train Loss: 0.1079, Val Loss: 0.0802\n",
      "Epoch 4/300 - Train Loss: 0.1038, Val Loss: 0.0787\n",
      "Epoch 5/300 - Train Loss: 0.1031, Val Loss: 0.0785\n",
      "Epoch 6/300 - Train Loss: 0.1033, Val Loss: 0.0829\n",
      "Epoch 7/300 - Train Loss: 0.0984, Val Loss: 0.0862\n",
      "Epoch 8/300 - Train Loss: 0.0989, Val Loss: 0.0902\n",
      "Epoch 9/300 - Train Loss: 0.0974, Val Loss: 0.0798\n",
      "Epoch 10/300 - Train Loss: 0.0989, Val Loss: 0.0818\n",
      "Epoch 11/300 - Train Loss: 0.0948, Val Loss: 0.0867\n",
      "Epoch 12/300 - Train Loss: 0.1008, Val Loss: 0.0777\n",
      "Epoch 13/300 - Train Loss: 0.0953, Val Loss: 0.0756\n",
      "Epoch 14/300 - Train Loss: 0.0938, Val Loss: 0.0809\n",
      "Epoch 15/300 - Train Loss: 0.0958, Val Loss: 0.0755\n",
      "Epoch 16/300 - Train Loss: 0.0948, Val Loss: 0.0802\n",
      "Epoch 17/300 - Train Loss: 0.0910, Val Loss: 0.0732\n",
      "Epoch 18/300 - Train Loss: 0.0934, Val Loss: 0.0716\n",
      "Epoch 19/300 - Train Loss: 0.0900, Val Loss: 0.0784\n",
      "Epoch 20/300 - Train Loss: 0.0891, Val Loss: 0.0758\n",
      "Epoch 21/300 - Train Loss: 0.0926, Val Loss: 0.0755\n",
      "Epoch 22/300 - Train Loss: 0.0906, Val Loss: 0.0746\n",
      "Epoch 23/300 - Train Loss: 0.0898, Val Loss: 0.0744\n",
      "Epoch 24/300 - Train Loss: 0.0909, Val Loss: 0.0709\n",
      "Epoch 25/300 - Train Loss: 0.0905, Val Loss: 0.0746\n",
      "Epoch 26/300 - Train Loss: 0.0895, Val Loss: 0.0771\n",
      "Epoch 27/300 - Train Loss: 0.0893, Val Loss: 0.0815\n",
      "Epoch 28/300 - Train Loss: 0.0881, Val Loss: 0.0762\n",
      "Epoch 29/300 - Train Loss: 0.0918, Val Loss: 0.0721\n",
      "Epoch 30/300 - Train Loss: 0.0903, Val Loss: 0.1039\n",
      "Epoch 31/300 - Train Loss: 0.0894, Val Loss: 0.0744\n",
      "Epoch 32/300 - Train Loss: 0.0850, Val Loss: 0.0781\n",
      "Epoch 33/300 - Train Loss: 0.0870, Val Loss: 0.0787\n",
      "Epoch 34/300 - Train Loss: 0.0876, Val Loss: 0.0719\n",
      "Epoch 35/300 - Train Loss: 0.0864, Val Loss: 0.0730\n",
      "Epoch 36/300 - Train Loss: 0.0867, Val Loss: 0.0789\n",
      "Epoch 37/300 - Train Loss: 0.0856, Val Loss: 0.0817\n",
      "Epoch 38/300 - Train Loss: 0.0856, Val Loss: 0.0997\n",
      "Epoch 39/300 - Train Loss: 0.0859, Val Loss: 0.0791\n",
      "Epoch 40/300 - Train Loss: 0.0860, Val Loss: 0.0771\n",
      "Epoch 41/300 - Train Loss: 0.0892, Val Loss: 0.0768\n",
      "Epoch 42/300 - Train Loss: 0.0842, Val Loss: 0.0768\n",
      "Epoch 43/300 - Train Loss: 0.0830, Val Loss: 0.0761\n",
      "Epoch 44/300 - Train Loss: 0.0864, Val Loss: 0.0759\n",
      "Epoch 45/300 - Train Loss: 0.0856, Val Loss: 0.0800\n",
      "Epoch 46/300 - Train Loss: 0.0861, Val Loss: 0.0706\n",
      "Epoch 47/300 - Train Loss: 0.0839, Val Loss: 0.0780\n",
      "Epoch 48/300 - Train Loss: 0.0856, Val Loss: 0.0762\n",
      "Epoch 49/300 - Train Loss: 0.0841, Val Loss: 0.0710\n",
      "Epoch 50/300 - Train Loss: 0.0841, Val Loss: 0.0809\n",
      "Epoch 51/300 - Train Loss: 0.0813, Val Loss: 0.0770\n",
      "Epoch 52/300 - Train Loss: 0.0837, Val Loss: 0.0696\n",
      "Epoch 53/300 - Train Loss: 0.0806, Val Loss: 0.0809\n",
      "Epoch 54/300 - Train Loss: 0.0802, Val Loss: 0.0759\n",
      "Epoch 55/300 - Train Loss: 0.0836, Val Loss: 0.0776\n",
      "Epoch 56/300 - Train Loss: 0.0820, Val Loss: 0.0817\n",
      "Epoch 57/300 - Train Loss: 0.0826, Val Loss: 0.0834\n",
      "Epoch 58/300 - Train Loss: 0.0842, Val Loss: 0.0784\n",
      "Epoch 59/300 - Train Loss: 0.0822, Val Loss: 0.0862\n",
      "Epoch 60/300 - Train Loss: 0.0844, Val Loss: 0.0790\n",
      "Epoch 61/300 - Train Loss: 0.0832, Val Loss: 0.0770\n",
      "Epoch 62/300 - Train Loss: 0.0810, Val Loss: 0.0855\n",
      "Epoch 63/300 - Train Loss: 0.0812, Val Loss: 0.0802\n",
      "Epoch 64/300 - Train Loss: 0.0832, Val Loss: 0.0691\n",
      "Epoch 65/300 - Train Loss: 0.0818, Val Loss: 0.0762\n",
      "Epoch 66/300 - Train Loss: 0.0832, Val Loss: 0.0827\n",
      "Epoch 67/300 - Train Loss: 0.0805, Val Loss: 0.0757\n",
      "Epoch 68/300 - Train Loss: 0.0811, Val Loss: 0.0742\n",
      "Epoch 69/300 - Train Loss: 0.0836, Val Loss: 0.0741\n",
      "Epoch 70/300 - Train Loss: 0.0820, Val Loss: 0.0749\n",
      "Epoch 71/300 - Train Loss: 0.0785, Val Loss: 0.0747\n",
      "Epoch 72/300 - Train Loss: 0.0786, Val Loss: 0.0742\n",
      "Epoch 73/300 - Train Loss: 0.0800, Val Loss: 0.0833\n",
      "Epoch 74/300 - Train Loss: 0.0798, Val Loss: 0.0717\n",
      "Epoch 75/300 - Train Loss: 0.0818, Val Loss: 0.0769\n",
      "Epoch 76/300 - Train Loss: 0.0806, Val Loss: 0.0756\n",
      "Epoch 77/300 - Train Loss: 0.0803, Val Loss: 0.0729\n",
      "Epoch 78/300 - Train Loss: 0.0801, Val Loss: 0.0773\n",
      "Epoch 79/300 - Train Loss: 0.0785, Val Loss: 0.0877\n",
      "Epoch 80/300 - Train Loss: 0.0810, Val Loss: 0.0732\n",
      "Epoch 81/300 - Train Loss: 0.0785, Val Loss: 0.0811\n",
      "Epoch 82/300 - Train Loss: 0.0799, Val Loss: 0.0739\n",
      "Epoch 83/300 - Train Loss: 0.0796, Val Loss: 0.0794\n",
      "Epoch 84/300 - Train Loss: 0.0786, Val Loss: 0.0895\n",
      "Epoch 85/300 - Train Loss: 0.0805, Val Loss: 0.0869\n",
      "Epoch 86/300 - Train Loss: 0.0796, Val Loss: 0.0828\n",
      "Epoch 87/300 - Train Loss: 0.0787, Val Loss: 0.0749\n",
      "Epoch 88/300 - Train Loss: 0.0765, Val Loss: 0.0769\n",
      "Epoch 89/300 - Train Loss: 0.0797, Val Loss: 0.0871\n",
      "Epoch 90/300 - Train Loss: 0.0785, Val Loss: 0.0775\n",
      "Epoch 91/300 - Train Loss: 0.0785, Val Loss: 0.0721\n",
      "Epoch 92/300 - Train Loss: 0.0758, Val Loss: 0.0769\n",
      "Epoch 93/300 - Train Loss: 0.0785, Val Loss: 0.0786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:24:34,841] Trial 353 finished with value: 0.9581722402187519 and parameters: {'F1': 4, 'F2': 8, 'D': 2, 'dropout': 0.15880790413179174, 'learning_rate': 0.0008774892365254062, 'batch_size': 32, 'weight_decay': 2.5811675733138536e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300 - Train Loss: 0.0763, Val Loss: 0.0818\n",
      "Early stopping at epoch 94\n",
      "Macro F1 Score: 0.9582, Macro Precision: 0.9529, Macro Recall: 0.9639\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.93      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 355\n",
      "Training with F1=4, F2=8, D=2, dropout=0.17677296582059976, LR=0.0009224925721695703, BS=32, WD=2.0306379003319395e-05\n",
      "Epoch 1/300 - Train Loss: 0.1912, Val Loss: 0.0948\n",
      "Epoch 2/300 - Train Loss: 0.1175, Val Loss: 0.0893\n",
      "Epoch 3/300 - Train Loss: 0.1109, Val Loss: 0.0763\n",
      "Epoch 4/300 - Train Loss: 0.1070, Val Loss: 0.0830\n",
      "Epoch 5/300 - Train Loss: 0.1065, Val Loss: 0.0888\n",
      "Epoch 6/300 - Train Loss: 0.1029, Val Loss: 0.0836\n",
      "Epoch 7/300 - Train Loss: 0.1009, Val Loss: 0.0851\n",
      "Epoch 8/300 - Train Loss: 0.1006, Val Loss: 0.0796\n",
      "Epoch 9/300 - Train Loss: 0.1001, Val Loss: 0.0844\n",
      "Epoch 10/300 - Train Loss: 0.1008, Val Loss: 0.0768\n",
      "Epoch 11/300 - Train Loss: 0.1024, Val Loss: 0.0830\n",
      "Epoch 12/300 - Train Loss: 0.0950, Val Loss: 0.0741\n",
      "Epoch 13/300 - Train Loss: 0.0966, Val Loss: 0.0725\n",
      "Epoch 14/300 - Train Loss: 0.0966, Val Loss: 0.0744\n",
      "Epoch 15/300 - Train Loss: 0.0961, Val Loss: 0.0819\n",
      "Epoch 16/300 - Train Loss: 0.0964, Val Loss: 0.0686\n",
      "Epoch 17/300 - Train Loss: 0.0950, Val Loss: 0.0834\n",
      "Epoch 18/300 - Train Loss: 0.0930, Val Loss: 0.0725\n",
      "Epoch 19/300 - Train Loss: 0.0926, Val Loss: 0.0743\n",
      "Epoch 20/300 - Train Loss: 0.0947, Val Loss: 0.0794\n",
      "Epoch 21/300 - Train Loss: 0.0924, Val Loss: 0.0759\n",
      "Epoch 22/300 - Train Loss: 0.0913, Val Loss: 0.0740\n",
      "Epoch 23/300 - Train Loss: 0.0911, Val Loss: 0.0846\n",
      "Epoch 24/300 - Train Loss: 0.0941, Val Loss: 0.0867\n",
      "Epoch 25/300 - Train Loss: 0.0931, Val Loss: 0.0822\n",
      "Epoch 26/300 - Train Loss: 0.0923, Val Loss: 0.0792\n",
      "Epoch 27/300 - Train Loss: 0.0919, Val Loss: 0.0710\n",
      "Epoch 28/300 - Train Loss: 0.0916, Val Loss: 0.0739\n",
      "Epoch 29/300 - Train Loss: 0.0940, Val Loss: 0.0782\n",
      "Epoch 30/300 - Train Loss: 0.0948, Val Loss: 0.0778\n",
      "Epoch 31/300 - Train Loss: 0.0925, Val Loss: 0.0723\n",
      "Epoch 32/300 - Train Loss: 0.0928, Val Loss: 0.0806\n",
      "Epoch 33/300 - Train Loss: 0.0912, Val Loss: 0.0708\n",
      "Epoch 34/300 - Train Loss: 0.0918, Val Loss: 0.0757\n",
      "Epoch 35/300 - Train Loss: 0.0908, Val Loss: 0.0819\n",
      "Epoch 36/300 - Train Loss: 0.0898, Val Loss: 0.0761\n",
      "Epoch 37/300 - Train Loss: 0.0903, Val Loss: 0.0707\n",
      "Epoch 38/300 - Train Loss: 0.0909, Val Loss: 0.0746\n",
      "Epoch 39/300 - Train Loss: 0.0904, Val Loss: 0.0763\n",
      "Epoch 40/300 - Train Loss: 0.0881, Val Loss: 0.0911\n",
      "Epoch 41/300 - Train Loss: 0.0898, Val Loss: 0.0741\n",
      "Epoch 42/300 - Train Loss: 0.0897, Val Loss: 0.0778\n",
      "Epoch 43/300 - Train Loss: 0.0890, Val Loss: 0.0732\n",
      "Epoch 44/300 - Train Loss: 0.0884, Val Loss: 0.0758\n",
      "Epoch 45/300 - Train Loss: 0.0895, Val Loss: 0.0769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:26:01,095] Trial 354 finished with value: 0.961168797532434 and parameters: {'F1': 4, 'F2': 8, 'D': 2, 'dropout': 0.17677296582059976, 'learning_rate': 0.0009224925721695703, 'batch_size': 32, 'weight_decay': 2.0306379003319395e-05}. Best is trial 285 with value: 0.9804401364947632.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300 - Train Loss: 0.0883, Val Loss: 0.0776\n",
      "Early stopping at epoch 46\n",
      "Macro F1 Score: 0.9612, Macro Precision: 0.9648, Macro Recall: 0.9578\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.93      0.92      0.93        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 356\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1604465584506531, LR=0.0007497847670531389, BS=32, WD=0.00021416718031719688\n",
      "Epoch 1/300 - Train Loss: 0.1616, Val Loss: 0.0762\n",
      "Epoch 2/300 - Train Loss: 0.0977, Val Loss: 0.0784\n",
      "Epoch 3/300 - Train Loss: 0.0935, Val Loss: 0.0790\n",
      "Epoch 4/300 - Train Loss: 0.0891, Val Loss: 0.0904\n",
      "Epoch 5/300 - Train Loss: 0.0855, Val Loss: 0.0720\n",
      "Epoch 6/300 - Train Loss: 0.0848, Val Loss: 0.0674\n",
      "Epoch 7/300 - Train Loss: 0.0845, Val Loss: 0.0674\n",
      "Epoch 8/300 - Train Loss: 0.0841, Val Loss: 0.0705\n",
      "Epoch 9/300 - Train Loss: 0.0817, Val Loss: 0.0709\n",
      "Epoch 10/300 - Train Loss: 0.0817, Val Loss: 0.0861\n",
      "Epoch 11/300 - Train Loss: 0.0829, Val Loss: 0.0670\n",
      "Epoch 12/300 - Train Loss: 0.0818, Val Loss: 0.0644\n",
      "Epoch 13/300 - Train Loss: 0.0807, Val Loss: 0.0727\n",
      "Epoch 14/300 - Train Loss: 0.0792, Val Loss: 0.0718\n",
      "Epoch 15/300 - Train Loss: 0.0797, Val Loss: 0.0657\n",
      "Epoch 16/300 - Train Loss: 0.0771, Val Loss: 0.0695\n",
      "Epoch 17/300 - Train Loss: 0.0774, Val Loss: 0.0775\n",
      "Epoch 18/300 - Train Loss: 0.0775, Val Loss: 0.0639\n",
      "Epoch 19/300 - Train Loss: 0.0774, Val Loss: 0.0767\n",
      "Epoch 20/300 - Train Loss: 0.0789, Val Loss: 0.0774\n",
      "Epoch 21/300 - Train Loss: 0.0784, Val Loss: 0.0666\n",
      "Epoch 22/300 - Train Loss: 0.0766, Val Loss: 0.0744\n",
      "Epoch 23/300 - Train Loss: 0.0753, Val Loss: 0.0708\n",
      "Epoch 24/300 - Train Loss: 0.0764, Val Loss: 0.0724\n",
      "Epoch 25/300 - Train Loss: 0.0779, Val Loss: 0.0756\n",
      "Epoch 26/300 - Train Loss: 0.0753, Val Loss: 0.0703\n",
      "Epoch 27/300 - Train Loss: 0.0748, Val Loss: 0.0711\n",
      "Epoch 28/300 - Train Loss: 0.0757, Val Loss: 0.0715\n",
      "Epoch 29/300 - Train Loss: 0.0754, Val Loss: 0.0682\n",
      "Epoch 30/300 - Train Loss: 0.0763, Val Loss: 0.0741\n",
      "Epoch 31/300 - Train Loss: 0.0731, Val Loss: 0.0692\n",
      "Epoch 32/300 - Train Loss: 0.0724, Val Loss: 0.0685\n",
      "Epoch 33/300 - Train Loss: 0.0730, Val Loss: 0.0767\n",
      "Epoch 34/300 - Train Loss: 0.0763, Val Loss: 0.0681\n",
      "Epoch 35/300 - Train Loss: 0.0733, Val Loss: 0.0668\n",
      "Epoch 36/300 - Train Loss: 0.0742, Val Loss: 0.0718\n",
      "Epoch 37/300 - Train Loss: 0.0732, Val Loss: 0.0762\n",
      "Epoch 38/300 - Train Loss: 0.0713, Val Loss: 0.0723\n",
      "Epoch 39/300 - Train Loss: 0.0710, Val Loss: 0.0704\n",
      "Epoch 40/300 - Train Loss: 0.0724, Val Loss: 0.0696\n",
      "Epoch 41/300 - Train Loss: 0.0734, Val Loss: 0.0669\n",
      "Epoch 42/300 - Train Loss: 0.0728, Val Loss: 0.0755\n",
      "Epoch 43/300 - Train Loss: 0.0726, Val Loss: 0.0763\n",
      "Epoch 44/300 - Train Loss: 0.0718, Val Loss: 0.0733\n",
      "Epoch 45/300 - Train Loss: 0.0730, Val Loss: 0.0785\n",
      "Epoch 46/300 - Train Loss: 0.0736, Val Loss: 0.0676\n",
      "Epoch 47/300 - Train Loss: 0.0700, Val Loss: 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:28:00,788] Trial 355 finished with value: 0.9823538998172442 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1604465584506531, 'learning_rate': 0.0007497847670531389, 'batch_size': 32, 'weight_decay': 0.00021416718031719688}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300 - Train Loss: 0.0710, Val Loss: 0.0764\n",
      "Early stopping at epoch 48\n",
      "Macro F1 Score: 0.9824, Macro Precision: 0.9809, Macro Recall: 0.9840\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.97      0.98      0.98        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.99      1443\n",
      "   macro avg       0.98      0.98      0.98      1443\n",
      "weighted avg       0.99      0.99      0.99      1443\n",
      "\n",
      "\n",
      "Trial 357\n",
      "Training with F1=4, F2=16, D=2, dropout=0.16024826442389475, LR=0.0005931755563139878, BS=32, WD=0.00021798890308165583\n",
      "Epoch 1/300 - Train Loss: 0.2094, Val Loss: 0.0958\n",
      "Epoch 2/300 - Train Loss: 0.1215, Val Loss: 0.0906\n",
      "Epoch 3/300 - Train Loss: 0.1109, Val Loss: 0.0825\n",
      "Epoch 4/300 - Train Loss: 0.1062, Val Loss: 0.0785\n",
      "Epoch 5/300 - Train Loss: 0.1001, Val Loss: 0.0827\n",
      "Epoch 6/300 - Train Loss: 0.1041, Val Loss: 0.0746\n",
      "Epoch 7/300 - Train Loss: 0.0988, Val Loss: 0.0767\n",
      "Epoch 8/300 - Train Loss: 0.0995, Val Loss: 0.0771\n",
      "Epoch 9/300 - Train Loss: 0.0985, Val Loss: 0.0752\n",
      "Epoch 10/300 - Train Loss: 0.0988, Val Loss: 0.0767\n",
      "Epoch 11/300 - Train Loss: 0.0941, Val Loss: 0.0723\n",
      "Epoch 12/300 - Train Loss: 0.0952, Val Loss: 0.0726\n",
      "Epoch 13/300 - Train Loss: 0.0940, Val Loss: 0.0706\n",
      "Epoch 14/300 - Train Loss: 0.0944, Val Loss: 0.0753\n",
      "Epoch 15/300 - Train Loss: 0.0933, Val Loss: 0.0741\n",
      "Epoch 16/300 - Train Loss: 0.0962, Val Loss: 0.0726\n",
      "Epoch 17/300 - Train Loss: 0.0919, Val Loss: 0.0784\n",
      "Epoch 18/300 - Train Loss: 0.0928, Val Loss: 0.0744\n",
      "Epoch 19/300 - Train Loss: 0.0910, Val Loss: 0.0698\n",
      "Epoch 20/300 - Train Loss: 0.0879, Val Loss: 0.0747\n",
      "Epoch 21/300 - Train Loss: 0.0912, Val Loss: 0.0717\n",
      "Epoch 22/300 - Train Loss: 0.0903, Val Loss: 0.0783\n",
      "Epoch 23/300 - Train Loss: 0.0883, Val Loss: 0.0764\n",
      "Epoch 24/300 - Train Loss: 0.0863, Val Loss: 0.0713\n",
      "Epoch 25/300 - Train Loss: 0.0892, Val Loss: 0.0681\n",
      "Epoch 26/300 - Train Loss: 0.0880, Val Loss: 0.0659\n",
      "Epoch 27/300 - Train Loss: 0.0901, Val Loss: 0.0757\n",
      "Epoch 28/300 - Train Loss: 0.0896, Val Loss: 0.0719\n",
      "Epoch 29/300 - Train Loss: 0.0866, Val Loss: 0.0705\n",
      "Epoch 30/300 - Train Loss: 0.0878, Val Loss: 0.0672\n",
      "Epoch 31/300 - Train Loss: 0.0857, Val Loss: 0.0708\n",
      "Epoch 32/300 - Train Loss: 0.0836, Val Loss: 0.0709\n",
      "Epoch 33/300 - Train Loss: 0.0850, Val Loss: 0.0694\n",
      "Epoch 34/300 - Train Loss: 0.0862, Val Loss: 0.0702\n",
      "Epoch 35/300 - Train Loss: 0.0856, Val Loss: 0.0740\n",
      "Epoch 36/300 - Train Loss: 0.0852, Val Loss: 0.0706\n",
      "Epoch 37/300 - Train Loss: 0.0852, Val Loss: 0.0769\n",
      "Epoch 38/300 - Train Loss: 0.0834, Val Loss: 0.0681\n",
      "Epoch 39/300 - Train Loss: 0.0840, Val Loss: 0.0748\n",
      "Epoch 40/300 - Train Loss: 0.0827, Val Loss: 0.0692\n",
      "Epoch 41/300 - Train Loss: 0.0834, Val Loss: 0.0762\n",
      "Epoch 42/300 - Train Loss: 0.0862, Val Loss: 0.0668\n",
      "Epoch 43/300 - Train Loss: 0.0848, Val Loss: 0.0838\n",
      "Epoch 44/300 - Train Loss: 0.0838, Val Loss: 0.0669\n",
      "Epoch 45/300 - Train Loss: 0.0821, Val Loss: 0.0712\n",
      "Epoch 46/300 - Train Loss: 0.0817, Val Loss: 0.0705\n",
      "Epoch 47/300 - Train Loss: 0.0797, Val Loss: 0.0668\n",
      "Epoch 48/300 - Train Loss: 0.0803, Val Loss: 0.0713\n",
      "Epoch 49/300 - Train Loss: 0.0815, Val Loss: 0.0691\n",
      "Epoch 50/300 - Train Loss: 0.0830, Val Loss: 0.0712\n",
      "Epoch 51/300 - Train Loss: 0.0787, Val Loss: 0.0676\n",
      "Epoch 52/300 - Train Loss: 0.0818, Val Loss: 0.0672\n",
      "Epoch 53/300 - Train Loss: 0.0817, Val Loss: 0.0699\n",
      "Epoch 54/300 - Train Loss: 0.0815, Val Loss: 0.0745\n",
      "Epoch 55/300 - Train Loss: 0.0807, Val Loss: 0.0691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:29:46,960] Trial 356 finished with value: 0.9693311352886473 and parameters: {'F1': 4, 'F2': 16, 'D': 2, 'dropout': 0.16024826442389475, 'learning_rate': 0.0005931755563139878, 'batch_size': 32, 'weight_decay': 0.00021798890308165583}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300 - Train Loss: 0.0823, Val Loss: 0.0784\n",
      "Early stopping at epoch 56\n",
      "Macro F1 Score: 0.9693, Macro Precision: 0.9639, Macro Recall: 0.9753\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 358\n",
      "Training with F1=4, F2=32, D=2, dropout=0.1719939121062775, LR=0.0007849307364480885, BS=32, WD=0.00020976774588900155\n",
      "Epoch 1/300 - Train Loss: 0.1878, Val Loss: 0.0972\n",
      "Epoch 2/300 - Train Loss: 0.1152, Val Loss: 0.0855\n",
      "Epoch 3/300 - Train Loss: 0.1091, Val Loss: 0.0802\n",
      "Epoch 4/300 - Train Loss: 0.1037, Val Loss: 0.0914\n",
      "Epoch 5/300 - Train Loss: 0.1041, Val Loss: 0.0827\n",
      "Epoch 6/300 - Train Loss: 0.1006, Val Loss: 0.0830\n",
      "Epoch 7/300 - Train Loss: 0.0964, Val Loss: 0.0874\n",
      "Epoch 8/300 - Train Loss: 0.0953, Val Loss: 0.0811\n",
      "Epoch 9/300 - Train Loss: 0.0930, Val Loss: 0.0788\n",
      "Epoch 10/300 - Train Loss: 0.0928, Val Loss: 0.0807\n",
      "Epoch 11/300 - Train Loss: 0.0933, Val Loss: 0.0781\n",
      "Epoch 12/300 - Train Loss: 0.0960, Val Loss: 0.0827\n",
      "Epoch 13/300 - Train Loss: 0.0946, Val Loss: 0.0838\n",
      "Epoch 14/300 - Train Loss: 0.0951, Val Loss: 0.0807\n",
      "Epoch 15/300 - Train Loss: 0.0907, Val Loss: 0.0795\n",
      "Epoch 16/300 - Train Loss: 0.0912, Val Loss: 0.0766\n",
      "Epoch 17/300 - Train Loss: 0.0895, Val Loss: 0.0754\n",
      "Epoch 18/300 - Train Loss: 0.0895, Val Loss: 0.0802\n",
      "Epoch 19/300 - Train Loss: 0.0876, Val Loss: 0.0774\n",
      "Epoch 20/300 - Train Loss: 0.0883, Val Loss: 0.0810\n",
      "Epoch 21/300 - Train Loss: 0.0912, Val Loss: 0.0787\n",
      "Epoch 22/300 - Train Loss: 0.0878, Val Loss: 0.0849\n",
      "Epoch 23/300 - Train Loss: 0.0849, Val Loss: 0.0753\n",
      "Epoch 24/300 - Train Loss: 0.0866, Val Loss: 0.0742\n",
      "Epoch 25/300 - Train Loss: 0.0883, Val Loss: 0.0873\n",
      "Epoch 26/300 - Train Loss: 0.0886, Val Loss: 0.0864\n",
      "Epoch 27/300 - Train Loss: 0.0833, Val Loss: 0.0808\n",
      "Epoch 28/300 - Train Loss: 0.0865, Val Loss: 0.1004\n",
      "Epoch 29/300 - Train Loss: 0.0871, Val Loss: 0.0772\n",
      "Epoch 30/300 - Train Loss: 0.0853, Val Loss: 0.0812\n",
      "Epoch 31/300 - Train Loss: 0.0839, Val Loss: 0.0753\n",
      "Epoch 32/300 - Train Loss: 0.0851, Val Loss: 0.0814\n",
      "Epoch 33/300 - Train Loss: 0.0840, Val Loss: 0.0784\n",
      "Epoch 34/300 - Train Loss: 0.0846, Val Loss: 0.0805\n",
      "Epoch 35/300 - Train Loss: 0.0832, Val Loss: 0.0743\n",
      "Epoch 36/300 - Train Loss: 0.0828, Val Loss: 0.0761\n",
      "Epoch 37/300 - Train Loss: 0.0841, Val Loss: 0.0780\n",
      "Epoch 38/300 - Train Loss: 0.0849, Val Loss: 0.0788\n",
      "Epoch 39/300 - Train Loss: 0.0838, Val Loss: 0.0782\n",
      "Epoch 40/300 - Train Loss: 0.0813, Val Loss: 0.0717\n",
      "Epoch 41/300 - Train Loss: 0.0815, Val Loss: 0.0867\n",
      "Epoch 42/300 - Train Loss: 0.0860, Val Loss: 0.0896\n",
      "Epoch 43/300 - Train Loss: 0.0828, Val Loss: 0.0784\n",
      "Epoch 44/300 - Train Loss: 0.0820, Val Loss: 0.0901\n",
      "Epoch 45/300 - Train Loss: 0.0782, Val Loss: 0.0744\n",
      "Epoch 46/300 - Train Loss: 0.0787, Val Loss: 0.0808\n",
      "Epoch 47/300 - Train Loss: 0.0805, Val Loss: 0.0772\n",
      "Epoch 48/300 - Train Loss: 0.0807, Val Loss: 0.0794\n",
      "Epoch 49/300 - Train Loss: 0.0780, Val Loss: 0.0802\n",
      "Epoch 50/300 - Train Loss: 0.0802, Val Loss: 0.0749\n",
      "Epoch 51/300 - Train Loss: 0.0777, Val Loss: 0.0758\n",
      "Epoch 52/300 - Train Loss: 0.0796, Val Loss: 0.0737\n",
      "Epoch 53/300 - Train Loss: 0.0786, Val Loss: 0.0820\n",
      "Epoch 54/300 - Train Loss: 0.0784, Val Loss: 0.0814\n",
      "Epoch 55/300 - Train Loss: 0.0801, Val Loss: 0.0770\n",
      "Epoch 56/300 - Train Loss: 0.0769, Val Loss: 0.0812\n",
      "Epoch 57/300 - Train Loss: 0.0810, Val Loss: 0.0768\n",
      "Epoch 58/300 - Train Loss: 0.0792, Val Loss: 0.0748\n",
      "Epoch 59/300 - Train Loss: 0.0782, Val Loss: 0.0744\n",
      "Epoch 60/300 - Train Loss: 0.0768, Val Loss: 0.0769\n",
      "Epoch 61/300 - Train Loss: 0.0804, Val Loss: 0.0770\n",
      "Epoch 62/300 - Train Loss: 0.0806, Val Loss: 0.0763\n",
      "Epoch 63/300 - Train Loss: 0.0767, Val Loss: 0.0770\n",
      "Epoch 64/300 - Train Loss: 0.0782, Val Loss: 0.0790\n",
      "Epoch 65/300 - Train Loss: 0.0763, Val Loss: 0.0765\n",
      "Epoch 66/300 - Train Loss: 0.0770, Val Loss: 0.0689\n",
      "Epoch 67/300 - Train Loss: 0.0760, Val Loss: 0.0736\n",
      "Epoch 68/300 - Train Loss: 0.0777, Val Loss: 0.0733\n",
      "Epoch 69/300 - Train Loss: 0.0754, Val Loss: 0.0752\n",
      "Epoch 70/300 - Train Loss: 0.0744, Val Loss: 0.0735\n",
      "Epoch 71/300 - Train Loss: 0.0744, Val Loss: 0.0821\n",
      "Epoch 72/300 - Train Loss: 0.0767, Val Loss: 0.0830\n",
      "Epoch 73/300 - Train Loss: 0.0767, Val Loss: 0.0711\n",
      "Epoch 74/300 - Train Loss: 0.0758, Val Loss: 0.0762\n",
      "Epoch 75/300 - Train Loss: 0.0758, Val Loss: 0.0748\n",
      "Epoch 76/300 - Train Loss: 0.0757, Val Loss: 0.0734\n",
      "Epoch 77/300 - Train Loss: 0.0747, Val Loss: 0.0766\n",
      "Epoch 78/300 - Train Loss: 0.0725, Val Loss: 0.0723\n",
      "Epoch 79/300 - Train Loss: 0.0720, Val Loss: 0.0756\n",
      "Epoch 80/300 - Train Loss: 0.0719, Val Loss: 0.0770\n",
      "Epoch 81/300 - Train Loss: 0.0723, Val Loss: 0.0777\n",
      "Epoch 82/300 - Train Loss: 0.0729, Val Loss: 0.0686\n",
      "Epoch 83/300 - Train Loss: 0.0716, Val Loss: 0.0722\n",
      "Epoch 84/300 - Train Loss: 0.0716, Val Loss: 0.0773\n",
      "Epoch 85/300 - Train Loss: 0.0731, Val Loss: 0.0715\n",
      "Epoch 86/300 - Train Loss: 0.0722, Val Loss: 0.0748\n",
      "Epoch 87/300 - Train Loss: 0.0726, Val Loss: 0.0750\n",
      "Epoch 88/300 - Train Loss: 0.0734, Val Loss: 0.0712\n",
      "Epoch 89/300 - Train Loss: 0.0704, Val Loss: 0.0734\n",
      "Epoch 90/300 - Train Loss: 0.0689, Val Loss: 0.0694\n",
      "Epoch 91/300 - Train Loss: 0.0713, Val Loss: 0.0816\n",
      "Epoch 92/300 - Train Loss: 0.0723, Val Loss: 0.0725\n",
      "Epoch 93/300 - Train Loss: 0.0717, Val Loss: 0.0744\n",
      "Epoch 94/300 - Train Loss: 0.0707, Val Loss: 0.0740\n",
      "Epoch 95/300 - Train Loss: 0.0695, Val Loss: 0.0726\n",
      "Epoch 96/300 - Train Loss: 0.0688, Val Loss: 0.0645\n",
      "Epoch 97/300 - Train Loss: 0.0674, Val Loss: 0.0684\n",
      "Epoch 98/300 - Train Loss: 0.0692, Val Loss: 0.0725\n",
      "Epoch 99/300 - Train Loss: 0.0688, Val Loss: 0.0888\n",
      "Epoch 100/300 - Train Loss: 0.0678, Val Loss: 0.0771\n",
      "Epoch 101/300 - Train Loss: 0.0690, Val Loss: 0.0741\n",
      "Epoch 102/300 - Train Loss: 0.0667, Val Loss: 0.0679\n",
      "Epoch 103/300 - Train Loss: 0.0647, Val Loss: 0.0676\n",
      "Epoch 104/300 - Train Loss: 0.0698, Val Loss: 0.0829\n",
      "Epoch 105/300 - Train Loss: 0.0679, Val Loss: 0.0753\n",
      "Epoch 106/300 - Train Loss: 0.0671, Val Loss: 0.0674\n",
      "Epoch 107/300 - Train Loss: 0.0654, Val Loss: 0.0730\n",
      "Epoch 108/300 - Train Loss: 0.0659, Val Loss: 0.0704\n",
      "Epoch 109/300 - Train Loss: 0.0659, Val Loss: 0.0724\n",
      "Epoch 110/300 - Train Loss: 0.0685, Val Loss: 0.0638\n",
      "Epoch 111/300 - Train Loss: 0.0669, Val Loss: 0.0658\n",
      "Epoch 112/300 - Train Loss: 0.0650, Val Loss: 0.0715\n",
      "Epoch 113/300 - Train Loss: 0.0679, Val Loss: 0.0686\n",
      "Epoch 114/300 - Train Loss: 0.0681, Val Loss: 0.0646\n",
      "Epoch 115/300 - Train Loss: 0.0638, Val Loss: 0.0791\n",
      "Epoch 116/300 - Train Loss: 0.0677, Val Loss: 0.0692\n",
      "Epoch 117/300 - Train Loss: 0.0643, Val Loss: 0.0755\n",
      "Epoch 118/300 - Train Loss: 0.0654, Val Loss: 0.0685\n",
      "Epoch 119/300 - Train Loss: 0.0654, Val Loss: 0.0678\n",
      "Epoch 120/300 - Train Loss: 0.0664, Val Loss: 0.0747\n",
      "Epoch 121/300 - Train Loss: 0.0632, Val Loss: 0.0707\n",
      "Epoch 122/300 - Train Loss: 0.0661, Val Loss: 0.0678\n",
      "Epoch 123/300 - Train Loss: 0.0640, Val Loss: 0.0676\n",
      "Epoch 124/300 - Train Loss: 0.0649, Val Loss: 0.0685\n",
      "Epoch 125/300 - Train Loss: 0.0638, Val Loss: 0.0690\n",
      "Epoch 126/300 - Train Loss: 0.0629, Val Loss: 0.0722\n",
      "Epoch 127/300 - Train Loss: 0.0637, Val Loss: 0.0639\n",
      "Epoch 128/300 - Train Loss: 0.0629, Val Loss: 0.0738\n",
      "Epoch 129/300 - Train Loss: 0.0641, Val Loss: 0.0740\n",
      "Epoch 130/300 - Train Loss: 0.0667, Val Loss: 0.0722\n",
      "Epoch 131/300 - Train Loss: 0.0623, Val Loss: 0.0738\n",
      "Epoch 132/300 - Train Loss: 0.0627, Val Loss: 0.0688\n",
      "Epoch 133/300 - Train Loss: 0.0616, Val Loss: 0.0714\n",
      "Epoch 134/300 - Train Loss: 0.0623, Val Loss: 0.0719\n",
      "Epoch 135/300 - Train Loss: 0.0614, Val Loss: 0.0778\n",
      "Epoch 136/300 - Train Loss: 0.0630, Val Loss: 0.0654\n",
      "Epoch 137/300 - Train Loss: 0.0622, Val Loss: 0.0658\n",
      "Epoch 138/300 - Train Loss: 0.0646, Val Loss: 0.0693\n",
      "Epoch 139/300 - Train Loss: 0.0646, Val Loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:34:12,709] Trial 357 finished with value: 0.9633068443212046 and parameters: {'F1': 4, 'F2': 32, 'D': 2, 'dropout': 0.1719939121062775, 'learning_rate': 0.0007849307364480885, 'batch_size': 32, 'weight_decay': 0.00020976774588900155}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 140/300 - Train Loss: 0.0670, Val Loss: 0.0752\n",
      "Early stopping at epoch 140\n",
      "Macro F1 Score: 0.9633, Macro Precision: 0.9664, Macro Recall: 0.9603\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.92      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 359\n",
      "Training with F1=4, F2=8, D=2, dropout=0.156375834841654, LR=0.0006684895895958724, BS=64, WD=0.00017969360311171097\n",
      "Epoch 1/300 - Train Loss: 0.2614, Val Loss: 0.1047\n",
      "Epoch 2/300 - Train Loss: 0.1159, Val Loss: 0.0889\n",
      "Epoch 3/300 - Train Loss: 0.1044, Val Loss: 0.0870\n",
      "Epoch 4/300 - Train Loss: 0.1001, Val Loss: 0.0819\n",
      "Epoch 5/300 - Train Loss: 0.1006, Val Loss: 0.0819\n",
      "Epoch 6/300 - Train Loss: 0.0981, Val Loss: 0.0801\n",
      "Epoch 7/300 - Train Loss: 0.0968, Val Loss: 0.0812\n",
      "Epoch 8/300 - Train Loss: 0.0975, Val Loss: 0.0779\n",
      "Epoch 9/300 - Train Loss: 0.0970, Val Loss: 0.0778\n",
      "Epoch 10/300 - Train Loss: 0.0936, Val Loss: 0.0841\n",
      "Epoch 11/300 - Train Loss: 0.0933, Val Loss: 0.0813\n",
      "Epoch 12/300 - Train Loss: 0.0906, Val Loss: 0.0755\n",
      "Epoch 13/300 - Train Loss: 0.0913, Val Loss: 0.0815\n",
      "Epoch 14/300 - Train Loss: 0.0922, Val Loss: 0.0784\n",
      "Epoch 15/300 - Train Loss: 0.0899, Val Loss: 0.0748\n",
      "Epoch 16/300 - Train Loss: 0.0891, Val Loss: 0.0832\n",
      "Epoch 17/300 - Train Loss: 0.0876, Val Loss: 0.0780\n",
      "Epoch 18/300 - Train Loss: 0.0903, Val Loss: 0.0778\n",
      "Epoch 19/300 - Train Loss: 0.0897, Val Loss: 0.0796\n",
      "Epoch 20/300 - Train Loss: 0.0880, Val Loss: 0.0806\n",
      "Epoch 21/300 - Train Loss: 0.0887, Val Loss: 0.0793\n",
      "Epoch 22/300 - Train Loss: 0.0867, Val Loss: 0.0762\n",
      "Epoch 23/300 - Train Loss: 0.0884, Val Loss: 0.0749\n",
      "Epoch 24/300 - Train Loss: 0.0870, Val Loss: 0.0774\n",
      "Epoch 25/300 - Train Loss: 0.0870, Val Loss: 0.0790\n",
      "Epoch 26/300 - Train Loss: 0.0881, Val Loss: 0.0775\n",
      "Epoch 27/300 - Train Loss: 0.0855, Val Loss: 0.0785\n",
      "Epoch 28/300 - Train Loss: 0.0874, Val Loss: 0.0821\n",
      "Epoch 29/300 - Train Loss: 0.0863, Val Loss: 0.0766\n",
      "Epoch 30/300 - Train Loss: 0.0851, Val Loss: 0.0758\n",
      "Epoch 31/300 - Train Loss: 0.0835, Val Loss: 0.0777\n",
      "Epoch 32/300 - Train Loss: 0.0854, Val Loss: 0.0756\n",
      "Epoch 33/300 - Train Loss: 0.0839, Val Loss: 0.0780\n",
      "Epoch 34/300 - Train Loss: 0.0856, Val Loss: 0.0757\n",
      "Epoch 35/300 - Train Loss: 0.0842, Val Loss: 0.0771\n",
      "Epoch 36/300 - Train Loss: 0.0827, Val Loss: 0.0779\n",
      "Epoch 37/300 - Train Loss: 0.0842, Val Loss: 0.0831\n",
      "Epoch 38/300 - Train Loss: 0.0839, Val Loss: 0.0780\n",
      "Epoch 39/300 - Train Loss: 0.0837, Val Loss: 0.0759\n",
      "Epoch 40/300 - Train Loss: 0.0836, Val Loss: 0.0761\n",
      "Epoch 41/300 - Train Loss: 0.0831, Val Loss: 0.0754\n",
      "Epoch 42/300 - Train Loss: 0.0849, Val Loss: 0.0763\n",
      "Epoch 43/300 - Train Loss: 0.0849, Val Loss: 0.0747\n",
      "Epoch 44/300 - Train Loss: 0.0835, Val Loss: 0.0741\n",
      "Epoch 45/300 - Train Loss: 0.0815, Val Loss: 0.0742\n",
      "Epoch 46/300 - Train Loss: 0.0813, Val Loss: 0.0798\n",
      "Epoch 47/300 - Train Loss: 0.0828, Val Loss: 0.0760\n",
      "Epoch 48/300 - Train Loss: 0.0816, Val Loss: 0.0782\n",
      "Epoch 49/300 - Train Loss: 0.0809, Val Loss: 0.0823\n",
      "Epoch 50/300 - Train Loss: 0.0811, Val Loss: 0.0815\n",
      "Epoch 51/300 - Train Loss: 0.0811, Val Loss: 0.0788\n",
      "Epoch 52/300 - Train Loss: 0.0822, Val Loss: 0.0790\n",
      "Epoch 53/300 - Train Loss: 0.0789, Val Loss: 0.0766\n",
      "Epoch 54/300 - Train Loss: 0.0820, Val Loss: 0.0783\n",
      "Epoch 55/300 - Train Loss: 0.0819, Val Loss: 0.0770\n",
      "Epoch 56/300 - Train Loss: 0.0828, Val Loss: 0.0781\n",
      "Epoch 57/300 - Train Loss: 0.0822, Val Loss: 0.0801\n",
      "Epoch 58/300 - Train Loss: 0.0816, Val Loss: 0.0762\n",
      "Epoch 59/300 - Train Loss: 0.0807, Val Loss: 0.0801\n",
      "Epoch 60/300 - Train Loss: 0.0802, Val Loss: 0.0731\n",
      "Epoch 61/300 - Train Loss: 0.0815, Val Loss: 0.0788\n",
      "Epoch 62/300 - Train Loss: 0.0811, Val Loss: 0.0853\n",
      "Epoch 63/300 - Train Loss: 0.0806, Val Loss: 0.0786\n",
      "Epoch 64/300 - Train Loss: 0.0796, Val Loss: 0.0776\n",
      "Epoch 65/300 - Train Loss: 0.0805, Val Loss: 0.0773\n",
      "Epoch 66/300 - Train Loss: 0.0797, Val Loss: 0.0800\n",
      "Epoch 67/300 - Train Loss: 0.0803, Val Loss: 0.0798\n",
      "Epoch 68/300 - Train Loss: 0.0816, Val Loss: 0.0744\n",
      "Epoch 69/300 - Train Loss: 0.0796, Val Loss: 0.0787\n",
      "Epoch 70/300 - Train Loss: 0.0787, Val Loss: 0.0780\n",
      "Epoch 71/300 - Train Loss: 0.0779, Val Loss: 0.0831\n",
      "Epoch 72/300 - Train Loss: 0.0793, Val Loss: 0.0756\n",
      "Epoch 73/300 - Train Loss: 0.0775, Val Loss: 0.0767\n",
      "Epoch 74/300 - Train Loss: 0.0797, Val Loss: 0.0795\n",
      "Epoch 75/300 - Train Loss: 0.0778, Val Loss: 0.0778\n",
      "Epoch 76/300 - Train Loss: 0.0778, Val Loss: 0.0747\n",
      "Epoch 77/300 - Train Loss: 0.0797, Val Loss: 0.0741\n",
      "Epoch 78/300 - Train Loss: 0.0793, Val Loss: 0.0804\n",
      "Epoch 79/300 - Train Loss: 0.0796, Val Loss: 0.0753\n",
      "Epoch 80/300 - Train Loss: 0.0799, Val Loss: 0.0767\n",
      "Epoch 81/300 - Train Loss: 0.0779, Val Loss: 0.0795\n",
      "Epoch 82/300 - Train Loss: 0.0791, Val Loss: 0.0775\n",
      "Epoch 83/300 - Train Loss: 0.0774, Val Loss: 0.0755\n",
      "Epoch 84/300 - Train Loss: 0.0789, Val Loss: 0.0762\n",
      "Epoch 85/300 - Train Loss: 0.0797, Val Loss: 0.0736\n",
      "Epoch 86/300 - Train Loss: 0.0773, Val Loss: 0.0738\n",
      "Epoch 87/300 - Train Loss: 0.0768, Val Loss: 0.0753\n",
      "Epoch 88/300 - Train Loss: 0.0789, Val Loss: 0.0769\n",
      "Epoch 89/300 - Train Loss: 0.0776, Val Loss: 0.0728\n",
      "Epoch 90/300 - Train Loss: 0.0761, Val Loss: 0.0789\n",
      "Epoch 91/300 - Train Loss: 0.0771, Val Loss: 0.0796\n",
      "Epoch 92/300 - Train Loss: 0.0781, Val Loss: 0.0771\n",
      "Epoch 93/300 - Train Loss: 0.0785, Val Loss: 0.0736\n",
      "Epoch 94/300 - Train Loss: 0.0793, Val Loss: 0.0780\n",
      "Epoch 95/300 - Train Loss: 0.0785, Val Loss: 0.0770\n",
      "Epoch 96/300 - Train Loss: 0.0766, Val Loss: 0.0804\n",
      "Epoch 97/300 - Train Loss: 0.0774, Val Loss: 0.0742\n",
      "Epoch 98/300 - Train Loss: 0.0775, Val Loss: 0.0791\n",
      "Epoch 99/300 - Train Loss: 0.0775, Val Loss: 0.0716\n",
      "Epoch 100/300 - Train Loss: 0.0768, Val Loss: 0.0829\n",
      "Epoch 101/300 - Train Loss: 0.0766, Val Loss: 0.0750\n",
      "Epoch 102/300 - Train Loss: 0.0777, Val Loss: 0.0742\n",
      "Epoch 103/300 - Train Loss: 0.0776, Val Loss: 0.0747\n",
      "Epoch 104/300 - Train Loss: 0.0769, Val Loss: 0.0783\n",
      "Epoch 105/300 - Train Loss: 0.0763, Val Loss: 0.0731\n",
      "Epoch 106/300 - Train Loss: 0.0758, Val Loss: 0.0770\n",
      "Epoch 107/300 - Train Loss: 0.0766, Val Loss: 0.0770\n",
      "Epoch 108/300 - Train Loss: 0.0786, Val Loss: 0.0786\n",
      "Epoch 109/300 - Train Loss: 0.0763, Val Loss: 0.0769\n",
      "Epoch 110/300 - Train Loss: 0.0768, Val Loss: 0.0735\n",
      "Epoch 111/300 - Train Loss: 0.0758, Val Loss: 0.0706\n",
      "Epoch 112/300 - Train Loss: 0.0762, Val Loss: 0.0764\n",
      "Epoch 113/300 - Train Loss: 0.0745, Val Loss: 0.0734\n",
      "Epoch 114/300 - Train Loss: 0.0777, Val Loss: 0.0744\n",
      "Epoch 115/300 - Train Loss: 0.0753, Val Loss: 0.0755\n",
      "Epoch 116/300 - Train Loss: 0.0754, Val Loss: 0.0772\n",
      "Epoch 117/300 - Train Loss: 0.0752, Val Loss: 0.0747\n",
      "Epoch 118/300 - Train Loss: 0.0749, Val Loss: 0.0726\n",
      "Epoch 119/300 - Train Loss: 0.0751, Val Loss: 0.0747\n",
      "Epoch 120/300 - Train Loss: 0.0760, Val Loss: 0.0768\n",
      "Epoch 121/300 - Train Loss: 0.0757, Val Loss: 0.0768\n",
      "Epoch 122/300 - Train Loss: 0.0756, Val Loss: 0.0727\n",
      "Epoch 123/300 - Train Loss: 0.0771, Val Loss: 0.0744\n",
      "Epoch 124/300 - Train Loss: 0.0773, Val Loss: 0.0740\n",
      "Epoch 125/300 - Train Loss: 0.0741, Val Loss: 0.0731\n",
      "Epoch 126/300 - Train Loss: 0.0779, Val Loss: 0.0778\n",
      "Epoch 127/300 - Train Loss: 0.0752, Val Loss: 0.0756\n",
      "Epoch 128/300 - Train Loss: 0.0747, Val Loss: 0.0757\n",
      "Epoch 129/300 - Train Loss: 0.0753, Val Loss: 0.0733\n",
      "Epoch 130/300 - Train Loss: 0.0756, Val Loss: 0.0828\n",
      "Epoch 131/300 - Train Loss: 0.0745, Val Loss: 0.0749\n",
      "Epoch 132/300 - Train Loss: 0.0753, Val Loss: 0.0728\n",
      "Epoch 133/300 - Train Loss: 0.0749, Val Loss: 0.0780\n",
      "Epoch 134/300 - Train Loss: 0.0756, Val Loss: 0.0780\n",
      "Epoch 135/300 - Train Loss: 0.0753, Val Loss: 0.0783\n",
      "Epoch 136/300 - Train Loss: 0.0769, Val Loss: 0.0718\n",
      "Epoch 137/300 - Train Loss: 0.0753, Val Loss: 0.0718\n",
      "Epoch 138/300 - Train Loss: 0.0756, Val Loss: 0.0778\n",
      "Epoch 139/300 - Train Loss: 0.0753, Val Loss: 0.0716\n",
      "Epoch 140/300 - Train Loss: 0.0751, Val Loss: 0.0779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:37:04,200] Trial 358 finished with value: 0.9726589748045574 and parameters: {'F1': 4, 'F2': 8, 'D': 2, 'dropout': 0.156375834841654, 'learning_rate': 0.0006684895895958724, 'batch_size': 64, 'weight_decay': 0.00017969360311171097}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/300 - Train Loss: 0.0757, Val Loss: 0.0752\n",
      "Early stopping at epoch 141\n",
      "Macro F1 Score: 0.9727, Macro Precision: 0.9690, Macro Recall: 0.9766\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 360\n",
      "Training with F1=32, F2=8, D=2, dropout=0.18020300226236255, LR=0.0007436816930317042, BS=32, WD=0.00019465452454919563\n",
      "Epoch 1/300 - Train Loss: 0.1731, Val Loss: 0.0774\n",
      "Epoch 2/300 - Train Loss: 0.1005, Val Loss: 0.0756\n",
      "Epoch 3/300 - Train Loss: 0.0907, Val Loss: 0.1043\n",
      "Epoch 4/300 - Train Loss: 0.0914, Val Loss: 0.0751\n",
      "Epoch 5/300 - Train Loss: 0.0877, Val Loss: 0.1046\n",
      "Epoch 6/300 - Train Loss: 0.0851, Val Loss: 0.0729\n",
      "Epoch 7/300 - Train Loss: 0.0865, Val Loss: 0.0709\n",
      "Epoch 8/300 - Train Loss: 0.0856, Val Loss: 0.0676\n",
      "Epoch 9/300 - Train Loss: 0.0838, Val Loss: 0.0749\n",
      "Epoch 10/300 - Train Loss: 0.0839, Val Loss: 0.0733\n",
      "Epoch 11/300 - Train Loss: 0.0825, Val Loss: 0.0828\n",
      "Epoch 12/300 - Train Loss: 0.0807, Val Loss: 0.0773\n",
      "Epoch 13/300 - Train Loss: 0.0812, Val Loss: 0.0851\n",
      "Epoch 14/300 - Train Loss: 0.0803, Val Loss: 0.0656\n",
      "Epoch 15/300 - Train Loss: 0.0852, Val Loss: 0.0771\n",
      "Epoch 16/300 - Train Loss: 0.0815, Val Loss: 0.0665\n",
      "Epoch 17/300 - Train Loss: 0.0801, Val Loss: 0.0837\n",
      "Epoch 18/300 - Train Loss: 0.0778, Val Loss: 0.0731\n",
      "Epoch 19/300 - Train Loss: 0.0780, Val Loss: 0.0755\n",
      "Epoch 20/300 - Train Loss: 0.0779, Val Loss: 0.0688\n",
      "Epoch 21/300 - Train Loss: 0.0795, Val Loss: 0.0776\n",
      "Epoch 22/300 - Train Loss: 0.0775, Val Loss: 0.0680\n",
      "Epoch 23/300 - Train Loss: 0.0775, Val Loss: 0.0719\n",
      "Epoch 24/300 - Train Loss: 0.0772, Val Loss: 0.0682\n",
      "Epoch 25/300 - Train Loss: 0.0750, Val Loss: 0.0681\n",
      "Epoch 26/300 - Train Loss: 0.0779, Val Loss: 0.0704\n",
      "Epoch 27/300 - Train Loss: 0.0761, Val Loss: 0.0662\n",
      "Epoch 28/300 - Train Loss: 0.0772, Val Loss: 0.0692\n",
      "Epoch 29/300 - Train Loss: 0.0767, Val Loss: 0.0725\n",
      "Epoch 30/300 - Train Loss: 0.0774, Val Loss: 0.0692\n",
      "Epoch 31/300 - Train Loss: 0.0756, Val Loss: 0.0675\n",
      "Epoch 32/300 - Train Loss: 0.0788, Val Loss: 0.0739\n",
      "Epoch 33/300 - Train Loss: 0.0750, Val Loss: 0.0716\n",
      "Epoch 34/300 - Train Loss: 0.0776, Val Loss: 0.0739\n",
      "Epoch 35/300 - Train Loss: 0.0774, Val Loss: 0.0810\n",
      "Epoch 36/300 - Train Loss: 0.0788, Val Loss: 0.0684\n",
      "Epoch 37/300 - Train Loss: 0.0737, Val Loss: 0.0766\n",
      "Epoch 38/300 - Train Loss: 0.0767, Val Loss: 0.0758\n",
      "Epoch 39/300 - Train Loss: 0.0725, Val Loss: 0.0700\n",
      "Epoch 40/300 - Train Loss: 0.0742, Val Loss: 0.0710\n",
      "Epoch 41/300 - Train Loss: 0.0742, Val Loss: 0.0685\n",
      "Epoch 42/300 - Train Loss: 0.0731, Val Loss: 0.0683\n",
      "Epoch 43/300 - Train Loss: 0.0757, Val Loss: 0.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:38:54,683] Trial 359 finished with value: 0.9730785258849015 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.18020300226236255, 'learning_rate': 0.0007436816930317042, 'batch_size': 32, 'weight_decay': 0.00019465452454919563}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300 - Train Loss: 0.0741, Val Loss: 0.0731\n",
      "Early stopping at epoch 44\n",
      "Macro F1 Score: 0.9731, Macro Precision: 0.9695, Macro Recall: 0.9770\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 361\n",
      "Training with F1=32, F2=8, D=2, dropout=0.15088179073077893, LR=0.0008437594474682261, BS=32, WD=0.0002971538535370179\n",
      "Epoch 1/300 - Train Loss: 0.1591, Val Loss: 0.0945\n",
      "Epoch 2/300 - Train Loss: 0.0969, Val Loss: 0.0742\n",
      "Epoch 3/300 - Train Loss: 0.0935, Val Loss: 0.0696\n",
      "Epoch 4/300 - Train Loss: 0.0910, Val Loss: 0.0732\n",
      "Epoch 5/300 - Train Loss: 0.0915, Val Loss: 0.0735\n",
      "Epoch 6/300 - Train Loss: 0.0876, Val Loss: 0.0652\n",
      "Epoch 7/300 - Train Loss: 0.0870, Val Loss: 0.0708\n",
      "Epoch 8/300 - Train Loss: 0.0863, Val Loss: 0.0901\n",
      "Epoch 9/300 - Train Loss: 0.0854, Val Loss: 0.0697\n",
      "Epoch 10/300 - Train Loss: 0.0854, Val Loss: 0.0761\n",
      "Epoch 11/300 - Train Loss: 0.0841, Val Loss: 0.0761\n",
      "Epoch 12/300 - Train Loss: 0.0843, Val Loss: 0.0789\n",
      "Epoch 13/300 - Train Loss: 0.0839, Val Loss: 0.0704\n",
      "Epoch 14/300 - Train Loss: 0.0827, Val Loss: 0.0795\n",
      "Epoch 15/300 - Train Loss: 0.0820, Val Loss: 0.0676\n",
      "Epoch 16/300 - Train Loss: 0.0806, Val Loss: 0.0701\n",
      "Epoch 17/300 - Train Loss: 0.0799, Val Loss: 0.0755\n",
      "Epoch 18/300 - Train Loss: 0.0813, Val Loss: 0.0733\n",
      "Epoch 19/300 - Train Loss: 0.0799, Val Loss: 0.0679\n",
      "Epoch 20/300 - Train Loss: 0.0805, Val Loss: 0.0691\n",
      "Epoch 21/300 - Train Loss: 0.0788, Val Loss: 0.0763\n",
      "Epoch 22/300 - Train Loss: 0.0800, Val Loss: 0.0719\n",
      "Epoch 23/300 - Train Loss: 0.0799, Val Loss: 0.0677\n",
      "Epoch 24/300 - Train Loss: 0.0795, Val Loss: 0.0808\n",
      "Epoch 25/300 - Train Loss: 0.0797, Val Loss: 0.0674\n",
      "Epoch 26/300 - Train Loss: 0.0790, Val Loss: 0.0700\n",
      "Epoch 27/300 - Train Loss: 0.0827, Val Loss: 0.0703\n",
      "Epoch 28/300 - Train Loss: 0.0798, Val Loss: 0.0738\n",
      "Epoch 29/300 - Train Loss: 0.0776, Val Loss: 0.0698\n",
      "Epoch 30/300 - Train Loss: 0.0770, Val Loss: 0.0910\n",
      "Epoch 31/300 - Train Loss: 0.0792, Val Loss: 0.0667\n",
      "Epoch 32/300 - Train Loss: 0.0754, Val Loss: 0.0756\n",
      "Epoch 33/300 - Train Loss: 0.0778, Val Loss: 0.0700\n",
      "Epoch 34/300 - Train Loss: 0.0783, Val Loss: 0.0708\n",
      "Epoch 35/300 - Train Loss: 0.0798, Val Loss: 0.0880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:40:24,847] Trial 360 finished with value: 0.9742994987620173 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.15088179073077893, 'learning_rate': 0.0008437594474682261, 'batch_size': 32, 'weight_decay': 0.0002971538535370179}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300 - Train Loss: 0.0792, Val Loss: 0.0720\n",
      "Early stopping at epoch 36\n",
      "Macro F1 Score: 0.9743, Macro Precision: 0.9754, Macro Recall: 0.9733\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       1.00      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 362\n",
      "Training with F1=32, F2=8, D=2, dropout=0.15150992731238058, LR=0.0008572428385285999, BS=32, WD=0.00024369410597969396\n",
      "Epoch 1/300 - Train Loss: 0.1508, Val Loss: 0.0828\n",
      "Epoch 2/300 - Train Loss: 0.0956, Val Loss: 0.0733\n",
      "Epoch 3/300 - Train Loss: 0.0937, Val Loss: 0.0776\n",
      "Epoch 4/300 - Train Loss: 0.0901, Val Loss: 0.0799\n",
      "Epoch 5/300 - Train Loss: 0.0866, Val Loss: 0.0864\n",
      "Epoch 6/300 - Train Loss: 0.0879, Val Loss: 0.0740\n",
      "Epoch 7/300 - Train Loss: 0.0833, Val Loss: 0.0730\n",
      "Epoch 8/300 - Train Loss: 0.0844, Val Loss: 0.0682\n",
      "Epoch 9/300 - Train Loss: 0.0861, Val Loss: 0.0832\n",
      "Epoch 10/300 - Train Loss: 0.0819, Val Loss: 0.0771\n",
      "Epoch 11/300 - Train Loss: 0.0871, Val Loss: 0.0799\n",
      "Epoch 12/300 - Train Loss: 0.0823, Val Loss: 0.0671\n",
      "Epoch 13/300 - Train Loss: 0.0830, Val Loss: 0.0706\n",
      "Epoch 14/300 - Train Loss: 0.0826, Val Loss: 0.0708\n",
      "Epoch 15/300 - Train Loss: 0.0839, Val Loss: 0.0799\n",
      "Epoch 16/300 - Train Loss: 0.0824, Val Loss: 0.0695\n",
      "Epoch 17/300 - Train Loss: 0.0828, Val Loss: 0.0688\n",
      "Epoch 18/300 - Train Loss: 0.0802, Val Loss: 0.0694\n",
      "Epoch 19/300 - Train Loss: 0.0829, Val Loss: 0.0704\n",
      "Epoch 20/300 - Train Loss: 0.0798, Val Loss: 0.0782\n",
      "Epoch 21/300 - Train Loss: 0.0783, Val Loss: 0.0699\n",
      "Epoch 22/300 - Train Loss: 0.0796, Val Loss: 0.0716\n",
      "Epoch 23/300 - Train Loss: 0.0808, Val Loss: 0.0726\n",
      "Epoch 24/300 - Train Loss: 0.0781, Val Loss: 0.0791\n",
      "Epoch 25/300 - Train Loss: 0.0802, Val Loss: 0.0704\n",
      "Epoch 26/300 - Train Loss: 0.0798, Val Loss: 0.0722\n",
      "Epoch 27/300 - Train Loss: 0.0796, Val Loss: 0.0732\n",
      "Epoch 28/300 - Train Loss: 0.0774, Val Loss: 0.0664\n",
      "Epoch 29/300 - Train Loss: 0.0789, Val Loss: 0.0782\n",
      "Epoch 30/300 - Train Loss: 0.0779, Val Loss: 0.0688\n",
      "Epoch 31/300 - Train Loss: 0.0769, Val Loss: 0.0738\n",
      "Epoch 32/300 - Train Loss: 0.0782, Val Loss: 0.0730\n",
      "Epoch 33/300 - Train Loss: 0.0777, Val Loss: 0.0716\n",
      "Epoch 34/300 - Train Loss: 0.0765, Val Loss: 0.0736\n",
      "Epoch 35/300 - Train Loss: 0.0768, Val Loss: 0.0700\n",
      "Epoch 36/300 - Train Loss: 0.0795, Val Loss: 0.0699\n",
      "Epoch 37/300 - Train Loss: 0.0776, Val Loss: 0.0728\n",
      "Epoch 38/300 - Train Loss: 0.0758, Val Loss: 0.0685\n",
      "Epoch 39/300 - Train Loss: 0.0776, Val Loss: 0.0703\n",
      "Epoch 40/300 - Train Loss: 0.0752, Val Loss: 0.0671\n",
      "Epoch 41/300 - Train Loss: 0.0761, Val Loss: 0.0740\n",
      "Epoch 42/300 - Train Loss: 0.0731, Val Loss: 0.0671\n",
      "Epoch 43/300 - Train Loss: 0.0744, Val Loss: 0.0669\n",
      "Epoch 44/300 - Train Loss: 0.0807, Val Loss: 0.0679\n",
      "Epoch 45/300 - Train Loss: 0.0766, Val Loss: 0.0745\n",
      "Epoch 46/300 - Train Loss: 0.0781, Val Loss: 0.0745\n",
      "Epoch 47/300 - Train Loss: 0.0755, Val Loss: 0.0675\n",
      "Epoch 48/300 - Train Loss: 0.0773, Val Loss: 0.0672\n",
      "Epoch 49/300 - Train Loss: 0.0748, Val Loss: 0.0686\n",
      "Epoch 50/300 - Train Loss: 0.0761, Val Loss: 0.0675\n",
      "Epoch 51/300 - Train Loss: 0.0716, Val Loss: 0.0774\n",
      "Epoch 52/300 - Train Loss: 0.0752, Val Loss: 0.0692\n",
      "Epoch 53/300 - Train Loss: 0.0735, Val Loss: 0.0703\n",
      "Epoch 54/300 - Train Loss: 0.0746, Val Loss: 0.0671\n",
      "Epoch 55/300 - Train Loss: 0.0774, Val Loss: 0.0748\n",
      "Epoch 56/300 - Train Loss: 0.0723, Val Loss: 0.0664\n",
      "Epoch 57/300 - Train Loss: 0.0718, Val Loss: 0.0823\n",
      "Epoch 58/300 - Train Loss: 0.0721, Val Loss: 0.0709\n",
      "Epoch 59/300 - Train Loss: 0.0727, Val Loss: 0.0728\n",
      "Epoch 60/300 - Train Loss: 0.0750, Val Loss: 0.0707\n",
      "Epoch 61/300 - Train Loss: 0.0729, Val Loss: 0.0700\n",
      "Epoch 62/300 - Train Loss: 0.0740, Val Loss: 0.0696\n",
      "Epoch 63/300 - Train Loss: 0.0720, Val Loss: 0.0703\n",
      "Epoch 64/300 - Train Loss: 0.0724, Val Loss: 0.0666\n",
      "Epoch 65/300 - Train Loss: 0.0712, Val Loss: 0.0760\n",
      "Epoch 66/300 - Train Loss: 0.0707, Val Loss: 0.0693\n",
      "Epoch 67/300 - Train Loss: 0.0722, Val Loss: 0.0661\n",
      "Epoch 68/300 - Train Loss: 0.0738, Val Loss: 0.0666\n",
      "Epoch 69/300 - Train Loss: 0.0727, Val Loss: 0.0736\n",
      "Epoch 70/300 - Train Loss: 0.0708, Val Loss: 0.0757\n",
      "Epoch 71/300 - Train Loss: 0.0745, Val Loss: 0.0674\n",
      "Epoch 72/300 - Train Loss: 0.0729, Val Loss: 0.0682\n",
      "Epoch 73/300 - Train Loss: 0.0719, Val Loss: 0.0730\n",
      "Epoch 74/300 - Train Loss: 0.0721, Val Loss: 0.0786\n",
      "Epoch 75/300 - Train Loss: 0.0734, Val Loss: 0.0737\n",
      "Epoch 76/300 - Train Loss: 0.0716, Val Loss: 0.0787\n",
      "Epoch 77/300 - Train Loss: 0.0741, Val Loss: 0.0687\n",
      "Epoch 78/300 - Train Loss: 0.0712, Val Loss: 0.0686\n",
      "Epoch 79/300 - Train Loss: 0.0727, Val Loss: 0.0695\n",
      "Epoch 80/300 - Train Loss: 0.0738, Val Loss: 0.0724\n",
      "Epoch 81/300 - Train Loss: 0.0706, Val Loss: 0.0759\n",
      "Epoch 82/300 - Train Loss: 0.0697, Val Loss: 0.0815\n",
      "Epoch 83/300 - Train Loss: 0.0727, Val Loss: 0.0715\n",
      "Epoch 84/300 - Train Loss: 0.0713, Val Loss: 0.0724\n",
      "Epoch 85/300 - Train Loss: 0.0704, Val Loss: 0.0737\n",
      "Epoch 86/300 - Train Loss: 0.0715, Val Loss: 0.0666\n",
      "Epoch 87/300 - Train Loss: 0.0721, Val Loss: 0.0724\n",
      "Epoch 88/300 - Train Loss: 0.0732, Val Loss: 0.0669\n",
      "Epoch 89/300 - Train Loss: 0.0732, Val Loss: 0.0742\n",
      "Epoch 90/300 - Train Loss: 0.0716, Val Loss: 0.0692\n",
      "Epoch 91/300 - Train Loss: 0.0712, Val Loss: 0.0772\n",
      "Epoch 92/300 - Train Loss: 0.0698, Val Loss: 0.0746\n",
      "Epoch 93/300 - Train Loss: 0.0701, Val Loss: 0.0688\n",
      "Epoch 94/300 - Train Loss: 0.0706, Val Loss: 0.0705\n",
      "Epoch 95/300 - Train Loss: 0.0699, Val Loss: 0.0757\n",
      "Epoch 96/300 - Train Loss: 0.0685, Val Loss: 0.0668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:44:27,558] Trial 361 finished with value: 0.9679318131224308 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.15150992731238058, 'learning_rate': 0.0008572428385285999, 'batch_size': 32, 'weight_decay': 0.00024369410597969396}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97/300 - Train Loss: 0.0710, Val Loss: 0.0771\n",
      "Early stopping at epoch 97\n",
      "Macro F1 Score: 0.9679, Macro Precision: 0.9622, Macro Recall: 0.9742\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 363\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1670060875549753, LR=0.0009536044809274749, BS=128, WD=0.0002814175070597885\n",
      "Epoch 1/300 - Train Loss: 0.2071, Val Loss: 0.0931\n",
      "Epoch 2/300 - Train Loss: 0.0959, Val Loss: 0.0854\n",
      "Epoch 3/300 - Train Loss: 0.0883, Val Loss: 0.0768\n",
      "Epoch 4/300 - Train Loss: 0.0847, Val Loss: 0.0782\n",
      "Epoch 5/300 - Train Loss: 0.0824, Val Loss: 0.0740\n",
      "Epoch 6/300 - Train Loss: 0.0803, Val Loss: 0.0928\n",
      "Epoch 7/300 - Train Loss: 0.0792, Val Loss: 0.0793\n",
      "Epoch 8/300 - Train Loss: 0.0778, Val Loss: 0.0750\n",
      "Epoch 9/300 - Train Loss: 0.0772, Val Loss: 0.0747\n",
      "Epoch 10/300 - Train Loss: 0.0756, Val Loss: 0.0761\n",
      "Epoch 11/300 - Train Loss: 0.0767, Val Loss: 0.0809\n",
      "Epoch 12/300 - Train Loss: 0.0765, Val Loss: 0.0730\n",
      "Epoch 13/300 - Train Loss: 0.0746, Val Loss: 0.0723\n",
      "Epoch 14/300 - Train Loss: 0.0734, Val Loss: 0.0725\n",
      "Epoch 15/300 - Train Loss: 0.0737, Val Loss: 0.0784\n",
      "Epoch 16/300 - Train Loss: 0.0750, Val Loss: 0.0743\n",
      "Epoch 17/300 - Train Loss: 0.0727, Val Loss: 0.0775\n",
      "Epoch 18/300 - Train Loss: 0.0750, Val Loss: 0.0747\n",
      "Epoch 19/300 - Train Loss: 0.0728, Val Loss: 0.0732\n",
      "Epoch 20/300 - Train Loss: 0.0740, Val Loss: 0.0789\n",
      "Epoch 21/300 - Train Loss: 0.0719, Val Loss: 0.0731\n",
      "Epoch 22/300 - Train Loss: 0.0729, Val Loss: 0.0779\n",
      "Epoch 23/300 - Train Loss: 0.0717, Val Loss: 0.0737\n",
      "Epoch 24/300 - Train Loss: 0.0704, Val Loss: 0.0748\n",
      "Epoch 25/300 - Train Loss: 0.0715, Val Loss: 0.0752\n",
      "Epoch 26/300 - Train Loss: 0.0709, Val Loss: 0.0781\n",
      "Epoch 27/300 - Train Loss: 0.0716, Val Loss: 0.0758\n",
      "Epoch 28/300 - Train Loss: 0.0695, Val Loss: 0.0730\n",
      "Epoch 29/300 - Train Loss: 0.0700, Val Loss: 0.0727\n",
      "Epoch 30/300 - Train Loss: 0.0701, Val Loss: 0.0772\n",
      "Epoch 31/300 - Train Loss: 0.0711, Val Loss: 0.0769\n",
      "Epoch 32/300 - Train Loss: 0.0701, Val Loss: 0.0795\n",
      "Epoch 33/300 - Train Loss: 0.0695, Val Loss: 0.0772\n",
      "Epoch 34/300 - Train Loss: 0.0698, Val Loss: 0.0731\n",
      "Epoch 35/300 - Train Loss: 0.0686, Val Loss: 0.0735\n",
      "Epoch 36/300 - Train Loss: 0.0697, Val Loss: 0.0748\n",
      "Epoch 37/300 - Train Loss: 0.0675, Val Loss: 0.0751\n",
      "Epoch 38/300 - Train Loss: 0.0707, Val Loss: 0.0721\n",
      "Epoch 39/300 - Train Loss: 0.0679, Val Loss: 0.0895\n",
      "Epoch 40/300 - Train Loss: 0.0697, Val Loss: 0.0756\n",
      "Epoch 41/300 - Train Loss: 0.0678, Val Loss: 0.0772\n",
      "Epoch 42/300 - Train Loss: 0.0682, Val Loss: 0.0725\n",
      "Epoch 43/300 - Train Loss: 0.0690, Val Loss: 0.0794\n",
      "Epoch 44/300 - Train Loss: 0.0678, Val Loss: 0.0726\n",
      "Epoch 45/300 - Train Loss: 0.0683, Val Loss: 0.0752\n",
      "Epoch 46/300 - Train Loss: 0.0689, Val Loss: 0.0730\n",
      "Epoch 47/300 - Train Loss: 0.0683, Val Loss: 0.0738\n",
      "Epoch 48/300 - Train Loss: 0.0670, Val Loss: 0.0735\n",
      "Epoch 49/300 - Train Loss: 0.0691, Val Loss: 0.0715\n",
      "Epoch 50/300 - Train Loss: 0.0664, Val Loss: 0.0740\n",
      "Epoch 51/300 - Train Loss: 0.0678, Val Loss: 0.0754\n",
      "Epoch 52/300 - Train Loss: 0.0669, Val Loss: 0.0719\n",
      "Epoch 53/300 - Train Loss: 0.0673, Val Loss: 0.0783\n",
      "Epoch 54/300 - Train Loss: 0.0685, Val Loss: 0.0772\n",
      "Epoch 55/300 - Train Loss: 0.0673, Val Loss: 0.0763\n",
      "Epoch 56/300 - Train Loss: 0.0677, Val Loss: 0.0755\n",
      "Epoch 57/300 - Train Loss: 0.0663, Val Loss: 0.0784\n",
      "Epoch 58/300 - Train Loss: 0.0663, Val Loss: 0.0775\n",
      "Epoch 59/300 - Train Loss: 0.0657, Val Loss: 0.0830\n",
      "Epoch 60/300 - Train Loss: 0.0673, Val Loss: 0.0779\n",
      "Epoch 61/300 - Train Loss: 0.0657, Val Loss: 0.0736\n",
      "Epoch 62/300 - Train Loss: 0.0660, Val Loss: 0.0829\n",
      "Epoch 63/300 - Train Loss: 0.0673, Val Loss: 0.0730\n",
      "Epoch 64/300 - Train Loss: 0.0656, Val Loss: 0.0740\n",
      "Epoch 65/300 - Train Loss: 0.0655, Val Loss: 0.0782\n",
      "Epoch 66/300 - Train Loss: 0.0667, Val Loss: 0.0767\n",
      "Epoch 67/300 - Train Loss: 0.0653, Val Loss: 0.0772\n",
      "Epoch 68/300 - Train Loss: 0.0649, Val Loss: 0.0828\n",
      "Epoch 69/300 - Train Loss: 0.0647, Val Loss: 0.0720\n",
      "Epoch 70/300 - Train Loss: 0.0662, Val Loss: 0.0784\n",
      "Epoch 71/300 - Train Loss: 0.0649, Val Loss: 0.0768\n",
      "Epoch 72/300 - Train Loss: 0.0654, Val Loss: 0.0814\n",
      "Epoch 73/300 - Train Loss: 0.0648, Val Loss: 0.0731\n",
      "Epoch 74/300 - Train Loss: 0.0652, Val Loss: 0.0819\n",
      "Epoch 75/300 - Train Loss: 0.0651, Val Loss: 0.0752\n",
      "Epoch 76/300 - Train Loss: 0.0643, Val Loss: 0.0808\n",
      "Epoch 77/300 - Train Loss: 0.0653, Val Loss: 0.0816\n",
      "Epoch 78/300 - Train Loss: 0.0661, Val Loss: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:47:08,683] Trial 362 finished with value: 0.9667867795506199 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1670060875549753, 'learning_rate': 0.0009536044809274749, 'batch_size': 128, 'weight_decay': 0.0002814175070597885}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/300 - Train Loss: 0.0641, Val Loss: 0.0735\n",
      "Early stopping at epoch 79\n",
      "Macro F1 Score: 0.9668, Macro Precision: 0.9633, Macro Recall: 0.9705\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 364\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1511465939607605, LR=0.0005647592021313578, BS=32, WD=0.0003084441601820701\n",
      "Epoch 1/300 - Train Loss: 0.1732, Val Loss: 0.0822\n",
      "Epoch 2/300 - Train Loss: 0.1023, Val Loss: 0.0701\n",
      "Epoch 3/300 - Train Loss: 0.0942, Val Loss: 0.0923\n",
      "Epoch 4/300 - Train Loss: 0.0929, Val Loss: 0.0885\n",
      "Epoch 5/300 - Train Loss: 0.0894, Val Loss: 0.0888\n",
      "Epoch 6/300 - Train Loss: 0.0891, Val Loss: 0.0847\n",
      "Epoch 7/300 - Train Loss: 0.0858, Val Loss: 0.0734\n",
      "Epoch 8/300 - Train Loss: 0.0861, Val Loss: 0.0714\n",
      "Epoch 9/300 - Train Loss: 0.0860, Val Loss: 0.0750\n",
      "Epoch 10/300 - Train Loss: 0.0844, Val Loss: 0.0774\n",
      "Epoch 11/300 - Train Loss: 0.0799, Val Loss: 0.0697\n",
      "Epoch 12/300 - Train Loss: 0.0824, Val Loss: 0.0744\n",
      "Epoch 13/300 - Train Loss: 0.0805, Val Loss: 0.0722\n",
      "Epoch 14/300 - Train Loss: 0.0830, Val Loss: 0.0704\n",
      "Epoch 15/300 - Train Loss: 0.0796, Val Loss: 0.0734\n",
      "Epoch 16/300 - Train Loss: 0.0795, Val Loss: 0.0743\n",
      "Epoch 17/300 - Train Loss: 0.0796, Val Loss: 0.0748\n",
      "Epoch 18/300 - Train Loss: 0.0819, Val Loss: 0.0673\n",
      "Epoch 19/300 - Train Loss: 0.0805, Val Loss: 0.0720\n",
      "Epoch 20/300 - Train Loss: 0.0790, Val Loss: 0.0718\n",
      "Epoch 21/300 - Train Loss: 0.0790, Val Loss: 0.0783\n",
      "Epoch 22/300 - Train Loss: 0.0788, Val Loss: 0.0693\n",
      "Epoch 23/300 - Train Loss: 0.0790, Val Loss: 0.0758\n",
      "Epoch 24/300 - Train Loss: 0.0792, Val Loss: 0.0719\n",
      "Epoch 25/300 - Train Loss: 0.0768, Val Loss: 0.0681\n",
      "Epoch 26/300 - Train Loss: 0.0791, Val Loss: 0.0725\n",
      "Epoch 27/300 - Train Loss: 0.0777, Val Loss: 0.0750\n",
      "Epoch 28/300 - Train Loss: 0.0798, Val Loss: 0.0726\n",
      "Epoch 29/300 - Train Loss: 0.0769, Val Loss: 0.0735\n",
      "Epoch 30/300 - Train Loss: 0.0756, Val Loss: 0.0768\n",
      "Epoch 31/300 - Train Loss: 0.0767, Val Loss: 0.0729\n",
      "Epoch 32/300 - Train Loss: 0.0794, Val Loss: 0.0702\n",
      "Epoch 33/300 - Train Loss: 0.0783, Val Loss: 0.0657\n",
      "Epoch 34/300 - Train Loss: 0.0779, Val Loss: 0.0674\n",
      "Epoch 35/300 - Train Loss: 0.0790, Val Loss: 0.0693\n",
      "Epoch 36/300 - Train Loss: 0.0763, Val Loss: 0.0719\n",
      "Epoch 37/300 - Train Loss: 0.0772, Val Loss: 0.0698\n",
      "Epoch 38/300 - Train Loss: 0.0769, Val Loss: 0.0711\n",
      "Epoch 39/300 - Train Loss: 0.0750, Val Loss: 0.0722\n",
      "Epoch 40/300 - Train Loss: 0.0763, Val Loss: 0.0776\n",
      "Epoch 41/300 - Train Loss: 0.0775, Val Loss: 0.0691\n",
      "Epoch 42/300 - Train Loss: 0.0737, Val Loss: 0.0693\n",
      "Epoch 43/300 - Train Loss: 0.0736, Val Loss: 0.0778\n",
      "Epoch 44/300 - Train Loss: 0.0742, Val Loss: 0.0677\n",
      "Epoch 45/300 - Train Loss: 0.0753, Val Loss: 0.0735\n",
      "Epoch 46/300 - Train Loss: 0.0746, Val Loss: 0.0714\n",
      "Epoch 47/300 - Train Loss: 0.0764, Val Loss: 0.0750\n",
      "Epoch 48/300 - Train Loss: 0.0741, Val Loss: 0.0688\n",
      "Epoch 49/300 - Train Loss: 0.0758, Val Loss: 0.0697\n",
      "Epoch 50/300 - Train Loss: 0.0764, Val Loss: 0.0735\n",
      "Epoch 51/300 - Train Loss: 0.0734, Val Loss: 0.0691\n",
      "Epoch 52/300 - Train Loss: 0.0747, Val Loss: 0.0749\n",
      "Epoch 53/300 - Train Loss: 0.0762, Val Loss: 0.0721\n",
      "Epoch 54/300 - Train Loss: 0.0739, Val Loss: 0.0709\n",
      "Epoch 55/300 - Train Loss: 0.0727, Val Loss: 0.0773\n",
      "Epoch 56/300 - Train Loss: 0.0764, Val Loss: 0.0741\n",
      "Epoch 57/300 - Train Loss: 0.0734, Val Loss: 0.0757\n",
      "Epoch 58/300 - Train Loss: 0.0754, Val Loss: 0.0727\n",
      "Epoch 59/300 - Train Loss: 0.0745, Val Loss: 0.0679\n",
      "Epoch 60/300 - Train Loss: 0.0741, Val Loss: 0.0702\n",
      "Epoch 61/300 - Train Loss: 0.0741, Val Loss: 0.0703\n",
      "Epoch 62/300 - Train Loss: 0.0739, Val Loss: 0.0696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:49:47,085] Trial 363 finished with value: 0.9677062589730072 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1511465939607605, 'learning_rate': 0.0005647592021313578, 'batch_size': 32, 'weight_decay': 0.0003084441601820701}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300 - Train Loss: 0.0728, Val Loss: 0.0703\n",
      "Early stopping at epoch 63\n",
      "Macro F1 Score: 0.9677, Macro Precision: 0.9640, Macro Recall: 0.9717\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 365\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1699515941296158, LR=0.0008037768219895918, BS=32, WD=0.000159238353333796\n",
      "Epoch 1/300 - Train Loss: 0.1533, Val Loss: 0.0777\n",
      "Epoch 2/300 - Train Loss: 0.0986, Val Loss: 0.0788\n",
      "Epoch 3/300 - Train Loss: 0.0927, Val Loss: 0.0877\n",
      "Epoch 4/300 - Train Loss: 0.0915, Val Loss: 0.0771\n",
      "Epoch 5/300 - Train Loss: 0.0876, Val Loss: 0.0733\n",
      "Epoch 6/300 - Train Loss: 0.0865, Val Loss: 0.0804\n",
      "Epoch 7/300 - Train Loss: 0.0851, Val Loss: 0.0744\n",
      "Epoch 8/300 - Train Loss: 0.0863, Val Loss: 0.0689\n",
      "Epoch 9/300 - Train Loss: 0.0842, Val Loss: 0.0715\n",
      "Epoch 10/300 - Train Loss: 0.0839, Val Loss: 0.0703\n",
      "Epoch 11/300 - Train Loss: 0.0824, Val Loss: 0.0752\n",
      "Epoch 12/300 - Train Loss: 0.0829, Val Loss: 0.0839\n",
      "Epoch 13/300 - Train Loss: 0.0823, Val Loss: 0.0698\n",
      "Epoch 14/300 - Train Loss: 0.0814, Val Loss: 0.0704\n",
      "Epoch 15/300 - Train Loss: 0.0797, Val Loss: 0.0737\n",
      "Epoch 16/300 - Train Loss: 0.0787, Val Loss: 0.0745\n",
      "Epoch 17/300 - Train Loss: 0.0768, Val Loss: 0.0771\n",
      "Epoch 18/300 - Train Loss: 0.0785, Val Loss: 0.0723\n",
      "Epoch 19/300 - Train Loss: 0.0802, Val Loss: 0.0795\n",
      "Epoch 20/300 - Train Loss: 0.0795, Val Loss: 0.0714\n",
      "Epoch 21/300 - Train Loss: 0.0761, Val Loss: 0.0743\n",
      "Epoch 22/300 - Train Loss: 0.0773, Val Loss: 0.0738\n",
      "Epoch 23/300 - Train Loss: 0.0770, Val Loss: 0.0769\n",
      "Epoch 24/300 - Train Loss: 0.0792, Val Loss: 0.0723\n",
      "Epoch 25/300 - Train Loss: 0.0795, Val Loss: 0.0854\n",
      "Epoch 26/300 - Train Loss: 0.0780, Val Loss: 0.0715\n",
      "Epoch 27/300 - Train Loss: 0.0754, Val Loss: 0.0692\n",
      "Epoch 28/300 - Train Loss: 0.0756, Val Loss: 0.0706\n",
      "Epoch 29/300 - Train Loss: 0.0768, Val Loss: 0.0723\n",
      "Epoch 30/300 - Train Loss: 0.0759, Val Loss: 0.0714\n",
      "Epoch 31/300 - Train Loss: 0.0745, Val Loss: 0.0734\n",
      "Epoch 32/300 - Train Loss: 0.0761, Val Loss: 0.0707\n",
      "Epoch 33/300 - Train Loss: 0.0742, Val Loss: 0.0640\n",
      "Epoch 34/300 - Train Loss: 0.0746, Val Loss: 0.0653\n",
      "Epoch 35/300 - Train Loss: 0.0748, Val Loss: 0.0741\n",
      "Epoch 36/300 - Train Loss: 0.0764, Val Loss: 0.0651\n",
      "Epoch 37/300 - Train Loss: 0.0778, Val Loss: 0.0792\n",
      "Epoch 38/300 - Train Loss: 0.0752, Val Loss: 0.0705\n",
      "Epoch 39/300 - Train Loss: 0.0750, Val Loss: 0.0753\n",
      "Epoch 40/300 - Train Loss: 0.0765, Val Loss: 0.0711\n",
      "Epoch 41/300 - Train Loss: 0.0742, Val Loss: 0.0704\n",
      "Epoch 42/300 - Train Loss: 0.0727, Val Loss: 0.0714\n",
      "Epoch 43/300 - Train Loss: 0.0747, Val Loss: 0.0825\n",
      "Epoch 44/300 - Train Loss: 0.0750, Val Loss: 0.0717\n",
      "Epoch 45/300 - Train Loss: 0.0745, Val Loss: 0.0806\n",
      "Epoch 46/300 - Train Loss: 0.0702, Val Loss: 0.0678\n",
      "Epoch 47/300 - Train Loss: 0.0734, Val Loss: 0.0796\n",
      "Epoch 48/300 - Train Loss: 0.0729, Val Loss: 0.0716\n",
      "Epoch 49/300 - Train Loss: 0.0738, Val Loss: 0.0693\n",
      "Epoch 50/300 - Train Loss: 0.0730, Val Loss: 0.0740\n",
      "Epoch 51/300 - Train Loss: 0.0713, Val Loss: 0.0746\n",
      "Epoch 52/300 - Train Loss: 0.0747, Val Loss: 0.0691\n",
      "Epoch 53/300 - Train Loss: 0.0730, Val Loss: 0.0741\n",
      "Epoch 54/300 - Train Loss: 0.0728, Val Loss: 0.0740\n",
      "Epoch 55/300 - Train Loss: 0.0708, Val Loss: 0.0686\n",
      "Epoch 56/300 - Train Loss: 0.0724, Val Loss: 0.0766\n",
      "Epoch 57/300 - Train Loss: 0.0709, Val Loss: 0.0693\n",
      "Epoch 58/300 - Train Loss: 0.0713, Val Loss: 0.0724\n",
      "Epoch 59/300 - Train Loss: 0.0714, Val Loss: 0.0684\n",
      "Epoch 60/300 - Train Loss: 0.0721, Val Loss: 0.0687\n",
      "Epoch 61/300 - Train Loss: 0.0728, Val Loss: 0.0716\n",
      "Epoch 62/300 - Train Loss: 0.0700, Val Loss: 0.0726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:52:24,540] Trial 364 finished with value: 0.9687716154184743 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1699515941296158, 'learning_rate': 0.0008037768219895918, 'batch_size': 32, 'weight_decay': 0.000159238353333796}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63/300 - Train Loss: 0.0731, Val Loss: 0.0788\n",
      "Early stopping at epoch 63\n",
      "Macro F1 Score: 0.9688, Macro Precision: 0.9648, Macro Recall: 0.9729\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 366\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1842636916726627, LR=0.0007340671516073711, BS=32, WD=0.00022486810567756647\n",
      "Epoch 1/300 - Train Loss: 0.1636, Val Loss: 0.0776\n",
      "Epoch 2/300 - Train Loss: 0.0992, Val Loss: 0.0860\n",
      "Epoch 3/300 - Train Loss: 0.0935, Val Loss: 0.0797\n",
      "Epoch 4/300 - Train Loss: 0.0905, Val Loss: 0.0735\n",
      "Epoch 5/300 - Train Loss: 0.0911, Val Loss: 0.0720\n",
      "Epoch 6/300 - Train Loss: 0.0867, Val Loss: 0.0799\n",
      "Epoch 7/300 - Train Loss: 0.0863, Val Loss: 0.0728\n",
      "Epoch 8/300 - Train Loss: 0.0862, Val Loss: 0.0716\n",
      "Epoch 9/300 - Train Loss: 0.0878, Val Loss: 0.0711\n",
      "Epoch 10/300 - Train Loss: 0.0837, Val Loss: 0.0812\n",
      "Epoch 11/300 - Train Loss: 0.0831, Val Loss: 0.0708\n",
      "Epoch 12/300 - Train Loss: 0.0823, Val Loss: 0.0724\n",
      "Epoch 13/300 - Train Loss: 0.0840, Val Loss: 0.0695\n",
      "Epoch 14/300 - Train Loss: 0.0809, Val Loss: 0.0770\n",
      "Epoch 15/300 - Train Loss: 0.0809, Val Loss: 0.0728\n",
      "Epoch 16/300 - Train Loss: 0.0823, Val Loss: 0.0791\n",
      "Epoch 17/300 - Train Loss: 0.0799, Val Loss: 0.0708\n",
      "Epoch 18/300 - Train Loss: 0.0816, Val Loss: 0.0769\n",
      "Epoch 19/300 - Train Loss: 0.0795, Val Loss: 0.0710\n",
      "Epoch 20/300 - Train Loss: 0.0798, Val Loss: 0.0706\n",
      "Epoch 21/300 - Train Loss: 0.0809, Val Loss: 0.0726\n",
      "Epoch 22/300 - Train Loss: 0.0798, Val Loss: 0.0722\n",
      "Epoch 23/300 - Train Loss: 0.0794, Val Loss: 0.0702\n",
      "Epoch 24/300 - Train Loss: 0.0788, Val Loss: 0.0727\n",
      "Epoch 25/300 - Train Loss: 0.0757, Val Loss: 0.0744\n",
      "Epoch 26/300 - Train Loss: 0.0768, Val Loss: 0.0655\n",
      "Epoch 27/300 - Train Loss: 0.0782, Val Loss: 0.0676\n",
      "Epoch 28/300 - Train Loss: 0.0788, Val Loss: 0.0745\n",
      "Epoch 29/300 - Train Loss: 0.0757, Val Loss: 0.0859\n",
      "Epoch 30/300 - Train Loss: 0.0766, Val Loss: 0.0760\n",
      "Epoch 31/300 - Train Loss: 0.0789, Val Loss: 0.0731\n",
      "Epoch 32/300 - Train Loss: 0.0786, Val Loss: 0.0733\n",
      "Epoch 33/300 - Train Loss: 0.0788, Val Loss: 0.0855\n",
      "Epoch 34/300 - Train Loss: 0.0766, Val Loss: 0.0696\n",
      "Epoch 35/300 - Train Loss: 0.0762, Val Loss: 0.0716\n",
      "Epoch 36/300 - Train Loss: 0.0778, Val Loss: 0.0735\n",
      "Epoch 37/300 - Train Loss: 0.0764, Val Loss: 0.0677\n",
      "Epoch 38/300 - Train Loss: 0.0749, Val Loss: 0.0674\n",
      "Epoch 39/300 - Train Loss: 0.0779, Val Loss: 0.0697\n",
      "Epoch 40/300 - Train Loss: 0.0751, Val Loss: 0.0685\n",
      "Epoch 41/300 - Train Loss: 0.0747, Val Loss: 0.0680\n",
      "Epoch 42/300 - Train Loss: 0.0769, Val Loss: 0.0695\n",
      "Epoch 43/300 - Train Loss: 0.0750, Val Loss: 0.0733\n",
      "Epoch 44/300 - Train Loss: 0.0739, Val Loss: 0.0740\n",
      "Epoch 45/300 - Train Loss: 0.0743, Val Loss: 0.0771\n",
      "Epoch 46/300 - Train Loss: 0.0739, Val Loss: 0.0652\n",
      "Epoch 47/300 - Train Loss: 0.0757, Val Loss: 0.0710\n",
      "Epoch 48/300 - Train Loss: 0.0738, Val Loss: 0.0719\n",
      "Epoch 49/300 - Train Loss: 0.0737, Val Loss: 0.0703\n",
      "Epoch 50/300 - Train Loss: 0.0739, Val Loss: 0.0727\n",
      "Epoch 51/300 - Train Loss: 0.0742, Val Loss: 0.0775\n",
      "Epoch 52/300 - Train Loss: 0.0734, Val Loss: 0.0806\n",
      "Epoch 53/300 - Train Loss: 0.0747, Val Loss: 0.0698\n",
      "Epoch 54/300 - Train Loss: 0.0764, Val Loss: 0.0689\n",
      "Epoch 55/300 - Train Loss: 0.0738, Val Loss: 0.0695\n",
      "Epoch 56/300 - Train Loss: 0.0751, Val Loss: 0.0735\n",
      "Epoch 57/300 - Train Loss: 0.0745, Val Loss: 0.0711\n",
      "Epoch 58/300 - Train Loss: 0.0745, Val Loss: 0.0639\n",
      "Epoch 59/300 - Train Loss: 0.0750, Val Loss: 0.0729\n",
      "Epoch 60/300 - Train Loss: 0.0724, Val Loss: 0.0680\n",
      "Epoch 61/300 - Train Loss: 0.0720, Val Loss: 0.0738\n",
      "Epoch 62/300 - Train Loss: 0.0725, Val Loss: 0.0688\n",
      "Epoch 63/300 - Train Loss: 0.0733, Val Loss: 0.0731\n",
      "Epoch 64/300 - Train Loss: 0.0733, Val Loss: 0.0682\n",
      "Epoch 65/300 - Train Loss: 0.0707, Val Loss: 0.0753\n",
      "Epoch 66/300 - Train Loss: 0.0720, Val Loss: 0.0716\n",
      "Epoch 67/300 - Train Loss: 0.0694, Val Loss: 0.0708\n",
      "Epoch 68/300 - Train Loss: 0.0714, Val Loss: 0.0676\n",
      "Epoch 69/300 - Train Loss: 0.0711, Val Loss: 0.0719\n",
      "Epoch 70/300 - Train Loss: 0.0708, Val Loss: 0.0669\n",
      "Epoch 71/300 - Train Loss: 0.0729, Val Loss: 0.0659\n",
      "Epoch 72/300 - Train Loss: 0.0728, Val Loss: 0.0658\n",
      "Epoch 73/300 - Train Loss: 0.0720, Val Loss: 0.0682\n",
      "Epoch 74/300 - Train Loss: 0.0716, Val Loss: 0.0731\n",
      "Epoch 75/300 - Train Loss: 0.0704, Val Loss: 0.0682\n",
      "Epoch 76/300 - Train Loss: 0.0702, Val Loss: 0.0666\n",
      "Epoch 77/300 - Train Loss: 0.0696, Val Loss: 0.0691\n",
      "Epoch 78/300 - Train Loss: 0.0722, Val Loss: 0.0723\n",
      "Epoch 79/300 - Train Loss: 0.0711, Val Loss: 0.0815\n",
      "Epoch 80/300 - Train Loss: 0.0716, Val Loss: 0.0711\n",
      "Epoch 81/300 - Train Loss: 0.0720, Val Loss: 0.0677\n",
      "Epoch 82/300 - Train Loss: 0.0706, Val Loss: 0.0708\n",
      "Epoch 83/300 - Train Loss: 0.0706, Val Loss: 0.0721\n",
      "Epoch 84/300 - Train Loss: 0.0688, Val Loss: 0.0662\n",
      "Epoch 85/300 - Train Loss: 0.0715, Val Loss: 0.0705\n",
      "Epoch 86/300 - Train Loss: 0.0713, Val Loss: 0.0735\n",
      "Epoch 87/300 - Train Loss: 0.0721, Val Loss: 0.0706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:56:04,559] Trial 365 finished with value: 0.9701984693080354 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1842636916726627, 'learning_rate': 0.0007340671516073711, 'batch_size': 32, 'weight_decay': 0.00022486810567756647}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/300 - Train Loss: 0.0723, Val Loss: 0.0659\n",
      "Early stopping at epoch 88\n",
      "Macro F1 Score: 0.9702, Macro Precision: 0.9601, Macro Recall: 0.9813\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 367\n",
      "Training with F1=32, F2=8, D=2, dropout=0.159356329922301, LR=0.0008744030438243869, BS=256, WD=0.00034536956669332794\n",
      "Epoch 1/300 - Train Loss: 0.2979, Val Loss: 0.1140\n",
      "Epoch 2/300 - Train Loss: 0.1011, Val Loss: 0.0777\n",
      "Epoch 3/300 - Train Loss: 0.0882, Val Loss: 0.0887\n",
      "Epoch 4/300 - Train Loss: 0.0836, Val Loss: 0.0715\n",
      "Epoch 5/300 - Train Loss: 0.0837, Val Loss: 0.0834\n",
      "Epoch 6/300 - Train Loss: 0.0791, Val Loss: 0.0740\n",
      "Epoch 7/300 - Train Loss: 0.0788, Val Loss: 0.0652\n",
      "Epoch 8/300 - Train Loss: 0.0769, Val Loss: 0.0686\n",
      "Epoch 9/300 - Train Loss: 0.0755, Val Loss: 0.0681\n",
      "Epoch 10/300 - Train Loss: 0.0766, Val Loss: 0.0694\n",
      "Epoch 11/300 - Train Loss: 0.0762, Val Loss: 0.0691\n",
      "Epoch 12/300 - Train Loss: 0.0723, Val Loss: 0.0620\n",
      "Epoch 13/300 - Train Loss: 0.0728, Val Loss: 0.0676\n",
      "Epoch 14/300 - Train Loss: 0.0742, Val Loss: 0.0649\n",
      "Epoch 15/300 - Train Loss: 0.0734, Val Loss: 0.0669\n",
      "Epoch 16/300 - Train Loss: 0.0728, Val Loss: 0.0755\n",
      "Epoch 17/300 - Train Loss: 0.0708, Val Loss: 0.0647\n",
      "Epoch 18/300 - Train Loss: 0.0705, Val Loss: 0.0626\n",
      "Epoch 19/300 - Train Loss: 0.0712, Val Loss: 0.0654\n",
      "Epoch 20/300 - Train Loss: 0.0706, Val Loss: 0.0670\n",
      "Epoch 21/300 - Train Loss: 0.0693, Val Loss: 0.0733\n",
      "Epoch 22/300 - Train Loss: 0.0703, Val Loss: 0.0683\n",
      "Epoch 23/300 - Train Loss: 0.0696, Val Loss: 0.0739\n",
      "Epoch 24/300 - Train Loss: 0.0680, Val Loss: 0.0760\n",
      "Epoch 25/300 - Train Loss: 0.0697, Val Loss: 0.0732\n",
      "Epoch 26/300 - Train Loss: 0.0682, Val Loss: 0.0745\n",
      "Epoch 27/300 - Train Loss: 0.0678, Val Loss: 0.0681\n",
      "Epoch 28/300 - Train Loss: 0.0669, Val Loss: 0.0693\n",
      "Epoch 29/300 - Train Loss: 0.0671, Val Loss: 0.0659\n",
      "Epoch 30/300 - Train Loss: 0.0664, Val Loss: 0.0650\n",
      "Epoch 31/300 - Train Loss: 0.0655, Val Loss: 0.0653\n",
      "Epoch 32/300 - Train Loss: 0.0685, Val Loss: 0.0677\n",
      "Epoch 33/300 - Train Loss: 0.0697, Val Loss: 0.0712\n",
      "Epoch 34/300 - Train Loss: 0.0654, Val Loss: 0.0675\n",
      "Epoch 35/300 - Train Loss: 0.0684, Val Loss: 0.0677\n",
      "Epoch 36/300 - Train Loss: 0.0663, Val Loss: 0.0697\n",
      "Epoch 37/300 - Train Loss: 0.0666, Val Loss: 0.0665\n",
      "Epoch 38/300 - Train Loss: 0.0655, Val Loss: 0.0670\n",
      "Epoch 39/300 - Train Loss: 0.0650, Val Loss: 0.0713\n",
      "Epoch 40/300 - Train Loss: 0.0650, Val Loss: 0.0701\n",
      "Epoch 41/300 - Train Loss: 0.0664, Val Loss: 0.0683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:57:29,321] Trial 366 finished with value: 0.9737502392361271 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.159356329922301, 'learning_rate': 0.0008744030438243869, 'batch_size': 256, 'weight_decay': 0.00034536956669332794}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300 - Train Loss: 0.0665, Val Loss: 0.0668\n",
      "Early stopping at epoch 42\n",
      "Macro F1 Score: 0.9738, Macro Precision: 0.9773, Macro Recall: 0.9704\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 368\n",
      "Training with F1=32, F2=8, D=2, dropout=0.14003830260276087, LR=0.0008194293763683009, BS=32, WD=2.2745374770428846e-05\n",
      "Epoch 1/300 - Train Loss: 0.1635, Val Loss: 0.0940\n",
      "Epoch 2/300 - Train Loss: 0.0975, Val Loss: 0.0761\n",
      "Epoch 3/300 - Train Loss: 0.0933, Val Loss: 0.1068\n",
      "Epoch 4/300 - Train Loss: 0.0890, Val Loss: 0.0786\n",
      "Epoch 5/300 - Train Loss: 0.0855, Val Loss: 0.0709\n",
      "Epoch 6/300 - Train Loss: 0.0856, Val Loss: 0.0672\n",
      "Epoch 7/300 - Train Loss: 0.0826, Val Loss: 0.0719\n",
      "Epoch 8/300 - Train Loss: 0.0813, Val Loss: 0.0733\n",
      "Epoch 9/300 - Train Loss: 0.0822, Val Loss: 0.0783\n",
      "Epoch 10/300 - Train Loss: 0.0816, Val Loss: 0.0856\n",
      "Epoch 11/300 - Train Loss: 0.0801, Val Loss: 0.0761\n",
      "Epoch 12/300 - Train Loss: 0.0781, Val Loss: 0.0695\n",
      "Epoch 13/300 - Train Loss: 0.0780, Val Loss: 0.0789\n",
      "Epoch 14/300 - Train Loss: 0.0787, Val Loss: 0.0698\n",
      "Epoch 15/300 - Train Loss: 0.0779, Val Loss: 0.0715\n",
      "Epoch 16/300 - Train Loss: 0.0767, Val Loss: 0.0708\n",
      "Epoch 17/300 - Train Loss: 0.0733, Val Loss: 0.0679\n",
      "Epoch 18/300 - Train Loss: 0.0774, Val Loss: 0.0720\n",
      "Epoch 19/300 - Train Loss: 0.0726, Val Loss: 0.0722\n",
      "Epoch 20/300 - Train Loss: 0.0731, Val Loss: 0.0754\n",
      "Epoch 21/300 - Train Loss: 0.0738, Val Loss: 0.0723\n",
      "Epoch 22/300 - Train Loss: 0.0730, Val Loss: 0.0750\n",
      "Epoch 23/300 - Train Loss: 0.0718, Val Loss: 0.0866\n",
      "Epoch 24/300 - Train Loss: 0.0705, Val Loss: 0.0690\n",
      "Epoch 25/300 - Train Loss: 0.0730, Val Loss: 0.0688\n",
      "Epoch 26/300 - Train Loss: 0.0729, Val Loss: 0.0779\n",
      "Epoch 27/300 - Train Loss: 0.0716, Val Loss: 0.0738\n",
      "Epoch 28/300 - Train Loss: 0.0699, Val Loss: 0.0725\n",
      "Epoch 29/300 - Train Loss: 0.0681, Val Loss: 0.0707\n",
      "Epoch 30/300 - Train Loss: 0.0693, Val Loss: 0.0718\n",
      "Epoch 31/300 - Train Loss: 0.0706, Val Loss: 0.0726\n",
      "Epoch 32/300 - Train Loss: 0.0705, Val Loss: 0.0723\n",
      "Epoch 33/300 - Train Loss: 0.0699, Val Loss: 0.0746\n",
      "Epoch 34/300 - Train Loss: 0.0681, Val Loss: 0.0738\n",
      "Epoch 35/300 - Train Loss: 0.0665, Val Loss: 0.0715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 22:58:59,017] Trial 367 finished with value: 0.972595222170774 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.14003830260276087, 'learning_rate': 0.0008194293763683009, 'batch_size': 32, 'weight_decay': 2.2745374770428846e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300 - Train Loss: 0.0660, Val Loss: 0.0714\n",
      "Early stopping at epoch 36\n",
      "Macro F1 Score: 0.9726, Macro Precision: 0.9686, Macro Recall: 0.9768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 369\n",
      "Training with F1=4, F2=8, D=2, dropout=0.12795594042413871, LR=0.0006363543432360937, BS=32, WD=1.8206060140821727e-05\n",
      "Epoch 1/300 - Train Loss: 0.2147, Val Loss: 0.0973\n",
      "Epoch 2/300 - Train Loss: 0.1137, Val Loss: 0.0874\n",
      "Epoch 3/300 - Train Loss: 0.1065, Val Loss: 0.0855\n",
      "Epoch 4/300 - Train Loss: 0.1042, Val Loss: 0.0783\n",
      "Epoch 5/300 - Train Loss: 0.1000, Val Loss: 0.0860\n",
      "Epoch 6/300 - Train Loss: 0.0988, Val Loss: 0.0746\n",
      "Epoch 7/300 - Train Loss: 0.1001, Val Loss: 0.0749\n",
      "Epoch 8/300 - Train Loss: 0.0987, Val Loss: 0.0780\n",
      "Epoch 9/300 - Train Loss: 0.0966, Val Loss: 0.0762\n",
      "Epoch 10/300 - Train Loss: 0.0952, Val Loss: 0.0807\n",
      "Epoch 11/300 - Train Loss: 0.0973, Val Loss: 0.0771\n",
      "Epoch 12/300 - Train Loss: 0.0926, Val Loss: 0.0821\n",
      "Epoch 13/300 - Train Loss: 0.0947, Val Loss: 0.0823\n",
      "Epoch 14/300 - Train Loss: 0.0952, Val Loss: 0.0744\n",
      "Epoch 15/300 - Train Loss: 0.0913, Val Loss: 0.0818\n",
      "Epoch 16/300 - Train Loss: 0.0916, Val Loss: 0.0778\n",
      "Epoch 17/300 - Train Loss: 0.0929, Val Loss: 0.0760\n",
      "Epoch 18/300 - Train Loss: 0.0895, Val Loss: 0.0796\n",
      "Epoch 19/300 - Train Loss: 0.0905, Val Loss: 0.0797\n",
      "Epoch 20/300 - Train Loss: 0.0900, Val Loss: 0.0754\n",
      "Epoch 21/300 - Train Loss: 0.0879, Val Loss: 0.0798\n",
      "Epoch 22/300 - Train Loss: 0.0913, Val Loss: 0.0763\n",
      "Epoch 23/300 - Train Loss: 0.0898, Val Loss: 0.0756\n",
      "Epoch 24/300 - Train Loss: 0.0878, Val Loss: 0.0725\n",
      "Epoch 25/300 - Train Loss: 0.0870, Val Loss: 0.0727\n",
      "Epoch 26/300 - Train Loss: 0.0868, Val Loss: 0.0723\n",
      "Epoch 27/300 - Train Loss: 0.0875, Val Loss: 0.0837\n",
      "Epoch 28/300 - Train Loss: 0.0873, Val Loss: 0.0756\n",
      "Epoch 29/300 - Train Loss: 0.0887, Val Loss: 0.0755\n",
      "Epoch 30/300 - Train Loss: 0.0868, Val Loss: 0.0738\n",
      "Epoch 31/300 - Train Loss: 0.0883, Val Loss: 0.0746\n",
      "Epoch 32/300 - Train Loss: 0.0882, Val Loss: 0.0854\n",
      "Epoch 33/300 - Train Loss: 0.0853, Val Loss: 0.0817\n",
      "Epoch 34/300 - Train Loss: 0.0883, Val Loss: 0.0808\n",
      "Epoch 35/300 - Train Loss: 0.0911, Val Loss: 0.0736\n",
      "Epoch 36/300 - Train Loss: 0.0845, Val Loss: 0.0760\n",
      "Epoch 37/300 - Train Loss: 0.0856, Val Loss: 0.0733\n",
      "Epoch 38/300 - Train Loss: 0.0850, Val Loss: 0.0744\n",
      "Epoch 39/300 - Train Loss: 0.0826, Val Loss: 0.0775\n",
      "Epoch 40/300 - Train Loss: 0.0862, Val Loss: 0.0795\n",
      "Epoch 41/300 - Train Loss: 0.0833, Val Loss: 0.0846\n",
      "Epoch 42/300 - Train Loss: 0.0839, Val Loss: 0.0752\n",
      "Epoch 43/300 - Train Loss: 0.0834, Val Loss: 0.0781\n",
      "Epoch 44/300 - Train Loss: 0.0816, Val Loss: 0.0712\n",
      "Epoch 45/300 - Train Loss: 0.0823, Val Loss: 0.0778\n",
      "Epoch 46/300 - Train Loss: 0.0839, Val Loss: 0.0808\n",
      "Epoch 47/300 - Train Loss: 0.0825, Val Loss: 0.0824\n",
      "Epoch 48/300 - Train Loss: 0.0844, Val Loss: 0.0701\n",
      "Epoch 49/300 - Train Loss: 0.0831, Val Loss: 0.0771\n",
      "Epoch 50/300 - Train Loss: 0.0810, Val Loss: 0.0745\n",
      "Epoch 51/300 - Train Loss: 0.0840, Val Loss: 0.0729\n",
      "Epoch 52/300 - Train Loss: 0.0816, Val Loss: 0.0714\n",
      "Epoch 53/300 - Train Loss: 0.0821, Val Loss: 0.0746\n",
      "Epoch 54/300 - Train Loss: 0.0814, Val Loss: 0.0742\n",
      "Epoch 55/300 - Train Loss: 0.0810, Val Loss: 0.0688\n",
      "Epoch 56/300 - Train Loss: 0.0828, Val Loss: 0.0792\n",
      "Epoch 57/300 - Train Loss: 0.0848, Val Loss: 0.0724\n",
      "Epoch 58/300 - Train Loss: 0.0834, Val Loss: 0.0714\n",
      "Epoch 59/300 - Train Loss: 0.0820, Val Loss: 0.0695\n",
      "Epoch 60/300 - Train Loss: 0.0814, Val Loss: 0.0785\n",
      "Epoch 61/300 - Train Loss: 0.0802, Val Loss: 0.0723\n",
      "Epoch 62/300 - Train Loss: 0.0810, Val Loss: 0.0711\n",
      "Epoch 63/300 - Train Loss: 0.0812, Val Loss: 0.0775\n",
      "Epoch 64/300 - Train Loss: 0.0779, Val Loss: 0.0725\n",
      "Epoch 65/300 - Train Loss: 0.0805, Val Loss: 0.0778\n",
      "Epoch 66/300 - Train Loss: 0.0834, Val Loss: 0.0717\n",
      "Epoch 67/300 - Train Loss: 0.0777, Val Loss: 0.0759\n",
      "Epoch 68/300 - Train Loss: 0.0803, Val Loss: 0.0713\n",
      "Epoch 69/300 - Train Loss: 0.0794, Val Loss: 0.0710\n",
      "Epoch 70/300 - Train Loss: 0.0787, Val Loss: 0.0729\n",
      "Epoch 71/300 - Train Loss: 0.0782, Val Loss: 0.0677\n",
      "Epoch 72/300 - Train Loss: 0.0800, Val Loss: 0.0741\n",
      "Epoch 73/300 - Train Loss: 0.0775, Val Loss: 0.0716\n",
      "Epoch 74/300 - Train Loss: 0.0792, Val Loss: 0.0700\n",
      "Epoch 75/300 - Train Loss: 0.0783, Val Loss: 0.0688\n",
      "Epoch 76/300 - Train Loss: 0.0770, Val Loss: 0.0689\n",
      "Epoch 77/300 - Train Loss: 0.0773, Val Loss: 0.0747\n",
      "Epoch 78/300 - Train Loss: 0.0745, Val Loss: 0.0741\n",
      "Epoch 79/300 - Train Loss: 0.0796, Val Loss: 0.0766\n",
      "Epoch 80/300 - Train Loss: 0.0783, Val Loss: 0.0801\n",
      "Epoch 81/300 - Train Loss: 0.0772, Val Loss: 0.0718\n",
      "Epoch 82/300 - Train Loss: 0.0764, Val Loss: 0.0704\n",
      "Epoch 83/300 - Train Loss: 0.0767, Val Loss: 0.0661\n",
      "Epoch 84/300 - Train Loss: 0.0795, Val Loss: 0.0689\n",
      "Epoch 85/300 - Train Loss: 0.0765, Val Loss: 0.0700\n",
      "Epoch 86/300 - Train Loss: 0.0759, Val Loss: 0.0709\n",
      "Epoch 87/300 - Train Loss: 0.0766, Val Loss: 0.0706\n",
      "Epoch 88/300 - Train Loss: 0.0755, Val Loss: 0.0757\n",
      "Epoch 89/300 - Train Loss: 0.0753, Val Loss: 0.0668\n",
      "Epoch 90/300 - Train Loss: 0.0751, Val Loss: 0.0747\n",
      "Epoch 91/300 - Train Loss: 0.0731, Val Loss: 0.0694\n",
      "Epoch 92/300 - Train Loss: 0.0769, Val Loss: 0.0698\n",
      "Epoch 93/300 - Train Loss: 0.0776, Val Loss: 0.0705\n",
      "Epoch 94/300 - Train Loss: 0.0762, Val Loss: 0.0661\n",
      "Epoch 95/300 - Train Loss: 0.0745, Val Loss: 0.0748\n",
      "Epoch 96/300 - Train Loss: 0.0752, Val Loss: 0.0715\n",
      "Epoch 97/300 - Train Loss: 0.0752, Val Loss: 0.0707\n",
      "Epoch 98/300 - Train Loss: 0.0752, Val Loss: 0.0749\n",
      "Epoch 99/300 - Train Loss: 0.0763, Val Loss: 0.0803\n",
      "Epoch 100/300 - Train Loss: 0.0777, Val Loss: 0.0742\n",
      "Epoch 101/300 - Train Loss: 0.0769, Val Loss: 0.0777\n",
      "Epoch 102/300 - Train Loss: 0.0752, Val Loss: 0.0723\n",
      "Epoch 103/300 - Train Loss: 0.0739, Val Loss: 0.0734\n",
      "Epoch 104/300 - Train Loss: 0.0777, Val Loss: 0.0717\n",
      "Epoch 105/300 - Train Loss: 0.0743, Val Loss: 0.0746\n",
      "Epoch 106/300 - Train Loss: 0.0740, Val Loss: 0.0710\n",
      "Epoch 107/300 - Train Loss: 0.0751, Val Loss: 0.0724\n",
      "Epoch 108/300 - Train Loss: 0.0759, Val Loss: 0.0713\n",
      "Epoch 109/300 - Train Loss: 0.0746, Val Loss: 0.0751\n",
      "Epoch 110/300 - Train Loss: 0.0765, Val Loss: 0.0719\n",
      "Epoch 111/300 - Train Loss: 0.0756, Val Loss: 0.0756\n",
      "Epoch 112/300 - Train Loss: 0.0732, Val Loss: 0.0672\n",
      "Epoch 113/300 - Train Loss: 0.0712, Val Loss: 0.0758\n",
      "Epoch 114/300 - Train Loss: 0.0743, Val Loss: 0.0655\n",
      "Epoch 115/300 - Train Loss: 0.0724, Val Loss: 0.0690\n",
      "Epoch 116/300 - Train Loss: 0.0736, Val Loss: 0.0660\n",
      "Epoch 117/300 - Train Loss: 0.0733, Val Loss: 0.0708\n",
      "Epoch 118/300 - Train Loss: 0.0746, Val Loss: 0.0737\n",
      "Epoch 119/300 - Train Loss: 0.0701, Val Loss: 0.0678\n",
      "Epoch 120/300 - Train Loss: 0.0724, Val Loss: 0.0657\n",
      "Epoch 121/300 - Train Loss: 0.0745, Val Loss: 0.0706\n",
      "Epoch 122/300 - Train Loss: 0.0727, Val Loss: 0.0710\n",
      "Epoch 123/300 - Train Loss: 0.0715, Val Loss: 0.0700\n",
      "Epoch 124/300 - Train Loss: 0.0721, Val Loss: 0.0755\n",
      "Epoch 125/300 - Train Loss: 0.0718, Val Loss: 0.0733\n",
      "Epoch 126/300 - Train Loss: 0.0729, Val Loss: 0.0698\n",
      "Epoch 127/300 - Train Loss: 0.0740, Val Loss: 0.0725\n",
      "Epoch 128/300 - Train Loss: 0.0731, Val Loss: 0.0702\n",
      "Epoch 129/300 - Train Loss: 0.0720, Val Loss: 0.0725\n",
      "Epoch 130/300 - Train Loss: 0.0742, Val Loss: 0.0686\n",
      "Epoch 131/300 - Train Loss: 0.0719, Val Loss: 0.0680\n",
      "Epoch 132/300 - Train Loss: 0.0730, Val Loss: 0.0687\n",
      "Epoch 133/300 - Train Loss: 0.0724, Val Loss: 0.0691\n",
      "Epoch 134/300 - Train Loss: 0.0720, Val Loss: 0.0710\n",
      "Epoch 135/300 - Train Loss: 0.0720, Val Loss: 0.0741\n",
      "Epoch 136/300 - Train Loss: 0.0738, Val Loss: 0.0705\n",
      "Epoch 137/300 - Train Loss: 0.0745, Val Loss: 0.0881\n",
      "Epoch 138/300 - Train Loss: 0.0714, Val Loss: 0.0728\n",
      "Epoch 139/300 - Train Loss: 0.0729, Val Loss: 0.0703\n",
      "Epoch 140/300 - Train Loss: 0.0754, Val Loss: 0.0707\n",
      "Epoch 141/300 - Train Loss: 0.0704, Val Loss: 0.0727\n",
      "Epoch 142/300 - Train Loss: 0.0730, Val Loss: 0.0721\n",
      "Epoch 143/300 - Train Loss: 0.0730, Val Loss: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:03:31,312] Trial 368 finished with value: 0.9733365355542577 and parameters: {'F1': 4, 'F2': 8, 'D': 2, 'dropout': 0.12795594042413871, 'learning_rate': 0.0006363543432360937, 'batch_size': 32, 'weight_decay': 1.8206060140821727e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 144/300 - Train Loss: 0.0711, Val Loss: 0.0677\n",
      "Early stopping at epoch 144\n",
      "Macro F1 Score: 0.9733, Macro Precision: 0.9651, Macro Recall: 0.9823\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.98      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 370\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1485888875527321, LR=0.0006359188356014147, BS=32, WD=0.0004177215273625524\n",
      "Epoch 1/300 - Train Loss: 0.1627, Val Loss: 0.0921\n",
      "Epoch 2/300 - Train Loss: 0.0997, Val Loss: 0.0709\n",
      "Epoch 3/300 - Train Loss: 0.0953, Val Loss: 0.0995\n",
      "Epoch 4/300 - Train Loss: 0.0920, Val Loss: 0.0951\n",
      "Epoch 5/300 - Train Loss: 0.0883, Val Loss: 0.0716\n",
      "Epoch 6/300 - Train Loss: 0.0889, Val Loss: 0.0773\n",
      "Epoch 7/300 - Train Loss: 0.0883, Val Loss: 0.0747\n",
      "Epoch 8/300 - Train Loss: 0.0883, Val Loss: 0.0683\n",
      "Epoch 9/300 - Train Loss: 0.0849, Val Loss: 0.0796\n",
      "Epoch 10/300 - Train Loss: 0.0845, Val Loss: 0.0708\n",
      "Epoch 11/300 - Train Loss: 0.0848, Val Loss: 0.0707\n",
      "Epoch 12/300 - Train Loss: 0.0818, Val Loss: 0.0694\n",
      "Epoch 13/300 - Train Loss: 0.0814, Val Loss: 0.0680\n",
      "Epoch 14/300 - Train Loss: 0.0854, Val Loss: 0.0748\n",
      "Epoch 15/300 - Train Loss: 0.0842, Val Loss: 0.0713\n",
      "Epoch 16/300 - Train Loss: 0.0830, Val Loss: 0.0700\n",
      "Epoch 17/300 - Train Loss: 0.0789, Val Loss: 0.0749\n",
      "Epoch 18/300 - Train Loss: 0.0802, Val Loss: 0.0739\n",
      "Epoch 19/300 - Train Loss: 0.0834, Val Loss: 0.0737\n",
      "Epoch 20/300 - Train Loss: 0.0825, Val Loss: 0.0741\n",
      "Epoch 21/300 - Train Loss: 0.0811, Val Loss: 0.0704\n",
      "Epoch 22/300 - Train Loss: 0.0808, Val Loss: 0.0701\n",
      "Epoch 23/300 - Train Loss: 0.0802, Val Loss: 0.0698\n",
      "Epoch 24/300 - Train Loss: 0.0817, Val Loss: 0.0798\n",
      "Epoch 25/300 - Train Loss: 0.0814, Val Loss: 0.0689\n",
      "Epoch 26/300 - Train Loss: 0.0794, Val Loss: 0.0712\n",
      "Epoch 27/300 - Train Loss: 0.0794, Val Loss: 0.0749\n",
      "Epoch 28/300 - Train Loss: 0.0815, Val Loss: 0.0772\n",
      "Epoch 29/300 - Train Loss: 0.0814, Val Loss: 0.0718\n",
      "Epoch 30/300 - Train Loss: 0.0813, Val Loss: 0.0755\n",
      "Epoch 31/300 - Train Loss: 0.0805, Val Loss: 0.0725\n",
      "Epoch 32/300 - Train Loss: 0.0783, Val Loss: 0.0719\n",
      "Epoch 33/300 - Train Loss: 0.0771, Val Loss: 0.0707\n",
      "Epoch 34/300 - Train Loss: 0.0787, Val Loss: 0.0756\n",
      "Epoch 35/300 - Train Loss: 0.0780, Val Loss: 0.0725\n",
      "Epoch 36/300 - Train Loss: 0.0790, Val Loss: 0.0748\n",
      "Epoch 37/300 - Train Loss: 0.0774, Val Loss: 0.0719\n",
      "Epoch 38/300 - Train Loss: 0.0769, Val Loss: 0.0666\n",
      "Epoch 39/300 - Train Loss: 0.0758, Val Loss: 0.0774\n",
      "Epoch 40/300 - Train Loss: 0.0801, Val Loss: 0.0684\n",
      "Epoch 41/300 - Train Loss: 0.0771, Val Loss: 0.0699\n",
      "Epoch 42/300 - Train Loss: 0.0757, Val Loss: 0.0765\n",
      "Epoch 43/300 - Train Loss: 0.0767, Val Loss: 0.0726\n",
      "Epoch 44/300 - Train Loss: 0.0783, Val Loss: 0.0688\n",
      "Epoch 45/300 - Train Loss: 0.0792, Val Loss: 0.0791\n",
      "Epoch 46/300 - Train Loss: 0.0789, Val Loss: 0.0695\n",
      "Epoch 47/300 - Train Loss: 0.0773, Val Loss: 0.0729\n",
      "Epoch 48/300 - Train Loss: 0.0781, Val Loss: 0.0727\n",
      "Epoch 49/300 - Train Loss: 0.0753, Val Loss: 0.0706\n",
      "Epoch 50/300 - Train Loss: 0.0769, Val Loss: 0.0731\n",
      "Epoch 51/300 - Train Loss: 0.0756, Val Loss: 0.0689\n",
      "Epoch 52/300 - Train Loss: 0.0753, Val Loss: 0.0702\n",
      "Epoch 53/300 - Train Loss: 0.0768, Val Loss: 0.0716\n",
      "Epoch 54/300 - Train Loss: 0.0757, Val Loss: 0.0749\n",
      "Epoch 55/300 - Train Loss: 0.0756, Val Loss: 0.0783\n",
      "Epoch 56/300 - Train Loss: 0.0756, Val Loss: 0.0764\n",
      "Epoch 57/300 - Train Loss: 0.0766, Val Loss: 0.0736\n",
      "Epoch 58/300 - Train Loss: 0.0731, Val Loss: 0.0683\n",
      "Epoch 59/300 - Train Loss: 0.0756, Val Loss: 0.0788\n",
      "Epoch 60/300 - Train Loss: 0.0763, Val Loss: 0.0827\n",
      "Epoch 61/300 - Train Loss: 0.0750, Val Loss: 0.0748\n",
      "Epoch 62/300 - Train Loss: 0.0755, Val Loss: 0.0730\n",
      "Epoch 63/300 - Train Loss: 0.0763, Val Loss: 0.0739\n",
      "Epoch 64/300 - Train Loss: 0.0772, Val Loss: 0.0708\n",
      "Epoch 65/300 - Train Loss: 0.0740, Val Loss: 0.0694\n",
      "Epoch 66/300 - Train Loss: 0.0732, Val Loss: 0.0743\n",
      "Epoch 67/300 - Train Loss: 0.0754, Val Loss: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:06:22,622] Trial 369 finished with value: 0.9737456552503575 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1485888875527321, 'learning_rate': 0.0006359188356014147, 'batch_size': 32, 'weight_decay': 0.0004177215273625524}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/300 - Train Loss: 0.0755, Val Loss: 0.0673\n",
      "Early stopping at epoch 68\n",
      "Macro F1 Score: 0.9737, Macro Precision: 0.9775, Macro Recall: 0.9703\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 371\n",
      "Training with F1=32, F2=8, D=2, dropout=0.16311278736409596, LR=0.000774149987438366, BS=32, WD=0.00023523019498634227\n",
      "Epoch 1/300 - Train Loss: 0.1542, Val Loss: 0.0900\n",
      "Epoch 2/300 - Train Loss: 0.1000, Val Loss: 0.1475\n",
      "Epoch 3/300 - Train Loss: 0.0929, Val Loss: 0.0820\n",
      "Epoch 4/300 - Train Loss: 0.0910, Val Loss: 0.0727\n",
      "Epoch 5/300 - Train Loss: 0.0893, Val Loss: 0.0744\n",
      "Epoch 6/300 - Train Loss: 0.0887, Val Loss: 0.0747\n",
      "Epoch 7/300 - Train Loss: 0.0862, Val Loss: 0.0784\n",
      "Epoch 8/300 - Train Loss: 0.0834, Val Loss: 0.0703\n",
      "Epoch 9/300 - Train Loss: 0.0849, Val Loss: 0.0746\n",
      "Epoch 10/300 - Train Loss: 0.0837, Val Loss: 0.0789\n",
      "Epoch 11/300 - Train Loss: 0.0837, Val Loss: 0.0669\n",
      "Epoch 12/300 - Train Loss: 0.0824, Val Loss: 0.0717\n",
      "Epoch 13/300 - Train Loss: 0.0841, Val Loss: 0.0779\n",
      "Epoch 14/300 - Train Loss: 0.0821, Val Loss: 0.0744\n",
      "Epoch 15/300 - Train Loss: 0.0814, Val Loss: 0.0667\n",
      "Epoch 16/300 - Train Loss: 0.0792, Val Loss: 0.0681\n",
      "Epoch 17/300 - Train Loss: 0.0830, Val Loss: 0.0702\n",
      "Epoch 18/300 - Train Loss: 0.0808, Val Loss: 0.0708\n",
      "Epoch 19/300 - Train Loss: 0.0800, Val Loss: 0.0756\n",
      "Epoch 20/300 - Train Loss: 0.0793, Val Loss: 0.0757\n",
      "Epoch 21/300 - Train Loss: 0.0804, Val Loss: 0.0739\n",
      "Epoch 22/300 - Train Loss: 0.0757, Val Loss: 0.0709\n",
      "Epoch 23/300 - Train Loss: 0.0762, Val Loss: 0.0668\n",
      "Epoch 24/300 - Train Loss: 0.0764, Val Loss: 0.0833\n",
      "Epoch 25/300 - Train Loss: 0.0756, Val Loss: 0.0729\n",
      "Epoch 26/300 - Train Loss: 0.0781, Val Loss: 0.0730\n",
      "Epoch 27/300 - Train Loss: 0.0761, Val Loss: 0.0715\n",
      "Epoch 28/300 - Train Loss: 0.0775, Val Loss: 0.0676\n",
      "Epoch 29/300 - Train Loss: 0.0769, Val Loss: 0.0845\n",
      "Epoch 30/300 - Train Loss: 0.0772, Val Loss: 0.0722\n",
      "Epoch 31/300 - Train Loss: 0.0757, Val Loss: 0.0683\n",
      "Epoch 32/300 - Train Loss: 0.0742, Val Loss: 0.0694\n",
      "Epoch 33/300 - Train Loss: 0.0750, Val Loss: 0.0709\n",
      "Epoch 34/300 - Train Loss: 0.0758, Val Loss: 0.0700\n",
      "Epoch 35/300 - Train Loss: 0.0731, Val Loss: 0.0708\n",
      "Epoch 36/300 - Train Loss: 0.0718, Val Loss: 0.0751\n",
      "Epoch 37/300 - Train Loss: 0.0746, Val Loss: 0.0706\n",
      "Epoch 38/300 - Train Loss: 0.0737, Val Loss: 0.0773\n",
      "Epoch 39/300 - Train Loss: 0.0750, Val Loss: 0.0696\n",
      "Epoch 40/300 - Train Loss: 0.0718, Val Loss: 0.0677\n",
      "Epoch 41/300 - Train Loss: 0.0725, Val Loss: 0.0656\n",
      "Epoch 42/300 - Train Loss: 0.0741, Val Loss: 0.0647\n",
      "Epoch 43/300 - Train Loss: 0.0756, Val Loss: 0.0658\n",
      "Epoch 44/300 - Train Loss: 0.0734, Val Loss: 0.0719\n",
      "Epoch 45/300 - Train Loss: 0.0719, Val Loss: 0.0669\n",
      "Epoch 46/300 - Train Loss: 0.0720, Val Loss: 0.0626\n",
      "Epoch 47/300 - Train Loss: 0.0710, Val Loss: 0.0688\n",
      "Epoch 48/300 - Train Loss: 0.0738, Val Loss: 0.0693\n",
      "Epoch 49/300 - Train Loss: 0.0711, Val Loss: 0.0706\n",
      "Epoch 50/300 - Train Loss: 0.0746, Val Loss: 0.0668\n",
      "Epoch 51/300 - Train Loss: 0.0727, Val Loss: 0.0757\n",
      "Epoch 52/300 - Train Loss: 0.0714, Val Loss: 0.0705\n",
      "Epoch 53/300 - Train Loss: 0.0717, Val Loss: 0.0665\n",
      "Epoch 54/300 - Train Loss: 0.0687, Val Loss: 0.0731\n",
      "Epoch 55/300 - Train Loss: 0.0731, Val Loss: 0.0656\n",
      "Epoch 56/300 - Train Loss: 0.0701, Val Loss: 0.0715\n",
      "Epoch 57/300 - Train Loss: 0.0707, Val Loss: 0.0677\n",
      "Epoch 58/300 - Train Loss: 0.0698, Val Loss: 0.0693\n",
      "Epoch 59/300 - Train Loss: 0.0684, Val Loss: 0.0667\n",
      "Epoch 60/300 - Train Loss: 0.0733, Val Loss: 0.0743\n",
      "Epoch 61/300 - Train Loss: 0.0709, Val Loss: 0.0713\n",
      "Epoch 62/300 - Train Loss: 0.0709, Val Loss: 0.0774\n",
      "Epoch 63/300 - Train Loss: 0.0680, Val Loss: 0.0763\n",
      "Epoch 64/300 - Train Loss: 0.0698, Val Loss: 0.0764\n",
      "Epoch 65/300 - Train Loss: 0.0726, Val Loss: 0.0652\n",
      "Epoch 66/300 - Train Loss: 0.0728, Val Loss: 0.0687\n",
      "Epoch 67/300 - Train Loss: 0.0705, Val Loss: 0.0728\n",
      "Epoch 68/300 - Train Loss: 0.0703, Val Loss: 0.0709\n",
      "Epoch 69/300 - Train Loss: 0.0706, Val Loss: 0.0681\n",
      "Epoch 70/300 - Train Loss: 0.0670, Val Loss: 0.0686\n",
      "Epoch 71/300 - Train Loss: 0.0676, Val Loss: 0.0732\n",
      "Epoch 72/300 - Train Loss: 0.0690, Val Loss: 0.0686\n",
      "Epoch 73/300 - Train Loss: 0.0694, Val Loss: 0.0727\n",
      "Epoch 74/300 - Train Loss: 0.0682, Val Loss: 0.0677\n",
      "Epoch 75/300 - Train Loss: 0.0687, Val Loss: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:09:36,055] Trial 370 finished with value: 0.9730160975916493 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.16311278736409596, 'learning_rate': 0.000774149987438366, 'batch_size': 32, 'weight_decay': 0.00023523019498634227}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300 - Train Loss: 0.0672, Val Loss: 0.0887\n",
      "Early stopping at epoch 76\n",
      "Macro F1 Score: 0.9730, Macro Precision: 0.9690, Macro Recall: 0.9773\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 372\n",
      "Training with F1=32, F2=8, D=2, dropout=0.13817088744051478, LR=0.0005052421041500623, BS=32, WD=0.00028603970663311886\n",
      "Epoch 1/300 - Train Loss: 0.1894, Val Loss: 0.0982\n",
      "Epoch 2/300 - Train Loss: 0.0972, Val Loss: 0.0771\n",
      "Epoch 3/300 - Train Loss: 0.0933, Val Loss: 0.0712\n",
      "Epoch 4/300 - Train Loss: 0.0888, Val Loss: 0.0841\n",
      "Epoch 5/300 - Train Loss: 0.0874, Val Loss: 0.0662\n",
      "Epoch 6/300 - Train Loss: 0.0847, Val Loss: 0.0676\n",
      "Epoch 7/300 - Train Loss: 0.0855, Val Loss: 0.0901\n",
      "Epoch 8/300 - Train Loss: 0.0823, Val Loss: 0.0675\n",
      "Epoch 9/300 - Train Loss: 0.0811, Val Loss: 0.0695\n",
      "Epoch 10/300 - Train Loss: 0.0820, Val Loss: 0.0720\n",
      "Epoch 11/300 - Train Loss: 0.0803, Val Loss: 0.0686\n",
      "Epoch 12/300 - Train Loss: 0.0819, Val Loss: 0.0685\n",
      "Epoch 13/300 - Train Loss: 0.0796, Val Loss: 0.0746\n",
      "Epoch 14/300 - Train Loss: 0.0805, Val Loss: 0.0834\n",
      "Epoch 15/300 - Train Loss: 0.0795, Val Loss: 0.0702\n",
      "Epoch 16/300 - Train Loss: 0.0784, Val Loss: 0.0744\n",
      "Epoch 17/300 - Train Loss: 0.0805, Val Loss: 0.0826\n",
      "Epoch 18/300 - Train Loss: 0.0774, Val Loss: 0.0694\n",
      "Epoch 19/300 - Train Loss: 0.0766, Val Loss: 0.0720\n",
      "Epoch 20/300 - Train Loss: 0.0781, Val Loss: 0.0693\n",
      "Epoch 21/300 - Train Loss: 0.0772, Val Loss: 0.0798\n",
      "Epoch 22/300 - Train Loss: 0.0769, Val Loss: 0.0738\n",
      "Epoch 23/300 - Train Loss: 0.0777, Val Loss: 0.0723\n",
      "Epoch 24/300 - Train Loss: 0.0770, Val Loss: 0.0689\n",
      "Epoch 25/300 - Train Loss: 0.0745, Val Loss: 0.0699\n",
      "Epoch 26/300 - Train Loss: 0.0759, Val Loss: 0.0716\n",
      "Epoch 27/300 - Train Loss: 0.0741, Val Loss: 0.0768\n",
      "Epoch 28/300 - Train Loss: 0.0758, Val Loss: 0.0698\n",
      "Epoch 29/300 - Train Loss: 0.0750, Val Loss: 0.0714\n",
      "Epoch 30/300 - Train Loss: 0.0760, Val Loss: 0.0706\n",
      "Epoch 31/300 - Train Loss: 0.0736, Val Loss: 0.0683\n",
      "Epoch 32/300 - Train Loss: 0.0746, Val Loss: 0.0746\n",
      "Epoch 33/300 - Train Loss: 0.0744, Val Loss: 0.0703\n",
      "Epoch 34/300 - Train Loss: 0.0742, Val Loss: 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:11:05,221] Trial 371 finished with value: 0.9701951649297458 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.13817088744051478, 'learning_rate': 0.0005052421041500623, 'batch_size': 32, 'weight_decay': 0.00028603970663311886}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/300 - Train Loss: 0.0727, Val Loss: 0.0695\n",
      "Early stopping at epoch 35\n",
      "Macro F1 Score: 0.9702, Macro Precision: 0.9639, Macro Recall: 0.9768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 373\n",
      "Training with F1=32, F2=8, D=2, dropout=0.17122650038416937, LR=0.0009566233823623613, BS=32, WD=0.00020006092666627405\n",
      "Epoch 1/300 - Train Loss: 0.1524, Val Loss: 0.1099\n",
      "Epoch 2/300 - Train Loss: 0.1025, Val Loss: 0.0750\n",
      "Epoch 3/300 - Train Loss: 0.0960, Val Loss: 0.0843\n",
      "Epoch 4/300 - Train Loss: 0.0928, Val Loss: 0.0720\n",
      "Epoch 5/300 - Train Loss: 0.0902, Val Loss: 0.0729\n",
      "Epoch 6/300 - Train Loss: 0.0900, Val Loss: 0.0786\n",
      "Epoch 7/300 - Train Loss: 0.0878, Val Loss: 0.1004\n",
      "Epoch 8/300 - Train Loss: 0.0874, Val Loss: 0.0694\n",
      "Epoch 9/300 - Train Loss: 0.0859, Val Loss: 0.0714\n",
      "Epoch 10/300 - Train Loss: 0.0874, Val Loss: 0.0721\n",
      "Epoch 11/300 - Train Loss: 0.0832, Val Loss: 0.0681\n",
      "Epoch 12/300 - Train Loss: 0.0848, Val Loss: 0.0792\n",
      "Epoch 13/300 - Train Loss: 0.0827, Val Loss: 0.0699\n",
      "Epoch 14/300 - Train Loss: 0.0834, Val Loss: 0.0757\n",
      "Epoch 15/300 - Train Loss: 0.0840, Val Loss: 0.0727\n",
      "Epoch 16/300 - Train Loss: 0.0790, Val Loss: 0.0687\n",
      "Epoch 17/300 - Train Loss: 0.0818, Val Loss: 0.0690\n",
      "Epoch 18/300 - Train Loss: 0.0821, Val Loss: 0.0742\n",
      "Epoch 19/300 - Train Loss: 0.0834, Val Loss: 0.0721\n",
      "Epoch 20/300 - Train Loss: 0.0819, Val Loss: 0.0812\n",
      "Epoch 21/300 - Train Loss: 0.0837, Val Loss: 0.0735\n",
      "Epoch 22/300 - Train Loss: 0.0788, Val Loss: 0.0722\n",
      "Epoch 23/300 - Train Loss: 0.0796, Val Loss: 0.0714\n",
      "Epoch 24/300 - Train Loss: 0.0791, Val Loss: 0.0662\n",
      "Epoch 25/300 - Train Loss: 0.0787, Val Loss: 0.0682\n",
      "Epoch 26/300 - Train Loss: 0.0779, Val Loss: 0.0738\n",
      "Epoch 27/300 - Train Loss: 0.0786, Val Loss: 0.0689\n",
      "Epoch 28/300 - Train Loss: 0.0769, Val Loss: 0.0688\n",
      "Epoch 29/300 - Train Loss: 0.0799, Val Loss: 0.0689\n",
      "Epoch 30/300 - Train Loss: 0.0771, Val Loss: 0.0677\n",
      "Epoch 31/300 - Train Loss: 0.0762, Val Loss: 0.0674\n",
      "Epoch 32/300 - Train Loss: 0.0766, Val Loss: 0.0669\n",
      "Epoch 33/300 - Train Loss: 0.0787, Val Loss: 0.0690\n",
      "Epoch 34/300 - Train Loss: 0.0788, Val Loss: 0.0798\n",
      "Epoch 35/300 - Train Loss: 0.0771, Val Loss: 0.0704\n",
      "Epoch 36/300 - Train Loss: 0.0777, Val Loss: 0.0696\n",
      "Epoch 37/300 - Train Loss: 0.0775, Val Loss: 0.0664\n",
      "Epoch 38/300 - Train Loss: 0.0791, Val Loss: 0.0836\n",
      "Epoch 39/300 - Train Loss: 0.0776, Val Loss: 0.0736\n",
      "Epoch 40/300 - Train Loss: 0.0770, Val Loss: 0.0696\n",
      "Epoch 41/300 - Train Loss: 0.0786, Val Loss: 0.0707\n",
      "Epoch 42/300 - Train Loss: 0.0752, Val Loss: 0.0754\n",
      "Epoch 43/300 - Train Loss: 0.0769, Val Loss: 0.0757\n",
      "Epoch 44/300 - Train Loss: 0.0775, Val Loss: 0.0741\n",
      "Epoch 45/300 - Train Loss: 0.0762, Val Loss: 0.0971\n",
      "Epoch 46/300 - Train Loss: 0.0741, Val Loss: 0.0793\n",
      "Epoch 47/300 - Train Loss: 0.0738, Val Loss: 0.0711\n",
      "Epoch 48/300 - Train Loss: 0.0752, Val Loss: 0.0699\n",
      "Epoch 49/300 - Train Loss: 0.0752, Val Loss: 0.0689\n",
      "Epoch 50/300 - Train Loss: 0.0755, Val Loss: 0.0685\n",
      "Epoch 51/300 - Train Loss: 0.0775, Val Loss: 0.0685\n",
      "Epoch 52/300 - Train Loss: 0.0765, Val Loss: 0.0682\n",
      "Epoch 53/300 - Train Loss: 0.0754, Val Loss: 0.0731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:13:21,512] Trial 372 finished with value: 0.969101454788611 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.17122650038416937, 'learning_rate': 0.0009566233823623613, 'batch_size': 32, 'weight_decay': 0.00020006092666627405}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/300 - Train Loss: 0.0720, Val Loss: 0.0751\n",
      "Early stopping at epoch 54\n",
      "Macro F1 Score: 0.9691, Macro Precision: 0.9612, Macro Recall: 0.9777\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 374\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1509812753932828, LR=0.0007263864033425966, BS=32, WD=3.848365546782461e-05\n",
      "Epoch 1/300 - Train Loss: 0.1572, Val Loss: 0.1860\n",
      "Epoch 2/300 - Train Loss: 0.0980, Val Loss: 0.0950\n",
      "Epoch 3/300 - Train Loss: 0.0927, Val Loss: 0.0812\n",
      "Epoch 4/300 - Train Loss: 0.0896, Val Loss: 0.0676\n",
      "Epoch 5/300 - Train Loss: 0.0859, Val Loss: 0.0766\n",
      "Epoch 6/300 - Train Loss: 0.0850, Val Loss: 0.0781\n",
      "Epoch 7/300 - Train Loss: 0.0816, Val Loss: 0.0764\n",
      "Epoch 8/300 - Train Loss: 0.0842, Val Loss: 0.0809\n",
      "Epoch 9/300 - Train Loss: 0.0815, Val Loss: 0.0719\n",
      "Epoch 10/300 - Train Loss: 0.0786, Val Loss: 0.0726\n",
      "Epoch 11/300 - Train Loss: 0.0833, Val Loss: 0.0913\n",
      "Epoch 12/300 - Train Loss: 0.0781, Val Loss: 0.0837\n",
      "Epoch 13/300 - Train Loss: 0.0787, Val Loss: 0.0770\n",
      "Epoch 14/300 - Train Loss: 0.0772, Val Loss: 0.0739\n",
      "Epoch 15/300 - Train Loss: 0.0745, Val Loss: 0.0774\n",
      "Epoch 16/300 - Train Loss: 0.0795, Val Loss: 0.0765\n",
      "Epoch 17/300 - Train Loss: 0.0758, Val Loss: 0.0760\n",
      "Epoch 18/300 - Train Loss: 0.0754, Val Loss: 0.0690\n",
      "Epoch 19/300 - Train Loss: 0.0733, Val Loss: 0.0697\n",
      "Epoch 20/300 - Train Loss: 0.0753, Val Loss: 0.0709\n",
      "Epoch 21/300 - Train Loss: 0.0771, Val Loss: 0.0756\n",
      "Epoch 22/300 - Train Loss: 0.0736, Val Loss: 0.0740\n",
      "Epoch 23/300 - Train Loss: 0.0713, Val Loss: 0.0721\n",
      "Epoch 24/300 - Train Loss: 0.0737, Val Loss: 0.0779\n",
      "Epoch 25/300 - Train Loss: 0.0724, Val Loss: 0.0861\n",
      "Epoch 26/300 - Train Loss: 0.0730, Val Loss: 0.0733\n",
      "Epoch 27/300 - Train Loss: 0.0707, Val Loss: 0.0817\n",
      "Epoch 28/300 - Train Loss: 0.0704, Val Loss: 0.0689\n",
      "Epoch 29/300 - Train Loss: 0.0695, Val Loss: 0.0725\n",
      "Epoch 30/300 - Train Loss: 0.0723, Val Loss: 0.0739\n",
      "Epoch 31/300 - Train Loss: 0.0672, Val Loss: 0.0761\n",
      "Epoch 32/300 - Train Loss: 0.0676, Val Loss: 0.0728\n",
      "Epoch 33/300 - Train Loss: 0.0666, Val Loss: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:14:47,460] Trial 373 finished with value: 0.9677062589730072 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1509812753932828, 'learning_rate': 0.0007263864033425966, 'batch_size': 32, 'weight_decay': 3.848365546782461e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300 - Train Loss: 0.0686, Val Loss: 0.0737\n",
      "Early stopping at epoch 34\n",
      "Macro F1 Score: 0.9677, Macro Precision: 0.9640, Macro Recall: 0.9717\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 375\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1842495032145528, LR=0.0008837834843319443, BS=32, WD=0.00016227349358622123\n",
      "Epoch 1/300 - Train Loss: 0.1667, Val Loss: 0.0814\n",
      "Epoch 2/300 - Train Loss: 0.1029, Val Loss: 0.1412\n",
      "Epoch 3/300 - Train Loss: 0.0954, Val Loss: 0.1346\n",
      "Epoch 4/300 - Train Loss: 0.0924, Val Loss: 0.0695\n",
      "Epoch 5/300 - Train Loss: 0.0880, Val Loss: 0.0803\n",
      "Epoch 6/300 - Train Loss: 0.0879, Val Loss: 0.0772\n",
      "Epoch 7/300 - Train Loss: 0.0871, Val Loss: 0.0731\n",
      "Epoch 8/300 - Train Loss: 0.0861, Val Loss: 0.0676\n",
      "Epoch 9/300 - Train Loss: 0.0846, Val Loss: 0.0755\n",
      "Epoch 10/300 - Train Loss: 0.0842, Val Loss: 0.0784\n",
      "Epoch 11/300 - Train Loss: 0.0831, Val Loss: 0.0673\n",
      "Epoch 12/300 - Train Loss: 0.0800, Val Loss: 0.0687\n",
      "Epoch 13/300 - Train Loss: 0.0843, Val Loss: 0.0733\n",
      "Epoch 14/300 - Train Loss: 0.0810, Val Loss: 0.0631\n",
      "Epoch 15/300 - Train Loss: 0.0833, Val Loss: 0.0695\n",
      "Epoch 16/300 - Train Loss: 0.0816, Val Loss: 0.0668\n",
      "Epoch 17/300 - Train Loss: 0.0804, Val Loss: 0.0638\n",
      "Epoch 18/300 - Train Loss: 0.0794, Val Loss: 0.0759\n",
      "Epoch 19/300 - Train Loss: 0.0819, Val Loss: 0.0779\n",
      "Epoch 20/300 - Train Loss: 0.0800, Val Loss: 0.0703\n",
      "Epoch 21/300 - Train Loss: 0.0766, Val Loss: 0.0711\n",
      "Epoch 22/300 - Train Loss: 0.0774, Val Loss: 0.0623\n",
      "Epoch 23/300 - Train Loss: 0.0793, Val Loss: 0.0656\n",
      "Epoch 24/300 - Train Loss: 0.0784, Val Loss: 0.0639\n",
      "Epoch 25/300 - Train Loss: 0.0764, Val Loss: 0.0638\n",
      "Epoch 26/300 - Train Loss: 0.0765, Val Loss: 0.0615\n",
      "Epoch 27/300 - Train Loss: 0.0788, Val Loss: 0.0648\n",
      "Epoch 28/300 - Train Loss: 0.0757, Val Loss: 0.0673\n",
      "Epoch 29/300 - Train Loss: 0.0770, Val Loss: 0.0697\n",
      "Epoch 30/300 - Train Loss: 0.0776, Val Loss: 0.0719\n",
      "Epoch 31/300 - Train Loss: 0.0753, Val Loss: 0.0653\n",
      "Epoch 32/300 - Train Loss: 0.0765, Val Loss: 0.0877\n",
      "Epoch 33/300 - Train Loss: 0.0758, Val Loss: 0.0737\n",
      "Epoch 34/300 - Train Loss: 0.0730, Val Loss: 0.0669\n",
      "Epoch 35/300 - Train Loss: 0.0746, Val Loss: 0.0647\n",
      "Epoch 36/300 - Train Loss: 0.0761, Val Loss: 0.0694\n",
      "Epoch 37/300 - Train Loss: 0.0743, Val Loss: 0.0679\n",
      "Epoch 38/300 - Train Loss: 0.0730, Val Loss: 0.0743\n",
      "Epoch 39/300 - Train Loss: 0.0731, Val Loss: 0.0693\n",
      "Epoch 40/300 - Train Loss: 0.0716, Val Loss: 0.0649\n",
      "Epoch 41/300 - Train Loss: 0.0738, Val Loss: 0.0660\n",
      "Epoch 42/300 - Train Loss: 0.0733, Val Loss: 0.0652\n",
      "Epoch 43/300 - Train Loss: 0.0731, Val Loss: 0.0681\n",
      "Epoch 44/300 - Train Loss: 0.0734, Val Loss: 0.0686\n",
      "Epoch 45/300 - Train Loss: 0.0731, Val Loss: 0.0703\n",
      "Epoch 46/300 - Train Loss: 0.0717, Val Loss: 0.0661\n",
      "Epoch 47/300 - Train Loss: 0.0728, Val Loss: 0.0646\n",
      "Epoch 48/300 - Train Loss: 0.0712, Val Loss: 0.0670\n",
      "Epoch 49/300 - Train Loss: 0.0694, Val Loss: 0.0708\n",
      "Epoch 50/300 - Train Loss: 0.0721, Val Loss: 0.0664\n",
      "Epoch 51/300 - Train Loss: 0.0733, Val Loss: 0.0643\n",
      "Epoch 52/300 - Train Loss: 0.0702, Val Loss: 0.0672\n",
      "Epoch 53/300 - Train Loss: 0.0714, Val Loss: 0.0747\n",
      "Epoch 54/300 - Train Loss: 0.0704, Val Loss: 0.0684\n",
      "Epoch 55/300 - Train Loss: 0.0725, Val Loss: 0.0743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:17:09,561] Trial 374 finished with value: 0.965635170373066 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1842495032145528, 'learning_rate': 0.0008837834843319443, 'batch_size': 32, 'weight_decay': 0.00016227349358622123}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300 - Train Loss: 0.0722, Val Loss: 0.0700\n",
      "Early stopping at epoch 56\n",
      "Macro F1 Score: 0.9656, Macro Precision: 0.9684, Macro Recall: 0.9630\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.93      0.92      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 376\n",
      "Training with F1=32, F2=8, D=2, dropout=0.12107874289567211, LR=0.0005691788880422673, BS=32, WD=0.00026326901343330677\n",
      "Epoch 1/300 - Train Loss: 0.1633, Val Loss: 0.0876\n",
      "Epoch 2/300 - Train Loss: 0.1012, Val Loss: 0.0782\n",
      "Epoch 3/300 - Train Loss: 0.0939, Val Loss: 0.0834\n",
      "Epoch 4/300 - Train Loss: 0.0891, Val Loss: 0.0708\n",
      "Epoch 5/300 - Train Loss: 0.0888, Val Loss: 0.0824\n",
      "Epoch 6/300 - Train Loss: 0.0856, Val Loss: 0.0772\n",
      "Epoch 7/300 - Train Loss: 0.0854, Val Loss: 0.0830\n",
      "Epoch 8/300 - Train Loss: 0.0841, Val Loss: 0.0814\n",
      "Epoch 9/300 - Train Loss: 0.0840, Val Loss: 0.0731\n",
      "Epoch 10/300 - Train Loss: 0.0813, Val Loss: 0.0706\n",
      "Epoch 11/300 - Train Loss: 0.0797, Val Loss: 0.0812\n",
      "Epoch 12/300 - Train Loss: 0.0787, Val Loss: 0.0642\n",
      "Epoch 13/300 - Train Loss: 0.0816, Val Loss: 0.0683\n",
      "Epoch 14/300 - Train Loss: 0.0772, Val Loss: 0.0665\n",
      "Epoch 15/300 - Train Loss: 0.0784, Val Loss: 0.0762\n",
      "Epoch 16/300 - Train Loss: 0.0775, Val Loss: 0.0706\n",
      "Epoch 17/300 - Train Loss: 0.0788, Val Loss: 0.0703\n",
      "Epoch 18/300 - Train Loss: 0.0798, Val Loss: 0.0769\n",
      "Epoch 19/300 - Train Loss: 0.0788, Val Loss: 0.0683\n",
      "Epoch 20/300 - Train Loss: 0.0765, Val Loss: 0.0728\n",
      "Epoch 21/300 - Train Loss: 0.0784, Val Loss: 0.0821\n",
      "Epoch 22/300 - Train Loss: 0.0751, Val Loss: 0.0760\n",
      "Epoch 23/300 - Train Loss: 0.0760, Val Loss: 0.0884\n",
      "Epoch 24/300 - Train Loss: 0.0780, Val Loss: 0.0723\n",
      "Epoch 25/300 - Train Loss: 0.0778, Val Loss: 0.0697\n",
      "Epoch 26/300 - Train Loss: 0.0750, Val Loss: 0.0683\n",
      "Epoch 27/300 - Train Loss: 0.0750, Val Loss: 0.0768\n",
      "Epoch 28/300 - Train Loss: 0.0730, Val Loss: 0.0678\n",
      "Epoch 29/300 - Train Loss: 0.0754, Val Loss: 0.0810\n",
      "Epoch 30/300 - Train Loss: 0.0728, Val Loss: 0.0720\n",
      "Epoch 31/300 - Train Loss: 0.0767, Val Loss: 0.0786\n",
      "Epoch 32/300 - Train Loss: 0.0732, Val Loss: 0.0737\n",
      "Epoch 33/300 - Train Loss: 0.0749, Val Loss: 0.0689\n",
      "Epoch 34/300 - Train Loss: 0.0710, Val Loss: 0.0699\n",
      "Epoch 35/300 - Train Loss: 0.0747, Val Loss: 0.0657\n",
      "Epoch 36/300 - Train Loss: 0.0717, Val Loss: 0.0706\n",
      "Epoch 37/300 - Train Loss: 0.0714, Val Loss: 0.0681\n",
      "Epoch 38/300 - Train Loss: 0.0759, Val Loss: 0.0857\n",
      "Epoch 39/300 - Train Loss: 0.0736, Val Loss: 0.0718\n",
      "Epoch 40/300 - Train Loss: 0.0726, Val Loss: 0.0665\n",
      "Epoch 41/300 - Train Loss: 0.0709, Val Loss: 0.0693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:18:56,138] Trial 375 finished with value: 0.9725039046617464 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.12107874289567211, 'learning_rate': 0.0005691788880422673, 'batch_size': 32, 'weight_decay': 0.00026326901343330677}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300 - Train Loss: 0.0728, Val Loss: 0.0694\n",
      "Early stopping at epoch 42\n",
      "Macro F1 Score: 0.9725, Macro Precision: 0.9709, Macro Recall: 0.9742\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.98      0.99       593\n",
      "\n",
      "    accuracy                           0.99      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.99      0.99      0.99      1443\n",
      "\n",
      "\n",
      "Trial 377\n",
      "Training with F1=32, F2=8, D=2, dropout=0.13761569986406713, LR=0.0004310500538894133, BS=32, WD=2.592468554659312e-05\n",
      "Epoch 1/300 - Train Loss: 0.1800, Val Loss: 0.0770\n",
      "Epoch 2/300 - Train Loss: 0.1029, Val Loss: 0.1012\n",
      "Epoch 3/300 - Train Loss: 0.0959, Val Loss: 0.0718\n",
      "Epoch 4/300 - Train Loss: 0.0900, Val Loss: 0.0779\n",
      "Epoch 5/300 - Train Loss: 0.0893, Val Loss: 0.0774\n",
      "Epoch 6/300 - Train Loss: 0.0846, Val Loss: 0.0706\n",
      "Epoch 7/300 - Train Loss: 0.0885, Val Loss: 0.0712\n",
      "Epoch 8/300 - Train Loss: 0.0833, Val Loss: 0.0707\n",
      "Epoch 9/300 - Train Loss: 0.0813, Val Loss: 0.0792\n",
      "Epoch 10/300 - Train Loss: 0.0816, Val Loss: 0.0774\n",
      "Epoch 11/300 - Train Loss: 0.0804, Val Loss: 0.0739\n",
      "Epoch 12/300 - Train Loss: 0.0802, Val Loss: 0.0683\n",
      "Epoch 13/300 - Train Loss: 0.0815, Val Loss: 0.0699\n",
      "Epoch 14/300 - Train Loss: 0.0797, Val Loss: 0.0731\n",
      "Epoch 15/300 - Train Loss: 0.0781, Val Loss: 0.0675\n",
      "Epoch 16/300 - Train Loss: 0.0767, Val Loss: 0.0773\n",
      "Epoch 17/300 - Train Loss: 0.0760, Val Loss: 0.0674\n",
      "Epoch 18/300 - Train Loss: 0.0759, Val Loss: 0.0676\n",
      "Epoch 19/300 - Train Loss: 0.0757, Val Loss: 0.0672\n",
      "Epoch 20/300 - Train Loss: 0.0725, Val Loss: 0.0749\n",
      "Epoch 21/300 - Train Loss: 0.0745, Val Loss: 0.0636\n",
      "Epoch 22/300 - Train Loss: 0.0742, Val Loss: 0.0737\n",
      "Epoch 23/300 - Train Loss: 0.0752, Val Loss: 0.0763\n",
      "Epoch 24/300 - Train Loss: 0.0762, Val Loss: 0.0697\n",
      "Epoch 25/300 - Train Loss: 0.0732, Val Loss: 0.0657\n",
      "Epoch 26/300 - Train Loss: 0.0713, Val Loss: 0.0716\n",
      "Epoch 27/300 - Train Loss: 0.0726, Val Loss: 0.0707\n",
      "Epoch 28/300 - Train Loss: 0.0700, Val Loss: 0.0652\n",
      "Epoch 29/300 - Train Loss: 0.0712, Val Loss: 0.0641\n",
      "Epoch 30/300 - Train Loss: 0.0678, Val Loss: 0.0722\n",
      "Epoch 31/300 - Train Loss: 0.0682, Val Loss: 0.0719\n",
      "Epoch 32/300 - Train Loss: 0.0696, Val Loss: 0.0706\n",
      "Epoch 33/300 - Train Loss: 0.0683, Val Loss: 0.0691\n",
      "Epoch 34/300 - Train Loss: 0.0680, Val Loss: 0.0674\n",
      "Epoch 35/300 - Train Loss: 0.0682, Val Loss: 0.0681\n",
      "Epoch 36/300 - Train Loss: 0.0663, Val Loss: 0.0632\n",
      "Epoch 37/300 - Train Loss: 0.0661, Val Loss: 0.0670\n",
      "Epoch 38/300 - Train Loss: 0.0661, Val Loss: 0.0656\n",
      "Epoch 39/300 - Train Loss: 0.0662, Val Loss: 0.0698\n",
      "Epoch 40/300 - Train Loss: 0.0671, Val Loss: 0.0717\n",
      "Epoch 41/300 - Train Loss: 0.0662, Val Loss: 0.0724\n",
      "Epoch 42/300 - Train Loss: 0.0674, Val Loss: 0.0736\n",
      "Epoch 43/300 - Train Loss: 0.0653, Val Loss: 0.0700\n",
      "Epoch 44/300 - Train Loss: 0.0649, Val Loss: 0.0640\n",
      "Epoch 45/300 - Train Loss: 0.0665, Val Loss: 0.0650\n",
      "Epoch 46/300 - Train Loss: 0.0645, Val Loss: 0.0671\n",
      "Epoch 47/300 - Train Loss: 0.0616, Val Loss: 0.0685\n",
      "Epoch 48/300 - Train Loss: 0.0635, Val Loss: 0.0684\n",
      "Epoch 49/300 - Train Loss: 0.0616, Val Loss: 0.0672\n",
      "Epoch 50/300 - Train Loss: 0.0635, Val Loss: 0.0665\n",
      "Epoch 51/300 - Train Loss: 0.0615, Val Loss: 0.0687\n",
      "Epoch 52/300 - Train Loss: 0.0629, Val Loss: 0.0687\n",
      "Epoch 53/300 - Train Loss: 0.0620, Val Loss: 0.0651\n",
      "Epoch 54/300 - Train Loss: 0.0621, Val Loss: 0.0714\n",
      "Epoch 55/300 - Train Loss: 0.0641, Val Loss: 0.0620\n",
      "Epoch 56/300 - Train Loss: 0.0622, Val Loss: 0.0676\n",
      "Epoch 57/300 - Train Loss: 0.0604, Val Loss: 0.0667\n",
      "Epoch 58/300 - Train Loss: 0.0631, Val Loss: 0.0675\n",
      "Epoch 59/300 - Train Loss: 0.0606, Val Loss: 0.0652\n",
      "Epoch 60/300 - Train Loss: 0.0608, Val Loss: 0.0669\n",
      "Epoch 61/300 - Train Loss: 0.0581, Val Loss: 0.0679\n",
      "Epoch 62/300 - Train Loss: 0.0620, Val Loss: 0.0671\n",
      "Epoch 63/300 - Train Loss: 0.0587, Val Loss: 0.0649\n",
      "Epoch 64/300 - Train Loss: 0.0578, Val Loss: 0.0671\n",
      "Epoch 65/300 - Train Loss: 0.0605, Val Loss: 0.0700\n",
      "Epoch 66/300 - Train Loss: 0.0572, Val Loss: 0.0674\n",
      "Epoch 67/300 - Train Loss: 0.0606, Val Loss: 0.0712\n",
      "Epoch 68/300 - Train Loss: 0.0601, Val Loss: 0.0669\n",
      "Epoch 69/300 - Train Loss: 0.0600, Val Loss: 0.0742\n",
      "Epoch 70/300 - Train Loss: 0.0585, Val Loss: 0.0785\n",
      "Epoch 71/300 - Train Loss: 0.0614, Val Loss: 0.0653\n",
      "Epoch 72/300 - Train Loss: 0.0579, Val Loss: 0.0682\n",
      "Epoch 73/300 - Train Loss: 0.0592, Val Loss: 0.0722\n",
      "Epoch 74/300 - Train Loss: 0.0572, Val Loss: 0.0699\n",
      "Epoch 75/300 - Train Loss: 0.0565, Val Loss: 0.0718\n",
      "Epoch 76/300 - Train Loss: 0.0562, Val Loss: 0.0683\n",
      "Epoch 77/300 - Train Loss: 0.0575, Val Loss: 0.0643\n",
      "Epoch 78/300 - Train Loss: 0.0589, Val Loss: 0.0746\n",
      "Epoch 79/300 - Train Loss: 0.0567, Val Loss: 0.0656\n",
      "Epoch 80/300 - Train Loss: 0.0574, Val Loss: 0.0658\n",
      "Epoch 81/300 - Train Loss: 0.0549, Val Loss: 0.0708\n",
      "Epoch 82/300 - Train Loss: 0.0591, Val Loss: 0.0682\n",
      "Epoch 83/300 - Train Loss: 0.0565, Val Loss: 0.0724\n",
      "Epoch 84/300 - Train Loss: 0.0562, Val Loss: 0.0692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:22:32,398] Trial 376 finished with value: 0.9686348685138482 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.13761569986406713, 'learning_rate': 0.0004310500538894133, 'batch_size': 32, 'weight_decay': 2.592468554659312e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300 - Train Loss: 0.0567, Val Loss: 0.0675\n",
      "Early stopping at epoch 85\n",
      "Macro F1 Score: 0.9686, Macro Precision: 0.9668, Macro Recall: 0.9705\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 378\n",
      "Training with F1=4, F2=8, D=2, dropout=0.16053823785872567, LR=0.00041017773810213775, BS=256, WD=3.536995155057059e-05\n",
      "Epoch 1/300 - Train Loss: 0.6294, Val Loss: 0.2360\n",
      "Epoch 2/300 - Train Loss: 0.1877, Val Loss: 0.1480\n",
      "Epoch 3/300 - Train Loss: 0.1517, Val Loss: 0.1210\n",
      "Epoch 4/300 - Train Loss: 0.1322, Val Loss: 0.1092\n",
      "Epoch 5/300 - Train Loss: 0.1212, Val Loss: 0.0991\n",
      "Epoch 6/300 - Train Loss: 0.1154, Val Loss: 0.0936\n",
      "Epoch 7/300 - Train Loss: 0.1089, Val Loss: 0.0887\n",
      "Epoch 8/300 - Train Loss: 0.1016, Val Loss: 0.0846\n",
      "Epoch 9/300 - Train Loss: 0.0963, Val Loss: 0.0812\n",
      "Epoch 10/300 - Train Loss: 0.0940, Val Loss: 0.0801\n",
      "Epoch 11/300 - Train Loss: 0.0922, Val Loss: 0.0801\n",
      "Epoch 12/300 - Train Loss: 0.0910, Val Loss: 0.0774\n",
      "Epoch 13/300 - Train Loss: 0.0907, Val Loss: 0.0766\n",
      "Epoch 14/300 - Train Loss: 0.0924, Val Loss: 0.0789\n",
      "Epoch 15/300 - Train Loss: 0.0899, Val Loss: 0.0757\n",
      "Epoch 16/300 - Train Loss: 0.0889, Val Loss: 0.0772\n",
      "Epoch 17/300 - Train Loss: 0.0883, Val Loss: 0.0750\n",
      "Epoch 18/300 - Train Loss: 0.0887, Val Loss: 0.0743\n",
      "Epoch 19/300 - Train Loss: 0.0888, Val Loss: 0.0743\n",
      "Epoch 20/300 - Train Loss: 0.0903, Val Loss: 0.0738\n",
      "Epoch 21/300 - Train Loss: 0.0879, Val Loss: 0.0756\n",
      "Epoch 22/300 - Train Loss: 0.0854, Val Loss: 0.0732\n",
      "Epoch 23/300 - Train Loss: 0.0857, Val Loss: 0.0739\n",
      "Epoch 24/300 - Train Loss: 0.0853, Val Loss: 0.0745\n",
      "Epoch 25/300 - Train Loss: 0.0859, Val Loss: 0.0751\n",
      "Epoch 26/300 - Train Loss: 0.0846, Val Loss: 0.0777\n",
      "Epoch 27/300 - Train Loss: 0.0851, Val Loss: 0.0741\n",
      "Epoch 28/300 - Train Loss: 0.0880, Val Loss: 0.0751\n",
      "Epoch 29/300 - Train Loss: 0.0831, Val Loss: 0.0735\n",
      "Epoch 30/300 - Train Loss: 0.0843, Val Loss: 0.0724\n",
      "Epoch 31/300 - Train Loss: 0.0850, Val Loss: 0.0735\n",
      "Epoch 32/300 - Train Loss: 0.0839, Val Loss: 0.0721\n",
      "Epoch 33/300 - Train Loss: 0.0850, Val Loss: 0.0758\n",
      "Epoch 34/300 - Train Loss: 0.0840, Val Loss: 0.0742\n",
      "Epoch 35/300 - Train Loss: 0.0825, Val Loss: 0.0746\n",
      "Epoch 36/300 - Train Loss: 0.0833, Val Loss: 0.0751\n",
      "Epoch 37/300 - Train Loss: 0.0834, Val Loss: 0.0735\n",
      "Epoch 38/300 - Train Loss: 0.0829, Val Loss: 0.0733\n",
      "Epoch 39/300 - Train Loss: 0.0824, Val Loss: 0.0744\n",
      "Epoch 40/300 - Train Loss: 0.0824, Val Loss: 0.0771\n",
      "Epoch 41/300 - Train Loss: 0.0813, Val Loss: 0.0738\n",
      "Epoch 42/300 - Train Loss: 0.0825, Val Loss: 0.0739\n",
      "Epoch 43/300 - Train Loss: 0.0795, Val Loss: 0.0743\n",
      "Epoch 44/300 - Train Loss: 0.0808, Val Loss: 0.0759\n",
      "Epoch 45/300 - Train Loss: 0.0811, Val Loss: 0.0747\n",
      "Epoch 46/300 - Train Loss: 0.0815, Val Loss: 0.0720\n",
      "Epoch 47/300 - Train Loss: 0.0807, Val Loss: 0.0754\n",
      "Epoch 48/300 - Train Loss: 0.0816, Val Loss: 0.0751\n",
      "Epoch 49/300 - Train Loss: 0.0797, Val Loss: 0.0735\n",
      "Epoch 50/300 - Train Loss: 0.0803, Val Loss: 0.0738\n",
      "Epoch 51/300 - Train Loss: 0.0794, Val Loss: 0.0746\n",
      "Epoch 52/300 - Train Loss: 0.0793, Val Loss: 0.0730\n",
      "Epoch 53/300 - Train Loss: 0.0783, Val Loss: 0.0751\n",
      "Epoch 54/300 - Train Loss: 0.0792, Val Loss: 0.0758\n",
      "Epoch 55/300 - Train Loss: 0.0790, Val Loss: 0.0726\n",
      "Epoch 56/300 - Train Loss: 0.0795, Val Loss: 0.0738\n",
      "Epoch 57/300 - Train Loss: 0.0798, Val Loss: 0.0722\n",
      "Epoch 58/300 - Train Loss: 0.0786, Val Loss: 0.0742\n",
      "Epoch 59/300 - Train Loss: 0.0801, Val Loss: 0.0743\n",
      "Epoch 60/300 - Train Loss: 0.0789, Val Loss: 0.0773\n",
      "Epoch 61/300 - Train Loss: 0.0783, Val Loss: 0.0751\n",
      "Epoch 62/300 - Train Loss: 0.0788, Val Loss: 0.0742\n",
      "Epoch 63/300 - Train Loss: 0.0802, Val Loss: 0.0729\n",
      "Epoch 64/300 - Train Loss: 0.0772, Val Loss: 0.0736\n",
      "Epoch 65/300 - Train Loss: 0.0778, Val Loss: 0.0739\n",
      "Epoch 66/300 - Train Loss: 0.0778, Val Loss: 0.0765\n",
      "Epoch 67/300 - Train Loss: 0.0794, Val Loss: 0.0712\n",
      "Epoch 68/300 - Train Loss: 0.0757, Val Loss: 0.0745\n",
      "Epoch 69/300 - Train Loss: 0.0802, Val Loss: 0.0731\n",
      "Epoch 70/300 - Train Loss: 0.0770, Val Loss: 0.0721\n",
      "Epoch 71/300 - Train Loss: 0.0781, Val Loss: 0.0736\n",
      "Epoch 72/300 - Train Loss: 0.0770, Val Loss: 0.0730\n",
      "Epoch 73/300 - Train Loss: 0.0781, Val Loss: 0.0737\n",
      "Epoch 74/300 - Train Loss: 0.0765, Val Loss: 0.0760\n",
      "Epoch 75/300 - Train Loss: 0.0769, Val Loss: 0.0731\n",
      "Epoch 76/300 - Train Loss: 0.0780, Val Loss: 0.0732\n",
      "Epoch 77/300 - Train Loss: 0.0795, Val Loss: 0.0729\n",
      "Epoch 78/300 - Train Loss: 0.0770, Val Loss: 0.0771\n",
      "Epoch 79/300 - Train Loss: 0.0764, Val Loss: 0.0775\n",
      "Epoch 80/300 - Train Loss: 0.0762, Val Loss: 0.0732\n",
      "Epoch 81/300 - Train Loss: 0.0765, Val Loss: 0.0717\n",
      "Epoch 82/300 - Train Loss: 0.0741, Val Loss: 0.0740\n",
      "Epoch 83/300 - Train Loss: 0.0764, Val Loss: 0.0701\n",
      "Epoch 84/300 - Train Loss: 0.0768, Val Loss: 0.0735\n",
      "Epoch 85/300 - Train Loss: 0.0760, Val Loss: 0.0699\n",
      "Epoch 86/300 - Train Loss: 0.0734, Val Loss: 0.0712\n",
      "Epoch 87/300 - Train Loss: 0.0750, Val Loss: 0.0735\n",
      "Epoch 88/300 - Train Loss: 0.0777, Val Loss: 0.0713\n",
      "Epoch 89/300 - Train Loss: 0.0748, Val Loss: 0.0737\n",
      "Epoch 90/300 - Train Loss: 0.0751, Val Loss: 0.0753\n",
      "Epoch 91/300 - Train Loss: 0.0757, Val Loss: 0.0718\n",
      "Epoch 92/300 - Train Loss: 0.0771, Val Loss: 0.0721\n",
      "Epoch 93/300 - Train Loss: 0.0751, Val Loss: 0.0737\n",
      "Epoch 94/300 - Train Loss: 0.0744, Val Loss: 0.0725\n",
      "Epoch 95/300 - Train Loss: 0.0752, Val Loss: 0.0748\n",
      "Epoch 96/300 - Train Loss: 0.0751, Val Loss: 0.0721\n",
      "Epoch 97/300 - Train Loss: 0.0742, Val Loss: 0.0759\n",
      "Epoch 98/300 - Train Loss: 0.0755, Val Loss: 0.0763\n",
      "Epoch 99/300 - Train Loss: 0.0767, Val Loss: 0.0748\n",
      "Epoch 100/300 - Train Loss: 0.0751, Val Loss: 0.0747\n",
      "Epoch 101/300 - Train Loss: 0.0760, Val Loss: 0.0734\n",
      "Epoch 102/300 - Train Loss: 0.0736, Val Loss: 0.0733\n",
      "Epoch 103/300 - Train Loss: 0.0750, Val Loss: 0.0762\n",
      "Epoch 104/300 - Train Loss: 0.0757, Val Loss: 0.0722\n",
      "Epoch 105/300 - Train Loss: 0.0751, Val Loss: 0.0733\n",
      "Epoch 106/300 - Train Loss: 0.0750, Val Loss: 0.0728\n",
      "Epoch 107/300 - Train Loss: 0.0757, Val Loss: 0.0733\n",
      "Epoch 108/300 - Train Loss: 0.0735, Val Loss: 0.0733\n",
      "Epoch 109/300 - Train Loss: 0.0751, Val Loss: 0.0734\n",
      "Epoch 110/300 - Train Loss: 0.0732, Val Loss: 0.0748\n",
      "Epoch 111/300 - Train Loss: 0.0764, Val Loss: 0.0773\n",
      "Epoch 112/300 - Train Loss: 0.0731, Val Loss: 0.0756\n",
      "Epoch 113/300 - Train Loss: 0.0731, Val Loss: 0.0709\n",
      "Epoch 114/300 - Train Loss: 0.0764, Val Loss: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:24:07,363] Trial 377 finished with value: 0.9675410851079905 and parameters: {'F1': 4, 'F2': 8, 'D': 2, 'dropout': 0.16053823785872567, 'learning_rate': 0.00041017773810213775, 'batch_size': 256, 'weight_decay': 3.536995155057059e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300 - Train Loss: 0.0738, Val Loss: 0.0762\n",
      "Early stopping at epoch 115\n",
      "Macro F1 Score: 0.9675, Macro Precision: 0.9598, Macro Recall: 0.9760\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 379\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1764116071690305, LR=0.0008295201048686657, BS=32, WD=0.00018955893603458994\n",
      "Epoch 1/300 - Train Loss: 0.1548, Val Loss: 0.0914\n",
      "Epoch 2/300 - Train Loss: 0.0982, Val Loss: 0.0715\n",
      "Epoch 3/300 - Train Loss: 0.0928, Val Loss: 0.1279\n",
      "Epoch 4/300 - Train Loss: 0.0888, Val Loss: 0.0784\n",
      "Epoch 5/300 - Train Loss: 0.0866, Val Loss: 0.0723\n",
      "Epoch 6/300 - Train Loss: 0.0885, Val Loss: 0.0816\n",
      "Epoch 7/300 - Train Loss: 0.0843, Val Loss: 0.0773\n",
      "Epoch 8/300 - Train Loss: 0.0841, Val Loss: 0.0681\n",
      "Epoch 9/300 - Train Loss: 0.0849, Val Loss: 0.0724\n",
      "Epoch 10/300 - Train Loss: 0.0844, Val Loss: 0.0762\n",
      "Epoch 11/300 - Train Loss: 0.0826, Val Loss: 0.0705\n",
      "Epoch 12/300 - Train Loss: 0.0834, Val Loss: 0.0740\n",
      "Epoch 13/300 - Train Loss: 0.0820, Val Loss: 0.0749\n",
      "Epoch 14/300 - Train Loss: 0.0811, Val Loss: 0.0703\n",
      "Epoch 15/300 - Train Loss: 0.0825, Val Loss: 0.0732\n",
      "Epoch 16/300 - Train Loss: 0.0811, Val Loss: 0.0686\n",
      "Epoch 17/300 - Train Loss: 0.0802, Val Loss: 0.0715\n",
      "Epoch 18/300 - Train Loss: 0.0806, Val Loss: 0.0766\n",
      "Epoch 19/300 - Train Loss: 0.0784, Val Loss: 0.0778\n",
      "Epoch 20/300 - Train Loss: 0.0786, Val Loss: 0.0720\n",
      "Epoch 21/300 - Train Loss: 0.0781, Val Loss: 0.0786\n",
      "Epoch 22/300 - Train Loss: 0.0780, Val Loss: 0.0826\n",
      "Epoch 23/300 - Train Loss: 0.0788, Val Loss: 0.0770\n",
      "Epoch 24/300 - Train Loss: 0.0786, Val Loss: 0.0769\n",
      "Epoch 25/300 - Train Loss: 0.0794, Val Loss: 0.0747\n",
      "Epoch 26/300 - Train Loss: 0.0777, Val Loss: 0.0690\n",
      "Epoch 27/300 - Train Loss: 0.0774, Val Loss: 0.0714\n",
      "Epoch 28/300 - Train Loss: 0.0801, Val Loss: 0.0731\n",
      "Epoch 29/300 - Train Loss: 0.0790, Val Loss: 0.0728\n",
      "Epoch 30/300 - Train Loss: 0.0758, Val Loss: 0.0735\n",
      "Epoch 31/300 - Train Loss: 0.0762, Val Loss: 0.0739\n",
      "Epoch 32/300 - Train Loss: 0.0768, Val Loss: 0.0788\n",
      "Epoch 33/300 - Train Loss: 0.0754, Val Loss: 0.0701\n",
      "Epoch 34/300 - Train Loss: 0.0772, Val Loss: 0.0790\n",
      "Epoch 35/300 - Train Loss: 0.0750, Val Loss: 0.0741\n",
      "Epoch 36/300 - Train Loss: 0.0751, Val Loss: 0.0770\n",
      "Epoch 37/300 - Train Loss: 0.0751, Val Loss: 0.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:25:43,920] Trial 378 finished with value: 0.9707520950157852 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1764116071690305, 'learning_rate': 0.0008295201048686657, 'batch_size': 32, 'weight_decay': 0.00018955893603458994}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300 - Train Loss: 0.0756, Val Loss: 0.0756\n",
      "Early stopping at epoch 38\n",
      "Macro F1 Score: 0.9708, Macro Precision: 0.9648, Macro Recall: 0.9771\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 380\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1498189494647674, LR=0.000726406068533781, BS=32, WD=0.0003609703021592477\n",
      "Epoch 1/300 - Train Loss: 0.1650, Val Loss: 0.0848\n",
      "Epoch 2/300 - Train Loss: 0.0970, Val Loss: 0.0863\n",
      "Epoch 3/300 - Train Loss: 0.0937, Val Loss: 0.0702\n",
      "Epoch 4/300 - Train Loss: 0.0907, Val Loss: 0.0764\n",
      "Epoch 5/300 - Train Loss: 0.0893, Val Loss: 0.0732\n",
      "Epoch 6/300 - Train Loss: 0.0879, Val Loss: 0.0717\n",
      "Epoch 7/300 - Train Loss: 0.0876, Val Loss: 0.0708\n",
      "Epoch 8/300 - Train Loss: 0.0847, Val Loss: 0.0726\n",
      "Epoch 9/300 - Train Loss: 0.0849, Val Loss: 0.0785\n",
      "Epoch 10/300 - Train Loss: 0.0846, Val Loss: 0.0713\n",
      "Epoch 11/300 - Train Loss: 0.0827, Val Loss: 0.0745\n",
      "Epoch 12/300 - Train Loss: 0.0858, Val Loss: 0.0680\n",
      "Epoch 13/300 - Train Loss: 0.0853, Val Loss: 0.0681\n",
      "Epoch 14/300 - Train Loss: 0.0837, Val Loss: 0.0684\n",
      "Epoch 15/300 - Train Loss: 0.0803, Val Loss: 0.0695\n",
      "Epoch 16/300 - Train Loss: 0.0823, Val Loss: 0.0659\n",
      "Epoch 17/300 - Train Loss: 0.0800, Val Loss: 0.0705\n",
      "Epoch 18/300 - Train Loss: 0.0816, Val Loss: 0.0701\n",
      "Epoch 19/300 - Train Loss: 0.0803, Val Loss: 0.0745\n",
      "Epoch 20/300 - Train Loss: 0.0799, Val Loss: 0.0668\n",
      "Epoch 21/300 - Train Loss: 0.0804, Val Loss: 0.0653\n",
      "Epoch 22/300 - Train Loss: 0.0801, Val Loss: 0.0687\n",
      "Epoch 23/300 - Train Loss: 0.0807, Val Loss: 0.0696\n",
      "Epoch 24/300 - Train Loss: 0.0797, Val Loss: 0.0736\n",
      "Epoch 25/300 - Train Loss: 0.0775, Val Loss: 0.0748\n",
      "Epoch 26/300 - Train Loss: 0.0794, Val Loss: 0.0760\n",
      "Epoch 27/300 - Train Loss: 0.0778, Val Loss: 0.0712\n",
      "Epoch 28/300 - Train Loss: 0.0781, Val Loss: 0.0715\n",
      "Epoch 29/300 - Train Loss: 0.0785, Val Loss: 0.0681\n",
      "Epoch 30/300 - Train Loss: 0.0783, Val Loss: 0.0750\n",
      "Epoch 31/300 - Train Loss: 0.0794, Val Loss: 0.0699\n",
      "Epoch 32/300 - Train Loss: 0.0776, Val Loss: 0.0734\n",
      "Epoch 33/300 - Train Loss: 0.0820, Val Loss: 0.0684\n",
      "Epoch 34/300 - Train Loss: 0.0756, Val Loss: 0.0648\n",
      "Epoch 35/300 - Train Loss: 0.0788, Val Loss: 0.0678\n",
      "Epoch 36/300 - Train Loss: 0.0772, Val Loss: 0.0702\n",
      "Epoch 37/300 - Train Loss: 0.0771, Val Loss: 0.0690\n",
      "Epoch 38/300 - Train Loss: 0.0778, Val Loss: 0.0688\n",
      "Epoch 39/300 - Train Loss: 0.0773, Val Loss: 0.0715\n",
      "Epoch 40/300 - Train Loss: 0.0770, Val Loss: 0.0688\n",
      "Epoch 41/300 - Train Loss: 0.0758, Val Loss: 0.0709\n",
      "Epoch 42/300 - Train Loss: 0.0765, Val Loss: 0.0661\n",
      "Epoch 43/300 - Train Loss: 0.0779, Val Loss: 0.0711\n",
      "Epoch 44/300 - Train Loss: 0.0781, Val Loss: 0.0674\n",
      "Epoch 45/300 - Train Loss: 0.0768, Val Loss: 0.0678\n",
      "Epoch 46/300 - Train Loss: 0.0776, Val Loss: 0.0701\n",
      "Epoch 47/300 - Train Loss: 0.0738, Val Loss: 0.0720\n",
      "Epoch 48/300 - Train Loss: 0.0733, Val Loss: 0.0703\n",
      "Epoch 49/300 - Train Loss: 0.0744, Val Loss: 0.0706\n",
      "Epoch 50/300 - Train Loss: 0.0740, Val Loss: 0.0793\n",
      "Epoch 51/300 - Train Loss: 0.0770, Val Loss: 0.0827\n",
      "Epoch 52/300 - Train Loss: 0.0761, Val Loss: 0.0759\n",
      "Epoch 53/300 - Train Loss: 0.0729, Val Loss: 0.0695\n",
      "Epoch 54/300 - Train Loss: 0.0745, Val Loss: 0.0762\n",
      "Epoch 55/300 - Train Loss: 0.0747, Val Loss: 0.0710\n",
      "Epoch 56/300 - Train Loss: 0.0770, Val Loss: 0.0659\n",
      "Epoch 57/300 - Train Loss: 0.0742, Val Loss: 0.0690\n",
      "Epoch 58/300 - Train Loss: 0.0776, Val Loss: 0.0843\n",
      "Epoch 59/300 - Train Loss: 0.0738, Val Loss: 0.0725\n",
      "Epoch 60/300 - Train Loss: 0.0741, Val Loss: 0.0695\n",
      "Epoch 61/300 - Train Loss: 0.0753, Val Loss: 0.0684\n",
      "Epoch 62/300 - Train Loss: 0.0772, Val Loss: 0.0724\n",
      "Epoch 63/300 - Train Loss: 0.0765, Val Loss: 0.0673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:28:26,076] Trial 379 finished with value: 0.965219979155886 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1498189494647674, 'learning_rate': 0.000726406068533781, 'batch_size': 32, 'weight_decay': 0.0003609703021592477}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/300 - Train Loss: 0.0753, Val Loss: 0.0765\n",
      "Early stopping at epoch 64\n",
      "Macro F1 Score: 0.9652, Macro Precision: 0.9731, Macro Recall: 0.9578\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.90      0.92        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 381\n",
      "Training with F1=32, F2=8, D=2, dropout=0.13452014984543034, LR=0.0006809797144418827, BS=32, WD=0.00013172756671264575\n",
      "Epoch 1/300 - Train Loss: 0.1705, Val Loss: 0.0794\n",
      "Epoch 2/300 - Train Loss: 0.0977, Val Loss: 0.0865\n",
      "Epoch 3/300 - Train Loss: 0.0896, Val Loss: 0.0747\n",
      "Epoch 4/300 - Train Loss: 0.0902, Val Loss: 0.0745\n",
      "Epoch 5/300 - Train Loss: 0.0871, Val Loss: 0.0715\n",
      "Epoch 6/300 - Train Loss: 0.0852, Val Loss: 0.0767\n",
      "Epoch 7/300 - Train Loss: 0.0840, Val Loss: 0.0743\n",
      "Epoch 8/300 - Train Loss: 0.0839, Val Loss: 0.0725\n",
      "Epoch 9/300 - Train Loss: 0.0840, Val Loss: 0.0723\n",
      "Epoch 10/300 - Train Loss: 0.0822, Val Loss: 0.0704\n",
      "Epoch 11/300 - Train Loss: 0.0797, Val Loss: 0.0707\n",
      "Epoch 12/300 - Train Loss: 0.0796, Val Loss: 0.0690\n",
      "Epoch 13/300 - Train Loss: 0.0805, Val Loss: 0.0776\n",
      "Epoch 14/300 - Train Loss: 0.0788, Val Loss: 0.0685\n",
      "Epoch 15/300 - Train Loss: 0.0763, Val Loss: 0.0671\n",
      "Epoch 16/300 - Train Loss: 0.0786, Val Loss: 0.0701\n",
      "Epoch 17/300 - Train Loss: 0.0788, Val Loss: 0.0643\n",
      "Epoch 18/300 - Train Loss: 0.0767, Val Loss: 0.0749\n",
      "Epoch 19/300 - Train Loss: 0.0785, Val Loss: 0.0695\n",
      "Epoch 20/300 - Train Loss: 0.0742, Val Loss: 0.0720\n",
      "Epoch 21/300 - Train Loss: 0.0755, Val Loss: 0.0684\n",
      "Epoch 22/300 - Train Loss: 0.0751, Val Loss: 0.0679\n",
      "Epoch 23/300 - Train Loss: 0.0749, Val Loss: 0.0717\n",
      "Epoch 24/300 - Train Loss: 0.0746, Val Loss: 0.0668\n",
      "Epoch 25/300 - Train Loss: 0.0730, Val Loss: 0.0694\n",
      "Epoch 26/300 - Train Loss: 0.0715, Val Loss: 0.0712\n",
      "Epoch 27/300 - Train Loss: 0.0724, Val Loss: 0.0694\n",
      "Epoch 28/300 - Train Loss: 0.0736, Val Loss: 0.0665\n",
      "Epoch 29/300 - Train Loss: 0.0704, Val Loss: 0.0732\n",
      "Epoch 30/300 - Train Loss: 0.0703, Val Loss: 0.0724\n",
      "Epoch 31/300 - Train Loss: 0.0711, Val Loss: 0.0676\n",
      "Epoch 32/300 - Train Loss: 0.0692, Val Loss: 0.0755\n",
      "Epoch 33/300 - Train Loss: 0.0727, Val Loss: 0.0677\n",
      "Epoch 34/300 - Train Loss: 0.0722, Val Loss: 0.0701\n",
      "Epoch 35/300 - Train Loss: 0.0682, Val Loss: 0.0679\n",
      "Epoch 36/300 - Train Loss: 0.0678, Val Loss: 0.0728\n",
      "Epoch 37/300 - Train Loss: 0.0710, Val Loss: 0.0684\n",
      "Epoch 38/300 - Train Loss: 0.0701, Val Loss: 0.0719\n",
      "Epoch 39/300 - Train Loss: 0.0697, Val Loss: 0.0745\n",
      "Epoch 40/300 - Train Loss: 0.0712, Val Loss: 0.0695\n",
      "Epoch 41/300 - Train Loss: 0.0704, Val Loss: 0.0712\n",
      "Epoch 42/300 - Train Loss: 0.0666, Val Loss: 0.0635\n",
      "Epoch 43/300 - Train Loss: 0.0676, Val Loss: 0.0672\n",
      "Epoch 44/300 - Train Loss: 0.0680, Val Loss: 0.0707\n",
      "Epoch 45/300 - Train Loss: 0.0682, Val Loss: 0.0698\n",
      "Epoch 46/300 - Train Loss: 0.0645, Val Loss: 0.0708\n",
      "Epoch 47/300 - Train Loss: 0.0638, Val Loss: 0.0712\n",
      "Epoch 48/300 - Train Loss: 0.0676, Val Loss: 0.0690\n",
      "Epoch 49/300 - Train Loss: 0.0668, Val Loss: 0.0745\n",
      "Epoch 50/300 - Train Loss: 0.0676, Val Loss: 0.0777\n",
      "Epoch 51/300 - Train Loss: 0.0662, Val Loss: 0.0694\n",
      "Epoch 52/300 - Train Loss: 0.0670, Val Loss: 0.0740\n",
      "Epoch 53/300 - Train Loss: 0.0669, Val Loss: 0.0734\n",
      "Epoch 54/300 - Train Loss: 0.0654, Val Loss: 0.0676\n",
      "Epoch 55/300 - Train Loss: 0.0659, Val Loss: 0.0770\n",
      "Epoch 56/300 - Train Loss: 0.0663, Val Loss: 0.0661\n",
      "Epoch 57/300 - Train Loss: 0.0672, Val Loss: 0.0694\n",
      "Epoch 58/300 - Train Loss: 0.0649, Val Loss: 0.0680\n",
      "Epoch 59/300 - Train Loss: 0.0642, Val Loss: 0.0717\n",
      "Epoch 60/300 - Train Loss: 0.0673, Val Loss: 0.0652\n",
      "Epoch 61/300 - Train Loss: 0.0647, Val Loss: 0.0697\n",
      "Epoch 62/300 - Train Loss: 0.0672, Val Loss: 0.0646\n",
      "Epoch 63/300 - Train Loss: 0.0631, Val Loss: 0.0650\n",
      "Epoch 64/300 - Train Loss: 0.0673, Val Loss: 0.0681\n",
      "Epoch 65/300 - Train Loss: 0.0650, Val Loss: 0.0753\n",
      "Epoch 66/300 - Train Loss: 0.0641, Val Loss: 0.0699\n",
      "Epoch 67/300 - Train Loss: 0.0612, Val Loss: 0.0742\n",
      "Epoch 68/300 - Train Loss: 0.0666, Val Loss: 0.0675\n",
      "Epoch 69/300 - Train Loss: 0.0632, Val Loss: 0.0666\n",
      "Epoch 70/300 - Train Loss: 0.0621, Val Loss: 0.0647\n",
      "Epoch 71/300 - Train Loss: 0.0654, Val Loss: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:31:27,757] Trial 380 finished with value: 0.9667943151171979 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.13452014984543034, 'learning_rate': 0.0006809797144418827, 'batch_size': 32, 'weight_decay': 0.00013172756671264575}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/300 - Train Loss: 0.0634, Val Loss: 0.0665\n",
      "Early stopping at epoch 72\n",
      "Macro F1 Score: 0.9668, Macro Precision: 0.9630, Macro Recall: 0.9708\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 382\n",
      "Training with F1=32, F2=8, D=2, dropout=0.19127318692133338, LR=0.0008636643789536869, BS=32, WD=5.473346435943806e-05\n",
      "Epoch 1/300 - Train Loss: 0.1527, Val Loss: 0.0774\n",
      "Epoch 2/300 - Train Loss: 0.0997, Val Loss: 0.0746\n",
      "Epoch 3/300 - Train Loss: 0.0920, Val Loss: 0.0759\n",
      "Epoch 4/300 - Train Loss: 0.0898, Val Loss: 0.0762\n",
      "Epoch 5/300 - Train Loss: 0.0879, Val Loss: 0.0702\n",
      "Epoch 6/300 - Train Loss: 0.0875, Val Loss: 0.0795\n",
      "Epoch 7/300 - Train Loss: 0.0856, Val Loss: 0.0831\n",
      "Epoch 8/300 - Train Loss: 0.0880, Val Loss: 0.0762\n",
      "Epoch 9/300 - Train Loss: 0.0867, Val Loss: 0.0705\n",
      "Epoch 10/300 - Train Loss: 0.0836, Val Loss: 0.0667\n",
      "Epoch 11/300 - Train Loss: 0.0825, Val Loss: 0.0708\n",
      "Epoch 12/300 - Train Loss: 0.0803, Val Loss: 0.0685\n",
      "Epoch 13/300 - Train Loss: 0.0793, Val Loss: 0.0784\n",
      "Epoch 14/300 - Train Loss: 0.0821, Val Loss: 0.0750\n",
      "Epoch 15/300 - Train Loss: 0.0787, Val Loss: 0.0749\n",
      "Epoch 16/300 - Train Loss: 0.0789, Val Loss: 0.0773\n",
      "Epoch 17/300 - Train Loss: 0.0784, Val Loss: 0.0668\n",
      "Epoch 18/300 - Train Loss: 0.0777, Val Loss: 0.0697\n",
      "Epoch 19/300 - Train Loss: 0.0763, Val Loss: 0.0642\n",
      "Epoch 20/300 - Train Loss: 0.0765, Val Loss: 0.0645\n",
      "Epoch 21/300 - Train Loss: 0.0765, Val Loss: 0.0695\n",
      "Epoch 22/300 - Train Loss: 0.0766, Val Loss: 0.0774\n",
      "Epoch 23/300 - Train Loss: 0.0754, Val Loss: 0.0660\n",
      "Epoch 24/300 - Train Loss: 0.0741, Val Loss: 0.0700\n",
      "Epoch 25/300 - Train Loss: 0.0790, Val Loss: 0.0701\n",
      "Epoch 26/300 - Train Loss: 0.0745, Val Loss: 0.0704\n",
      "Epoch 27/300 - Train Loss: 0.0742, Val Loss: 0.0706\n",
      "Epoch 28/300 - Train Loss: 0.0747, Val Loss: 0.0715\n",
      "Epoch 29/300 - Train Loss: 0.0727, Val Loss: 0.0725\n",
      "Epoch 30/300 - Train Loss: 0.0747, Val Loss: 0.0704\n",
      "Epoch 31/300 - Train Loss: 0.0750, Val Loss: 0.0666\n",
      "Epoch 32/300 - Train Loss: 0.0722, Val Loss: 0.0680\n",
      "Epoch 33/300 - Train Loss: 0.0725, Val Loss: 0.0685\n",
      "Epoch 34/300 - Train Loss: 0.0716, Val Loss: 0.0691\n",
      "Epoch 35/300 - Train Loss: 0.0714, Val Loss: 0.0668\n",
      "Epoch 36/300 - Train Loss: 0.0738, Val Loss: 0.0763\n",
      "Epoch 37/300 - Train Loss: 0.0736, Val Loss: 0.0721\n",
      "Epoch 38/300 - Train Loss: 0.0748, Val Loss: 0.0690\n",
      "Epoch 39/300 - Train Loss: 0.0729, Val Loss: 0.0684\n",
      "Epoch 40/300 - Train Loss: 0.0751, Val Loss: 0.0669\n",
      "Epoch 41/300 - Train Loss: 0.0715, Val Loss: 0.0674\n",
      "Epoch 42/300 - Train Loss: 0.0697, Val Loss: 0.0710\n",
      "Epoch 43/300 - Train Loss: 0.0705, Val Loss: 0.0734\n",
      "Epoch 44/300 - Train Loss: 0.0746, Val Loss: 0.0744\n",
      "Epoch 45/300 - Train Loss: 0.0698, Val Loss: 0.0777\n",
      "Epoch 46/300 - Train Loss: 0.0705, Val Loss: 0.0669\n",
      "Epoch 47/300 - Train Loss: 0.0686, Val Loss: 0.0674\n",
      "Epoch 48/300 - Train Loss: 0.0701, Val Loss: 0.0699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:33:32,275] Trial 381 finished with value: 0.9762236650656781 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.19127318692133338, 'learning_rate': 0.0008636643789536869, 'batch_size': 32, 'weight_decay': 5.473346435943806e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/300 - Train Loss: 0.0725, Val Loss: 0.0704\n",
      "Early stopping at epoch 49\n",
      "Macro F1 Score: 0.9762, Macro Precision: 0.9795, Macro Recall: 0.9731\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 383\n",
      "Training with F1=8, F2=8, D=2, dropout=0.1909378591617453, LR=0.0009625853338755896, BS=32, WD=4.707416507233552e-05\n",
      "Epoch 1/300 - Train Loss: 0.1636, Val Loss: 0.0837\n",
      "Epoch 2/300 - Train Loss: 0.1068, Val Loss: 0.0860\n",
      "Epoch 3/300 - Train Loss: 0.1024, Val Loss: 0.0820\n",
      "Epoch 4/300 - Train Loss: 0.0986, Val Loss: 0.0764\n",
      "Epoch 5/300 - Train Loss: 0.0957, Val Loss: 0.1026\n",
      "Epoch 6/300 - Train Loss: 0.0949, Val Loss: 0.0802\n",
      "Epoch 7/300 - Train Loss: 0.0960, Val Loss: 0.0718\n",
      "Epoch 8/300 - Train Loss: 0.0934, Val Loss: 0.0737\n",
      "Epoch 9/300 - Train Loss: 0.0931, Val Loss: 0.0810\n",
      "Epoch 10/300 - Train Loss: 0.0907, Val Loss: 0.0756\n",
      "Epoch 11/300 - Train Loss: 0.0911, Val Loss: 0.0752\n",
      "Epoch 12/300 - Train Loss: 0.0907, Val Loss: 0.0769\n",
      "Epoch 13/300 - Train Loss: 0.0884, Val Loss: 0.0712\n",
      "Epoch 14/300 - Train Loss: 0.0896, Val Loss: 0.0775\n",
      "Epoch 15/300 - Train Loss: 0.0882, Val Loss: 0.0776\n",
      "Epoch 16/300 - Train Loss: 0.0864, Val Loss: 0.0768\n",
      "Epoch 17/300 - Train Loss: 0.0871, Val Loss: 0.0829\n",
      "Epoch 18/300 - Train Loss: 0.0878, Val Loss: 0.0739\n",
      "Epoch 19/300 - Train Loss: 0.0827, Val Loss: 0.0746\n",
      "Epoch 20/300 - Train Loss: 0.0863, Val Loss: 0.0689\n",
      "Epoch 21/300 - Train Loss: 0.0847, Val Loss: 0.0763\n",
      "Epoch 22/300 - Train Loss: 0.0863, Val Loss: 0.0839\n",
      "Epoch 23/300 - Train Loss: 0.0849, Val Loss: 0.0803\n",
      "Epoch 24/300 - Train Loss: 0.0846, Val Loss: 0.0748\n",
      "Epoch 25/300 - Train Loss: 0.0843, Val Loss: 0.0745\n",
      "Epoch 26/300 - Train Loss: 0.0851, Val Loss: 0.0754\n",
      "Epoch 27/300 - Train Loss: 0.0834, Val Loss: 0.0797\n",
      "Epoch 28/300 - Train Loss: 0.0837, Val Loss: 0.0741\n",
      "Epoch 29/300 - Train Loss: 0.0825, Val Loss: 0.0835\n",
      "Epoch 30/300 - Train Loss: 0.0833, Val Loss: 0.0865\n",
      "Epoch 31/300 - Train Loss: 0.0848, Val Loss: 0.0703\n",
      "Epoch 32/300 - Train Loss: 0.0824, Val Loss: 0.0830\n",
      "Epoch 33/300 - Train Loss: 0.0819, Val Loss: 0.0765\n",
      "Epoch 34/300 - Train Loss: 0.0834, Val Loss: 0.0711\n",
      "Epoch 35/300 - Train Loss: 0.0799, Val Loss: 0.0865\n",
      "Epoch 36/300 - Train Loss: 0.0804, Val Loss: 0.0729\n",
      "Epoch 37/300 - Train Loss: 0.0813, Val Loss: 0.0763\n",
      "Epoch 38/300 - Train Loss: 0.0804, Val Loss: 0.0716\n",
      "Epoch 39/300 - Train Loss: 0.0813, Val Loss: 0.0725\n",
      "Epoch 40/300 - Train Loss: 0.0822, Val Loss: 0.0766\n",
      "Epoch 41/300 - Train Loss: 0.0778, Val Loss: 0.0846\n",
      "Epoch 42/300 - Train Loss: 0.0806, Val Loss: 0.0748\n",
      "Epoch 43/300 - Train Loss: 0.0810, Val Loss: 0.0750\n",
      "Epoch 44/300 - Train Loss: 0.0827, Val Loss: 0.0830\n",
      "Epoch 45/300 - Train Loss: 0.0796, Val Loss: 0.1111\n",
      "Epoch 46/300 - Train Loss: 0.0800, Val Loss: 0.0755\n",
      "Epoch 47/300 - Train Loss: 0.0779, Val Loss: 0.0753\n",
      "Epoch 48/300 - Train Loss: 0.0786, Val Loss: 0.0779\n",
      "Epoch 49/300 - Train Loss: 0.0790, Val Loss: 0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:35:07,682] Trial 382 finished with value: 0.9693440297639809 and parameters: {'F1': 8, 'F2': 8, 'D': 2, 'dropout': 0.1909378591617453, 'learning_rate': 0.0009625853338755896, 'batch_size': 32, 'weight_decay': 4.707416507233552e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300 - Train Loss: 0.0792, Val Loss: 0.0720\n",
      "Early stopping at epoch 50\n",
      "Macro F1 Score: 0.9693, Macro Precision: 0.9634, Macro Recall: 0.9757\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 384\n",
      "Training with F1=32, F2=8, D=2, dropout=0.21375054296670123, LR=0.0008662212524655295, BS=64, WD=0.0002386294055950547\n",
      "Epoch 1/300 - Train Loss: 0.1758, Val Loss: 0.0804\n",
      "Epoch 2/300 - Train Loss: 0.0975, Val Loss: 0.0844\n",
      "Epoch 3/300 - Train Loss: 0.0893, Val Loss: 0.0781\n",
      "Epoch 4/300 - Train Loss: 0.0872, Val Loss: 0.0871\n",
      "Epoch 5/300 - Train Loss: 0.0858, Val Loss: 0.0798\n",
      "Epoch 6/300 - Train Loss: 0.0834, Val Loss: 0.0865\n",
      "Epoch 7/300 - Train Loss: 0.0830, Val Loss: 0.0726\n",
      "Epoch 8/300 - Train Loss: 0.0824, Val Loss: 0.0726\n",
      "Epoch 9/300 - Train Loss: 0.0830, Val Loss: 0.0703\n",
      "Epoch 10/300 - Train Loss: 0.0810, Val Loss: 0.0715\n",
      "Epoch 11/300 - Train Loss: 0.0798, Val Loss: 0.0731\n",
      "Epoch 12/300 - Train Loss: 0.0777, Val Loss: 0.0766\n",
      "Epoch 13/300 - Train Loss: 0.0785, Val Loss: 0.0771\n",
      "Epoch 14/300 - Train Loss: 0.0772, Val Loss: 0.0776\n",
      "Epoch 15/300 - Train Loss: 0.0765, Val Loss: 0.0798\n",
      "Epoch 16/300 - Train Loss: 0.0768, Val Loss: 0.0719\n",
      "Epoch 17/300 - Train Loss: 0.0758, Val Loss: 0.0719\n",
      "Epoch 18/300 - Train Loss: 0.0764, Val Loss: 0.0679\n",
      "Epoch 19/300 - Train Loss: 0.0757, Val Loss: 0.0684\n",
      "Epoch 20/300 - Train Loss: 0.0754, Val Loss: 0.0700\n",
      "Epoch 21/300 - Train Loss: 0.0756, Val Loss: 0.0715\n",
      "Epoch 22/300 - Train Loss: 0.0764, Val Loss: 0.0725\n",
      "Epoch 23/300 - Train Loss: 0.0746, Val Loss: 0.0778\n",
      "Epoch 24/300 - Train Loss: 0.0736, Val Loss: 0.0708\n",
      "Epoch 25/300 - Train Loss: 0.0728, Val Loss: 0.0713\n",
      "Epoch 26/300 - Train Loss: 0.0746, Val Loss: 0.0698\n",
      "Epoch 27/300 - Train Loss: 0.0740, Val Loss: 0.0766\n",
      "Epoch 28/300 - Train Loss: 0.0732, Val Loss: 0.0714\n",
      "Epoch 29/300 - Train Loss: 0.0729, Val Loss: 0.0744\n",
      "Epoch 30/300 - Train Loss: 0.0736, Val Loss: 0.0714\n",
      "Epoch 31/300 - Train Loss: 0.0750, Val Loss: 0.0724\n",
      "Epoch 32/300 - Train Loss: 0.0744, Val Loss: 0.0728\n",
      "Epoch 33/300 - Train Loss: 0.0722, Val Loss: 0.0741\n",
      "Epoch 34/300 - Train Loss: 0.0732, Val Loss: 0.0705\n",
      "Epoch 35/300 - Train Loss: 0.0718, Val Loss: 0.0744\n",
      "Epoch 36/300 - Train Loss: 0.0735, Val Loss: 0.0738\n",
      "Epoch 37/300 - Train Loss: 0.0732, Val Loss: 0.0726\n",
      "Epoch 38/300 - Train Loss: 0.0722, Val Loss: 0.0792\n",
      "Epoch 39/300 - Train Loss: 0.0718, Val Loss: 0.0678\n",
      "Epoch 40/300 - Train Loss: 0.0712, Val Loss: 0.0788\n",
      "Epoch 41/300 - Train Loss: 0.0733, Val Loss: 0.0692\n",
      "Epoch 42/300 - Train Loss: 0.0732, Val Loss: 0.0722\n",
      "Epoch 43/300 - Train Loss: 0.0717, Val Loss: 0.0728\n",
      "Epoch 44/300 - Train Loss: 0.0727, Val Loss: 0.0757\n",
      "Epoch 45/300 - Train Loss: 0.0709, Val Loss: 0.0736\n",
      "Epoch 46/300 - Train Loss: 0.0721, Val Loss: 0.0727\n",
      "Epoch 47/300 - Train Loss: 0.0710, Val Loss: 0.0703\n",
      "Epoch 48/300 - Train Loss: 0.0718, Val Loss: 0.0732\n",
      "Epoch 49/300 - Train Loss: 0.0719, Val Loss: 0.0715\n",
      "Epoch 50/300 - Train Loss: 0.0722, Val Loss: 0.0745\n",
      "Epoch 51/300 - Train Loss: 0.0718, Val Loss: 0.0732\n",
      "Epoch 52/300 - Train Loss: 0.0718, Val Loss: 0.0721\n",
      "Epoch 53/300 - Train Loss: 0.0721, Val Loss: 0.0732\n",
      "Epoch 54/300 - Train Loss: 0.0712, Val Loss: 0.0775\n",
      "Epoch 55/300 - Train Loss: 0.0720, Val Loss: 0.0697\n",
      "Epoch 56/300 - Train Loss: 0.0713, Val Loss: 0.0770\n",
      "Epoch 57/300 - Train Loss: 0.0713, Val Loss: 0.0731\n",
      "Epoch 58/300 - Train Loss: 0.0711, Val Loss: 0.0755\n",
      "Epoch 59/300 - Train Loss: 0.0712, Val Loss: 0.0697\n",
      "Epoch 60/300 - Train Loss: 0.0706, Val Loss: 0.0737\n",
      "Epoch 61/300 - Train Loss: 0.0702, Val Loss: 0.0727\n",
      "Epoch 62/300 - Train Loss: 0.0712, Val Loss: 0.0708\n",
      "Epoch 63/300 - Train Loss: 0.0689, Val Loss: 0.0710\n",
      "Epoch 64/300 - Train Loss: 0.0708, Val Loss: 0.0701\n",
      "Epoch 65/300 - Train Loss: 0.0711, Val Loss: 0.0716\n",
      "Epoch 66/300 - Train Loss: 0.0701, Val Loss: 0.0703\n",
      "Epoch 67/300 - Train Loss: 0.0686, Val Loss: 0.0704\n",
      "Epoch 68/300 - Train Loss: 0.0694, Val Loss: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:37:39,919] Trial 383 finished with value: 0.9675496461210747 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.21375054296670123, 'learning_rate': 0.0008662212524655295, 'batch_size': 64, 'weight_decay': 0.0002386294055950547}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.0696, Val Loss: 0.0758\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9675, Macro Precision: 0.9595, Macro Recall: 0.9763\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 385\n",
      "Training with F1=32, F2=8, D=2, dropout=0.19047447993088729, LR=0.0009127625655805389, BS=32, WD=4.2299274171804356e-05\n",
      "Epoch 1/300 - Train Loss: 0.1530, Val Loss: 0.0834\n",
      "Epoch 2/300 - Train Loss: 0.0999, Val Loss: 0.0812\n",
      "Epoch 3/300 - Train Loss: 0.0916, Val Loss: 0.0882\n",
      "Epoch 4/300 - Train Loss: 0.0897, Val Loss: 0.0735\n",
      "Epoch 5/300 - Train Loss: 0.0899, Val Loss: 0.0767\n",
      "Epoch 6/300 - Train Loss: 0.0876, Val Loss: 0.0723\n",
      "Epoch 7/300 - Train Loss: 0.0881, Val Loss: 0.0725\n",
      "Epoch 8/300 - Train Loss: 0.0854, Val Loss: 0.0743\n",
      "Epoch 9/300 - Train Loss: 0.0860, Val Loss: 0.0722\n",
      "Epoch 10/300 - Train Loss: 0.0839, Val Loss: 0.0746\n",
      "Epoch 11/300 - Train Loss: 0.0813, Val Loss: 0.0727\n",
      "Epoch 12/300 - Train Loss: 0.0848, Val Loss: 0.0813\n",
      "Epoch 13/300 - Train Loss: 0.0797, Val Loss: 0.0681\n",
      "Epoch 14/300 - Train Loss: 0.0814, Val Loss: 0.0795\n",
      "Epoch 15/300 - Train Loss: 0.0803, Val Loss: 0.0763\n",
      "Epoch 16/300 - Train Loss: 0.0800, Val Loss: 0.0771\n",
      "Epoch 17/300 - Train Loss: 0.0778, Val Loss: 0.0738\n",
      "Epoch 18/300 - Train Loss: 0.0809, Val Loss: 0.0764\n",
      "Epoch 19/300 - Train Loss: 0.0785, Val Loss: 0.0700\n",
      "Epoch 20/300 - Train Loss: 0.0785, Val Loss: 0.0772\n",
      "Epoch 21/300 - Train Loss: 0.0775, Val Loss: 0.0720\n",
      "Epoch 22/300 - Train Loss: 0.0748, Val Loss: 0.0729\n",
      "Epoch 23/300 - Train Loss: 0.0730, Val Loss: 0.0716\n",
      "Epoch 24/300 - Train Loss: 0.0765, Val Loss: 0.0689\n",
      "Epoch 25/300 - Train Loss: 0.0769, Val Loss: 0.0691\n",
      "Epoch 26/300 - Train Loss: 0.0757, Val Loss: 0.0711\n",
      "Epoch 27/300 - Train Loss: 0.0744, Val Loss: 0.0753\n",
      "Epoch 28/300 - Train Loss: 0.0745, Val Loss: 0.0680\n",
      "Epoch 29/300 - Train Loss: 0.0742, Val Loss: 0.0770\n",
      "Epoch 30/300 - Train Loss: 0.0729, Val Loss: 0.0729\n",
      "Epoch 31/300 - Train Loss: 0.0716, Val Loss: 0.0747\n",
      "Epoch 32/300 - Train Loss: 0.0737, Val Loss: 0.0748\n",
      "Epoch 33/300 - Train Loss: 0.0706, Val Loss: 0.0752\n",
      "Epoch 34/300 - Train Loss: 0.0722, Val Loss: 0.0722\n",
      "Epoch 35/300 - Train Loss: 0.0749, Val Loss: 0.0713\n",
      "Epoch 36/300 - Train Loss: 0.0702, Val Loss: 0.0791\n",
      "Epoch 37/300 - Train Loss: 0.0711, Val Loss: 0.0700\n",
      "Epoch 38/300 - Train Loss: 0.0721, Val Loss: 0.0732\n",
      "Epoch 39/300 - Train Loss: 0.0736, Val Loss: 0.0714\n",
      "Epoch 40/300 - Train Loss: 0.0696, Val Loss: 0.0751\n",
      "Epoch 41/300 - Train Loss: 0.0693, Val Loss: 0.0783\n",
      "Epoch 42/300 - Train Loss: 0.0744, Val Loss: 0.0841\n",
      "Epoch 43/300 - Train Loss: 0.0711, Val Loss: 0.0701\n",
      "Epoch 44/300 - Train Loss: 0.0685, Val Loss: 0.0729\n",
      "Epoch 45/300 - Train Loss: 0.0702, Val Loss: 0.0688\n",
      "Epoch 46/300 - Train Loss: 0.0688, Val Loss: 0.0777\n",
      "Epoch 47/300 - Train Loss: 0.0691, Val Loss: 0.0782\n",
      "Epoch 48/300 - Train Loss: 0.0683, Val Loss: 0.0709\n",
      "Epoch 49/300 - Train Loss: 0.0686, Val Loss: 0.0699\n",
      "Epoch 50/300 - Train Loss: 0.0687, Val Loss: 0.0673\n",
      "Epoch 51/300 - Train Loss: 0.0662, Val Loss: 0.0723\n",
      "Epoch 52/300 - Train Loss: 0.0687, Val Loss: 0.0735\n",
      "Epoch 53/300 - Train Loss: 0.0653, Val Loss: 0.0671\n",
      "Epoch 54/300 - Train Loss: 0.0666, Val Loss: 0.0779\n",
      "Epoch 55/300 - Train Loss: 0.0654, Val Loss: 0.0826\n",
      "Epoch 56/300 - Train Loss: 0.0656, Val Loss: 0.0775\n",
      "Epoch 57/300 - Train Loss: 0.0657, Val Loss: 0.0750\n",
      "Epoch 58/300 - Train Loss: 0.0648, Val Loss: 0.0670\n",
      "Epoch 59/300 - Train Loss: 0.0659, Val Loss: 0.0690\n",
      "Epoch 60/300 - Train Loss: 0.0643, Val Loss: 0.0752\n",
      "Epoch 61/300 - Train Loss: 0.0664, Val Loss: 0.0729\n",
      "Epoch 62/300 - Train Loss: 0.0659, Val Loss: 0.0674\n",
      "Epoch 63/300 - Train Loss: 0.0678, Val Loss: 0.0717\n",
      "Epoch 64/300 - Train Loss: 0.0658, Val Loss: 0.0699\n",
      "Epoch 65/300 - Train Loss: 0.0656, Val Loss: 0.0704\n",
      "Epoch 66/300 - Train Loss: 0.0665, Val Loss: 0.0692\n",
      "Epoch 67/300 - Train Loss: 0.0643, Val Loss: 0.0700\n",
      "Epoch 68/300 - Train Loss: 0.0668, Val Loss: 0.0721\n",
      "Epoch 69/300 - Train Loss: 0.0655, Val Loss: 0.0735\n",
      "Epoch 70/300 - Train Loss: 0.0649, Val Loss: 0.0679\n",
      "Epoch 71/300 - Train Loss: 0.0640, Val Loss: 0.0772\n",
      "Epoch 72/300 - Train Loss: 0.0639, Val Loss: 0.0690\n",
      "Epoch 73/300 - Train Loss: 0.0655, Val Loss: 0.0703\n",
      "Epoch 74/300 - Train Loss: 0.0660, Val Loss: 0.0749\n",
      "Epoch 75/300 - Train Loss: 0.0644, Val Loss: 0.0892\n",
      "Epoch 76/300 - Train Loss: 0.0651, Val Loss: 0.0706\n",
      "Epoch 77/300 - Train Loss: 0.0629, Val Loss: 0.0676\n",
      "Epoch 78/300 - Train Loss: 0.0624, Val Loss: 0.0731\n",
      "Epoch 79/300 - Train Loss: 0.0631, Val Loss: 0.0669\n",
      "Epoch 80/300 - Train Loss: 0.0659, Val Loss: 0.0727\n",
      "Epoch 81/300 - Train Loss: 0.0643, Val Loss: 0.0763\n",
      "Epoch 82/300 - Train Loss: 0.0637, Val Loss: 0.0734\n",
      "Epoch 83/300 - Train Loss: 0.0626, Val Loss: 0.0760\n",
      "Epoch 84/300 - Train Loss: 0.0624, Val Loss: 0.0718\n",
      "Epoch 85/300 - Train Loss: 0.0597, Val Loss: 0.0720\n",
      "Epoch 86/300 - Train Loss: 0.0602, Val Loss: 0.0683\n",
      "Epoch 87/300 - Train Loss: 0.0649, Val Loss: 0.0707\n",
      "Epoch 88/300 - Train Loss: 0.0613, Val Loss: 0.0690\n",
      "Epoch 89/300 - Train Loss: 0.0621, Val Loss: 0.0721\n",
      "Epoch 90/300 - Train Loss: 0.0620, Val Loss: 0.0738\n",
      "Epoch 91/300 - Train Loss: 0.0604, Val Loss: 0.0769\n",
      "Epoch 92/300 - Train Loss: 0.0635, Val Loss: 0.0729\n",
      "Epoch 93/300 - Train Loss: 0.0594, Val Loss: 0.0755\n",
      "Epoch 94/300 - Train Loss: 0.0602, Val Loss: 0.0673\n",
      "Epoch 95/300 - Train Loss: 0.0655, Val Loss: 0.0935\n",
      "Epoch 96/300 - Train Loss: 0.0601, Val Loss: 0.0716\n",
      "Epoch 97/300 - Train Loss: 0.0605, Val Loss: 0.0778\n",
      "Epoch 98/300 - Train Loss: 0.0638, Val Loss: 0.0742\n",
      "Epoch 99/300 - Train Loss: 0.0617, Val Loss: 0.0725\n",
      "Epoch 100/300 - Train Loss: 0.0592, Val Loss: 0.0767\n",
      "Epoch 101/300 - Train Loss: 0.0596, Val Loss: 0.0729\n",
      "Epoch 102/300 - Train Loss: 0.0578, Val Loss: 0.0709\n",
      "Epoch 103/300 - Train Loss: 0.0588, Val Loss: 0.0733\n",
      "Epoch 104/300 - Train Loss: 0.0592, Val Loss: 0.0717\n",
      "Epoch 105/300 - Train Loss: 0.0585, Val Loss: 0.0744\n",
      "Epoch 106/300 - Train Loss: 0.0616, Val Loss: 0.0767\n",
      "Epoch 107/300 - Train Loss: 0.0597, Val Loss: 0.0765\n",
      "Epoch 108/300 - Train Loss: 0.0609, Val Loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:42:13,738] Trial 384 finished with value: 0.9752479624909531 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.19047447993088729, 'learning_rate': 0.0009127625655805389, 'batch_size': 32, 'weight_decay': 4.2299274171804356e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 109/300 - Train Loss: 0.0577, Val Loss: 0.0733\n",
      "Early stopping at epoch 109\n",
      "Macro F1 Score: 0.9752, Macro Precision: 0.9690, Macro Recall: 0.9819\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.98      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 386\n",
      "Training with F1=32, F2=8, D=2, dropout=0.19767825863748387, LR=0.0007957800145966251, BS=32, WD=4.346361229671358e-05\n",
      "Epoch 1/300 - Train Loss: 0.1620, Val Loss: 0.0804\n",
      "Epoch 2/300 - Train Loss: 0.1007, Val Loss: 0.0847\n",
      "Epoch 3/300 - Train Loss: 0.0960, Val Loss: 0.0722\n",
      "Epoch 4/300 - Train Loss: 0.0943, Val Loss: 0.0829\n",
      "Epoch 5/300 - Train Loss: 0.0905, Val Loss: 0.0804\n",
      "Epoch 6/300 - Train Loss: 0.0892, Val Loss: 0.0667\n",
      "Epoch 7/300 - Train Loss: 0.0884, Val Loss: 0.0778\n",
      "Epoch 8/300 - Train Loss: 0.0867, Val Loss: 0.0700\n",
      "Epoch 9/300 - Train Loss: 0.0869, Val Loss: 0.0794\n",
      "Epoch 10/300 - Train Loss: 0.0848, Val Loss: 0.0728\n",
      "Epoch 11/300 - Train Loss: 0.0843, Val Loss: 0.0877\n",
      "Epoch 12/300 - Train Loss: 0.0827, Val Loss: 0.0692\n",
      "Epoch 13/300 - Train Loss: 0.0811, Val Loss: 0.0826\n",
      "Epoch 14/300 - Train Loss: 0.0800, Val Loss: 0.0723\n",
      "Epoch 15/300 - Train Loss: 0.0803, Val Loss: 0.0860\n",
      "Epoch 16/300 - Train Loss: 0.0787, Val Loss: 0.0701\n",
      "Epoch 17/300 - Train Loss: 0.0806, Val Loss: 0.0701\n",
      "Epoch 18/300 - Train Loss: 0.0786, Val Loss: 0.0671\n",
      "Epoch 19/300 - Train Loss: 0.0791, Val Loss: 0.0688\n",
      "Epoch 20/300 - Train Loss: 0.0794, Val Loss: 0.0699\n",
      "Epoch 21/300 - Train Loss: 0.0796, Val Loss: 0.0739\n",
      "Epoch 22/300 - Train Loss: 0.0796, Val Loss: 0.0700\n",
      "Epoch 23/300 - Train Loss: 0.0767, Val Loss: 0.0691\n",
      "Epoch 24/300 - Train Loss: 0.0757, Val Loss: 0.0739\n",
      "Epoch 25/300 - Train Loss: 0.0754, Val Loss: 0.0722\n",
      "Epoch 26/300 - Train Loss: 0.0767, Val Loss: 0.0670\n",
      "Epoch 27/300 - Train Loss: 0.0778, Val Loss: 0.0709\n",
      "Epoch 28/300 - Train Loss: 0.0757, Val Loss: 0.0708\n",
      "Epoch 29/300 - Train Loss: 0.0771, Val Loss: 0.0741\n",
      "Epoch 30/300 - Train Loss: 0.0777, Val Loss: 0.0786\n",
      "Epoch 31/300 - Train Loss: 0.0747, Val Loss: 0.0788\n",
      "Epoch 32/300 - Train Loss: 0.0739, Val Loss: 0.0666\n",
      "Epoch 33/300 - Train Loss: 0.0731, Val Loss: 0.0676\n",
      "Epoch 34/300 - Train Loss: 0.0724, Val Loss: 0.0711\n",
      "Epoch 35/300 - Train Loss: 0.0746, Val Loss: 0.0760\n",
      "Epoch 36/300 - Train Loss: 0.0755, Val Loss: 0.0780\n",
      "Epoch 37/300 - Train Loss: 0.0723, Val Loss: 0.0683\n",
      "Epoch 38/300 - Train Loss: 0.0712, Val Loss: 0.0764\n",
      "Epoch 39/300 - Train Loss: 0.0728, Val Loss: 0.0733\n",
      "Epoch 40/300 - Train Loss: 0.0736, Val Loss: 0.0655\n",
      "Epoch 41/300 - Train Loss: 0.0743, Val Loss: 0.0695\n",
      "Epoch 42/300 - Train Loss: 0.0761, Val Loss: 0.0779\n",
      "Epoch 43/300 - Train Loss: 0.0687, Val Loss: 0.0693\n",
      "Epoch 44/300 - Train Loss: 0.0716, Val Loss: 0.0762\n",
      "Epoch 45/300 - Train Loss: 0.0712, Val Loss: 0.0769\n",
      "Epoch 46/300 - Train Loss: 0.0704, Val Loss: 0.0735\n",
      "Epoch 47/300 - Train Loss: 0.0713, Val Loss: 0.0744\n",
      "Epoch 48/300 - Train Loss: 0.0694, Val Loss: 0.0683\n",
      "Epoch 49/300 - Train Loss: 0.0679, Val Loss: 0.0726\n",
      "Epoch 50/300 - Train Loss: 0.0695, Val Loss: 0.0767\n",
      "Epoch 51/300 - Train Loss: 0.0712, Val Loss: 0.0681\n",
      "Epoch 52/300 - Train Loss: 0.0675, Val Loss: 0.0739\n",
      "Epoch 53/300 - Train Loss: 0.0701, Val Loss: 0.0766\n",
      "Epoch 54/300 - Train Loss: 0.0685, Val Loss: 0.0703\n",
      "Epoch 55/300 - Train Loss: 0.0706, Val Loss: 0.0749\n",
      "Epoch 56/300 - Train Loss: 0.0707, Val Loss: 0.0749\n",
      "Epoch 57/300 - Train Loss: 0.0682, Val Loss: 0.0742\n",
      "Epoch 58/300 - Train Loss: 0.0682, Val Loss: 0.0746\n",
      "Epoch 59/300 - Train Loss: 0.0700, Val Loss: 0.0699\n",
      "Epoch 60/300 - Train Loss: 0.0685, Val Loss: 0.0690\n",
      "Epoch 61/300 - Train Loss: 0.0680, Val Loss: 0.0788\n",
      "Epoch 62/300 - Train Loss: 0.0688, Val Loss: 0.0792\n",
      "Epoch 63/300 - Train Loss: 0.0695, Val Loss: 0.0772\n",
      "Epoch 64/300 - Train Loss: 0.0677, Val Loss: 0.0741\n",
      "Epoch 65/300 - Train Loss: 0.0690, Val Loss: 0.0764\n",
      "Epoch 66/300 - Train Loss: 0.0679, Val Loss: 0.0726\n",
      "Epoch 67/300 - Train Loss: 0.0662, Val Loss: 0.0871\n",
      "Epoch 68/300 - Train Loss: 0.0669, Val Loss: 0.0754\n",
      "Epoch 69/300 - Train Loss: 0.0664, Val Loss: 0.0742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:45:10,490] Trial 385 finished with value: 0.9768569398683321 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.19767825863748387, 'learning_rate': 0.0007957800145966251, 'batch_size': 32, 'weight_decay': 4.346361229671358e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70/300 - Train Loss: 0.0689, Val Loss: 0.0785\n",
      "Early stopping at epoch 70\n",
      "Macro F1 Score: 0.9769, Macro Precision: 0.9666, Macro Recall: 0.9882\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      1.00      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.99      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 387\n",
      "Training with F1=32, F2=8, D=2, dropout=0.208739932272693, LR=0.0009863446756451647, BS=32, WD=4.271626860456865e-05\n",
      "Epoch 1/300 - Train Loss: 0.1536, Val Loss: 0.0830\n",
      "Epoch 2/300 - Train Loss: 0.1026, Val Loss: 0.0823\n",
      "Epoch 3/300 - Train Loss: 0.0959, Val Loss: 0.0743\n",
      "Epoch 4/300 - Train Loss: 0.0924, Val Loss: 0.0712\n",
      "Epoch 5/300 - Train Loss: 0.0887, Val Loss: 0.0720\n",
      "Epoch 6/300 - Train Loss: 0.0872, Val Loss: 0.0805\n",
      "Epoch 7/300 - Train Loss: 0.0865, Val Loss: 0.0732\n",
      "Epoch 8/300 - Train Loss: 0.0892, Val Loss: 0.0766\n",
      "Epoch 9/300 - Train Loss: 0.0862, Val Loss: 0.0762\n",
      "Epoch 10/300 - Train Loss: 0.0850, Val Loss: 0.0712\n",
      "Epoch 11/300 - Train Loss: 0.0833, Val Loss: 0.0749\n",
      "Epoch 12/300 - Train Loss: 0.0805, Val Loss: 0.0699\n",
      "Epoch 13/300 - Train Loss: 0.0850, Val Loss: 0.0760\n",
      "Epoch 14/300 - Train Loss: 0.0802, Val Loss: 0.0709\n",
      "Epoch 15/300 - Train Loss: 0.0800, Val Loss: 0.0716\n",
      "Epoch 16/300 - Train Loss: 0.0778, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.0802, Val Loss: 0.0714\n",
      "Epoch 18/300 - Train Loss: 0.0813, Val Loss: 0.0762\n",
      "Epoch 19/300 - Train Loss: 0.0784, Val Loss: 0.0729\n",
      "Epoch 20/300 - Train Loss: 0.0781, Val Loss: 0.0740\n",
      "Epoch 21/300 - Train Loss: 0.0784, Val Loss: 0.0700\n",
      "Epoch 22/300 - Train Loss: 0.0761, Val Loss: 0.0765\n",
      "Epoch 23/300 - Train Loss: 0.0760, Val Loss: 0.0815\n",
      "Epoch 24/300 - Train Loss: 0.0781, Val Loss: 0.0896\n",
      "Epoch 25/300 - Train Loss: 0.0778, Val Loss: 0.0752\n",
      "Epoch 26/300 - Train Loss: 0.0776, Val Loss: 0.0709\n",
      "Epoch 27/300 - Train Loss: 0.0742, Val Loss: 0.0693\n",
      "Epoch 28/300 - Train Loss: 0.0769, Val Loss: 0.0704\n",
      "Epoch 29/300 - Train Loss: 0.0740, Val Loss: 0.0711\n",
      "Epoch 30/300 - Train Loss: 0.0752, Val Loss: 0.0716\n",
      "Epoch 31/300 - Train Loss: 0.0779, Val Loss: 0.0760\n",
      "Epoch 32/300 - Train Loss: 0.0745, Val Loss: 0.0724\n",
      "Epoch 33/300 - Train Loss: 0.0752, Val Loss: 0.0749\n",
      "Epoch 34/300 - Train Loss: 0.0760, Val Loss: 0.0667\n",
      "Epoch 35/300 - Train Loss: 0.0754, Val Loss: 0.0749\n",
      "Epoch 36/300 - Train Loss: 0.0758, Val Loss: 0.0708\n",
      "Epoch 37/300 - Train Loss: 0.0749, Val Loss: 0.0796\n",
      "Epoch 38/300 - Train Loss: 0.0766, Val Loss: 0.0681\n",
      "Epoch 39/300 - Train Loss: 0.0744, Val Loss: 0.0722\n",
      "Epoch 40/300 - Train Loss: 0.0735, Val Loss: 0.0730\n",
      "Epoch 41/300 - Train Loss: 0.0717, Val Loss: 0.0706\n",
      "Epoch 42/300 - Train Loss: 0.0735, Val Loss: 0.0712\n",
      "Epoch 43/300 - Train Loss: 0.0734, Val Loss: 0.0826\n",
      "Epoch 44/300 - Train Loss: 0.0743, Val Loss: 0.0730\n",
      "Epoch 45/300 - Train Loss: 0.0746, Val Loss: 0.0759\n",
      "Epoch 46/300 - Train Loss: 0.0751, Val Loss: 0.0694\n",
      "Epoch 47/300 - Train Loss: 0.0748, Val Loss: 0.0722\n",
      "Epoch 48/300 - Train Loss: 0.0706, Val Loss: 0.0659\n",
      "Epoch 49/300 - Train Loss: 0.0739, Val Loss: 0.0678\n",
      "Epoch 50/300 - Train Loss: 0.0730, Val Loss: 0.0713\n",
      "Epoch 51/300 - Train Loss: 0.0727, Val Loss: 0.0925\n",
      "Epoch 52/300 - Train Loss: 0.0708, Val Loss: 0.0774\n",
      "Epoch 53/300 - Train Loss: 0.0730, Val Loss: 0.0691\n",
      "Epoch 54/300 - Train Loss: 0.0762, Val Loss: 0.0783\n",
      "Epoch 55/300 - Train Loss: 0.0724, Val Loss: 0.0747\n",
      "Epoch 56/300 - Train Loss: 0.0678, Val Loss: 0.0777\n",
      "Epoch 57/300 - Train Loss: 0.0691, Val Loss: 0.0707\n",
      "Epoch 58/300 - Train Loss: 0.0710, Val Loss: 0.0785\n",
      "Epoch 59/300 - Train Loss: 0.0706, Val Loss: 0.0790\n",
      "Epoch 60/300 - Train Loss: 0.0736, Val Loss: 0.0710\n",
      "Epoch 61/300 - Train Loss: 0.0740, Val Loss: 0.0796\n",
      "Epoch 62/300 - Train Loss: 0.0707, Val Loss: 0.0758\n",
      "Epoch 63/300 - Train Loss: 0.0691, Val Loss: 0.0809\n",
      "Epoch 64/300 - Train Loss: 0.0714, Val Loss: 0.0679\n",
      "Epoch 65/300 - Train Loss: 0.0695, Val Loss: 0.0693\n",
      "Epoch 66/300 - Train Loss: 0.0666, Val Loss: 0.0713\n",
      "Epoch 67/300 - Train Loss: 0.0705, Val Loss: 0.0740\n",
      "Epoch 68/300 - Train Loss: 0.0736, Val Loss: 0.0771\n",
      "Epoch 69/300 - Train Loss: 0.0693, Val Loss: 0.0774\n",
      "Epoch 70/300 - Train Loss: 0.0678, Val Loss: 0.0709\n",
      "Epoch 71/300 - Train Loss: 0.0721, Val Loss: 0.0883\n",
      "Epoch 72/300 - Train Loss: 0.0691, Val Loss: 0.0754\n",
      "Epoch 73/300 - Train Loss: 0.0701, Val Loss: 0.0728\n",
      "Epoch 74/300 - Train Loss: 0.0695, Val Loss: 0.0678\n",
      "Epoch 75/300 - Train Loss: 0.0686, Val Loss: 0.0657\n",
      "Epoch 76/300 - Train Loss: 0.0676, Val Loss: 0.0718\n",
      "Epoch 77/300 - Train Loss: 0.0713, Val Loss: 0.0678\n",
      "Epoch 78/300 - Train Loss: 0.0693, Val Loss: 0.0730\n",
      "Epoch 79/300 - Train Loss: 0.0690, Val Loss: 0.0656\n",
      "Epoch 80/300 - Train Loss: 0.0692, Val Loss: 0.0699\n",
      "Epoch 81/300 - Train Loss: 0.0671, Val Loss: 0.0719\n",
      "Epoch 82/300 - Train Loss: 0.0691, Val Loss: 0.0695\n",
      "Epoch 83/300 - Train Loss: 0.0684, Val Loss: 0.0711\n",
      "Epoch 84/300 - Train Loss: 0.0679, Val Loss: 0.0742\n",
      "Epoch 85/300 - Train Loss: 0.0647, Val Loss: 0.0736\n",
      "Epoch 86/300 - Train Loss: 0.0658, Val Loss: 0.0688\n",
      "Epoch 87/300 - Train Loss: 0.0683, Val Loss: 0.0715\n",
      "Epoch 88/300 - Train Loss: 0.0662, Val Loss: 0.0716\n",
      "Epoch 89/300 - Train Loss: 0.0667, Val Loss: 0.0755\n",
      "Epoch 90/300 - Train Loss: 0.0656, Val Loss: 0.0729\n",
      "Epoch 91/300 - Train Loss: 0.0673, Val Loss: 0.0653\n",
      "Epoch 92/300 - Train Loss: 0.0675, Val Loss: 0.0651\n",
      "Epoch 93/300 - Train Loss: 0.0634, Val Loss: 0.0672\n",
      "Epoch 94/300 - Train Loss: 0.0646, Val Loss: 0.0683\n",
      "Epoch 95/300 - Train Loss: 0.0654, Val Loss: 0.0719\n",
      "Epoch 96/300 - Train Loss: 0.0697, Val Loss: 0.0710\n",
      "Epoch 97/300 - Train Loss: 0.0669, Val Loss: 0.0756\n",
      "Epoch 98/300 - Train Loss: 0.0637, Val Loss: 0.0734\n",
      "Epoch 99/300 - Train Loss: 0.0637, Val Loss: 0.0854\n",
      "Epoch 100/300 - Train Loss: 0.0639, Val Loss: 0.0691\n",
      "Epoch 101/300 - Train Loss: 0.0658, Val Loss: 0.0714\n",
      "Epoch 102/300 - Train Loss: 0.0651, Val Loss: 0.0689\n",
      "Epoch 103/300 - Train Loss: 0.0635, Val Loss: 0.0732\n",
      "Epoch 104/300 - Train Loss: 0.0651, Val Loss: 0.0745\n",
      "Epoch 105/300 - Train Loss: 0.0647, Val Loss: 0.0692\n",
      "Epoch 106/300 - Train Loss: 0.0658, Val Loss: 0.0811\n",
      "Epoch 107/300 - Train Loss: 0.0645, Val Loss: 0.0679\n",
      "Epoch 108/300 - Train Loss: 0.0639, Val Loss: 0.0693\n",
      "Epoch 109/300 - Train Loss: 0.0650, Val Loss: 0.0675\n",
      "Epoch 110/300 - Train Loss: 0.0644, Val Loss: 0.0693\n",
      "Epoch 111/300 - Train Loss: 0.0620, Val Loss: 0.0706\n",
      "Epoch 112/300 - Train Loss: 0.0648, Val Loss: 0.0734\n",
      "Epoch 113/300 - Train Loss: 0.0641, Val Loss: 0.0695\n",
      "Epoch 114/300 - Train Loss: 0.0643, Val Loss: 0.0687\n",
      "Epoch 115/300 - Train Loss: 0.0645, Val Loss: 0.0708\n",
      "Epoch 116/300 - Train Loss: 0.0642, Val Loss: 0.0803\n",
      "Epoch 117/300 - Train Loss: 0.0621, Val Loss: 0.0659\n",
      "Epoch 118/300 - Train Loss: 0.0633, Val Loss: 0.0681\n",
      "Epoch 119/300 - Train Loss: 0.0650, Val Loss: 0.0740\n",
      "Epoch 120/300 - Train Loss: 0.0648, Val Loss: 0.0722\n",
      "Epoch 121/300 - Train Loss: 0.0621, Val Loss: 0.0703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:50:16,485] Trial 386 finished with value: 0.9755299325687211 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.208739932272693, 'learning_rate': 0.0009863446756451647, 'batch_size': 32, 'weight_decay': 4.271626860456865e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/300 - Train Loss: 0.0615, Val Loss: 0.0685\n",
      "Early stopping at epoch 122\n",
      "Macro F1 Score: 0.9755, Macro Precision: 0.9739, Macro Recall: 0.9773\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.97      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 388\n",
      "Training with F1=32, F2=8, D=2, dropout=0.20275162931243979, LR=0.0009076632541007421, BS=32, WD=4.235191321114186e-05\n",
      "Epoch 1/300 - Train Loss: 0.1541, Val Loss: 0.0804\n",
      "Epoch 2/300 - Train Loss: 0.0985, Val Loss: 0.0760\n",
      "Epoch 3/300 - Train Loss: 0.0957, Val Loss: 0.0794\n",
      "Epoch 4/300 - Train Loss: 0.0892, Val Loss: 0.0703\n",
      "Epoch 5/300 - Train Loss: 0.0883, Val Loss: 0.0835\n",
      "Epoch 6/300 - Train Loss: 0.0876, Val Loss: 0.0769\n",
      "Epoch 7/300 - Train Loss: 0.0904, Val Loss: 0.0657\n",
      "Epoch 8/300 - Train Loss: 0.0856, Val Loss: 0.0712\n",
      "Epoch 9/300 - Train Loss: 0.0823, Val Loss: 0.0704\n",
      "Epoch 10/300 - Train Loss: 0.0827, Val Loss: 0.0770\n",
      "Epoch 11/300 - Train Loss: 0.0816, Val Loss: 0.0672\n",
      "Epoch 12/300 - Train Loss: 0.0820, Val Loss: 0.0685\n",
      "Epoch 13/300 - Train Loss: 0.0830, Val Loss: 0.0691\n",
      "Epoch 14/300 - Train Loss: 0.0794, Val Loss: 0.0691\n",
      "Epoch 15/300 - Train Loss: 0.0810, Val Loss: 0.0670\n",
      "Epoch 16/300 - Train Loss: 0.0777, Val Loss: 0.0673\n",
      "Epoch 17/300 - Train Loss: 0.0804, Val Loss: 0.0669\n",
      "Epoch 18/300 - Train Loss: 0.0771, Val Loss: 0.0684\n",
      "Epoch 19/300 - Train Loss: 0.0762, Val Loss: 0.0688\n",
      "Epoch 20/300 - Train Loss: 0.0741, Val Loss: 0.0717\n",
      "Epoch 21/300 - Train Loss: 0.0766, Val Loss: 0.0675\n",
      "Epoch 22/300 - Train Loss: 0.0743, Val Loss: 0.0695\n",
      "Epoch 23/300 - Train Loss: 0.0737, Val Loss: 0.0680\n",
      "Epoch 24/300 - Train Loss: 0.0743, Val Loss: 0.0677\n",
      "Epoch 25/300 - Train Loss: 0.0764, Val Loss: 0.0679\n",
      "Epoch 26/300 - Train Loss: 0.0736, Val Loss: 0.0757\n",
      "Epoch 27/300 - Train Loss: 0.0753, Val Loss: 0.0797\n",
      "Epoch 28/300 - Train Loss: 0.0745, Val Loss: 0.0691\n",
      "Epoch 29/300 - Train Loss: 0.0722, Val Loss: 0.0723\n",
      "Epoch 30/300 - Train Loss: 0.0727, Val Loss: 0.0680\n",
      "Epoch 31/300 - Train Loss: 0.0709, Val Loss: 0.0730\n",
      "Epoch 32/300 - Train Loss: 0.0718, Val Loss: 0.0697\n",
      "Epoch 33/300 - Train Loss: 0.0717, Val Loss: 0.0708\n",
      "Epoch 34/300 - Train Loss: 0.0699, Val Loss: 0.0745\n",
      "Epoch 35/300 - Train Loss: 0.0724, Val Loss: 0.0742\n",
      "Epoch 36/300 - Train Loss: 0.0679, Val Loss: 0.0638\n",
      "Epoch 37/300 - Train Loss: 0.0707, Val Loss: 0.0702\n",
      "Epoch 38/300 - Train Loss: 0.0723, Val Loss: 0.0692\n",
      "Epoch 39/300 - Train Loss: 0.0710, Val Loss: 0.0701\n",
      "Epoch 40/300 - Train Loss: 0.0715, Val Loss: 0.0763\n",
      "Epoch 41/300 - Train Loss: 0.0706, Val Loss: 0.0704\n",
      "Epoch 42/300 - Train Loss: 0.0692, Val Loss: 0.0740\n",
      "Epoch 43/300 - Train Loss: 0.0707, Val Loss: 0.0784\n",
      "Epoch 44/300 - Train Loss: 0.0687, Val Loss: 0.0713\n",
      "Epoch 45/300 - Train Loss: 0.0682, Val Loss: 0.0733\n",
      "Epoch 46/300 - Train Loss: 0.0686, Val Loss: 0.0724\n",
      "Epoch 47/300 - Train Loss: 0.0694, Val Loss: 0.0692\n",
      "Epoch 48/300 - Train Loss: 0.0671, Val Loss: 0.0749\n",
      "Epoch 49/300 - Train Loss: 0.0689, Val Loss: 0.0713\n",
      "Epoch 50/300 - Train Loss: 0.0688, Val Loss: 0.0681\n",
      "Epoch 51/300 - Train Loss: 0.0690, Val Loss: 0.0773\n",
      "Epoch 52/300 - Train Loss: 0.0666, Val Loss: 0.0735\n",
      "Epoch 53/300 - Train Loss: 0.0689, Val Loss: 0.0694\n",
      "Epoch 54/300 - Train Loss: 0.0691, Val Loss: 0.0714\n",
      "Epoch 55/300 - Train Loss: 0.0679, Val Loss: 0.0718\n",
      "Epoch 56/300 - Train Loss: 0.0684, Val Loss: 0.0677\n",
      "Epoch 57/300 - Train Loss: 0.0675, Val Loss: 0.0715\n",
      "Epoch 58/300 - Train Loss: 0.0669, Val Loss: 0.0793\n",
      "Epoch 59/300 - Train Loss: 0.0649, Val Loss: 0.0751\n",
      "Epoch 60/300 - Train Loss: 0.0664, Val Loss: 0.0710\n",
      "Epoch 61/300 - Train Loss: 0.0655, Val Loss: 0.0667\n",
      "Epoch 62/300 - Train Loss: 0.0693, Val Loss: 0.0693\n",
      "Epoch 63/300 - Train Loss: 0.0666, Val Loss: 0.0723\n",
      "Epoch 64/300 - Train Loss: 0.0671, Val Loss: 0.0723\n",
      "Epoch 65/300 - Train Loss: 0.0652, Val Loss: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:53:01,578] Trial 387 finished with value: 0.96451822442542 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.20275162931243979, 'learning_rate': 0.0009076632541007421, 'batch_size': 32, 'weight_decay': 4.235191321114186e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/300 - Train Loss: 0.0640, Val Loss: 0.0678\n",
      "Early stopping at epoch 66\n",
      "Macro F1 Score: 0.9645, Macro Precision: 0.9582, Macro Recall: 0.9712\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 389\n",
      "Training with F1=32, F2=8, D=2, dropout=0.21107551387447765, LR=0.0009890643543036483, BS=128, WD=5.2915222646069825e-05\n",
      "Epoch 1/300 - Train Loss: 0.2317, Val Loss: 0.0917\n",
      "Epoch 2/300 - Train Loss: 0.0963, Val Loss: 0.0922\n",
      "Epoch 3/300 - Train Loss: 0.0880, Val Loss: 0.0781\n",
      "Epoch 4/300 - Train Loss: 0.0829, Val Loss: 0.0921\n",
      "Epoch 5/300 - Train Loss: 0.0814, Val Loss: 0.0779\n",
      "Epoch 6/300 - Train Loss: 0.0804, Val Loss: 0.0891\n",
      "Epoch 7/300 - Train Loss: 0.0793, Val Loss: 0.0812\n",
      "Epoch 8/300 - Train Loss: 0.0771, Val Loss: 0.0785\n",
      "Epoch 9/300 - Train Loss: 0.0779, Val Loss: 0.0735\n",
      "Epoch 10/300 - Train Loss: 0.0757, Val Loss: 0.0737\n",
      "Epoch 11/300 - Train Loss: 0.0771, Val Loss: 0.0775\n",
      "Epoch 12/300 - Train Loss: 0.0737, Val Loss: 0.0766\n",
      "Epoch 13/300 - Train Loss: 0.0742, Val Loss: 0.0785\n",
      "Epoch 14/300 - Train Loss: 0.0744, Val Loss: 0.0760\n",
      "Epoch 15/300 - Train Loss: 0.0734, Val Loss: 0.0853\n",
      "Epoch 16/300 - Train Loss: 0.0733, Val Loss: 0.0714\n",
      "Epoch 17/300 - Train Loss: 0.0721, Val Loss: 0.0753\n",
      "Epoch 18/300 - Train Loss: 0.0712, Val Loss: 0.0759\n",
      "Epoch 19/300 - Train Loss: 0.0711, Val Loss: 0.0730\n",
      "Epoch 20/300 - Train Loss: 0.0700, Val Loss: 0.0786\n",
      "Epoch 21/300 - Train Loss: 0.0717, Val Loss: 0.0814\n",
      "Epoch 22/300 - Train Loss: 0.0696, Val Loss: 0.0745\n",
      "Epoch 23/300 - Train Loss: 0.0683, Val Loss: 0.0783\n",
      "Epoch 24/300 - Train Loss: 0.0687, Val Loss: 0.0753\n",
      "Epoch 25/300 - Train Loss: 0.0703, Val Loss: 0.0739\n",
      "Epoch 26/300 - Train Loss: 0.0685, Val Loss: 0.0747\n",
      "Epoch 27/300 - Train Loss: 0.0694, Val Loss: 0.0802\n",
      "Epoch 28/300 - Train Loss: 0.0675, Val Loss: 0.0717\n",
      "Epoch 29/300 - Train Loss: 0.0685, Val Loss: 0.0723\n",
      "Epoch 30/300 - Train Loss: 0.0686, Val Loss: 0.0815\n",
      "Epoch 31/300 - Train Loss: 0.0676, Val Loss: 0.0739\n",
      "Epoch 32/300 - Train Loss: 0.0671, Val Loss: 0.0741\n",
      "Epoch 33/300 - Train Loss: 0.0664, Val Loss: 0.0765\n",
      "Epoch 34/300 - Train Loss: 0.0652, Val Loss: 0.0728\n",
      "Epoch 35/300 - Train Loss: 0.0655, Val Loss: 0.0796\n",
      "Epoch 36/300 - Train Loss: 0.0673, Val Loss: 0.0701\n",
      "Epoch 37/300 - Train Loss: 0.0659, Val Loss: 0.0724\n",
      "Epoch 38/300 - Train Loss: 0.0657, Val Loss: 0.0758\n",
      "Epoch 39/300 - Train Loss: 0.0656, Val Loss: 0.0763\n",
      "Epoch 40/300 - Train Loss: 0.0646, Val Loss: 0.0733\n",
      "Epoch 41/300 - Train Loss: 0.0638, Val Loss: 0.0730\n",
      "Epoch 42/300 - Train Loss: 0.0638, Val Loss: 0.0755\n",
      "Epoch 43/300 - Train Loss: 0.0668, Val Loss: 0.0693\n",
      "Epoch 44/300 - Train Loss: 0.0643, Val Loss: 0.0747\n",
      "Epoch 45/300 - Train Loss: 0.0634, Val Loss: 0.0744\n",
      "Epoch 46/300 - Train Loss: 0.0643, Val Loss: 0.0769\n",
      "Epoch 47/300 - Train Loss: 0.0621, Val Loss: 0.0838\n",
      "Epoch 48/300 - Train Loss: 0.0634, Val Loss: 0.0795\n",
      "Epoch 49/300 - Train Loss: 0.0629, Val Loss: 0.0733\n",
      "Epoch 50/300 - Train Loss: 0.0621, Val Loss: 0.0767\n",
      "Epoch 51/300 - Train Loss: 0.0641, Val Loss: 0.0776\n",
      "Epoch 52/300 - Train Loss: 0.0632, Val Loss: 0.0704\n",
      "Epoch 53/300 - Train Loss: 0.0618, Val Loss: 0.0736\n",
      "Epoch 54/300 - Train Loss: 0.0608, Val Loss: 0.0722\n",
      "Epoch 55/300 - Train Loss: 0.0620, Val Loss: 0.0737\n",
      "Epoch 56/300 - Train Loss: 0.0617, Val Loss: 0.0772\n",
      "Epoch 57/300 - Train Loss: 0.0608, Val Loss: 0.0747\n",
      "Epoch 58/300 - Train Loss: 0.0597, Val Loss: 0.0747\n",
      "Epoch 59/300 - Train Loss: 0.0609, Val Loss: 0.0748\n",
      "Epoch 60/300 - Train Loss: 0.0619, Val Loss: 0.0753\n",
      "Epoch 61/300 - Train Loss: 0.0621, Val Loss: 0.0757\n",
      "Epoch 62/300 - Train Loss: 0.0587, Val Loss: 0.0793\n",
      "Epoch 63/300 - Train Loss: 0.0594, Val Loss: 0.0811\n",
      "Epoch 64/300 - Train Loss: 0.0585, Val Loss: 0.0741\n",
      "Epoch 65/300 - Train Loss: 0.0597, Val Loss: 0.0772\n",
      "Epoch 66/300 - Train Loss: 0.0588, Val Loss: 0.0776\n",
      "Epoch 67/300 - Train Loss: 0.0593, Val Loss: 0.0764\n",
      "Epoch 68/300 - Train Loss: 0.0588, Val Loss: 0.0785\n",
      "Epoch 69/300 - Train Loss: 0.0578, Val Loss: 0.0827\n",
      "Epoch 70/300 - Train Loss: 0.0605, Val Loss: 0.0745\n",
      "Epoch 71/300 - Train Loss: 0.0606, Val Loss: 0.0757\n",
      "Epoch 72/300 - Train Loss: 0.0593, Val Loss: 0.0729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:55:30,260] Trial 388 finished with value: 0.9686057998078459 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.21107551387447765, 'learning_rate': 0.0009890643543036483, 'batch_size': 128, 'weight_decay': 5.2915222646069825e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/300 - Train Loss: 0.0570, Val Loss: 0.0805\n",
      "Early stopping at epoch 73\n",
      "Macro F1 Score: 0.9686, Macro Precision: 0.9608, Macro Recall: 0.9771\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 390\n",
      "Training with F1=32, F2=8, D=2, dropout=0.19464551026733984, LR=0.0008283370547128397, BS=32, WD=4.7378592725709666e-05\n",
      "Epoch 1/300 - Train Loss: 0.1618, Val Loss: 0.1111\n",
      "Epoch 2/300 - Train Loss: 0.1018, Val Loss: 0.0713\n",
      "Epoch 3/300 - Train Loss: 0.0954, Val Loss: 0.0750\n",
      "Epoch 4/300 - Train Loss: 0.0922, Val Loss: 0.0687\n",
      "Epoch 5/300 - Train Loss: 0.0890, Val Loss: 0.0749\n",
      "Epoch 6/300 - Train Loss: 0.0882, Val Loss: 0.1148\n",
      "Epoch 7/300 - Train Loss: 0.0853, Val Loss: 0.0726\n",
      "Epoch 8/300 - Train Loss: 0.0856, Val Loss: 0.0695\n",
      "Epoch 9/300 - Train Loss: 0.0845, Val Loss: 0.0734\n",
      "Epoch 10/300 - Train Loss: 0.0858, Val Loss: 0.0694\n",
      "Epoch 11/300 - Train Loss: 0.0819, Val Loss: 0.0713\n",
      "Epoch 12/300 - Train Loss: 0.0809, Val Loss: 0.0690\n",
      "Epoch 13/300 - Train Loss: 0.0801, Val Loss: 0.0658\n",
      "Epoch 14/300 - Train Loss: 0.0797, Val Loss: 0.0893\n",
      "Epoch 15/300 - Train Loss: 0.0812, Val Loss: 0.0734\n",
      "Epoch 16/300 - Train Loss: 0.0796, Val Loss: 0.0776\n",
      "Epoch 17/300 - Train Loss: 0.0769, Val Loss: 0.0750\n",
      "Epoch 18/300 - Train Loss: 0.0778, Val Loss: 0.0668\n",
      "Epoch 19/300 - Train Loss: 0.0766, Val Loss: 0.0741\n",
      "Epoch 20/300 - Train Loss: 0.0741, Val Loss: 0.0759\n",
      "Epoch 21/300 - Train Loss: 0.0774, Val Loss: 0.0729\n",
      "Epoch 22/300 - Train Loss: 0.0764, Val Loss: 0.0741\n",
      "Epoch 23/300 - Train Loss: 0.0769, Val Loss: 0.0693\n",
      "Epoch 24/300 - Train Loss: 0.0754, Val Loss: 0.0819\n",
      "Epoch 25/300 - Train Loss: 0.0790, Val Loss: 0.0699\n",
      "Epoch 26/300 - Train Loss: 0.0773, Val Loss: 0.0713\n",
      "Epoch 27/300 - Train Loss: 0.0753, Val Loss: 0.0724\n",
      "Epoch 28/300 - Train Loss: 0.0741, Val Loss: 0.0724\n",
      "Epoch 29/300 - Train Loss: 0.0771, Val Loss: 0.0679\n",
      "Epoch 30/300 - Train Loss: 0.0740, Val Loss: 0.0715\n",
      "Epoch 31/300 - Train Loss: 0.0736, Val Loss: 0.0710\n",
      "Epoch 32/300 - Train Loss: 0.0743, Val Loss: 0.0709\n",
      "Epoch 33/300 - Train Loss: 0.0743, Val Loss: 0.0724\n",
      "Epoch 34/300 - Train Loss: 0.0724, Val Loss: 0.0674\n",
      "Epoch 35/300 - Train Loss: 0.0727, Val Loss: 0.0751\n",
      "Epoch 36/300 - Train Loss: 0.0745, Val Loss: 0.0732\n",
      "Epoch 37/300 - Train Loss: 0.0731, Val Loss: 0.0684\n",
      "Epoch 38/300 - Train Loss: 0.0751, Val Loss: 0.0694\n",
      "Epoch 39/300 - Train Loss: 0.0710, Val Loss: 0.0719\n",
      "Epoch 40/300 - Train Loss: 0.0725, Val Loss: 0.0732\n",
      "Epoch 41/300 - Train Loss: 0.0738, Val Loss: 0.0782\n",
      "Epoch 42/300 - Train Loss: 0.0750, Val Loss: 0.0757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-01 23:57:18,068] Trial 389 finished with value: 0.9749508383103533 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.19464551026733984, 'learning_rate': 0.0008283370547128397, 'batch_size': 32, 'weight_decay': 4.7378592725709666e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/300 - Train Loss: 0.0729, Val Loss: 0.0753\n",
      "Early stopping at epoch 43\n",
      "Macro F1 Score: 0.9750, Macro Precision: 0.9738, Macro Recall: 0.9763\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.97      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 391\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1948016395167802, LR=0.0009196461857996225, BS=32, WD=4.693677707942322e-05\n",
      "Epoch 1/300 - Train Loss: 0.1548, Val Loss: 0.0812\n",
      "Epoch 2/300 - Train Loss: 0.0998, Val Loss: 0.0706\n",
      "Epoch 3/300 - Train Loss: 0.0948, Val Loss: 0.0809\n",
      "Epoch 4/300 - Train Loss: 0.0920, Val Loss: 0.0853\n",
      "Epoch 5/300 - Train Loss: 0.0910, Val Loss: 0.0751\n",
      "Epoch 6/300 - Train Loss: 0.0856, Val Loss: 0.0959\n",
      "Epoch 7/300 - Train Loss: 0.0873, Val Loss: 0.0738\n",
      "Epoch 8/300 - Train Loss: 0.0874, Val Loss: 0.0707\n",
      "Epoch 9/300 - Train Loss: 0.0858, Val Loss: 0.0696\n",
      "Epoch 10/300 - Train Loss: 0.0823, Val Loss: 0.0818\n",
      "Epoch 11/300 - Train Loss: 0.0831, Val Loss: 0.0836\n",
      "Epoch 12/300 - Train Loss: 0.0828, Val Loss: 0.0783\n",
      "Epoch 13/300 - Train Loss: 0.0808, Val Loss: 0.0783\n",
      "Epoch 14/300 - Train Loss: 0.0811, Val Loss: 0.0716\n",
      "Epoch 15/300 - Train Loss: 0.0814, Val Loss: 0.0845\n",
      "Epoch 16/300 - Train Loss: 0.0784, Val Loss: 0.0756\n",
      "Epoch 17/300 - Train Loss: 0.0824, Val Loss: 0.0687\n",
      "Epoch 18/300 - Train Loss: 0.0812, Val Loss: 0.0755\n",
      "Epoch 19/300 - Train Loss: 0.0773, Val Loss: 0.0698\n",
      "Epoch 20/300 - Train Loss: 0.0779, Val Loss: 0.0752\n",
      "Epoch 21/300 - Train Loss: 0.0812, Val Loss: 0.0790\n",
      "Epoch 22/300 - Train Loss: 0.0762, Val Loss: 0.0719\n",
      "Epoch 23/300 - Train Loss: 0.0770, Val Loss: 0.0717\n",
      "Epoch 24/300 - Train Loss: 0.0754, Val Loss: 0.0713\n",
      "Epoch 25/300 - Train Loss: 0.0759, Val Loss: 0.0776\n",
      "Epoch 26/300 - Train Loss: 0.0781, Val Loss: 0.0714\n",
      "Epoch 27/300 - Train Loss: 0.0748, Val Loss: 0.0726\n",
      "Epoch 28/300 - Train Loss: 0.0737, Val Loss: 0.0707\n",
      "Epoch 29/300 - Train Loss: 0.0748, Val Loss: 0.0677\n",
      "Epoch 30/300 - Train Loss: 0.0737, Val Loss: 0.0685\n",
      "Epoch 31/300 - Train Loss: 0.0740, Val Loss: 0.0760\n",
      "Epoch 32/300 - Train Loss: 0.0732, Val Loss: 0.0745\n",
      "Epoch 33/300 - Train Loss: 0.0728, Val Loss: 0.0780\n",
      "Epoch 34/300 - Train Loss: 0.0766, Val Loss: 0.0758\n",
      "Epoch 35/300 - Train Loss: 0.0743, Val Loss: 0.0682\n",
      "Epoch 36/300 - Train Loss: 0.0731, Val Loss: 0.0730\n",
      "Epoch 37/300 - Train Loss: 0.0717, Val Loss: 0.0728\n",
      "Epoch 38/300 - Train Loss: 0.0736, Val Loss: 0.0668\n",
      "Epoch 39/300 - Train Loss: 0.0719, Val Loss: 0.0750\n",
      "Epoch 40/300 - Train Loss: 0.0722, Val Loss: 0.0693\n",
      "Epoch 41/300 - Train Loss: 0.0717, Val Loss: 0.0842\n",
      "Epoch 42/300 - Train Loss: 0.0711, Val Loss: 0.0722\n",
      "Epoch 43/300 - Train Loss: 0.0706, Val Loss: 0.0736\n",
      "Epoch 44/300 - Train Loss: 0.0722, Val Loss: 0.0840\n",
      "Epoch 45/300 - Train Loss: 0.0747, Val Loss: 0.0784\n",
      "Epoch 46/300 - Train Loss: 0.0709, Val Loss: 0.0835\n",
      "Epoch 47/300 - Train Loss: 0.0692, Val Loss: 0.0701\n",
      "Epoch 48/300 - Train Loss: 0.0713, Val Loss: 0.0843\n",
      "Epoch 49/300 - Train Loss: 0.0682, Val Loss: 0.0806\n",
      "Epoch 50/300 - Train Loss: 0.0708, Val Loss: 0.0747\n",
      "Epoch 51/300 - Train Loss: 0.0709, Val Loss: 0.0717\n",
      "Epoch 52/300 - Train Loss: 0.0694, Val Loss: 0.0747\n",
      "Epoch 53/300 - Train Loss: 0.0709, Val Loss: 0.0859\n",
      "Epoch 54/300 - Train Loss: 0.0706, Val Loss: 0.0679\n",
      "Epoch 55/300 - Train Loss: 0.0726, Val Loss: 0.0738\n",
      "Epoch 56/300 - Train Loss: 0.0695, Val Loss: 0.0728\n",
      "Epoch 57/300 - Train Loss: 0.0680, Val Loss: 0.0759\n",
      "Epoch 58/300 - Train Loss: 0.0705, Val Loss: 0.0760\n",
      "Epoch 59/300 - Train Loss: 0.0714, Val Loss: 0.0668\n",
      "Epoch 60/300 - Train Loss: 0.0687, Val Loss: 0.0758\n",
      "Epoch 61/300 - Train Loss: 0.0637, Val Loss: 0.0778\n",
      "Epoch 62/300 - Train Loss: 0.0663, Val Loss: 0.0733\n",
      "Epoch 63/300 - Train Loss: 0.0698, Val Loss: 0.0727\n",
      "Epoch 64/300 - Train Loss: 0.0678, Val Loss: 0.0716\n",
      "Epoch 65/300 - Train Loss: 0.0652, Val Loss: 0.0743\n",
      "Epoch 66/300 - Train Loss: 0.0697, Val Loss: 0.0746\n",
      "Epoch 67/300 - Train Loss: 0.0675, Val Loss: 0.0768\n",
      "Epoch 68/300 - Train Loss: 0.0676, Val Loss: 0.0729\n",
      "Epoch 69/300 - Train Loss: 0.0670, Val Loss: 0.0724\n",
      "Epoch 70/300 - Train Loss: 0.0671, Val Loss: 0.0772\n",
      "Epoch 71/300 - Train Loss: 0.0669, Val Loss: 0.0762\n",
      "Epoch 72/300 - Train Loss: 0.0675, Val Loss: 0.0755\n",
      "Epoch 73/300 - Train Loss: 0.0653, Val Loss: 0.0748\n",
      "Epoch 74/300 - Train Loss: 0.0687, Val Loss: 0.0723\n",
      "Epoch 75/300 - Train Loss: 0.0666, Val Loss: 0.0735\n",
      "Epoch 76/300 - Train Loss: 0.0650, Val Loss: 0.0841\n",
      "Epoch 77/300 - Train Loss: 0.0663, Val Loss: 0.0786\n",
      "Epoch 78/300 - Train Loss: 0.0683, Val Loss: 0.0783\n",
      "Epoch 79/300 - Train Loss: 0.0654, Val Loss: 0.0765\n",
      "Epoch 80/300 - Train Loss: 0.0647, Val Loss: 0.0727\n",
      "Epoch 81/300 - Train Loss: 0.0645, Val Loss: 0.0745\n",
      "Epoch 82/300 - Train Loss: 0.0638, Val Loss: 0.0711\n",
      "Epoch 83/300 - Train Loss: 0.0633, Val Loss: 0.0730\n",
      "Epoch 84/300 - Train Loss: 0.0645, Val Loss: 0.0733\n",
      "Epoch 85/300 - Train Loss: 0.0654, Val Loss: 0.0778\n",
      "Epoch 86/300 - Train Loss: 0.0674, Val Loss: 0.0706\n",
      "Epoch 87/300 - Train Loss: 0.0639, Val Loss: 0.0733\n",
      "Epoch 88/300 - Train Loss: 0.0643, Val Loss: 0.0774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:01:01,146] Trial 390 finished with value: 0.9702016753698977 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1948016395167802, 'learning_rate': 0.0009196461857996225, 'batch_size': 32, 'weight_decay': 4.693677707942322e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/300 - Train Loss: 0.0669, Val Loss: 0.0742\n",
      "Early stopping at epoch 89\n",
      "Macro F1 Score: 0.9702, Macro Precision: 0.9600, Macro Recall: 0.9815\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 392\n",
      "Training with F1=32, F2=8, D=2, dropout=0.21834196152047144, LR=0.0007801112649673548, BS=32, WD=4.075053202034202e-05\n",
      "Epoch 1/300 - Train Loss: 0.1595, Val Loss: 0.0908\n",
      "Epoch 2/300 - Train Loss: 0.1005, Val Loss: 0.0795\n",
      "Epoch 3/300 - Train Loss: 0.0954, Val Loss: 0.0795\n",
      "Epoch 4/300 - Train Loss: 0.0937, Val Loss: 0.0753\n",
      "Epoch 5/300 - Train Loss: 0.0893, Val Loss: 0.0705\n",
      "Epoch 6/300 - Train Loss: 0.0862, Val Loss: 0.0766\n",
      "Epoch 7/300 - Train Loss: 0.0863, Val Loss: 0.0788\n",
      "Epoch 8/300 - Train Loss: 0.0841, Val Loss: 0.0768\n",
      "Epoch 9/300 - Train Loss: 0.0853, Val Loss: 0.0742\n",
      "Epoch 10/300 - Train Loss: 0.0857, Val Loss: 0.0714\n",
      "Epoch 11/300 - Train Loss: 0.0838, Val Loss: 0.0743\n",
      "Epoch 12/300 - Train Loss: 0.0827, Val Loss: 0.0822\n",
      "Epoch 13/300 - Train Loss: 0.0819, Val Loss: 0.0709\n",
      "Epoch 14/300 - Train Loss: 0.0805, Val Loss: 0.0789\n",
      "Epoch 15/300 - Train Loss: 0.0784, Val Loss: 0.0811\n",
      "Epoch 16/300 - Train Loss: 0.0773, Val Loss: 0.0796\n",
      "Epoch 17/300 - Train Loss: 0.0821, Val Loss: 0.0717\n",
      "Epoch 18/300 - Train Loss: 0.0804, Val Loss: 0.0734\n",
      "Epoch 19/300 - Train Loss: 0.0809, Val Loss: 0.0705\n",
      "Epoch 20/300 - Train Loss: 0.0773, Val Loss: 0.0726\n",
      "Epoch 21/300 - Train Loss: 0.0778, Val Loss: 0.0668\n",
      "Epoch 22/300 - Train Loss: 0.0787, Val Loss: 0.0887\n",
      "Epoch 23/300 - Train Loss: 0.0765, Val Loss: 0.0733\n",
      "Epoch 24/300 - Train Loss: 0.0802, Val Loss: 0.0771\n",
      "Epoch 25/300 - Train Loss: 0.0742, Val Loss: 0.0772\n",
      "Epoch 26/300 - Train Loss: 0.0766, Val Loss: 0.0801\n",
      "Epoch 27/300 - Train Loss: 0.0785, Val Loss: 0.0705\n",
      "Epoch 28/300 - Train Loss: 0.0763, Val Loss: 0.0777\n",
      "Epoch 29/300 - Train Loss: 0.0748, Val Loss: 0.0716\n",
      "Epoch 30/300 - Train Loss: 0.0747, Val Loss: 0.0698\n",
      "Epoch 31/300 - Train Loss: 0.0749, Val Loss: 0.0777\n",
      "Epoch 32/300 - Train Loss: 0.0748, Val Loss: 0.0744\n",
      "Epoch 33/300 - Train Loss: 0.0739, Val Loss: 0.0735\n",
      "Epoch 34/300 - Train Loss: 0.0759, Val Loss: 0.0734\n",
      "Epoch 35/300 - Train Loss: 0.0741, Val Loss: 0.0822\n",
      "Epoch 36/300 - Train Loss: 0.0732, Val Loss: 0.0691\n",
      "Epoch 37/300 - Train Loss: 0.0756, Val Loss: 0.0718\n",
      "Epoch 38/300 - Train Loss: 0.0749, Val Loss: 0.0694\n",
      "Epoch 39/300 - Train Loss: 0.0704, Val Loss: 0.0706\n",
      "Epoch 40/300 - Train Loss: 0.0723, Val Loss: 0.0756\n",
      "Epoch 41/300 - Train Loss: 0.0717, Val Loss: 0.0748\n",
      "Epoch 42/300 - Train Loss: 0.0734, Val Loss: 0.0724\n",
      "Epoch 43/300 - Train Loss: 0.0734, Val Loss: 0.0720\n",
      "Epoch 44/300 - Train Loss: 0.0715, Val Loss: 0.0757\n",
      "Epoch 45/300 - Train Loss: 0.0727, Val Loss: 0.0858\n",
      "Epoch 46/300 - Train Loss: 0.0710, Val Loss: 0.0694\n",
      "Epoch 47/300 - Train Loss: 0.0714, Val Loss: 0.0697\n",
      "Epoch 48/300 - Train Loss: 0.0726, Val Loss: 0.0700\n",
      "Epoch 49/300 - Train Loss: 0.0730, Val Loss: 0.0769\n",
      "Epoch 50/300 - Train Loss: 0.0726, Val Loss: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:03:09,439] Trial 391 finished with value: 0.9724011570085013 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.21834196152047144, 'learning_rate': 0.0007801112649673548, 'batch_size': 32, 'weight_decay': 4.075053202034202e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300 - Train Loss: 0.0721, Val Loss: 0.0751\n",
      "Early stopping at epoch 51\n",
      "Macro F1 Score: 0.9724, Macro Precision: 0.9734, Macro Recall: 0.9715\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 393\n",
      "Training with F1=32, F2=8, D=2, dropout=0.20579407776681732, LR=0.0009817629033320515, BS=256, WD=4.832764747896104e-05\n",
      "Epoch 1/300 - Train Loss: 0.2954, Val Loss: 0.1091\n",
      "Epoch 2/300 - Train Loss: 0.1067, Val Loss: 0.0822\n",
      "Epoch 3/300 - Train Loss: 0.0914, Val Loss: 0.0860\n",
      "Epoch 4/300 - Train Loss: 0.0862, Val Loss: 0.0777\n",
      "Epoch 5/300 - Train Loss: 0.0834, Val Loss: 0.0678\n",
      "Epoch 6/300 - Train Loss: 0.0811, Val Loss: 0.0783\n",
      "Epoch 7/300 - Train Loss: 0.0801, Val Loss: 0.0678\n",
      "Epoch 8/300 - Train Loss: 0.0777, Val Loss: 0.0686\n",
      "Epoch 9/300 - Train Loss: 0.0782, Val Loss: 0.0724\n",
      "Epoch 10/300 - Train Loss: 0.0756, Val Loss: 0.0664\n",
      "Epoch 11/300 - Train Loss: 0.0746, Val Loss: 0.0666\n",
      "Epoch 12/300 - Train Loss: 0.0724, Val Loss: 0.0679\n",
      "Epoch 13/300 - Train Loss: 0.0751, Val Loss: 0.0684\n",
      "Epoch 14/300 - Train Loss: 0.0737, Val Loss: 0.0788\n",
      "Epoch 15/300 - Train Loss: 0.0723, Val Loss: 0.0710\n",
      "Epoch 16/300 - Train Loss: 0.0724, Val Loss: 0.0677\n",
      "Epoch 17/300 - Train Loss: 0.0721, Val Loss: 0.0742\n",
      "Epoch 18/300 - Train Loss: 0.0703, Val Loss: 0.0694\n",
      "Epoch 19/300 - Train Loss: 0.0708, Val Loss: 0.0648\n",
      "Epoch 20/300 - Train Loss: 0.0694, Val Loss: 0.0695\n",
      "Epoch 21/300 - Train Loss: 0.0710, Val Loss: 0.0684\n",
      "Epoch 22/300 - Train Loss: 0.0686, Val Loss: 0.0646\n",
      "Epoch 23/300 - Train Loss: 0.0695, Val Loss: 0.0671\n",
      "Epoch 24/300 - Train Loss: 0.0682, Val Loss: 0.0674\n",
      "Epoch 25/300 - Train Loss: 0.0682, Val Loss: 0.0679\n",
      "Epoch 26/300 - Train Loss: 0.0678, Val Loss: 0.0654\n",
      "Epoch 27/300 - Train Loss: 0.0681, Val Loss: 0.0676\n",
      "Epoch 28/300 - Train Loss: 0.0685, Val Loss: 0.0709\n",
      "Epoch 29/300 - Train Loss: 0.0674, Val Loss: 0.0710\n",
      "Epoch 30/300 - Train Loss: 0.0657, Val Loss: 0.0648\n",
      "Epoch 31/300 - Train Loss: 0.0657, Val Loss: 0.0697\n",
      "Epoch 32/300 - Train Loss: 0.0651, Val Loss: 0.0687\n",
      "Epoch 33/300 - Train Loss: 0.0660, Val Loss: 0.0676\n",
      "Epoch 34/300 - Train Loss: 0.0655, Val Loss: 0.0667\n",
      "Epoch 35/300 - Train Loss: 0.0652, Val Loss: 0.0653\n",
      "Epoch 36/300 - Train Loss: 0.0636, Val Loss: 0.0657\n",
      "Epoch 37/300 - Train Loss: 0.0640, Val Loss: 0.0676\n",
      "Epoch 38/300 - Train Loss: 0.0634, Val Loss: 0.0673\n",
      "Epoch 39/300 - Train Loss: 0.0646, Val Loss: 0.0668\n",
      "Epoch 40/300 - Train Loss: 0.0618, Val Loss: 0.0690\n",
      "Epoch 41/300 - Train Loss: 0.0625, Val Loss: 0.0642\n",
      "Epoch 42/300 - Train Loss: 0.0619, Val Loss: 0.0660\n",
      "Epoch 43/300 - Train Loss: 0.0632, Val Loss: 0.0717\n",
      "Epoch 44/300 - Train Loss: 0.0628, Val Loss: 0.0728\n",
      "Epoch 45/300 - Train Loss: 0.0630, Val Loss: 0.0665\n",
      "Epoch 46/300 - Train Loss: 0.0619, Val Loss: 0.0650\n",
      "Epoch 47/300 - Train Loss: 0.0637, Val Loss: 0.0662\n",
      "Epoch 48/300 - Train Loss: 0.0604, Val Loss: 0.0664\n",
      "Epoch 49/300 - Train Loss: 0.0627, Val Loss: 0.0656\n",
      "Epoch 50/300 - Train Loss: 0.0613, Val Loss: 0.0683\n",
      "Epoch 51/300 - Train Loss: 0.0629, Val Loss: 0.0664\n",
      "Epoch 52/300 - Train Loss: 0.0618, Val Loss: 0.0661\n",
      "Epoch 53/300 - Train Loss: 0.0591, Val Loss: 0.0706\n",
      "Epoch 54/300 - Train Loss: 0.0598, Val Loss: 0.0670\n",
      "Epoch 55/300 - Train Loss: 0.0596, Val Loss: 0.0656\n",
      "Epoch 56/300 - Train Loss: 0.0598, Val Loss: 0.0677\n",
      "Epoch 57/300 - Train Loss: 0.0615, Val Loss: 0.0672\n",
      "Epoch 58/300 - Train Loss: 0.0608, Val Loss: 0.0678\n",
      "Epoch 59/300 - Train Loss: 0.0590, Val Loss: 0.0718\n",
      "Epoch 60/300 - Train Loss: 0.0579, Val Loss: 0.0678\n",
      "Epoch 61/300 - Train Loss: 0.0588, Val Loss: 0.0647\n",
      "Epoch 62/300 - Train Loss: 0.0588, Val Loss: 0.0674\n",
      "Epoch 63/300 - Train Loss: 0.0586, Val Loss: 0.0703\n",
      "Epoch 64/300 - Train Loss: 0.0581, Val Loss: 0.0677\n",
      "Epoch 65/300 - Train Loss: 0.0588, Val Loss: 0.0670\n",
      "Epoch 66/300 - Train Loss: 0.0569, Val Loss: 0.0709\n",
      "Epoch 67/300 - Train Loss: 0.0600, Val Loss: 0.0648\n",
      "Epoch 68/300 - Train Loss: 0.0607, Val Loss: 0.0693\n",
      "Epoch 69/300 - Train Loss: 0.0581, Val Loss: 0.0687\n",
      "Epoch 70/300 - Train Loss: 0.0578, Val Loss: 0.0709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:05:32,831] Trial 392 finished with value: 0.9663680329833628 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.20579407776681732, 'learning_rate': 0.0009817629033320515, 'batch_size': 256, 'weight_decay': 4.832764747896104e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300 - Train Loss: 0.0585, Val Loss: 0.0655\n",
      "Early stopping at epoch 71\n",
      "Macro F1 Score: 0.9664, Macro Precision: 0.9560, Macro Recall: 0.9777\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 394\n",
      "Training with F1=4, F2=8, D=2, dropout=0.19333777650223347, LR=0.000831486714310928, BS=32, WD=5.503399254415542e-05\n",
      "Epoch 1/300 - Train Loss: 0.2015, Val Loss: 0.0893\n",
      "Epoch 2/300 - Train Loss: 0.1176, Val Loss: 0.0814\n",
      "Epoch 3/300 - Train Loss: 0.1071, Val Loss: 0.0745\n",
      "Epoch 4/300 - Train Loss: 0.1068, Val Loss: 0.0754\n",
      "Epoch 5/300 - Train Loss: 0.1030, Val Loss: 0.0847\n",
      "Epoch 6/300 - Train Loss: 0.1007, Val Loss: 0.0813\n",
      "Epoch 7/300 - Train Loss: 0.1009, Val Loss: 0.0729\n",
      "Epoch 8/300 - Train Loss: 0.0973, Val Loss: 0.0679\n",
      "Epoch 9/300 - Train Loss: 0.1005, Val Loss: 0.0721\n",
      "Epoch 10/300 - Train Loss: 0.0985, Val Loss: 0.0805\n",
      "Epoch 11/300 - Train Loss: 0.0985, Val Loss: 0.0795\n",
      "Epoch 12/300 - Train Loss: 0.0959, Val Loss: 0.0780\n",
      "Epoch 13/300 - Train Loss: 0.0977, Val Loss: 0.0725\n",
      "Epoch 14/300 - Train Loss: 0.0947, Val Loss: 0.0821\n",
      "Epoch 15/300 - Train Loss: 0.0941, Val Loss: 0.0718\n",
      "Epoch 16/300 - Train Loss: 0.0933, Val Loss: 0.0791\n",
      "Epoch 17/300 - Train Loss: 0.0934, Val Loss: 0.0764\n",
      "Epoch 18/300 - Train Loss: 0.0954, Val Loss: 0.0804\n",
      "Epoch 19/300 - Train Loss: 0.0958, Val Loss: 0.0839\n",
      "Epoch 20/300 - Train Loss: 0.0928, Val Loss: 0.0707\n",
      "Epoch 21/300 - Train Loss: 0.0898, Val Loss: 0.0789\n",
      "Epoch 22/300 - Train Loss: 0.0921, Val Loss: 0.0803\n",
      "Epoch 23/300 - Train Loss: 0.0906, Val Loss: 0.0755\n",
      "Epoch 24/300 - Train Loss: 0.0930, Val Loss: 0.0736\n",
      "Epoch 25/300 - Train Loss: 0.0914, Val Loss: 0.0750\n",
      "Epoch 26/300 - Train Loss: 0.0913, Val Loss: 0.0725\n",
      "Epoch 27/300 - Train Loss: 0.0885, Val Loss: 0.0845\n",
      "Epoch 28/300 - Train Loss: 0.0903, Val Loss: 0.0760\n",
      "Epoch 29/300 - Train Loss: 0.0898, Val Loss: 0.0744\n",
      "Epoch 30/300 - Train Loss: 0.0912, Val Loss: 0.0757\n",
      "Epoch 31/300 - Train Loss: 0.0919, Val Loss: 0.0846\n",
      "Epoch 32/300 - Train Loss: 0.0890, Val Loss: 0.0721\n",
      "Epoch 33/300 - Train Loss: 0.0886, Val Loss: 0.0822\n",
      "Epoch 34/300 - Train Loss: 0.0893, Val Loss: 0.0726\n",
      "Epoch 35/300 - Train Loss: 0.0902, Val Loss: 0.0751\n",
      "Epoch 36/300 - Train Loss: 0.0894, Val Loss: 0.0805\n",
      "Epoch 37/300 - Train Loss: 0.0867, Val Loss: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:06:45,217] Trial 393 finished with value: 0.9621621272756914 and parameters: {'F1': 4, 'F2': 8, 'D': 2, 'dropout': 0.19333777650223347, 'learning_rate': 0.000831486714310928, 'batch_size': 32, 'weight_decay': 5.503399254415542e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300 - Train Loss: 0.0886, Val Loss: 0.0742\n",
      "Early stopping at epoch 38\n",
      "Macro F1 Score: 0.9622, Macro Precision: 0.9656, Macro Recall: 0.9589\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.93      0.92      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 395\n",
      "Training with F1=32, F2=8, D=2, dropout=0.188427285848272, LR=0.0008426516899421054, BS=32, WD=4.112394546698721e-05\n",
      "Epoch 1/300 - Train Loss: 0.1555, Val Loss: 0.0759\n",
      "Epoch 2/300 - Train Loss: 0.0955, Val Loss: 0.0738\n",
      "Epoch 3/300 - Train Loss: 0.0933, Val Loss: 0.0733\n",
      "Epoch 4/300 - Train Loss: 0.0890, Val Loss: 0.0734\n",
      "Epoch 5/300 - Train Loss: 0.0881, Val Loss: 0.0716\n",
      "Epoch 6/300 - Train Loss: 0.0894, Val Loss: 0.0813\n",
      "Epoch 7/300 - Train Loss: 0.0849, Val Loss: 0.1009\n",
      "Epoch 8/300 - Train Loss: 0.0858, Val Loss: 0.0759\n",
      "Epoch 9/300 - Train Loss: 0.0825, Val Loss: 0.0711\n",
      "Epoch 10/300 - Train Loss: 0.0804, Val Loss: 0.0698\n",
      "Epoch 11/300 - Train Loss: 0.0814, Val Loss: 0.0683\n",
      "Epoch 12/300 - Train Loss: 0.0831, Val Loss: 0.0739\n",
      "Epoch 13/300 - Train Loss: 0.0820, Val Loss: 0.0690\n",
      "Epoch 14/300 - Train Loss: 0.0809, Val Loss: 0.0750\n",
      "Epoch 15/300 - Train Loss: 0.0805, Val Loss: 0.0720\n",
      "Epoch 16/300 - Train Loss: 0.0788, Val Loss: 0.0685\n",
      "Epoch 17/300 - Train Loss: 0.0758, Val Loss: 0.0762\n",
      "Epoch 18/300 - Train Loss: 0.0793, Val Loss: 0.0728\n",
      "Epoch 19/300 - Train Loss: 0.0755, Val Loss: 0.0658\n",
      "Epoch 20/300 - Train Loss: 0.0771, Val Loss: 0.0731\n",
      "Epoch 21/300 - Train Loss: 0.0773, Val Loss: 0.0661\n",
      "Epoch 22/300 - Train Loss: 0.0774, Val Loss: 0.0695\n",
      "Epoch 23/300 - Train Loss: 0.0766, Val Loss: 0.0735\n",
      "Epoch 24/300 - Train Loss: 0.0758, Val Loss: 0.0723\n",
      "Epoch 25/300 - Train Loss: 0.0741, Val Loss: 0.0752\n",
      "Epoch 26/300 - Train Loss: 0.0761, Val Loss: 0.0763\n",
      "Epoch 27/300 - Train Loss: 0.0769, Val Loss: 0.0728\n",
      "Epoch 28/300 - Train Loss: 0.0735, Val Loss: 0.0760\n",
      "Epoch 29/300 - Train Loss: 0.0750, Val Loss: 0.0676\n",
      "Epoch 30/300 - Train Loss: 0.0732, Val Loss: 0.0753\n",
      "Epoch 31/300 - Train Loss: 0.0716, Val Loss: 0.0697\n",
      "Epoch 32/300 - Train Loss: 0.0743, Val Loss: 0.0699\n",
      "Epoch 33/300 - Train Loss: 0.0724, Val Loss: 0.0782\n",
      "Epoch 34/300 - Train Loss: 0.0730, Val Loss: 0.0667\n",
      "Epoch 35/300 - Train Loss: 0.0706, Val Loss: 0.0705\n",
      "Epoch 36/300 - Train Loss: 0.0704, Val Loss: 0.0784\n",
      "Epoch 37/300 - Train Loss: 0.0709, Val Loss: 0.0739\n",
      "Epoch 38/300 - Train Loss: 0.0704, Val Loss: 0.0754\n",
      "Epoch 39/300 - Train Loss: 0.0712, Val Loss: 0.0699\n",
      "Epoch 40/300 - Train Loss: 0.0704, Val Loss: 0.0668\n",
      "Epoch 41/300 - Train Loss: 0.0716, Val Loss: 0.0656\n",
      "Epoch 42/300 - Train Loss: 0.0702, Val Loss: 0.0713\n",
      "Epoch 43/300 - Train Loss: 0.0707, Val Loss: 0.0668\n",
      "Epoch 44/300 - Train Loss: 0.0701, Val Loss: 0.0641\n",
      "Epoch 45/300 - Train Loss: 0.0697, Val Loss: 0.0785\n",
      "Epoch 46/300 - Train Loss: 0.0685, Val Loss: 0.0689\n",
      "Epoch 47/300 - Train Loss: 0.0693, Val Loss: 0.0665\n",
      "Epoch 48/300 - Train Loss: 0.0673, Val Loss: 0.0640\n",
      "Epoch 49/300 - Train Loss: 0.0702, Val Loss: 0.0716\n",
      "Epoch 50/300 - Train Loss: 0.0686, Val Loss: 0.0601\n",
      "Epoch 51/300 - Train Loss: 0.0688, Val Loss: 0.0658\n",
      "Epoch 52/300 - Train Loss: 0.0683, Val Loss: 0.0687\n",
      "Epoch 53/300 - Train Loss: 0.0692, Val Loss: 0.0670\n",
      "Epoch 54/300 - Train Loss: 0.0677, Val Loss: 0.0648\n",
      "Epoch 55/300 - Train Loss: 0.0719, Val Loss: 0.0654\n",
      "Epoch 56/300 - Train Loss: 0.0668, Val Loss: 0.0671\n",
      "Epoch 57/300 - Train Loss: 0.0670, Val Loss: 0.0681\n",
      "Epoch 58/300 - Train Loss: 0.0687, Val Loss: 0.0630\n",
      "Epoch 59/300 - Train Loss: 0.0682, Val Loss: 0.0729\n",
      "Epoch 60/300 - Train Loss: 0.0678, Val Loss: 0.0699\n",
      "Epoch 61/300 - Train Loss: 0.0676, Val Loss: 0.0687\n",
      "Epoch 62/300 - Train Loss: 0.0686, Val Loss: 0.0674\n",
      "Epoch 63/300 - Train Loss: 0.0670, Val Loss: 0.0662\n",
      "Epoch 64/300 - Train Loss: 0.0667, Val Loss: 0.0693\n",
      "Epoch 65/300 - Train Loss: 0.0656, Val Loss: 0.0735\n",
      "Epoch 66/300 - Train Loss: 0.0628, Val Loss: 0.0697\n",
      "Epoch 67/300 - Train Loss: 0.0663, Val Loss: 0.0671\n",
      "Epoch 68/300 - Train Loss: 0.0652, Val Loss: 0.0693\n",
      "Epoch 69/300 - Train Loss: 0.0648, Val Loss: 0.0736\n",
      "Epoch 70/300 - Train Loss: 0.0673, Val Loss: 0.0651\n",
      "Epoch 71/300 - Train Loss: 0.0646, Val Loss: 0.0667\n",
      "Epoch 72/300 - Train Loss: 0.0639, Val Loss: 0.0652\n",
      "Epoch 73/300 - Train Loss: 0.0659, Val Loss: 0.0676\n",
      "Epoch 74/300 - Train Loss: 0.0654, Val Loss: 0.0756\n",
      "Epoch 75/300 - Train Loss: 0.0639, Val Loss: 0.0716\n",
      "Epoch 76/300 - Train Loss: 0.0626, Val Loss: 0.0720\n",
      "Epoch 77/300 - Train Loss: 0.0644, Val Loss: 0.0765\n",
      "Epoch 78/300 - Train Loss: 0.0656, Val Loss: 0.0711\n",
      "Epoch 79/300 - Train Loss: 0.0631, Val Loss: 0.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:10:06,938] Trial 394 finished with value: 0.9752447389838391 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.188427285848272, 'learning_rate': 0.0008426516899421054, 'batch_size': 32, 'weight_decay': 4.112394546698721e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300 - Train Loss: 0.0640, Val Loss: 0.0686\n",
      "Early stopping at epoch 80\n",
      "Macro F1 Score: 0.9752, Macro Precision: 0.9692, Macro Recall: 0.9817\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.98      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 396\n",
      "Training with F1=32, F2=8, D=2, dropout=0.19702288731822212, LR=0.0007715604587195585, BS=32, WD=3.984488329469654e-05\n",
      "Epoch 1/300 - Train Loss: 0.1678, Val Loss: 0.0997\n",
      "Epoch 2/300 - Train Loss: 0.0976, Val Loss: 0.0793\n",
      "Epoch 3/300 - Train Loss: 0.0936, Val Loss: 0.0830\n",
      "Epoch 4/300 - Train Loss: 0.0901, Val Loss: 0.0834\n",
      "Epoch 5/300 - Train Loss: 0.0892, Val Loss: 0.0693\n",
      "Epoch 6/300 - Train Loss: 0.0846, Val Loss: 0.0713\n",
      "Epoch 7/300 - Train Loss: 0.0878, Val Loss: 0.0754\n",
      "Epoch 8/300 - Train Loss: 0.0844, Val Loss: 0.0677\n",
      "Epoch 9/300 - Train Loss: 0.0802, Val Loss: 0.0702\n",
      "Epoch 10/300 - Train Loss: 0.0824, Val Loss: 0.0702\n",
      "Epoch 11/300 - Train Loss: 0.0824, Val Loss: 0.0761\n",
      "Epoch 12/300 - Train Loss: 0.0837, Val Loss: 0.0676\n",
      "Epoch 13/300 - Train Loss: 0.0801, Val Loss: 0.0729\n",
      "Epoch 14/300 - Train Loss: 0.0793, Val Loss: 0.0700\n",
      "Epoch 15/300 - Train Loss: 0.0782, Val Loss: 0.0731\n",
      "Epoch 16/300 - Train Loss: 0.0788, Val Loss: 0.0821\n",
      "Epoch 17/300 - Train Loss: 0.0763, Val Loss: 0.0737\n",
      "Epoch 18/300 - Train Loss: 0.0766, Val Loss: 0.0837\n",
      "Epoch 19/300 - Train Loss: 0.0773, Val Loss: 0.0727\n",
      "Epoch 20/300 - Train Loss: 0.0772, Val Loss: 0.0751\n",
      "Epoch 21/300 - Train Loss: 0.0794, Val Loss: 0.0694\n",
      "Epoch 22/300 - Train Loss: 0.0747, Val Loss: 0.0668\n",
      "Epoch 23/300 - Train Loss: 0.0729, Val Loss: 0.0687\n",
      "Epoch 24/300 - Train Loss: 0.0750, Val Loss: 0.0673\n",
      "Epoch 25/300 - Train Loss: 0.0760, Val Loss: 0.0754\n",
      "Epoch 26/300 - Train Loss: 0.0753, Val Loss: 0.0796\n",
      "Epoch 27/300 - Train Loss: 0.0735, Val Loss: 0.0740\n",
      "Epoch 28/300 - Train Loss: 0.0717, Val Loss: 0.0696\n",
      "Epoch 29/300 - Train Loss: 0.0700, Val Loss: 0.0773\n",
      "Epoch 30/300 - Train Loss: 0.0735, Val Loss: 0.0726\n",
      "Epoch 31/300 - Train Loss: 0.0704, Val Loss: 0.0719\n",
      "Epoch 32/300 - Train Loss: 0.0717, Val Loss: 0.0734\n",
      "Epoch 33/300 - Train Loss: 0.0706, Val Loss: 0.0728\n",
      "Epoch 34/300 - Train Loss: 0.0721, Val Loss: 0.0797\n",
      "Epoch 35/300 - Train Loss: 0.0711, Val Loss: 0.0745\n",
      "Epoch 36/300 - Train Loss: 0.0692, Val Loss: 0.0729\n",
      "Epoch 37/300 - Train Loss: 0.0684, Val Loss: 0.0711\n",
      "Epoch 38/300 - Train Loss: 0.0694, Val Loss: 0.0712\n",
      "Epoch 39/300 - Train Loss: 0.0676, Val Loss: 0.0683\n",
      "Epoch 40/300 - Train Loss: 0.0672, Val Loss: 0.0696\n",
      "Epoch 41/300 - Train Loss: 0.0698, Val Loss: 0.0689\n",
      "Epoch 42/300 - Train Loss: 0.0687, Val Loss: 0.0699\n",
      "Epoch 43/300 - Train Loss: 0.0680, Val Loss: 0.0759\n",
      "Epoch 44/300 - Train Loss: 0.0661, Val Loss: 0.0668\n",
      "Epoch 45/300 - Train Loss: 0.0676, Val Loss: 0.0730\n",
      "Epoch 46/300 - Train Loss: 0.0682, Val Loss: 0.0710\n",
      "Epoch 47/300 - Train Loss: 0.0668, Val Loss: 0.0707\n",
      "Epoch 48/300 - Train Loss: 0.0649, Val Loss: 0.0691\n",
      "Epoch 49/300 - Train Loss: 0.0657, Val Loss: 0.0691\n",
      "Epoch 50/300 - Train Loss: 0.0676, Val Loss: 0.0693\n",
      "Epoch 51/300 - Train Loss: 0.0633, Val Loss: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:12:17,036] Trial 395 finished with value: 0.9728448535936854 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.19702288731822212, 'learning_rate': 0.0007715604587195585, 'batch_size': 32, 'weight_decay': 3.984488329469654e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300 - Train Loss: 0.0672, Val Loss: 0.0714\n",
      "Early stopping at epoch 52\n",
      "Macro F1 Score: 0.9728, Macro Precision: 0.9645, Macro Recall: 0.9819\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.98      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 397\n",
      "Training with F1=32, F2=8, D=2, dropout=0.18677357943887077, LR=0.0009935471030215405, BS=32, WD=3.747907459138005e-05\n",
      "Epoch 1/300 - Train Loss: 0.1555, Val Loss: 0.0806\n",
      "Epoch 2/300 - Train Loss: 0.0965, Val Loss: 0.0946\n",
      "Epoch 3/300 - Train Loss: 0.0940, Val Loss: 0.0724\n",
      "Epoch 4/300 - Train Loss: 0.0909, Val Loss: 0.0729\n",
      "Epoch 5/300 - Train Loss: 0.0906, Val Loss: 0.0761\n",
      "Epoch 6/300 - Train Loss: 0.0867, Val Loss: 0.0747\n",
      "Epoch 7/300 - Train Loss: 0.0888, Val Loss: 0.0708\n",
      "Epoch 8/300 - Train Loss: 0.0842, Val Loss: 0.0660\n",
      "Epoch 9/300 - Train Loss: 0.0838, Val Loss: 0.0730\n",
      "Epoch 10/300 - Train Loss: 0.0844, Val Loss: 0.0699\n",
      "Epoch 11/300 - Train Loss: 0.0850, Val Loss: 0.0784\n",
      "Epoch 12/300 - Train Loss: 0.0825, Val Loss: 0.0778\n",
      "Epoch 13/300 - Train Loss: 0.0815, Val Loss: 0.0730\n",
      "Epoch 14/300 - Train Loss: 0.0779, Val Loss: 0.0687\n",
      "Epoch 15/300 - Train Loss: 0.0802, Val Loss: 0.0715\n",
      "Epoch 16/300 - Train Loss: 0.0769, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.0785, Val Loss: 0.1256\n",
      "Epoch 18/300 - Train Loss: 0.0789, Val Loss: 0.0707\n",
      "Epoch 19/300 - Train Loss: 0.0793, Val Loss: 0.0814\n",
      "Epoch 20/300 - Train Loss: 0.0761, Val Loss: 0.0749\n",
      "Epoch 21/300 - Train Loss: 0.0793, Val Loss: 0.0769\n",
      "Epoch 22/300 - Train Loss: 0.0740, Val Loss: 0.0761\n",
      "Epoch 23/300 - Train Loss: 0.0755, Val Loss: 0.0724\n",
      "Epoch 24/300 - Train Loss: 0.0771, Val Loss: 0.0751\n",
      "Epoch 25/300 - Train Loss: 0.0732, Val Loss: 0.0748\n",
      "Epoch 26/300 - Train Loss: 0.0748, Val Loss: 0.0706\n",
      "Epoch 27/300 - Train Loss: 0.0749, Val Loss: 0.0686\n",
      "Epoch 28/300 - Train Loss: 0.0753, Val Loss: 0.0726\n",
      "Epoch 29/300 - Train Loss: 0.0757, Val Loss: 0.0777\n",
      "Epoch 30/300 - Train Loss: 0.0715, Val Loss: 0.0687\n",
      "Epoch 31/300 - Train Loss: 0.0744, Val Loss: 0.0774\n",
      "Epoch 32/300 - Train Loss: 0.0759, Val Loss: 0.0666\n",
      "Epoch 33/300 - Train Loss: 0.0753, Val Loss: 0.0679\n",
      "Epoch 34/300 - Train Loss: 0.0726, Val Loss: 0.0707\n",
      "Epoch 35/300 - Train Loss: 0.0728, Val Loss: 0.0752\n",
      "Epoch 36/300 - Train Loss: 0.0735, Val Loss: 0.0697\n",
      "Epoch 37/300 - Train Loss: 0.0748, Val Loss: 0.0676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:13:52,468] Trial 396 finished with value: 0.9648288783717497 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.18677357943887077, 'learning_rate': 0.0009935471030215405, 'batch_size': 32, 'weight_decay': 3.747907459138005e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300 - Train Loss: 0.0725, Val Loss: 0.0716\n",
      "Early stopping at epoch 38\n",
      "Macro F1 Score: 0.9648, Macro Precision: 0.9607, Macro Recall: 0.9691\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.98      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 398\n",
      "Training with F1=32, F2=8, D=2, dropout=0.20795740014109546, LR=0.0009041554420523165, BS=32, WD=4.15843244032849e-05\n",
      "Epoch 1/300 - Train Loss: 0.1567, Val Loss: 0.0902\n",
      "Epoch 2/300 - Train Loss: 0.1003, Val Loss: 0.1022\n",
      "Epoch 3/300 - Train Loss: 0.0950, Val Loss: 0.0845\n",
      "Epoch 4/300 - Train Loss: 0.0923, Val Loss: 0.0725\n",
      "Epoch 5/300 - Train Loss: 0.0895, Val Loss: 0.0682\n",
      "Epoch 6/300 - Train Loss: 0.0888, Val Loss: 0.0720\n",
      "Epoch 7/300 - Train Loss: 0.0864, Val Loss: 0.0761\n",
      "Epoch 8/300 - Train Loss: 0.0846, Val Loss: 0.0751\n",
      "Epoch 9/300 - Train Loss: 0.0860, Val Loss: 0.0796\n",
      "Epoch 10/300 - Train Loss: 0.0826, Val Loss: 0.0737\n",
      "Epoch 11/300 - Train Loss: 0.0817, Val Loss: 0.0820\n",
      "Epoch 12/300 - Train Loss: 0.0816, Val Loss: 0.0748\n",
      "Epoch 13/300 - Train Loss: 0.0835, Val Loss: 0.0707\n",
      "Epoch 14/300 - Train Loss: 0.0815, Val Loss: 0.0722\n",
      "Epoch 15/300 - Train Loss: 0.0795, Val Loss: 0.0754\n",
      "Epoch 16/300 - Train Loss: 0.0795, Val Loss: 0.0712\n",
      "Epoch 17/300 - Train Loss: 0.0782, Val Loss: 0.0705\n",
      "Epoch 18/300 - Train Loss: 0.0816, Val Loss: 0.0707\n",
      "Epoch 19/300 - Train Loss: 0.0808, Val Loss: 0.0793\n",
      "Epoch 20/300 - Train Loss: 0.0801, Val Loss: 0.0735\n",
      "Epoch 21/300 - Train Loss: 0.0798, Val Loss: 0.0789\n",
      "Epoch 22/300 - Train Loss: 0.0763, Val Loss: 0.0704\n",
      "Epoch 23/300 - Train Loss: 0.0779, Val Loss: 0.0711\n",
      "Epoch 24/300 - Train Loss: 0.0788, Val Loss: 0.0708\n",
      "Epoch 25/300 - Train Loss: 0.0777, Val Loss: 0.0759\n",
      "Epoch 26/300 - Train Loss: 0.0750, Val Loss: 0.0729\n",
      "Epoch 27/300 - Train Loss: 0.0769, Val Loss: 0.0714\n",
      "Epoch 28/300 - Train Loss: 0.0763, Val Loss: 0.0723\n",
      "Epoch 29/300 - Train Loss: 0.0745, Val Loss: 0.0720\n",
      "Epoch 30/300 - Train Loss: 0.0753, Val Loss: 0.0806\n",
      "Epoch 31/300 - Train Loss: 0.0770, Val Loss: 0.0759\n",
      "Epoch 32/300 - Train Loss: 0.0762, Val Loss: 0.0814\n",
      "Epoch 33/300 - Train Loss: 0.0750, Val Loss: 0.0753\n",
      "Epoch 34/300 - Train Loss: 0.0740, Val Loss: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:15:20,446] Trial 397 finished with value: 0.9668016368766814 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.20795740014109546, 'learning_rate': 0.0009041554420523165, 'batch_size': 32, 'weight_decay': 4.15843244032849e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/300 - Train Loss: 0.0767, Val Loss: 0.0798\n",
      "Early stopping at epoch 35\n",
      "Macro F1 Score: 0.9668, Macro Precision: 0.9627, Macro Recall: 0.9711\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 399\n",
      "Training with F1=32, F2=8, D=2, dropout=0.18818456275531426, LR=0.0008354285049499549, BS=32, WD=3.363325646213781e-05\n",
      "Epoch 1/300 - Train Loss: 0.1547, Val Loss: 0.0725\n",
      "Epoch 2/300 - Train Loss: 0.0975, Val Loss: 0.1034\n",
      "Epoch 3/300 - Train Loss: 0.0912, Val Loss: 0.0701\n",
      "Epoch 4/300 - Train Loss: 0.0904, Val Loss: 0.0717\n",
      "Epoch 5/300 - Train Loss: 0.0874, Val Loss: 0.0825\n",
      "Epoch 6/300 - Train Loss: 0.0847, Val Loss: 0.0805\n",
      "Epoch 7/300 - Train Loss: 0.0845, Val Loss: 0.0780\n",
      "Epoch 8/300 - Train Loss: 0.0831, Val Loss: 0.0706\n",
      "Epoch 9/300 - Train Loss: 0.0860, Val Loss: 0.0694\n",
      "Epoch 10/300 - Train Loss: 0.0814, Val Loss: 0.0747\n",
      "Epoch 11/300 - Train Loss: 0.0801, Val Loss: 0.0774\n",
      "Epoch 12/300 - Train Loss: 0.0818, Val Loss: 0.0731\n",
      "Epoch 13/300 - Train Loss: 0.0789, Val Loss: 0.0771\n",
      "Epoch 14/300 - Train Loss: 0.0819, Val Loss: 0.0674\n",
      "Epoch 15/300 - Train Loss: 0.0791, Val Loss: 0.0723\n",
      "Epoch 16/300 - Train Loss: 0.0769, Val Loss: 0.0708\n",
      "Epoch 17/300 - Train Loss: 0.0769, Val Loss: 0.0718\n",
      "Epoch 18/300 - Train Loss: 0.0791, Val Loss: 0.0729\n",
      "Epoch 19/300 - Train Loss: 0.0776, Val Loss: 0.0731\n",
      "Epoch 20/300 - Train Loss: 0.0752, Val Loss: 0.0724\n",
      "Epoch 21/300 - Train Loss: 0.0752, Val Loss: 0.0704\n",
      "Epoch 22/300 - Train Loss: 0.0738, Val Loss: 0.0730\n",
      "Epoch 23/300 - Train Loss: 0.0742, Val Loss: 0.0719\n",
      "Epoch 24/300 - Train Loss: 0.0756, Val Loss: 0.0662\n",
      "Epoch 25/300 - Train Loss: 0.0721, Val Loss: 0.0710\n",
      "Epoch 26/300 - Train Loss: 0.0747, Val Loss: 0.0722\n",
      "Epoch 27/300 - Train Loss: 0.0739, Val Loss: 0.0725\n",
      "Epoch 28/300 - Train Loss: 0.0730, Val Loss: 0.0735\n",
      "Epoch 29/300 - Train Loss: 0.0718, Val Loss: 0.0694\n",
      "Epoch 30/300 - Train Loss: 0.0710, Val Loss: 0.0724\n",
      "Epoch 31/300 - Train Loss: 0.0693, Val Loss: 0.0716\n",
      "Epoch 32/300 - Train Loss: 0.0719, Val Loss: 0.0719\n",
      "Epoch 33/300 - Train Loss: 0.0693, Val Loss: 0.0713\n",
      "Epoch 34/300 - Train Loss: 0.0692, Val Loss: 0.0684\n",
      "Epoch 35/300 - Train Loss: 0.0698, Val Loss: 0.0752\n",
      "Epoch 36/300 - Train Loss: 0.0688, Val Loss: 0.0780\n",
      "Epoch 37/300 - Train Loss: 0.0695, Val Loss: 0.0666\n",
      "Epoch 38/300 - Train Loss: 0.0699, Val Loss: 0.0752\n",
      "Epoch 39/300 - Train Loss: 0.0682, Val Loss: 0.0682\n",
      "Epoch 40/300 - Train Loss: 0.0706, Val Loss: 0.0717\n",
      "Epoch 41/300 - Train Loss: 0.0680, Val Loss: 0.0675\n",
      "Epoch 42/300 - Train Loss: 0.0678, Val Loss: 0.0710\n",
      "Epoch 43/300 - Train Loss: 0.0679, Val Loss: 0.0661\n",
      "Epoch 44/300 - Train Loss: 0.0688, Val Loss: 0.0748\n",
      "Epoch 45/300 - Train Loss: 0.0708, Val Loss: 0.0682\n",
      "Epoch 46/300 - Train Loss: 0.0659, Val Loss: 0.0696\n",
      "Epoch 47/300 - Train Loss: 0.0689, Val Loss: 0.0705\n",
      "Epoch 48/300 - Train Loss: 0.0667, Val Loss: 0.0715\n",
      "Epoch 49/300 - Train Loss: 0.0683, Val Loss: 0.0720\n",
      "Epoch 50/300 - Train Loss: 0.0658, Val Loss: 0.0724\n",
      "Epoch 51/300 - Train Loss: 0.0673, Val Loss: 0.0711\n",
      "Epoch 52/300 - Train Loss: 0.0657, Val Loss: 0.0719\n",
      "Epoch 53/300 - Train Loss: 0.0669, Val Loss: 0.0745\n",
      "Epoch 54/300 - Train Loss: 0.0672, Val Loss: 0.0674\n",
      "Epoch 55/300 - Train Loss: 0.0653, Val Loss: 0.0701\n",
      "Epoch 56/300 - Train Loss: 0.0616, Val Loss: 0.0683\n",
      "Epoch 57/300 - Train Loss: 0.0649, Val Loss: 0.0691\n",
      "Epoch 58/300 - Train Loss: 0.0637, Val Loss: 0.0717\n",
      "Epoch 59/300 - Train Loss: 0.0641, Val Loss: 0.0685\n",
      "Epoch 60/300 - Train Loss: 0.0636, Val Loss: 0.0695\n",
      "Epoch 61/300 - Train Loss: 0.0645, Val Loss: 0.0655\n",
      "Epoch 62/300 - Train Loss: 0.0631, Val Loss: 0.0706\n",
      "Epoch 63/300 - Train Loss: 0.0631, Val Loss: 0.0711\n",
      "Epoch 64/300 - Train Loss: 0.0656, Val Loss: 0.0674\n",
      "Epoch 65/300 - Train Loss: 0.0619, Val Loss: 0.0662\n",
      "Epoch 66/300 - Train Loss: 0.0646, Val Loss: 0.0730\n",
      "Epoch 67/300 - Train Loss: 0.0610, Val Loss: 0.0745\n",
      "Epoch 68/300 - Train Loss: 0.0652, Val Loss: 0.0673\n",
      "Epoch 69/300 - Train Loss: 0.0662, Val Loss: 0.0756\n",
      "Epoch 70/300 - Train Loss: 0.0612, Val Loss: 0.0694\n",
      "Epoch 71/300 - Train Loss: 0.0625, Val Loss: 0.0664\n",
      "Epoch 72/300 - Train Loss: 0.0597, Val Loss: 0.0721\n",
      "Epoch 73/300 - Train Loss: 0.0632, Val Loss: 0.0685\n",
      "Epoch 74/300 - Train Loss: 0.0634, Val Loss: 0.0703\n",
      "Epoch 75/300 - Train Loss: 0.0633, Val Loss: 0.0727\n",
      "Epoch 76/300 - Train Loss: 0.0596, Val Loss: 0.0652\n",
      "Epoch 77/300 - Train Loss: 0.0626, Val Loss: 0.0710\n",
      "Epoch 78/300 - Train Loss: 0.0612, Val Loss: 0.0664\n",
      "Epoch 79/300 - Train Loss: 0.0609, Val Loss: 0.0687\n",
      "Epoch 80/300 - Train Loss: 0.0606, Val Loss: 0.0658\n",
      "Epoch 81/300 - Train Loss: 0.0586, Val Loss: 0.0702\n",
      "Epoch 82/300 - Train Loss: 0.0598, Val Loss: 0.0684\n",
      "Epoch 83/300 - Train Loss: 0.0604, Val Loss: 0.0678\n",
      "Epoch 84/300 - Train Loss: 0.0585, Val Loss: 0.0702\n",
      "Epoch 85/300 - Train Loss: 0.0577, Val Loss: 0.0726\n",
      "Epoch 86/300 - Train Loss: 0.0596, Val Loss: 0.0747\n",
      "Epoch 87/300 - Train Loss: 0.0613, Val Loss: 0.0656\n",
      "Epoch 88/300 - Train Loss: 0.0588, Val Loss: 0.0736\n",
      "Epoch 89/300 - Train Loss: 0.0647, Val Loss: 0.0701\n",
      "Epoch 90/300 - Train Loss: 0.0596, Val Loss: 0.0659\n",
      "Epoch 91/300 - Train Loss: 0.0602, Val Loss: 0.0726\n",
      "Epoch 92/300 - Train Loss: 0.0590, Val Loss: 0.0632\n",
      "Epoch 93/300 - Train Loss: 0.0579, Val Loss: 0.0740\n",
      "Epoch 94/300 - Train Loss: 0.0598, Val Loss: 0.0642\n",
      "Epoch 95/300 - Train Loss: 0.0585, Val Loss: 0.0709\n",
      "Epoch 96/300 - Train Loss: 0.0593, Val Loss: 0.0672\n",
      "Epoch 97/300 - Train Loss: 0.0576, Val Loss: 0.0718\n",
      "Epoch 98/300 - Train Loss: 0.0565, Val Loss: 0.0734\n",
      "Epoch 99/300 - Train Loss: 0.0591, Val Loss: 0.0716\n",
      "Epoch 100/300 - Train Loss: 0.0570, Val Loss: 0.0826\n",
      "Epoch 101/300 - Train Loss: 0.0542, Val Loss: 0.0672\n",
      "Epoch 102/300 - Train Loss: 0.0583, Val Loss: 0.0678\n",
      "Epoch 103/300 - Train Loss: 0.0588, Val Loss: 0.0683\n",
      "Epoch 104/300 - Train Loss: 0.0557, Val Loss: 0.0651\n",
      "Epoch 105/300 - Train Loss: 0.0620, Val Loss: 0.0729\n",
      "Epoch 106/300 - Train Loss: 0.0567, Val Loss: 0.0685\n",
      "Epoch 107/300 - Train Loss: 0.0563, Val Loss: 0.0704\n",
      "Epoch 108/300 - Train Loss: 0.0542, Val Loss: 0.0754\n",
      "Epoch 109/300 - Train Loss: 0.0566, Val Loss: 0.0748\n",
      "Epoch 110/300 - Train Loss: 0.0561, Val Loss: 0.0730\n",
      "Epoch 111/300 - Train Loss: 0.0575, Val Loss: 0.0739\n",
      "Epoch 112/300 - Train Loss: 0.0536, Val Loss: 0.0737\n",
      "Epoch 113/300 - Train Loss: 0.0545, Val Loss: 0.0732\n",
      "Epoch 114/300 - Train Loss: 0.0568, Val Loss: 0.0634\n",
      "Epoch 115/300 - Train Loss: 0.0561, Val Loss: 0.0661\n",
      "Epoch 116/300 - Train Loss: 0.0536, Val Loss: 0.0665\n",
      "Epoch 117/300 - Train Loss: 0.0546, Val Loss: 0.0727\n",
      "Epoch 118/300 - Train Loss: 0.0566, Val Loss: 0.0645\n",
      "Epoch 119/300 - Train Loss: 0.0536, Val Loss: 0.0653\n",
      "Epoch 120/300 - Train Loss: 0.0572, Val Loss: 0.0645\n",
      "Epoch 121/300 - Train Loss: 0.0559, Val Loss: 0.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:20:27,608] Trial 398 finished with value: 0.9691263363389607 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.18818456275531426, 'learning_rate': 0.0008354285049499549, 'batch_size': 32, 'weight_decay': 3.363325646213781e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/300 - Train Loss: 0.0564, Val Loss: 0.0692\n",
      "Early stopping at epoch 122\n",
      "Macro F1 Score: 0.9691, Macro Precision: 0.9674, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 400\n",
      "Training with F1=32, F2=8, D=2, dropout=0.19985907458190688, LR=0.0007606762887033571, BS=32, WD=4.5246329874950225e-05\n",
      "Epoch 1/300 - Train Loss: 0.1671, Val Loss: 0.0753\n",
      "Epoch 2/300 - Train Loss: 0.0986, Val Loss: 0.0778\n",
      "Epoch 3/300 - Train Loss: 0.0916, Val Loss: 0.0751\n",
      "Epoch 4/300 - Train Loss: 0.0905, Val Loss: 0.0785\n",
      "Epoch 5/300 - Train Loss: 0.0915, Val Loss: 0.0672\n",
      "Epoch 6/300 - Train Loss: 0.0857, Val Loss: 0.0718\n",
      "Epoch 7/300 - Train Loss: 0.0864, Val Loss: 0.0691\n",
      "Epoch 8/300 - Train Loss: 0.0872, Val Loss: 0.0719\n",
      "Epoch 9/300 - Train Loss: 0.0840, Val Loss: 0.0677\n",
      "Epoch 10/300 - Train Loss: 0.0829, Val Loss: 0.0729\n",
      "Epoch 11/300 - Train Loss: 0.0834, Val Loss: 0.0754\n",
      "Epoch 12/300 - Train Loss: 0.0798, Val Loss: 0.0687\n",
      "Epoch 13/300 - Train Loss: 0.0816, Val Loss: 0.0704\n",
      "Epoch 14/300 - Train Loss: 0.0803, Val Loss: 0.0696\n",
      "Epoch 15/300 - Train Loss: 0.0810, Val Loss: 0.0665\n",
      "Epoch 16/300 - Train Loss: 0.0785, Val Loss: 0.0720\n",
      "Epoch 17/300 - Train Loss: 0.0807, Val Loss: 0.0689\n",
      "Epoch 18/300 - Train Loss: 0.0805, Val Loss: 0.0695\n",
      "Epoch 19/300 - Train Loss: 0.0780, Val Loss: 0.0684\n",
      "Epoch 20/300 - Train Loss: 0.0765, Val Loss: 0.0690\n",
      "Epoch 21/300 - Train Loss: 0.0760, Val Loss: 0.0724\n",
      "Epoch 22/300 - Train Loss: 0.0764, Val Loss: 0.0744\n",
      "Epoch 23/300 - Train Loss: 0.0754, Val Loss: 0.0705\n",
      "Epoch 24/300 - Train Loss: 0.0767, Val Loss: 0.0759\n",
      "Epoch 25/300 - Train Loss: 0.0763, Val Loss: 0.0890\n",
      "Epoch 26/300 - Train Loss: 0.0734, Val Loss: 0.0704\n",
      "Epoch 27/300 - Train Loss: 0.0723, Val Loss: 0.0741\n",
      "Epoch 28/300 - Train Loss: 0.0721, Val Loss: 0.0729\n",
      "Epoch 29/300 - Train Loss: 0.0731, Val Loss: 0.0808\n",
      "Epoch 30/300 - Train Loss: 0.0721, Val Loss: 0.0738\n",
      "Epoch 31/300 - Train Loss: 0.0733, Val Loss: 0.0685\n",
      "Epoch 32/300 - Train Loss: 0.0732, Val Loss: 0.0720\n",
      "Epoch 33/300 - Train Loss: 0.0728, Val Loss: 0.0679\n",
      "Epoch 34/300 - Train Loss: 0.0731, Val Loss: 0.0754\n",
      "Epoch 35/300 - Train Loss: 0.0713, Val Loss: 0.0803\n",
      "Epoch 36/300 - Train Loss: 0.0725, Val Loss: 0.0711\n",
      "Epoch 37/300 - Train Loss: 0.0739, Val Loss: 0.0732\n",
      "Epoch 38/300 - Train Loss: 0.0727, Val Loss: 0.0762\n",
      "Epoch 39/300 - Train Loss: 0.0736, Val Loss: 0.0723\n",
      "Epoch 40/300 - Train Loss: 0.0697, Val Loss: 0.0734\n",
      "Epoch 41/300 - Train Loss: 0.0701, Val Loss: 0.0734\n",
      "Epoch 42/300 - Train Loss: 0.0710, Val Loss: 0.0831\n",
      "Epoch 43/300 - Train Loss: 0.0709, Val Loss: 0.0720\n",
      "Epoch 44/300 - Train Loss: 0.0694, Val Loss: 0.0725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:22:21,772] Trial 399 finished with value: 0.9703317503331456 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.19985907458190688, 'learning_rate': 0.0007606762887033571, 'batch_size': 32, 'weight_decay': 4.5246329874950225e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/300 - Train Loss: 0.0682, Val Loss: 0.0797\n",
      "Early stopping at epoch 45\n",
      "Macro F1 Score: 0.9703, Macro Precision: 0.9644, Macro Recall: 0.9767\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 401\n",
      "Training with F1=32, F2=8, D=2, dropout=0.18205449351494718, LR=0.0006982763709962923, BS=32, WD=5.5211009316937715e-05\n",
      "Epoch 1/300 - Train Loss: 0.1605, Val Loss: 0.0895\n",
      "Epoch 2/300 - Train Loss: 0.0981, Val Loss: 0.0740\n",
      "Epoch 3/300 - Train Loss: 0.0928, Val Loss: 0.0729\n",
      "Epoch 4/300 - Train Loss: 0.0918, Val Loss: 0.0782\n",
      "Epoch 5/300 - Train Loss: 0.0869, Val Loss: 0.0725\n",
      "Epoch 6/300 - Train Loss: 0.0873, Val Loss: 0.0886\n",
      "Epoch 7/300 - Train Loss: 0.0878, Val Loss: 0.0716\n",
      "Epoch 8/300 - Train Loss: 0.0850, Val Loss: 0.0734\n",
      "Epoch 9/300 - Train Loss: 0.0836, Val Loss: 0.0702\n",
      "Epoch 10/300 - Train Loss: 0.0851, Val Loss: 0.0703\n",
      "Epoch 11/300 - Train Loss: 0.0838, Val Loss: 0.0735\n",
      "Epoch 12/300 - Train Loss: 0.0797, Val Loss: 0.0737\n",
      "Epoch 13/300 - Train Loss: 0.0811, Val Loss: 0.0702\n",
      "Epoch 14/300 - Train Loss: 0.0804, Val Loss: 0.0836\n",
      "Epoch 15/300 - Train Loss: 0.0788, Val Loss: 0.0835\n",
      "Epoch 16/300 - Train Loss: 0.0783, Val Loss: 0.0742\n",
      "Epoch 17/300 - Train Loss: 0.0784, Val Loss: 0.0678\n",
      "Epoch 18/300 - Train Loss: 0.0765, Val Loss: 0.0710\n",
      "Epoch 19/300 - Train Loss: 0.0781, Val Loss: 0.0694\n",
      "Epoch 20/300 - Train Loss: 0.0775, Val Loss: 0.0754\n",
      "Epoch 21/300 - Train Loss: 0.0785, Val Loss: 0.0658\n",
      "Epoch 22/300 - Train Loss: 0.0780, Val Loss: 0.0709\n",
      "Epoch 23/300 - Train Loss: 0.0761, Val Loss: 0.0708\n",
      "Epoch 24/300 - Train Loss: 0.0748, Val Loss: 0.0673\n",
      "Epoch 25/300 - Train Loss: 0.0731, Val Loss: 0.0668\n",
      "Epoch 26/300 - Train Loss: 0.0742, Val Loss: 0.0715\n",
      "Epoch 27/300 - Train Loss: 0.0731, Val Loss: 0.0716\n",
      "Epoch 28/300 - Train Loss: 0.0761, Val Loss: 0.0727\n",
      "Epoch 29/300 - Train Loss: 0.0748, Val Loss: 0.0711\n",
      "Epoch 30/300 - Train Loss: 0.0729, Val Loss: 0.0655\n",
      "Epoch 31/300 - Train Loss: 0.0737, Val Loss: 0.0715\n",
      "Epoch 32/300 - Train Loss: 0.0743, Val Loss: 0.0716\n",
      "Epoch 33/300 - Train Loss: 0.0719, Val Loss: 0.0709\n",
      "Epoch 34/300 - Train Loss: 0.0711, Val Loss: 0.0673\n",
      "Epoch 35/300 - Train Loss: 0.0730, Val Loss: 0.0699\n",
      "Epoch 36/300 - Train Loss: 0.0720, Val Loss: 0.0699\n",
      "Epoch 37/300 - Train Loss: 0.0702, Val Loss: 0.0745\n",
      "Epoch 38/300 - Train Loss: 0.0732, Val Loss: 0.0694\n",
      "Epoch 39/300 - Train Loss: 0.0728, Val Loss: 0.0699\n",
      "Epoch 40/300 - Train Loss: 0.0695, Val Loss: 0.0744\n",
      "Epoch 41/300 - Train Loss: 0.0676, Val Loss: 0.0697\n",
      "Epoch 42/300 - Train Loss: 0.0714, Val Loss: 0.0703\n",
      "Epoch 43/300 - Train Loss: 0.0680, Val Loss: 0.0676\n",
      "Epoch 44/300 - Train Loss: 0.0685, Val Loss: 0.0668\n",
      "Epoch 45/300 - Train Loss: 0.0679, Val Loss: 0.0743\n",
      "Epoch 46/300 - Train Loss: 0.0693, Val Loss: 0.0648\n",
      "Epoch 47/300 - Train Loss: 0.0705, Val Loss: 0.0826\n",
      "Epoch 48/300 - Train Loss: 0.0672, Val Loss: 0.0688\n",
      "Epoch 49/300 - Train Loss: 0.0686, Val Loss: 0.0826\n",
      "Epoch 50/300 - Train Loss: 0.0667, Val Loss: 0.0695\n",
      "Epoch 51/300 - Train Loss: 0.0689, Val Loss: 0.0746\n",
      "Epoch 52/300 - Train Loss: 0.0650, Val Loss: 0.0719\n",
      "Epoch 53/300 - Train Loss: 0.0663, Val Loss: 0.0704\n",
      "Epoch 54/300 - Train Loss: 0.0650, Val Loss: 0.0772\n",
      "Epoch 55/300 - Train Loss: 0.0649, Val Loss: 0.0727\n",
      "Epoch 56/300 - Train Loss: 0.0673, Val Loss: 0.0811\n",
      "Epoch 57/300 - Train Loss: 0.0672, Val Loss: 0.0681\n",
      "Epoch 58/300 - Train Loss: 0.0659, Val Loss: 0.0832\n",
      "Epoch 59/300 - Train Loss: 0.0667, Val Loss: 0.0716\n",
      "Epoch 60/300 - Train Loss: 0.0645, Val Loss: 0.0791\n",
      "Epoch 61/300 - Train Loss: 0.0639, Val Loss: 0.0717\n",
      "Epoch 62/300 - Train Loss: 0.0633, Val Loss: 0.0754\n",
      "Epoch 63/300 - Train Loss: 0.0653, Val Loss: 0.0807\n",
      "Epoch 64/300 - Train Loss: 0.0645, Val Loss: 0.0672\n",
      "Epoch 65/300 - Train Loss: 0.0642, Val Loss: 0.0674\n",
      "Epoch 66/300 - Train Loss: 0.0665, Val Loss: 0.0689\n",
      "Epoch 67/300 - Train Loss: 0.0643, Val Loss: 0.0693\n",
      "Epoch 68/300 - Train Loss: 0.0632, Val Loss: 0.0716\n",
      "Epoch 69/300 - Train Loss: 0.0658, Val Loss: 0.0801\n",
      "Epoch 70/300 - Train Loss: 0.0627, Val Loss: 0.0774\n",
      "Epoch 71/300 - Train Loss: 0.0647, Val Loss: 0.0714\n",
      "Epoch 72/300 - Train Loss: 0.0641, Val Loss: 0.0680\n",
      "Epoch 73/300 - Train Loss: 0.0637, Val Loss: 0.0700\n",
      "Epoch 74/300 - Train Loss: 0.0642, Val Loss: 0.0781\n",
      "Epoch 75/300 - Train Loss: 0.0631, Val Loss: 0.0717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:25:34,367] Trial 400 finished with value: 0.9664631507415281 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.18205449351494718, 'learning_rate': 0.0006982763709962923, 'batch_size': 32, 'weight_decay': 5.5211009316937715e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300 - Train Loss: 0.0624, Val Loss: 0.0730\n",
      "Early stopping at epoch 76\n",
      "Macro F1 Score: 0.9665, Macro Precision: 0.9666, Macro Recall: 0.9663\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 402\n",
      "Training with F1=32, F2=8, D=2, dropout=0.22107502052827074, LR=0.0008680441952519408, BS=32, WD=4.446592276464173e-05\n",
      "Epoch 1/300 - Train Loss: 0.1616, Val Loss: 0.0996\n",
      "Epoch 2/300 - Train Loss: 0.0999, Val Loss: 0.0762\n",
      "Epoch 3/300 - Train Loss: 0.0935, Val Loss: 0.0875\n",
      "Epoch 4/300 - Train Loss: 0.0943, Val Loss: 0.0726\n",
      "Epoch 5/300 - Train Loss: 0.0904, Val Loss: 0.0748\n",
      "Epoch 6/300 - Train Loss: 0.0884, Val Loss: 0.0719\n",
      "Epoch 7/300 - Train Loss: 0.0879, Val Loss: 0.0775\n",
      "Epoch 8/300 - Train Loss: 0.0846, Val Loss: 0.0796\n",
      "Epoch 9/300 - Train Loss: 0.0856, Val Loss: 0.0753\n",
      "Epoch 10/300 - Train Loss: 0.0856, Val Loss: 0.0789\n",
      "Epoch 11/300 - Train Loss: 0.0843, Val Loss: 0.0709\n",
      "Epoch 12/300 - Train Loss: 0.0833, Val Loss: 0.0783\n",
      "Epoch 13/300 - Train Loss: 0.0812, Val Loss: 0.0756\n",
      "Epoch 14/300 - Train Loss: 0.0823, Val Loss: 0.0704\n",
      "Epoch 15/300 - Train Loss: 0.0814, Val Loss: 0.0804\n",
      "Epoch 16/300 - Train Loss: 0.0818, Val Loss: 0.0724\n",
      "Epoch 17/300 - Train Loss: 0.0780, Val Loss: 0.0763\n",
      "Epoch 18/300 - Train Loss: 0.0793, Val Loss: 0.0727\n",
      "Epoch 19/300 - Train Loss: 0.0791, Val Loss: 0.0799\n",
      "Epoch 20/300 - Train Loss: 0.0793, Val Loss: 0.0734\n",
      "Epoch 21/300 - Train Loss: 0.0770, Val Loss: 0.0781\n",
      "Epoch 22/300 - Train Loss: 0.0778, Val Loss: 0.0796\n",
      "Epoch 23/300 - Train Loss: 0.0773, Val Loss: 0.0744\n",
      "Epoch 24/300 - Train Loss: 0.0809, Val Loss: 0.0726\n",
      "Epoch 25/300 - Train Loss: 0.0774, Val Loss: 0.0745\n",
      "Epoch 26/300 - Train Loss: 0.0753, Val Loss: 0.0798\n",
      "Epoch 27/300 - Train Loss: 0.0810, Val Loss: 0.0726\n",
      "Epoch 28/300 - Train Loss: 0.0751, Val Loss: 0.0793\n",
      "Epoch 29/300 - Train Loss: 0.0774, Val Loss: 0.0762\n",
      "Epoch 30/300 - Train Loss: 0.0763, Val Loss: 0.0751\n",
      "Epoch 31/300 - Train Loss: 0.0746, Val Loss: 0.0739\n",
      "Epoch 32/300 - Train Loss: 0.0742, Val Loss: 0.1000\n",
      "Epoch 33/300 - Train Loss: 0.0757, Val Loss: 0.0776\n",
      "Epoch 34/300 - Train Loss: 0.0775, Val Loss: 0.0729\n",
      "Epoch 35/300 - Train Loss: 0.0753, Val Loss: 0.0847\n",
      "Epoch 36/300 - Train Loss: 0.0750, Val Loss: 0.0790\n",
      "Epoch 37/300 - Train Loss: 0.0716, Val Loss: 0.0736\n",
      "Epoch 38/300 - Train Loss: 0.0754, Val Loss: 0.0708\n",
      "Epoch 39/300 - Train Loss: 0.0710, Val Loss: 0.0721\n",
      "Epoch 40/300 - Train Loss: 0.0752, Val Loss: 0.1014\n",
      "Epoch 41/300 - Train Loss: 0.0728, Val Loss: 0.0763\n",
      "Epoch 42/300 - Train Loss: 0.0728, Val Loss: 0.0729\n",
      "Epoch 43/300 - Train Loss: 0.0725, Val Loss: 0.0764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:27:25,899] Trial 401 finished with value: 0.9677815532238739 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.22107502052827074, 'learning_rate': 0.0008680441952519408, 'batch_size': 32, 'weight_decay': 4.446592276464173e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300 - Train Loss: 0.0740, Val Loss: 0.0795\n",
      "Early stopping at epoch 44\n",
      "Macro F1 Score: 0.9678, Macro Precision: 0.9640, Macro Recall: 0.9718\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 403\n",
      "Training with F1=32, F2=8, D=2, dropout=0.18270151445101146, LR=0.0008944656628286094, BS=32, WD=3.7529181162096235e-05\n",
      "Epoch 1/300 - Train Loss: 0.1595, Val Loss: 0.0809\n",
      "Epoch 2/300 - Train Loss: 0.0987, Val Loss: 0.0738\n",
      "Epoch 3/300 - Train Loss: 0.0942, Val Loss: 0.0725\n",
      "Epoch 4/300 - Train Loss: 0.0894, Val Loss: 0.0716\n",
      "Epoch 5/300 - Train Loss: 0.0860, Val Loss: 0.0792\n",
      "Epoch 6/300 - Train Loss: 0.0848, Val Loss: 0.0750\n",
      "Epoch 7/300 - Train Loss: 0.0847, Val Loss: 0.0692\n",
      "Epoch 8/300 - Train Loss: 0.0869, Val Loss: 0.0681\n",
      "Epoch 9/300 - Train Loss: 0.0830, Val Loss: 0.0737\n",
      "Epoch 10/300 - Train Loss: 0.0823, Val Loss: 0.0716\n",
      "Epoch 11/300 - Train Loss: 0.0801, Val Loss: 0.0794\n",
      "Epoch 12/300 - Train Loss: 0.0803, Val Loss: 0.0718\n",
      "Epoch 13/300 - Train Loss: 0.0808, Val Loss: 0.0721\n",
      "Epoch 14/300 - Train Loss: 0.0810, Val Loss: 0.0679\n",
      "Epoch 15/300 - Train Loss: 0.0804, Val Loss: 0.0691\n",
      "Epoch 16/300 - Train Loss: 0.0766, Val Loss: 0.0722\n",
      "Epoch 17/300 - Train Loss: 0.0750, Val Loss: 0.0763\n",
      "Epoch 18/300 - Train Loss: 0.0768, Val Loss: 0.0703\n",
      "Epoch 19/300 - Train Loss: 0.0797, Val Loss: 0.0647\n",
      "Epoch 20/300 - Train Loss: 0.0773, Val Loss: 0.0727\n",
      "Epoch 21/300 - Train Loss: 0.0744, Val Loss: 0.0667\n",
      "Epoch 22/300 - Train Loss: 0.0733, Val Loss: 0.0806\n",
      "Epoch 23/300 - Train Loss: 0.0736, Val Loss: 0.0719\n",
      "Epoch 24/300 - Train Loss: 0.0720, Val Loss: 0.0675\n",
      "Epoch 25/300 - Train Loss: 0.0752, Val Loss: 0.0743\n",
      "Epoch 26/300 - Train Loss: 0.0747, Val Loss: 0.0638\n",
      "Epoch 27/300 - Train Loss: 0.0728, Val Loss: 0.0706\n",
      "Epoch 28/300 - Train Loss: 0.0742, Val Loss: 0.0682\n",
      "Epoch 29/300 - Train Loss: 0.0705, Val Loss: 0.0670\n",
      "Epoch 30/300 - Train Loss: 0.0715, Val Loss: 0.0723\n",
      "Epoch 31/300 - Train Loss: 0.0725, Val Loss: 0.0718\n",
      "Epoch 32/300 - Train Loss: 0.0721, Val Loss: 0.0681\n",
      "Epoch 33/300 - Train Loss: 0.0684, Val Loss: 0.0708\n",
      "Epoch 34/300 - Train Loss: 0.0688, Val Loss: 0.0737\n",
      "Epoch 35/300 - Train Loss: 0.0732, Val Loss: 0.0657\n",
      "Epoch 36/300 - Train Loss: 0.0704, Val Loss: 0.0792\n",
      "Epoch 37/300 - Train Loss: 0.0709, Val Loss: 0.0706\n",
      "Epoch 38/300 - Train Loss: 0.0664, Val Loss: 0.0695\n",
      "Epoch 39/300 - Train Loss: 0.0674, Val Loss: 0.0700\n",
      "Epoch 40/300 - Train Loss: 0.0670, Val Loss: 0.0745\n",
      "Epoch 41/300 - Train Loss: 0.0680, Val Loss: 0.0697\n",
      "Epoch 42/300 - Train Loss: 0.0673, Val Loss: 0.0792\n",
      "Epoch 43/300 - Train Loss: 0.0678, Val Loss: 0.0748\n",
      "Epoch 44/300 - Train Loss: 0.0664, Val Loss: 0.0705\n",
      "Epoch 45/300 - Train Loss: 0.0662, Val Loss: 0.0686\n",
      "Epoch 46/300 - Train Loss: 0.0673, Val Loss: 0.0736\n",
      "Epoch 47/300 - Train Loss: 0.0658, Val Loss: 0.0733\n",
      "Epoch 48/300 - Train Loss: 0.0671, Val Loss: 0.0687\n",
      "Epoch 49/300 - Train Loss: 0.0650, Val Loss: 0.0763\n",
      "Epoch 50/300 - Train Loss: 0.0699, Val Loss: 0.0677\n",
      "Epoch 51/300 - Train Loss: 0.0663, Val Loss: 0.0756\n",
      "Epoch 52/300 - Train Loss: 0.0650, Val Loss: 0.0815\n",
      "Epoch 53/300 - Train Loss: 0.0668, Val Loss: 0.0680\n",
      "Epoch 54/300 - Train Loss: 0.0658, Val Loss: 0.0778\n",
      "Epoch 55/300 - Train Loss: 0.0664, Val Loss: 0.0677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:29:46,692] Trial 402 finished with value: 0.9645104933180465 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.18270151445101146, 'learning_rate': 0.0008944656628286094, 'batch_size': 32, 'weight_decay': 3.7529181162096235e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/300 - Train Loss: 0.0639, Val Loss: 0.0695\n",
      "Early stopping at epoch 56\n",
      "Macro F1 Score: 0.9645, Macro Precision: 0.9585, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 404\n",
      "Training with F1=32, F2=8, D=2, dropout=0.2109483737804863, LR=0.0009925228051141934, BS=32, WD=2.2687433688535088e-05\n",
      "Epoch 1/300 - Train Loss: 0.1570, Val Loss: 0.0909\n",
      "Epoch 2/300 - Train Loss: 0.0970, Val Loss: 0.1182\n",
      "Epoch 3/300 - Train Loss: 0.0936, Val Loss: 0.0789\n",
      "Epoch 4/300 - Train Loss: 0.0922, Val Loss: 0.0735\n",
      "Epoch 5/300 - Train Loss: 0.0909, Val Loss: 0.0785\n",
      "Epoch 6/300 - Train Loss: 0.0905, Val Loss: 0.0735\n",
      "Epoch 7/300 - Train Loss: 0.0850, Val Loss: 0.0710\n",
      "Epoch 8/300 - Train Loss: 0.0847, Val Loss: 0.0705\n",
      "Epoch 9/300 - Train Loss: 0.0837, Val Loss: 0.0721\n",
      "Epoch 10/300 - Train Loss: 0.0842, Val Loss: 0.0700\n",
      "Epoch 11/300 - Train Loss: 0.0830, Val Loss: 0.0820\n",
      "Epoch 12/300 - Train Loss: 0.0843, Val Loss: 0.0771\n",
      "Epoch 13/300 - Train Loss: 0.0816, Val Loss: 0.0692\n",
      "Epoch 14/300 - Train Loss: 0.0831, Val Loss: 0.0740\n",
      "Epoch 15/300 - Train Loss: 0.0820, Val Loss: 0.0642\n",
      "Epoch 16/300 - Train Loss: 0.0802, Val Loss: 0.0680\n",
      "Epoch 17/300 - Train Loss: 0.0796, Val Loss: 0.0720\n",
      "Epoch 18/300 - Train Loss: 0.0777, Val Loss: 0.0694\n",
      "Epoch 19/300 - Train Loss: 0.0772, Val Loss: 0.0681\n",
      "Epoch 20/300 - Train Loss: 0.0776, Val Loss: 0.0640\n",
      "Epoch 21/300 - Train Loss: 0.0785, Val Loss: 0.0659\n",
      "Epoch 22/300 - Train Loss: 0.0769, Val Loss: 0.0714\n",
      "Epoch 23/300 - Train Loss: 0.0748, Val Loss: 0.0654\n",
      "Epoch 24/300 - Train Loss: 0.0763, Val Loss: 0.0693\n",
      "Epoch 25/300 - Train Loss: 0.0756, Val Loss: 0.0742\n",
      "Epoch 26/300 - Train Loss: 0.0746, Val Loss: 0.0700\n",
      "Epoch 27/300 - Train Loss: 0.0761, Val Loss: 0.0683\n",
      "Epoch 28/300 - Train Loss: 0.0733, Val Loss: 0.0677\n",
      "Epoch 29/300 - Train Loss: 0.0744, Val Loss: 0.0717\n",
      "Epoch 30/300 - Train Loss: 0.0727, Val Loss: 0.0655\n",
      "Epoch 31/300 - Train Loss: 0.0738, Val Loss: 0.0715\n",
      "Epoch 32/300 - Train Loss: 0.0759, Val Loss: 0.0785\n",
      "Epoch 33/300 - Train Loss: 0.0713, Val Loss: 0.0704\n",
      "Epoch 34/300 - Train Loss: 0.0755, Val Loss: 0.0689\n",
      "Epoch 35/300 - Train Loss: 0.0738, Val Loss: 0.0721\n",
      "Epoch 36/300 - Train Loss: 0.0703, Val Loss: 0.0687\n",
      "Epoch 37/300 - Train Loss: 0.0717, Val Loss: 0.0734\n",
      "Epoch 38/300 - Train Loss: 0.0727, Val Loss: 0.0685\n",
      "Epoch 39/300 - Train Loss: 0.0730, Val Loss: 0.0757\n",
      "Epoch 40/300 - Train Loss: 0.0712, Val Loss: 0.0714\n",
      "Epoch 41/300 - Train Loss: 0.0735, Val Loss: 0.0710\n",
      "Epoch 42/300 - Train Loss: 0.0701, Val Loss: 0.0788\n",
      "Epoch 43/300 - Train Loss: 0.0700, Val Loss: 0.0825\n",
      "Epoch 44/300 - Train Loss: 0.0723, Val Loss: 0.0815\n",
      "Epoch 45/300 - Train Loss: 0.0707, Val Loss: 0.0679\n",
      "Epoch 46/300 - Train Loss: 0.0689, Val Loss: 0.0623\n",
      "Epoch 47/300 - Train Loss: 0.0671, Val Loss: 0.0736\n",
      "Epoch 48/300 - Train Loss: 0.0711, Val Loss: 0.0653\n",
      "Epoch 49/300 - Train Loss: 0.0659, Val Loss: 0.0704\n",
      "Epoch 50/300 - Train Loss: 0.0692, Val Loss: 0.0664\n",
      "Epoch 51/300 - Train Loss: 0.0694, Val Loss: 0.0656\n",
      "Epoch 52/300 - Train Loss: 0.0703, Val Loss: 0.0724\n",
      "Epoch 53/300 - Train Loss: 0.0713, Val Loss: 0.0811\n",
      "Epoch 54/300 - Train Loss: 0.0685, Val Loss: 0.0683\n",
      "Epoch 55/300 - Train Loss: 0.0662, Val Loss: 0.0640\n",
      "Epoch 56/300 - Train Loss: 0.0671, Val Loss: 0.0764\n",
      "Epoch 57/300 - Train Loss: 0.0659, Val Loss: 0.0710\n",
      "Epoch 58/300 - Train Loss: 0.0664, Val Loss: 0.0677\n",
      "Epoch 59/300 - Train Loss: 0.0669, Val Loss: 0.0708\n",
      "Epoch 60/300 - Train Loss: 0.0653, Val Loss: 0.0697\n",
      "Epoch 61/300 - Train Loss: 0.0657, Val Loss: 0.0654\n",
      "Epoch 62/300 - Train Loss: 0.0654, Val Loss: 0.0662\n",
      "Epoch 63/300 - Train Loss: 0.0680, Val Loss: 0.0654\n",
      "Epoch 64/300 - Train Loss: 0.0627, Val Loss: 0.0746\n",
      "Epoch 65/300 - Train Loss: 0.0643, Val Loss: 0.0711\n",
      "Epoch 66/300 - Train Loss: 0.0669, Val Loss: 0.0694\n",
      "Epoch 67/300 - Train Loss: 0.0631, Val Loss: 0.0729\n",
      "Epoch 68/300 - Train Loss: 0.0627, Val Loss: 0.0708\n",
      "Epoch 69/300 - Train Loss: 0.0651, Val Loss: 0.0702\n",
      "Epoch 70/300 - Train Loss: 0.0652, Val Loss: 0.0681\n",
      "Epoch 71/300 - Train Loss: 0.0634, Val Loss: 0.0705\n",
      "Epoch 72/300 - Train Loss: 0.0642, Val Loss: 0.0713\n",
      "Epoch 73/300 - Train Loss: 0.0647, Val Loss: 0.0675\n",
      "Epoch 74/300 - Train Loss: 0.0637, Val Loss: 0.0690\n",
      "Epoch 75/300 - Train Loss: 0.0648, Val Loss: 0.0644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:32:58,284] Trial 403 finished with value: 0.9689199716830478 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.2109483737804863, 'learning_rate': 0.0009925228051141934, 'batch_size': 32, 'weight_decay': 2.2687433688535088e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300 - Train Loss: 0.0617, Val Loss: 0.0666\n",
      "Early stopping at epoch 76\n",
      "Macro F1 Score: 0.9689, Macro Precision: 0.9632, Macro Recall: 0.9752\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 405\n",
      "Training with F1=32, F2=8, D=2, dropout=0.19148097167063172, LR=0.0007470482202876422, BS=32, WD=5.5191892521836345e-05\n",
      "Epoch 1/300 - Train Loss: 0.1640, Val Loss: 0.0832\n",
      "Epoch 2/300 - Train Loss: 0.0991, Val Loss: 0.0800\n",
      "Epoch 3/300 - Train Loss: 0.0936, Val Loss: 0.0781\n",
      "Epoch 4/300 - Train Loss: 0.0904, Val Loss: 0.0822\n",
      "Epoch 5/300 - Train Loss: 0.0909, Val Loss: 0.0709\n",
      "Epoch 6/300 - Train Loss: 0.0865, Val Loss: 0.0731\n",
      "Epoch 7/300 - Train Loss: 0.0839, Val Loss: 0.0954\n",
      "Epoch 8/300 - Train Loss: 0.0845, Val Loss: 0.0752\n",
      "Epoch 9/300 - Train Loss: 0.0855, Val Loss: 0.0808\n",
      "Epoch 10/300 - Train Loss: 0.0817, Val Loss: 0.0708\n",
      "Epoch 11/300 - Train Loss: 0.0816, Val Loss: 0.0766\n",
      "Epoch 12/300 - Train Loss: 0.0801, Val Loss: 0.0739\n",
      "Epoch 13/300 - Train Loss: 0.0800, Val Loss: 0.0747\n",
      "Epoch 14/300 - Train Loss: 0.0778, Val Loss: 0.0723\n",
      "Epoch 15/300 - Train Loss: 0.0789, Val Loss: 0.0702\n",
      "Epoch 16/300 - Train Loss: 0.0779, Val Loss: 0.0692\n",
      "Epoch 17/300 - Train Loss: 0.0776, Val Loss: 0.0817\n",
      "Epoch 18/300 - Train Loss: 0.0754, Val Loss: 0.0807\n",
      "Epoch 19/300 - Train Loss: 0.0769, Val Loss: 0.0720\n",
      "Epoch 20/300 - Train Loss: 0.0770, Val Loss: 0.0780\n",
      "Epoch 21/300 - Train Loss: 0.0767, Val Loss: 0.0738\n",
      "Epoch 22/300 - Train Loss: 0.0787, Val Loss: 0.0751\n",
      "Epoch 23/300 - Train Loss: 0.0756, Val Loss: 0.0727\n",
      "Epoch 24/300 - Train Loss: 0.0740, Val Loss: 0.0666\n",
      "Epoch 25/300 - Train Loss: 0.0742, Val Loss: 0.0713\n",
      "Epoch 26/300 - Train Loss: 0.0753, Val Loss: 0.0680\n",
      "Epoch 27/300 - Train Loss: 0.0725, Val Loss: 0.0717\n",
      "Epoch 28/300 - Train Loss: 0.0730, Val Loss: 0.0666\n",
      "Epoch 29/300 - Train Loss: 0.0746, Val Loss: 0.0702\n",
      "Epoch 30/300 - Train Loss: 0.0711, Val Loss: 0.0711\n",
      "Epoch 31/300 - Train Loss: 0.0749, Val Loss: 0.0731\n",
      "Epoch 32/300 - Train Loss: 0.0726, Val Loss: 0.0701\n",
      "Epoch 33/300 - Train Loss: 0.0736, Val Loss: 0.0722\n",
      "Epoch 34/300 - Train Loss: 0.0731, Val Loss: 0.0744\n",
      "Epoch 35/300 - Train Loss: 0.0719, Val Loss: 0.0706\n",
      "Epoch 36/300 - Train Loss: 0.0696, Val Loss: 0.0701\n",
      "Epoch 37/300 - Train Loss: 0.0733, Val Loss: 0.0738\n",
      "Epoch 38/300 - Train Loss: 0.0710, Val Loss: 0.0763\n",
      "Epoch 39/300 - Train Loss: 0.0715, Val Loss: 0.0715\n",
      "Epoch 40/300 - Train Loss: 0.0702, Val Loss: 0.0696\n",
      "Epoch 41/300 - Train Loss: 0.0682, Val Loss: 0.0802\n",
      "Epoch 42/300 - Train Loss: 0.0707, Val Loss: 0.0698\n",
      "Epoch 43/300 - Train Loss: 0.0690, Val Loss: 0.0764\n",
      "Epoch 44/300 - Train Loss: 0.0682, Val Loss: 0.0729\n",
      "Epoch 45/300 - Train Loss: 0.0706, Val Loss: 0.0682\n",
      "Epoch 46/300 - Train Loss: 0.0692, Val Loss: 0.0716\n",
      "Epoch 47/300 - Train Loss: 0.0677, Val Loss: 0.0710\n",
      "Epoch 48/300 - Train Loss: 0.0673, Val Loss: 0.0822\n",
      "Epoch 49/300 - Train Loss: 0.0702, Val Loss: 0.0704\n",
      "Epoch 50/300 - Train Loss: 0.0686, Val Loss: 0.0678\n",
      "Epoch 51/300 - Train Loss: 0.0668, Val Loss: 0.0728\n",
      "Epoch 52/300 - Train Loss: 0.0688, Val Loss: 0.0765\n",
      "Epoch 53/300 - Train Loss: 0.0669, Val Loss: 0.0677\n",
      "Epoch 54/300 - Train Loss: 0.0649, Val Loss: 0.0689\n",
      "Epoch 55/300 - Train Loss: 0.0670, Val Loss: 0.0713\n",
      "Epoch 56/300 - Train Loss: 0.0648, Val Loss: 0.0701\n",
      "Epoch 57/300 - Train Loss: 0.0654, Val Loss: 0.0720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:35:23,200] Trial 404 finished with value: 0.9672861791578423 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.19148097167063172, 'learning_rate': 0.0007470482202876422, 'batch_size': 32, 'weight_decay': 5.5191892521836345e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300 - Train Loss: 0.0676, Val Loss: 0.0709\n",
      "Early stopping at epoch 58\n",
      "Macro F1 Score: 0.9673, Macro Precision: 0.9636, Macro Recall: 0.9712\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 406\n",
      "Training with F1=32, F2=8, D=2, dropout=0.20017949796803625, LR=0.0008431553374517401, BS=256, WD=3.3379966512209696e-05\n",
      "Epoch 1/300 - Train Loss: 0.3285, Val Loss: 0.1489\n",
      "Epoch 2/300 - Train Loss: 0.1108, Val Loss: 0.1028\n",
      "Epoch 3/300 - Train Loss: 0.0951, Val Loss: 0.1850\n",
      "Epoch 4/300 - Train Loss: 0.0900, Val Loss: 0.0737\n",
      "Epoch 5/300 - Train Loss: 0.0848, Val Loss: 0.0724\n",
      "Epoch 6/300 - Train Loss: 0.0810, Val Loss: 0.0783\n",
      "Epoch 7/300 - Train Loss: 0.0810, Val Loss: 0.0717\n",
      "Epoch 8/300 - Train Loss: 0.0805, Val Loss: 0.0748\n",
      "Epoch 9/300 - Train Loss: 0.0779, Val Loss: 0.0712\n",
      "Epoch 10/300 - Train Loss: 0.0777, Val Loss: 0.0744\n",
      "Epoch 11/300 - Train Loss: 0.0767, Val Loss: 0.0688\n",
      "Epoch 12/300 - Train Loss: 0.0751, Val Loss: 0.0690\n",
      "Epoch 13/300 - Train Loss: 0.0755, Val Loss: 0.0736\n",
      "Epoch 14/300 - Train Loss: 0.0749, Val Loss: 0.0730\n",
      "Epoch 15/300 - Train Loss: 0.0727, Val Loss: 0.0753\n",
      "Epoch 16/300 - Train Loss: 0.0723, Val Loss: 0.0794\n",
      "Epoch 17/300 - Train Loss: 0.0754, Val Loss: 0.0926\n",
      "Epoch 18/300 - Train Loss: 0.0729, Val Loss: 0.0712\n",
      "Epoch 19/300 - Train Loss: 0.0720, Val Loss: 0.0701\n",
      "Epoch 20/300 - Train Loss: 0.0709, Val Loss: 0.0672\n",
      "Epoch 21/300 - Train Loss: 0.0707, Val Loss: 0.0688\n",
      "Epoch 22/300 - Train Loss: 0.0694, Val Loss: 0.0717\n",
      "Epoch 23/300 - Train Loss: 0.0709, Val Loss: 0.0660\n",
      "Epoch 24/300 - Train Loss: 0.0708, Val Loss: 0.0701\n",
      "Epoch 25/300 - Train Loss: 0.0702, Val Loss: 0.0706\n",
      "Epoch 26/300 - Train Loss: 0.0681, Val Loss: 0.0727\n",
      "Epoch 27/300 - Train Loss: 0.0685, Val Loss: 0.0681\n",
      "Epoch 28/300 - Train Loss: 0.0697, Val Loss: 0.0751\n",
      "Epoch 29/300 - Train Loss: 0.0679, Val Loss: 0.0761\n",
      "Epoch 30/300 - Train Loss: 0.0688, Val Loss: 0.0748\n",
      "Epoch 31/300 - Train Loss: 0.0670, Val Loss: 0.0765\n",
      "Epoch 32/300 - Train Loss: 0.0685, Val Loss: 0.0701\n",
      "Epoch 33/300 - Train Loss: 0.0680, Val Loss: 0.0715\n",
      "Epoch 34/300 - Train Loss: 0.0666, Val Loss: 0.0700\n",
      "Epoch 35/300 - Train Loss: 0.0684, Val Loss: 0.0723\n",
      "Epoch 36/300 - Train Loss: 0.0673, Val Loss: 0.0743\n",
      "Epoch 37/300 - Train Loss: 0.0659, Val Loss: 0.0730\n",
      "Epoch 38/300 - Train Loss: 0.0666, Val Loss: 0.0717\n",
      "Epoch 39/300 - Train Loss: 0.0659, Val Loss: 0.0713\n",
      "Epoch 40/300 - Train Loss: 0.0649, Val Loss: 0.0681\n",
      "Epoch 41/300 - Train Loss: 0.0654, Val Loss: 0.0699\n",
      "Epoch 42/300 - Train Loss: 0.0642, Val Loss: 0.0745\n",
      "Epoch 43/300 - Train Loss: 0.0642, Val Loss: 0.0731\n",
      "Epoch 44/300 - Train Loss: 0.0640, Val Loss: 0.0759\n",
      "Epoch 45/300 - Train Loss: 0.0658, Val Loss: 0.0702\n",
      "Epoch 46/300 - Train Loss: 0.0620, Val Loss: 0.0724\n",
      "Epoch 47/300 - Train Loss: 0.0641, Val Loss: 0.0738\n",
      "Epoch 48/300 - Train Loss: 0.0625, Val Loss: 0.0762\n",
      "Epoch 49/300 - Train Loss: 0.0632, Val Loss: 0.0728\n",
      "Epoch 50/300 - Train Loss: 0.0632, Val Loss: 0.0709\n",
      "Epoch 51/300 - Train Loss: 0.0648, Val Loss: 0.0719\n",
      "Epoch 52/300 - Train Loss: 0.0621, Val Loss: 0.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:37:10,214] Trial 405 finished with value: 0.9709305603430126 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.20017949796803625, 'learning_rate': 0.0008431553374517401, 'batch_size': 256, 'weight_decay': 3.3379966512209696e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/300 - Train Loss: 0.0628, Val Loss: 0.0745\n",
      "Early stopping at epoch 53\n",
      "Macro F1 Score: 0.9709, Macro Precision: 0.9715, Macro Recall: 0.9704\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 407\n",
      "Training with F1=32, F2=8, D=2, dropout=0.17799518550929194, LR=0.0007935562570006904, BS=32, WD=4.952299693449275e-05\n",
      "Epoch 1/300 - Train Loss: 0.1576, Val Loss: 0.0796\n",
      "Epoch 2/300 - Train Loss: 0.0959, Val Loss: 0.0866\n",
      "Epoch 3/300 - Train Loss: 0.0981, Val Loss: 0.0932\n",
      "Epoch 4/300 - Train Loss: 0.0917, Val Loss: 0.0709\n",
      "Epoch 5/300 - Train Loss: 0.0896, Val Loss: 0.0759\n",
      "Epoch 6/300 - Train Loss: 0.0862, Val Loss: 0.0705\n",
      "Epoch 7/300 - Train Loss: 0.0883, Val Loss: 0.0781\n",
      "Epoch 8/300 - Train Loss: 0.0862, Val Loss: 0.0730\n",
      "Epoch 9/300 - Train Loss: 0.0832, Val Loss: 0.0755\n",
      "Epoch 10/300 - Train Loss: 0.0845, Val Loss: 0.0725\n",
      "Epoch 11/300 - Train Loss: 0.0836, Val Loss: 0.0770\n",
      "Epoch 12/300 - Train Loss: 0.0804, Val Loss: 0.0730\n",
      "Epoch 13/300 - Train Loss: 0.0796, Val Loss: 0.0697\n",
      "Epoch 14/300 - Train Loss: 0.0817, Val Loss: 0.0678\n",
      "Epoch 15/300 - Train Loss: 0.0797, Val Loss: 0.0712\n",
      "Epoch 16/300 - Train Loss: 0.0782, Val Loss: 0.0702\n",
      "Epoch 17/300 - Train Loss: 0.0778, Val Loss: 0.0711\n",
      "Epoch 18/300 - Train Loss: 0.0778, Val Loss: 0.0695\n",
      "Epoch 19/300 - Train Loss: 0.0762, Val Loss: 0.0765\n",
      "Epoch 20/300 - Train Loss: 0.0791, Val Loss: 0.0740\n",
      "Epoch 21/300 - Train Loss: 0.0761, Val Loss: 0.0692\n",
      "Epoch 22/300 - Train Loss: 0.0757, Val Loss: 0.0792\n",
      "Epoch 23/300 - Train Loss: 0.0759, Val Loss: 0.0764\n",
      "Epoch 24/300 - Train Loss: 0.0740, Val Loss: 0.0761\n",
      "Epoch 25/300 - Train Loss: 0.0734, Val Loss: 0.0718\n",
      "Epoch 26/300 - Train Loss: 0.0740, Val Loss: 0.0697\n",
      "Epoch 27/300 - Train Loss: 0.0721, Val Loss: 0.0726\n",
      "Epoch 28/300 - Train Loss: 0.0729, Val Loss: 0.0726\n",
      "Epoch 29/300 - Train Loss: 0.0749, Val Loss: 0.0692\n",
      "Epoch 30/300 - Train Loss: 0.0734, Val Loss: 0.0722\n",
      "Epoch 31/300 - Train Loss: 0.0743, Val Loss: 0.0702\n",
      "Epoch 32/300 - Train Loss: 0.0708, Val Loss: 0.0716\n",
      "Epoch 33/300 - Train Loss: 0.0731, Val Loss: 0.0698\n",
      "Epoch 34/300 - Train Loss: 0.0740, Val Loss: 0.0688\n",
      "Epoch 35/300 - Train Loss: 0.0707, Val Loss: 0.0719\n",
      "Epoch 36/300 - Train Loss: 0.0719, Val Loss: 0.0681\n",
      "Epoch 37/300 - Train Loss: 0.0707, Val Loss: 0.0679\n",
      "Epoch 38/300 - Train Loss: 0.0701, Val Loss: 0.0683\n",
      "Epoch 39/300 - Train Loss: 0.0698, Val Loss: 0.0686\n",
      "Epoch 40/300 - Train Loss: 0.0684, Val Loss: 0.0729\n",
      "Epoch 41/300 - Train Loss: 0.0727, Val Loss: 0.0775\n",
      "Epoch 42/300 - Train Loss: 0.0698, Val Loss: 0.0681\n",
      "Epoch 43/300 - Train Loss: 0.0695, Val Loss: 0.0679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:39:01,007] Trial 406 finished with value: 0.9710703391415191 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.17799518550929194, 'learning_rate': 0.0007935562570006904, 'batch_size': 32, 'weight_decay': 4.952299693449275e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300 - Train Loss: 0.0682, Val Loss: 0.0717\n",
      "Early stopping at epoch 44\n",
      "Macro F1 Score: 0.9711, Macro Precision: 0.9771, Macro Recall: 0.9653\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 408\n",
      "Training with F1=32, F2=8, D=2, dropout=0.17462074180535, LR=0.0008965395557671684, BS=32, WD=5.9964242472642385e-05\n",
      "Epoch 1/300 - Train Loss: 0.1587, Val Loss: 0.0781\n",
      "Epoch 2/300 - Train Loss: 0.0988, Val Loss: 0.0802\n",
      "Epoch 3/300 - Train Loss: 0.0942, Val Loss: 0.0730\n",
      "Epoch 4/300 - Train Loss: 0.0898, Val Loss: 0.1061\n",
      "Epoch 5/300 - Train Loss: 0.0899, Val Loss: 0.0853\n",
      "Epoch 6/300 - Train Loss: 0.0864, Val Loss: 0.0718\n",
      "Epoch 7/300 - Train Loss: 0.0859, Val Loss: 0.0690\n",
      "Epoch 8/300 - Train Loss: 0.0821, Val Loss: 0.0804\n",
      "Epoch 9/300 - Train Loss: 0.0826, Val Loss: 0.0723\n",
      "Epoch 10/300 - Train Loss: 0.0830, Val Loss: 0.0713\n",
      "Epoch 11/300 - Train Loss: 0.0830, Val Loss: 0.0747\n",
      "Epoch 12/300 - Train Loss: 0.0822, Val Loss: 0.0652\n",
      "Epoch 13/300 - Train Loss: 0.0812, Val Loss: 0.0736\n",
      "Epoch 14/300 - Train Loss: 0.0804, Val Loss: 0.0727\n",
      "Epoch 15/300 - Train Loss: 0.0791, Val Loss: 0.0754\n",
      "Epoch 16/300 - Train Loss: 0.0814, Val Loss: 0.0721\n",
      "Epoch 17/300 - Train Loss: 0.0784, Val Loss: 0.0779\n",
      "Epoch 18/300 - Train Loss: 0.0789, Val Loss: 0.0761\n",
      "Epoch 19/300 - Train Loss: 0.0777, Val Loss: 0.0731\n",
      "Epoch 20/300 - Train Loss: 0.0767, Val Loss: 0.0727\n",
      "Epoch 21/300 - Train Loss: 0.0771, Val Loss: 0.0709\n",
      "Epoch 22/300 - Train Loss: 0.0769, Val Loss: 0.0714\n",
      "Epoch 23/300 - Train Loss: 0.0743, Val Loss: 0.0787\n",
      "Epoch 24/300 - Train Loss: 0.0739, Val Loss: 0.0724\n",
      "Epoch 25/300 - Train Loss: 0.0742, Val Loss: 0.0780\n",
      "Epoch 26/300 - Train Loss: 0.0747, Val Loss: 0.0711\n",
      "Epoch 27/300 - Train Loss: 0.0757, Val Loss: 0.0736\n",
      "Epoch 28/300 - Train Loss: 0.0726, Val Loss: 0.0709\n",
      "Epoch 29/300 - Train Loss: 0.0742, Val Loss: 0.0738\n",
      "Epoch 30/300 - Train Loss: 0.0742, Val Loss: 0.0732\n",
      "Epoch 31/300 - Train Loss: 0.0749, Val Loss: 0.0732\n",
      "Epoch 32/300 - Train Loss: 0.0737, Val Loss: 0.0752\n",
      "Epoch 33/300 - Train Loss: 0.0726, Val Loss: 0.0719\n",
      "Epoch 34/300 - Train Loss: 0.0746, Val Loss: 0.0711\n",
      "Epoch 35/300 - Train Loss: 0.0726, Val Loss: 0.0755\n",
      "Epoch 36/300 - Train Loss: 0.0709, Val Loss: 0.0758\n",
      "Epoch 37/300 - Train Loss: 0.0734, Val Loss: 0.0725\n",
      "Epoch 38/300 - Train Loss: 0.0730, Val Loss: 0.0792\n",
      "Epoch 39/300 - Train Loss: 0.0726, Val Loss: 0.0701\n",
      "Epoch 40/300 - Train Loss: 0.0694, Val Loss: 0.0669\n",
      "Epoch 41/300 - Train Loss: 0.0688, Val Loss: 0.0762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:40:46,601] Trial 407 finished with value: 0.9706461058200618 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.17462074180535, 'learning_rate': 0.0008965395557671684, 'batch_size': 32, 'weight_decay': 5.9964242472642385e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300 - Train Loss: 0.0721, Val Loss: 0.0766\n",
      "Early stopping at epoch 42\n",
      "Macro F1 Score: 0.9706, Macro Precision: 0.9736, Macro Recall: 0.9677\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 409\n",
      "Training with F1=4, F2=8, D=2, dropout=0.19149937299501532, LR=0.0007119498468192078, BS=64, WD=4.0546790715162056e-05\n",
      "Epoch 1/300 - Train Loss: 0.2556, Val Loss: 0.1226\n",
      "Epoch 2/300 - Train Loss: 0.1283, Val Loss: 0.0883\n",
      "Epoch 3/300 - Train Loss: 0.1064, Val Loss: 0.0810\n",
      "Epoch 4/300 - Train Loss: 0.0996, Val Loss: 0.0779\n",
      "Epoch 5/300 - Train Loss: 0.0980, Val Loss: 0.0838\n",
      "Epoch 6/300 - Train Loss: 0.0993, Val Loss: 0.0821\n",
      "Epoch 7/300 - Train Loss: 0.0950, Val Loss: 0.0872\n",
      "Epoch 8/300 - Train Loss: 0.0920, Val Loss: 0.0810\n",
      "Epoch 9/300 - Train Loss: 0.0933, Val Loss: 0.0802\n",
      "Epoch 10/300 - Train Loss: 0.0934, Val Loss: 0.0778\n",
      "Epoch 11/300 - Train Loss: 0.0925, Val Loss: 0.0816\n",
      "Epoch 12/300 - Train Loss: 0.0906, Val Loss: 0.0831\n",
      "Epoch 13/300 - Train Loss: 0.0895, Val Loss: 0.0780\n",
      "Epoch 14/300 - Train Loss: 0.0919, Val Loss: 0.0846\n",
      "Epoch 15/300 - Train Loss: 0.0893, Val Loss: 0.0819\n",
      "Epoch 16/300 - Train Loss: 0.0901, Val Loss: 0.0797\n",
      "Epoch 17/300 - Train Loss: 0.0879, Val Loss: 0.0822\n",
      "Epoch 18/300 - Train Loss: 0.0872, Val Loss: 0.0808\n",
      "Epoch 19/300 - Train Loss: 0.0879, Val Loss: 0.0789\n",
      "Epoch 20/300 - Train Loss: 0.0877, Val Loss: 0.0809\n",
      "Epoch 21/300 - Train Loss: 0.0876, Val Loss: 0.0860\n",
      "Epoch 22/300 - Train Loss: 0.0866, Val Loss: 0.0841\n",
      "Epoch 23/300 - Train Loss: 0.0865, Val Loss: 0.0825\n",
      "Epoch 24/300 - Train Loss: 0.0854, Val Loss: 0.0827\n",
      "Epoch 25/300 - Train Loss: 0.0865, Val Loss: 0.0809\n",
      "Epoch 26/300 - Train Loss: 0.0861, Val Loss: 0.0783\n",
      "Epoch 27/300 - Train Loss: 0.0850, Val Loss: 0.0768\n",
      "Epoch 28/300 - Train Loss: 0.0852, Val Loss: 0.0870\n",
      "Epoch 29/300 - Train Loss: 0.0856, Val Loss: 0.0776\n",
      "Epoch 30/300 - Train Loss: 0.0839, Val Loss: 0.0811\n",
      "Epoch 31/300 - Train Loss: 0.0856, Val Loss: 0.0810\n",
      "Epoch 32/300 - Train Loss: 0.0852, Val Loss: 0.0837\n",
      "Epoch 33/300 - Train Loss: 0.0841, Val Loss: 0.0842\n",
      "Epoch 34/300 - Train Loss: 0.0829, Val Loss: 0.0805\n",
      "Epoch 35/300 - Train Loss: 0.0827, Val Loss: 0.0825\n",
      "Epoch 36/300 - Train Loss: 0.0831, Val Loss: 0.0804\n",
      "Epoch 37/300 - Train Loss: 0.0837, Val Loss: 0.0836\n",
      "Epoch 38/300 - Train Loss: 0.0817, Val Loss: 0.0828\n",
      "Epoch 39/300 - Train Loss: 0.0842, Val Loss: 0.0851\n",
      "Epoch 40/300 - Train Loss: 0.0826, Val Loss: 0.0828\n",
      "Epoch 41/300 - Train Loss: 0.0828, Val Loss: 0.0782\n",
      "Epoch 42/300 - Train Loss: 0.0833, Val Loss: 0.0849\n",
      "Epoch 43/300 - Train Loss: 0.0808, Val Loss: 0.0881\n",
      "Epoch 44/300 - Train Loss: 0.0821, Val Loss: 0.0828\n",
      "Epoch 45/300 - Train Loss: 0.0811, Val Loss: 0.0822\n",
      "Epoch 46/300 - Train Loss: 0.0826, Val Loss: 0.0860\n",
      "Epoch 47/300 - Train Loss: 0.0829, Val Loss: 0.0817\n",
      "Epoch 48/300 - Train Loss: 0.0813, Val Loss: 0.0811\n",
      "Epoch 49/300 - Train Loss: 0.0829, Val Loss: 0.0820\n",
      "Epoch 50/300 - Train Loss: 0.0802, Val Loss: 0.0822\n",
      "Epoch 51/300 - Train Loss: 0.0818, Val Loss: 0.0844\n",
      "Epoch 52/300 - Train Loss: 0.0805, Val Loss: 0.0820\n",
      "Epoch 53/300 - Train Loss: 0.0803, Val Loss: 0.0898\n",
      "Epoch 54/300 - Train Loss: 0.0795, Val Loss: 0.0840\n",
      "Epoch 55/300 - Train Loss: 0.0813, Val Loss: 0.0759\n",
      "Epoch 56/300 - Train Loss: 0.0810, Val Loss: 0.0903\n",
      "Epoch 57/300 - Train Loss: 0.0813, Val Loss: 0.0819\n",
      "Epoch 58/300 - Train Loss: 0.0795, Val Loss: 0.0850\n",
      "Epoch 59/300 - Train Loss: 0.0799, Val Loss: 0.0784\n",
      "Epoch 60/300 - Train Loss: 0.0784, Val Loss: 0.0795\n",
      "Epoch 61/300 - Train Loss: 0.0802, Val Loss: 0.0800\n",
      "Epoch 62/300 - Train Loss: 0.0799, Val Loss: 0.0766\n",
      "Epoch 63/300 - Train Loss: 0.0816, Val Loss: 0.0803\n",
      "Epoch 64/300 - Train Loss: 0.0779, Val Loss: 0.0969\n",
      "Epoch 65/300 - Train Loss: 0.0802, Val Loss: 0.0832\n",
      "Epoch 66/300 - Train Loss: 0.0803, Val Loss: 0.0817\n",
      "Epoch 67/300 - Train Loss: 0.0793, Val Loss: 0.0790\n",
      "Epoch 68/300 - Train Loss: 0.0788, Val Loss: 0.0840\n",
      "Epoch 69/300 - Train Loss: 0.0793, Val Loss: 0.0842\n",
      "Epoch 70/300 - Train Loss: 0.0775, Val Loss: 0.0785\n",
      "Epoch 71/300 - Train Loss: 0.0790, Val Loss: 0.0850\n",
      "Epoch 72/300 - Train Loss: 0.0784, Val Loss: 0.0916\n",
      "Epoch 73/300 - Train Loss: 0.0792, Val Loss: 0.0805\n",
      "Epoch 74/300 - Train Loss: 0.0791, Val Loss: 0.0814\n",
      "Epoch 75/300 - Train Loss: 0.0795, Val Loss: 0.0850\n",
      "Epoch 76/300 - Train Loss: 0.0782, Val Loss: 0.0831\n",
      "Epoch 77/300 - Train Loss: 0.0812, Val Loss: 0.0769\n",
      "Epoch 78/300 - Train Loss: 0.0785, Val Loss: 0.0851\n",
      "Epoch 79/300 - Train Loss: 0.0795, Val Loss: 0.0904\n",
      "Epoch 80/300 - Train Loss: 0.0779, Val Loss: 0.0854\n",
      "Epoch 81/300 - Train Loss: 0.0773, Val Loss: 0.0882\n",
      "Epoch 82/300 - Train Loss: 0.0790, Val Loss: 0.0849\n",
      "Epoch 83/300 - Train Loss: 0.0794, Val Loss: 0.0822\n",
      "Epoch 84/300 - Train Loss: 0.0800, Val Loss: 0.0812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:42:30,248] Trial 408 finished with value: 0.9661283510340114 and parameters: {'F1': 4, 'F2': 8, 'D': 2, 'dropout': 0.19149937299501532, 'learning_rate': 0.0007119498468192078, 'batch_size': 64, 'weight_decay': 4.0546790715162056e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85/300 - Train Loss: 0.0762, Val Loss: 0.0808\n",
      "Early stopping at epoch 85\n",
      "Macro F1 Score: 0.9661, Macro Precision: 0.9586, Macro Recall: 0.9744\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 410\n",
      "Training with F1=32, F2=8, D=2, dropout=0.21413563461704765, LR=0.0006362896156838979, BS=32, WD=2.8354401954874414e-05\n",
      "Epoch 1/300 - Train Loss: 0.1784, Val Loss: 0.1052\n",
      "Epoch 2/300 - Train Loss: 0.1030, Val Loss: 0.0765\n",
      "Epoch 3/300 - Train Loss: 0.0947, Val Loss: 0.0751\n",
      "Epoch 4/300 - Train Loss: 0.0917, Val Loss: 0.0716\n",
      "Epoch 5/300 - Train Loss: 0.0916, Val Loss: 0.0784\n",
      "Epoch 6/300 - Train Loss: 0.0886, Val Loss: 0.0754\n",
      "Epoch 7/300 - Train Loss: 0.0880, Val Loss: 0.0866\n",
      "Epoch 8/300 - Train Loss: 0.0827, Val Loss: 0.0774\n",
      "Epoch 9/300 - Train Loss: 0.0846, Val Loss: 0.0762\n",
      "Epoch 10/300 - Train Loss: 0.0840, Val Loss: 0.0687\n",
      "Epoch 11/300 - Train Loss: 0.0842, Val Loss: 0.0698\n",
      "Epoch 12/300 - Train Loss: 0.0827, Val Loss: 0.0845\n",
      "Epoch 13/300 - Train Loss: 0.0816, Val Loss: 0.0693\n",
      "Epoch 14/300 - Train Loss: 0.0800, Val Loss: 0.0703\n",
      "Epoch 15/300 - Train Loss: 0.0827, Val Loss: 0.0744\n",
      "Epoch 16/300 - Train Loss: 0.0814, Val Loss: 0.0759\n",
      "Epoch 17/300 - Train Loss: 0.0797, Val Loss: 0.0758\n",
      "Epoch 18/300 - Train Loss: 0.0819, Val Loss: 0.0811\n",
      "Epoch 19/300 - Train Loss: 0.0798, Val Loss: 0.0723\n",
      "Epoch 20/300 - Train Loss: 0.0778, Val Loss: 0.0769\n",
      "Epoch 21/300 - Train Loss: 0.0772, Val Loss: 0.0736\n",
      "Epoch 22/300 - Train Loss: 0.0790, Val Loss: 0.0706\n",
      "Epoch 23/300 - Train Loss: 0.0780, Val Loss: 0.0712\n",
      "Epoch 24/300 - Train Loss: 0.0760, Val Loss: 0.0741\n",
      "Epoch 25/300 - Train Loss: 0.0787, Val Loss: 0.0685\n",
      "Epoch 26/300 - Train Loss: 0.0796, Val Loss: 0.0763\n",
      "Epoch 27/300 - Train Loss: 0.0743, Val Loss: 0.0719\n",
      "Epoch 28/300 - Train Loss: 0.0759, Val Loss: 0.0710\n",
      "Epoch 29/300 - Train Loss: 0.0744, Val Loss: 0.0734\n",
      "Epoch 30/300 - Train Loss: 0.0750, Val Loss: 0.0675\n",
      "Epoch 31/300 - Train Loss: 0.0753, Val Loss: 0.0726\n",
      "Epoch 32/300 - Train Loss: 0.0742, Val Loss: 0.0753\n",
      "Epoch 33/300 - Train Loss: 0.0760, Val Loss: 0.0747\n",
      "Epoch 34/300 - Train Loss: 0.0753, Val Loss: 0.0751\n",
      "Epoch 35/300 - Train Loss: 0.0745, Val Loss: 0.0724\n",
      "Epoch 36/300 - Train Loss: 0.0751, Val Loss: 0.0761\n",
      "Epoch 37/300 - Train Loss: 0.0727, Val Loss: 0.0715\n",
      "Epoch 38/300 - Train Loss: 0.0734, Val Loss: 0.0701\n",
      "Epoch 39/300 - Train Loss: 0.0711, Val Loss: 0.0709\n",
      "Epoch 40/300 - Train Loss: 0.0716, Val Loss: 0.0677\n",
      "Epoch 41/300 - Train Loss: 0.0731, Val Loss: 0.0761\n",
      "Epoch 42/300 - Train Loss: 0.0733, Val Loss: 0.0705\n",
      "Epoch 43/300 - Train Loss: 0.0719, Val Loss: 0.0684\n",
      "Epoch 44/300 - Train Loss: 0.0716, Val Loss: 0.0727\n",
      "Epoch 45/300 - Train Loss: 0.0703, Val Loss: 0.0729\n",
      "Epoch 46/300 - Train Loss: 0.0709, Val Loss: 0.0776\n",
      "Epoch 47/300 - Train Loss: 0.0681, Val Loss: 0.0707\n",
      "Epoch 48/300 - Train Loss: 0.0720, Val Loss: 0.0722\n",
      "Epoch 49/300 - Train Loss: 0.0709, Val Loss: 0.0740\n",
      "Epoch 50/300 - Train Loss: 0.0707, Val Loss: 0.0685\n",
      "Epoch 51/300 - Train Loss: 0.0724, Val Loss: 0.0760\n",
      "Epoch 52/300 - Train Loss: 0.0682, Val Loss: 0.0713\n",
      "Epoch 53/300 - Train Loss: 0.0707, Val Loss: 0.0718\n",
      "Epoch 54/300 - Train Loss: 0.0704, Val Loss: 0.0690\n",
      "Epoch 55/300 - Train Loss: 0.0713, Val Loss: 0.0715\n",
      "Epoch 56/300 - Train Loss: 0.0684, Val Loss: 0.0704\n",
      "Epoch 57/300 - Train Loss: 0.0681, Val Loss: 0.0721\n",
      "Epoch 58/300 - Train Loss: 0.0694, Val Loss: 0.0715\n",
      "Epoch 59/300 - Train Loss: 0.0683, Val Loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:45:01,725] Trial 409 finished with value: 0.9637311556264966 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.21413563461704765, 'learning_rate': 0.0006362896156838979, 'batch_size': 32, 'weight_decay': 2.8354401954874414e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300 - Train Loss: 0.0695, Val Loss: 0.0685\n",
      "Early stopping at epoch 60\n",
      "Macro F1 Score: 0.9637, Macro Precision: 0.9517, Macro Recall: 0.9771\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 411\n",
      "Training with F1=32, F2=8, D=2, dropout=0.17350109718883194, LR=0.0008507583008311922, BS=32, WD=4.830757250891309e-05\n",
      "Epoch 1/300 - Train Loss: 0.1604, Val Loss: 0.0772\n",
      "Epoch 2/300 - Train Loss: 0.0996, Val Loss: 0.0815\n",
      "Epoch 3/300 - Train Loss: 0.0956, Val Loss: 0.0700\n",
      "Epoch 4/300 - Train Loss: 0.0926, Val Loss: 0.0703\n",
      "Epoch 5/300 - Train Loss: 0.0875, Val Loss: 0.0684\n",
      "Epoch 6/300 - Train Loss: 0.0884, Val Loss: 0.0711\n",
      "Epoch 7/300 - Train Loss: 0.0857, Val Loss: 0.0706\n",
      "Epoch 8/300 - Train Loss: 0.0848, Val Loss: 0.0675\n",
      "Epoch 9/300 - Train Loss: 0.0845, Val Loss: 0.0792\n",
      "Epoch 10/300 - Train Loss: 0.0821, Val Loss: 0.0745\n",
      "Epoch 11/300 - Train Loss: 0.0810, Val Loss: 0.0717\n",
      "Epoch 12/300 - Train Loss: 0.0796, Val Loss: 0.0727\n",
      "Epoch 13/300 - Train Loss: 0.0804, Val Loss: 0.0720\n",
      "Epoch 14/300 - Train Loss: 0.0812, Val Loss: 0.0760\n",
      "Epoch 15/300 - Train Loss: 0.0760, Val Loss: 0.0745\n",
      "Epoch 16/300 - Train Loss: 0.0783, Val Loss: 0.0691\n",
      "Epoch 17/300 - Train Loss: 0.0759, Val Loss: 0.0722\n",
      "Epoch 18/300 - Train Loss: 0.0760, Val Loss: 0.0723\n",
      "Epoch 19/300 - Train Loss: 0.0766, Val Loss: 0.0741\n",
      "Epoch 20/300 - Train Loss: 0.0761, Val Loss: 0.0745\n",
      "Epoch 21/300 - Train Loss: 0.0758, Val Loss: 0.0703\n",
      "Epoch 22/300 - Train Loss: 0.0769, Val Loss: 0.0747\n",
      "Epoch 23/300 - Train Loss: 0.0754, Val Loss: 0.0739\n",
      "Epoch 24/300 - Train Loss: 0.0750, Val Loss: 0.0719\n",
      "Epoch 25/300 - Train Loss: 0.0735, Val Loss: 0.0725\n",
      "Epoch 26/300 - Train Loss: 0.0710, Val Loss: 0.0679\n",
      "Epoch 27/300 - Train Loss: 0.0749, Val Loss: 0.0668\n",
      "Epoch 28/300 - Train Loss: 0.0729, Val Loss: 0.0661\n",
      "Epoch 29/300 - Train Loss: 0.0730, Val Loss: 0.0721\n",
      "Epoch 30/300 - Train Loss: 0.0721, Val Loss: 0.0734\n",
      "Epoch 31/300 - Train Loss: 0.0738, Val Loss: 0.0701\n",
      "Epoch 32/300 - Train Loss: 0.0721, Val Loss: 0.0833\n",
      "Epoch 33/300 - Train Loss: 0.0714, Val Loss: 0.0718\n",
      "Epoch 34/300 - Train Loss: 0.0706, Val Loss: 0.0713\n",
      "Epoch 35/300 - Train Loss: 0.0700, Val Loss: 0.0730\n",
      "Epoch 36/300 - Train Loss: 0.0708, Val Loss: 0.0785\n",
      "Epoch 37/300 - Train Loss: 0.0700, Val Loss: 0.0720\n",
      "Epoch 38/300 - Train Loss: 0.0706, Val Loss: 0.0726\n",
      "Epoch 39/300 - Train Loss: 0.0712, Val Loss: 0.0763\n",
      "Epoch 40/300 - Train Loss: 0.0693, Val Loss: 0.0672\n",
      "Epoch 41/300 - Train Loss: 0.0669, Val Loss: 0.0671\n",
      "Epoch 42/300 - Train Loss: 0.0676, Val Loss: 0.0673\n",
      "Epoch 43/300 - Train Loss: 0.0673, Val Loss: 0.0719\n",
      "Epoch 44/300 - Train Loss: 0.0671, Val Loss: 0.0689\n",
      "Epoch 45/300 - Train Loss: 0.0668, Val Loss: 0.0646\n",
      "Epoch 46/300 - Train Loss: 0.0665, Val Loss: 0.0729\n",
      "Epoch 47/300 - Train Loss: 0.0658, Val Loss: 0.0698\n",
      "Epoch 48/300 - Train Loss: 0.0684, Val Loss: 0.0702\n",
      "Epoch 49/300 - Train Loss: 0.0673, Val Loss: 0.0716\n",
      "Epoch 50/300 - Train Loss: 0.0656, Val Loss: 0.0715\n",
      "Epoch 51/300 - Train Loss: 0.0651, Val Loss: 0.0702\n",
      "Epoch 52/300 - Train Loss: 0.0672, Val Loss: 0.0691\n",
      "Epoch 53/300 - Train Loss: 0.0663, Val Loss: 0.0671\n",
      "Epoch 54/300 - Train Loss: 0.0652, Val Loss: 0.0719\n",
      "Epoch 55/300 - Train Loss: 0.0639, Val Loss: 0.0686\n",
      "Epoch 56/300 - Train Loss: 0.0688, Val Loss: 0.0692\n",
      "Epoch 57/300 - Train Loss: 0.0668, Val Loss: 0.0751\n",
      "Epoch 58/300 - Train Loss: 0.0661, Val Loss: 0.0705\n",
      "Epoch 59/300 - Train Loss: 0.0652, Val Loss: 0.0764\n",
      "Epoch 60/300 - Train Loss: 0.0653, Val Loss: 0.0786\n",
      "Epoch 61/300 - Train Loss: 0.0646, Val Loss: 0.0684\n",
      "Epoch 62/300 - Train Loss: 0.0634, Val Loss: 0.0728\n",
      "Epoch 63/300 - Train Loss: 0.0629, Val Loss: 0.0658\n",
      "Epoch 64/300 - Train Loss: 0.0641, Val Loss: 0.0726\n",
      "Epoch 65/300 - Train Loss: 0.0635, Val Loss: 0.0694\n",
      "Epoch 66/300 - Train Loss: 0.0626, Val Loss: 0.0771\n",
      "Epoch 67/300 - Train Loss: 0.0619, Val Loss: 0.0718\n",
      "Epoch 68/300 - Train Loss: 0.0638, Val Loss: 0.0679\n",
      "Epoch 69/300 - Train Loss: 0.0632, Val Loss: 0.0755\n",
      "Epoch 70/300 - Train Loss: 0.0618, Val Loss: 0.0685\n",
      "Epoch 71/300 - Train Loss: 0.0629, Val Loss: 0.0692\n",
      "Epoch 72/300 - Train Loss: 0.0615, Val Loss: 0.0670\n",
      "Epoch 73/300 - Train Loss: 0.0629, Val Loss: 0.0714\n",
      "Epoch 74/300 - Train Loss: 0.0639, Val Loss: 0.0771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:48:10,621] Trial 410 finished with value: 0.9716789988177515 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.17350109718883194, 'learning_rate': 0.0008507583008311922, 'batch_size': 32, 'weight_decay': 4.830757250891309e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300 - Train Loss: 0.0625, Val Loss: 0.0700\n",
      "Early stopping at epoch 75\n",
      "Macro F1 Score: 0.9717, Macro Precision: 0.9677, Macro Recall: 0.9759\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 412\n",
      "Training with F1=32, F2=8, D=2, dropout=0.18201469534101974, LR=0.0008065725085366461, BS=32, WD=4.27420433923688e-05\n",
      "Epoch 1/300 - Train Loss: 0.1535, Val Loss: 0.0760\n",
      "Epoch 2/300 - Train Loss: 0.0973, Val Loss: 0.0821\n",
      "Epoch 3/300 - Train Loss: 0.0924, Val Loss: 0.0704\n",
      "Epoch 4/300 - Train Loss: 0.0896, Val Loss: 0.0751\n",
      "Epoch 5/300 - Train Loss: 0.0878, Val Loss: 0.0740\n",
      "Epoch 6/300 - Train Loss: 0.0850, Val Loss: 0.0851\n",
      "Epoch 7/300 - Train Loss: 0.0855, Val Loss: 0.0738\n",
      "Epoch 8/300 - Train Loss: 0.0833, Val Loss: 0.0708\n",
      "Epoch 9/300 - Train Loss: 0.0844, Val Loss: 0.1924\n",
      "Epoch 10/300 - Train Loss: 0.0815, Val Loss: 0.0911\n",
      "Epoch 11/300 - Train Loss: 0.0829, Val Loss: 0.0717\n",
      "Epoch 12/300 - Train Loss: 0.0841, Val Loss: 0.0820\n",
      "Epoch 13/300 - Train Loss: 0.0805, Val Loss: 0.0747\n",
      "Epoch 14/300 - Train Loss: 0.0803, Val Loss: 0.0742\n",
      "Epoch 15/300 - Train Loss: 0.0766, Val Loss: 0.0735\n",
      "Epoch 16/300 - Train Loss: 0.0767, Val Loss: 0.0750\n",
      "Epoch 17/300 - Train Loss: 0.0770, Val Loss: 0.0743\n",
      "Epoch 18/300 - Train Loss: 0.0762, Val Loss: 0.0729\n",
      "Epoch 19/300 - Train Loss: 0.0739, Val Loss: 0.0690\n",
      "Epoch 20/300 - Train Loss: 0.0757, Val Loss: 0.0720\n",
      "Epoch 21/300 - Train Loss: 0.0748, Val Loss: 0.0670\n",
      "Epoch 22/300 - Train Loss: 0.0741, Val Loss: 0.0675\n",
      "Epoch 23/300 - Train Loss: 0.0741, Val Loss: 0.0670\n",
      "Epoch 24/300 - Train Loss: 0.0710, Val Loss: 0.0659\n",
      "Epoch 25/300 - Train Loss: 0.0736, Val Loss: 0.0714\n",
      "Epoch 26/300 - Train Loss: 0.0713, Val Loss: 0.0721\n",
      "Epoch 27/300 - Train Loss: 0.0706, Val Loss: 0.0847\n",
      "Epoch 28/300 - Train Loss: 0.0711, Val Loss: 0.0677\n",
      "Epoch 29/300 - Train Loss: 0.0707, Val Loss: 0.0685\n",
      "Epoch 30/300 - Train Loss: 0.0723, Val Loss: 0.0658\n",
      "Epoch 31/300 - Train Loss: 0.0708, Val Loss: 0.0674\n",
      "Epoch 32/300 - Train Loss: 0.0686, Val Loss: 0.0715\n",
      "Epoch 33/300 - Train Loss: 0.0691, Val Loss: 0.0755\n",
      "Epoch 34/300 - Train Loss: 0.0711, Val Loss: 0.0719\n",
      "Epoch 35/300 - Train Loss: 0.0683, Val Loss: 0.0671\n",
      "Epoch 36/300 - Train Loss: 0.0700, Val Loss: 0.0709\n",
      "Epoch 37/300 - Train Loss: 0.0703, Val Loss: 0.0807\n",
      "Epoch 38/300 - Train Loss: 0.0671, Val Loss: 0.0707\n",
      "Epoch 39/300 - Train Loss: 0.0670, Val Loss: 0.0784\n",
      "Epoch 40/300 - Train Loss: 0.0694, Val Loss: 0.0671\n",
      "Epoch 41/300 - Train Loss: 0.0693, Val Loss: 0.0693\n",
      "Epoch 42/300 - Train Loss: 0.0682, Val Loss: 0.0793\n",
      "Epoch 43/300 - Train Loss: 0.0701, Val Loss: 0.0752\n",
      "Epoch 44/300 - Train Loss: 0.0661, Val Loss: 0.0732\n",
      "Epoch 45/300 - Train Loss: 0.0687, Val Loss: 0.0683\n",
      "Epoch 46/300 - Train Loss: 0.0650, Val Loss: 0.0739\n",
      "Epoch 47/300 - Train Loss: 0.0648, Val Loss: 0.0680\n",
      "Epoch 48/300 - Train Loss: 0.0636, Val Loss: 0.0737\n",
      "Epoch 49/300 - Train Loss: 0.0654, Val Loss: 0.0727\n",
      "Epoch 50/300 - Train Loss: 0.0644, Val Loss: 0.0775\n",
      "Epoch 51/300 - Train Loss: 0.0666, Val Loss: 0.0685\n",
      "Epoch 52/300 - Train Loss: 0.0645, Val Loss: 0.0678\n",
      "Epoch 53/300 - Train Loss: 0.0648, Val Loss: 0.0777\n",
      "Epoch 54/300 - Train Loss: 0.0660, Val Loss: 0.0750\n",
      "Epoch 55/300 - Train Loss: 0.0641, Val Loss: 0.0804\n",
      "Epoch 56/300 - Train Loss: 0.0691, Val Loss: 0.0743\n",
      "Epoch 57/300 - Train Loss: 0.0632, Val Loss: 0.0726\n",
      "Epoch 58/300 - Train Loss: 0.0633, Val Loss: 0.0707\n",
      "Epoch 59/300 - Train Loss: 0.0628, Val Loss: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:50:42,848] Trial 411 finished with value: 0.9689264699076473 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.18201469534101974, 'learning_rate': 0.0008065725085366461, 'batch_size': 32, 'weight_decay': 4.27420433923688e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300 - Train Loss: 0.0648, Val Loss: 0.0752\n",
      "Early stopping at epoch 60\n",
      "Macro F1 Score: 0.9689, Macro Precision: 0.9769, Macro Recall: 0.9614\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.92      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 413\n",
      "Training with F1=32, F2=8, D=2, dropout=0.20289022849404031, LR=0.000998143273691313, BS=32, WD=3.654803337327932e-05\n",
      "Epoch 1/300 - Train Loss: 0.1538, Val Loss: 0.0752\n",
      "Epoch 2/300 - Train Loss: 0.1007, Val Loss: 0.0844\n",
      "Epoch 3/300 - Train Loss: 0.0956, Val Loss: 0.0769\n",
      "Epoch 4/300 - Train Loss: 0.0915, Val Loss: 0.0741\n",
      "Epoch 5/300 - Train Loss: 0.0884, Val Loss: 0.0749\n",
      "Epoch 6/300 - Train Loss: 0.0871, Val Loss: 0.0797\n",
      "Epoch 7/300 - Train Loss: 0.0853, Val Loss: 0.0700\n",
      "Epoch 8/300 - Train Loss: 0.0856, Val Loss: 0.0729\n",
      "Epoch 9/300 - Train Loss: 0.0847, Val Loss: 0.0813\n",
      "Epoch 10/300 - Train Loss: 0.0850, Val Loss: 0.0725\n",
      "Epoch 11/300 - Train Loss: 0.0847, Val Loss: 0.0713\n",
      "Epoch 12/300 - Train Loss: 0.0821, Val Loss: 0.0870\n",
      "Epoch 13/300 - Train Loss: 0.0791, Val Loss: 0.0697\n",
      "Epoch 14/300 - Train Loss: 0.0782, Val Loss: 0.0789\n",
      "Epoch 15/300 - Train Loss: 0.0780, Val Loss: 0.0697\n",
      "Epoch 16/300 - Train Loss: 0.0797, Val Loss: 0.0853\n",
      "Epoch 17/300 - Train Loss: 0.0820, Val Loss: 0.0714\n",
      "Epoch 18/300 - Train Loss: 0.0768, Val Loss: 0.0782\n",
      "Epoch 19/300 - Train Loss: 0.0755, Val Loss: 0.0808\n",
      "Epoch 20/300 - Train Loss: 0.0760, Val Loss: 0.0805\n",
      "Epoch 21/300 - Train Loss: 0.0769, Val Loss: 0.0691\n",
      "Epoch 22/300 - Train Loss: 0.0781, Val Loss: 0.0685\n",
      "Epoch 23/300 - Train Loss: 0.0766, Val Loss: 0.0802\n",
      "Epoch 24/300 - Train Loss: 0.0752, Val Loss: 0.0723\n",
      "Epoch 25/300 - Train Loss: 0.0776, Val Loss: 0.0704\n",
      "Epoch 26/300 - Train Loss: 0.0749, Val Loss: 0.0764\n",
      "Epoch 27/300 - Train Loss: 0.0746, Val Loss: 0.0716\n",
      "Epoch 28/300 - Train Loss: 0.0724, Val Loss: 0.0719\n",
      "Epoch 29/300 - Train Loss: 0.0767, Val Loss: 0.0715\n",
      "Epoch 30/300 - Train Loss: 0.0744, Val Loss: 0.0679\n",
      "Epoch 31/300 - Train Loss: 0.0751, Val Loss: 0.0735\n",
      "Epoch 32/300 - Train Loss: 0.0742, Val Loss: 0.0670\n",
      "Epoch 33/300 - Train Loss: 0.0741, Val Loss: 0.0720\n",
      "Epoch 34/300 - Train Loss: 0.0729, Val Loss: 0.0710\n",
      "Epoch 35/300 - Train Loss: 0.0700, Val Loss: 0.0714\n",
      "Epoch 36/300 - Train Loss: 0.0739, Val Loss: 0.0761\n",
      "Epoch 37/300 - Train Loss: 0.0713, Val Loss: 0.0777\n",
      "Epoch 38/300 - Train Loss: 0.0734, Val Loss: 0.0674\n",
      "Epoch 39/300 - Train Loss: 0.0732, Val Loss: 0.0721\n",
      "Epoch 40/300 - Train Loss: 0.0716, Val Loss: 0.0670\n",
      "Epoch 41/300 - Train Loss: 0.0707, Val Loss: 0.0694\n",
      "Epoch 42/300 - Train Loss: 0.0713, Val Loss: 0.0762\n",
      "Epoch 43/300 - Train Loss: 0.0700, Val Loss: 0.0793\n",
      "Epoch 44/300 - Train Loss: 0.0697, Val Loss: 0.0759\n",
      "Epoch 45/300 - Train Loss: 0.0695, Val Loss: 0.0694\n",
      "Epoch 46/300 - Train Loss: 0.0685, Val Loss: 0.0704\n",
      "Epoch 47/300 - Train Loss: 0.0704, Val Loss: 0.0657\n",
      "Epoch 48/300 - Train Loss: 0.0714, Val Loss: 0.0798\n",
      "Epoch 49/300 - Train Loss: 0.0673, Val Loss: 0.0721\n",
      "Epoch 50/300 - Train Loss: 0.0704, Val Loss: 0.0718\n",
      "Epoch 51/300 - Train Loss: 0.0697, Val Loss: 0.0693\n",
      "Epoch 52/300 - Train Loss: 0.0686, Val Loss: 0.0758\n",
      "Epoch 53/300 - Train Loss: 0.0688, Val Loss: 0.0688\n",
      "Epoch 54/300 - Train Loss: 0.0681, Val Loss: 0.0717\n",
      "Epoch 55/300 - Train Loss: 0.0688, Val Loss: 0.0725\n",
      "Epoch 56/300 - Train Loss: 0.0663, Val Loss: 0.0686\n",
      "Epoch 57/300 - Train Loss: 0.0682, Val Loss: 0.0695\n",
      "Epoch 58/300 - Train Loss: 0.0680, Val Loss: 0.0726\n",
      "Epoch 59/300 - Train Loss: 0.0715, Val Loss: 0.0744\n",
      "Epoch 60/300 - Train Loss: 0.0694, Val Loss: 0.0705\n",
      "Epoch 61/300 - Train Loss: 0.0691, Val Loss: 0.0808\n",
      "Epoch 62/300 - Train Loss: 0.0672, Val Loss: 0.0768\n",
      "Epoch 63/300 - Train Loss: 0.0706, Val Loss: 0.0891\n",
      "Epoch 64/300 - Train Loss: 0.0692, Val Loss: 0.0763\n",
      "Epoch 65/300 - Train Loss: 0.0691, Val Loss: 0.0703\n",
      "Epoch 66/300 - Train Loss: 0.0686, Val Loss: 0.0765\n",
      "Epoch 67/300 - Train Loss: 0.0689, Val Loss: 0.0799\n",
      "Epoch 68/300 - Train Loss: 0.0658, Val Loss: 0.0694\n",
      "Epoch 69/300 - Train Loss: 0.0681, Val Loss: 0.0717\n",
      "Epoch 70/300 - Train Loss: 0.0684, Val Loss: 0.0804\n",
      "Epoch 71/300 - Train Loss: 0.0676, Val Loss: 0.0675\n",
      "Epoch 72/300 - Train Loss: 0.0654, Val Loss: 0.0690\n",
      "Epoch 73/300 - Train Loss: 0.0679, Val Loss: 0.0694\n",
      "Epoch 74/300 - Train Loss: 0.0653, Val Loss: 0.0730\n",
      "Epoch 75/300 - Train Loss: 0.0633, Val Loss: 0.0708\n",
      "Epoch 76/300 - Train Loss: 0.0641, Val Loss: 0.0799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:53:58,155] Trial 412 finished with value: 0.9600171727133183 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.20289022849404031, 'learning_rate': 0.000998143273691313, 'batch_size': 32, 'weight_decay': 3.654803337327932e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300 - Train Loss: 0.0682, Val Loss: 0.0814\n",
      "Early stopping at epoch 77\n",
      "Macro F1 Score: 0.9600, Macro Precision: 0.9517, Macro Recall: 0.9690\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 414\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1668057066007297, LR=0.0009158725454608817, BS=32, WD=6.432674950886333e-05\n",
      "Epoch 1/300 - Train Loss: 0.1555, Val Loss: 0.1232\n",
      "Epoch 2/300 - Train Loss: 0.0984, Val Loss: 0.0756\n",
      "Epoch 3/300 - Train Loss: 0.0966, Val Loss: 0.0767\n",
      "Epoch 4/300 - Train Loss: 0.0891, Val Loss: 0.1046\n",
      "Epoch 5/300 - Train Loss: 0.0861, Val Loss: 0.0741\n",
      "Epoch 6/300 - Train Loss: 0.0873, Val Loss: 0.0736\n",
      "Epoch 7/300 - Train Loss: 0.0848, Val Loss: 0.0700\n",
      "Epoch 8/300 - Train Loss: 0.0854, Val Loss: 0.0698\n",
      "Epoch 9/300 - Train Loss: 0.0803, Val Loss: 0.0712\n",
      "Epoch 10/300 - Train Loss: 0.0824, Val Loss: 0.0771\n",
      "Epoch 11/300 - Train Loss: 0.0837, Val Loss: 0.0766\n",
      "Epoch 12/300 - Train Loss: 0.0814, Val Loss: 0.0654\n",
      "Epoch 13/300 - Train Loss: 0.0793, Val Loss: 0.0688\n",
      "Epoch 14/300 - Train Loss: 0.0797, Val Loss: 0.0691\n",
      "Epoch 15/300 - Train Loss: 0.0795, Val Loss: 0.0744\n",
      "Epoch 16/300 - Train Loss: 0.0789, Val Loss: 0.0735\n",
      "Epoch 17/300 - Train Loss: 0.0769, Val Loss: 0.0793\n",
      "Epoch 18/300 - Train Loss: 0.0773, Val Loss: 0.0743\n",
      "Epoch 19/300 - Train Loss: 0.0772, Val Loss: 0.0708\n",
      "Epoch 20/300 - Train Loss: 0.0769, Val Loss: 0.0726\n",
      "Epoch 21/300 - Train Loss: 0.0777, Val Loss: 0.0725\n",
      "Epoch 22/300 - Train Loss: 0.0747, Val Loss: 0.0764\n",
      "Epoch 23/300 - Train Loss: 0.0744, Val Loss: 0.0738\n",
      "Epoch 24/300 - Train Loss: 0.0758, Val Loss: 0.0803\n",
      "Epoch 25/300 - Train Loss: 0.0757, Val Loss: 0.0689\n",
      "Epoch 26/300 - Train Loss: 0.0733, Val Loss: 0.0724\n",
      "Epoch 27/300 - Train Loss: 0.0743, Val Loss: 0.0789\n",
      "Epoch 28/300 - Train Loss: 0.0760, Val Loss: 0.0684\n",
      "Epoch 29/300 - Train Loss: 0.0753, Val Loss: 0.0760\n",
      "Epoch 30/300 - Train Loss: 0.0717, Val Loss: 0.0720\n",
      "Epoch 31/300 - Train Loss: 0.0731, Val Loss: 0.0707\n",
      "Epoch 32/300 - Train Loss: 0.0712, Val Loss: 0.0717\n",
      "Epoch 33/300 - Train Loss: 0.0710, Val Loss: 0.0733\n",
      "Epoch 34/300 - Train Loss: 0.0722, Val Loss: 0.0707\n",
      "Epoch 35/300 - Train Loss: 0.0725, Val Loss: 0.0736\n",
      "Epoch 36/300 - Train Loss: 0.0708, Val Loss: 0.0860\n",
      "Epoch 37/300 - Train Loss: 0.0720, Val Loss: 0.0786\n",
      "Epoch 38/300 - Train Loss: 0.0692, Val Loss: 0.0737\n",
      "Epoch 39/300 - Train Loss: 0.0718, Val Loss: 0.0785\n",
      "Epoch 40/300 - Train Loss: 0.0676, Val Loss: 0.0678\n",
      "Epoch 41/300 - Train Loss: 0.0724, Val Loss: 0.0702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:55:43,616] Trial 413 finished with value: 0.970260491999113 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1668057066007297, 'learning_rate': 0.0009158725454608817, 'batch_size': 32, 'weight_decay': 6.432674950886333e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300 - Train Loss: 0.0724, Val Loss: 0.0725\n",
      "Early stopping at epoch 42\n",
      "Macro F1 Score: 0.9703, Macro Precision: 0.9642, Macro Recall: 0.9767\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 415\n",
      "Training with F1=32, F2=8, D=2, dropout=0.18923954480517846, LR=0.0003421582166428454, BS=128, WD=5.174549762331336e-05\n",
      "Epoch 1/300 - Train Loss: 0.3493, Val Loss: 0.1374\n",
      "Epoch 2/300 - Train Loss: 0.1258, Val Loss: 0.1153\n",
      "Epoch 3/300 - Train Loss: 0.1029, Val Loss: 0.0921\n",
      "Epoch 4/300 - Train Loss: 0.0938, Val Loss: 0.0876\n",
      "Epoch 5/300 - Train Loss: 0.0909, Val Loss: 0.0853\n",
      "Epoch 6/300 - Train Loss: 0.0869, Val Loss: 0.0950\n",
      "Epoch 7/300 - Train Loss: 0.0846, Val Loss: 0.0810\n",
      "Epoch 8/300 - Train Loss: 0.0831, Val Loss: 0.0824\n",
      "Epoch 9/300 - Train Loss: 0.0805, Val Loss: 0.0787\n",
      "Epoch 10/300 - Train Loss: 0.0793, Val Loss: 0.0805\n",
      "Epoch 11/300 - Train Loss: 0.0793, Val Loss: 0.0776\n",
      "Epoch 12/300 - Train Loss: 0.0776, Val Loss: 0.0737\n",
      "Epoch 13/300 - Train Loss: 0.0782, Val Loss: 0.0779\n",
      "Epoch 14/300 - Train Loss: 0.0766, Val Loss: 0.0825\n",
      "Epoch 15/300 - Train Loss: 0.0758, Val Loss: 0.0770\n",
      "Epoch 16/300 - Train Loss: 0.0751, Val Loss: 0.0777\n",
      "Epoch 17/300 - Train Loss: 0.0764, Val Loss: 0.0754\n",
      "Epoch 18/300 - Train Loss: 0.0745, Val Loss: 0.0797\n",
      "Epoch 19/300 - Train Loss: 0.0728, Val Loss: 0.0747\n",
      "Epoch 20/300 - Train Loss: 0.0731, Val Loss: 0.0733\n",
      "Epoch 21/300 - Train Loss: 0.0731, Val Loss: 0.0759\n",
      "Epoch 22/300 - Train Loss: 0.0719, Val Loss: 0.0702\n",
      "Epoch 23/300 - Train Loss: 0.0709, Val Loss: 0.0718\n",
      "Epoch 24/300 - Train Loss: 0.0700, Val Loss: 0.0776\n",
      "Epoch 25/300 - Train Loss: 0.0703, Val Loss: 0.0744\n",
      "Epoch 26/300 - Train Loss: 0.0710, Val Loss: 0.0713\n",
      "Epoch 27/300 - Train Loss: 0.0694, Val Loss: 0.0758\n",
      "Epoch 28/300 - Train Loss: 0.0699, Val Loss: 0.0713\n",
      "Epoch 29/300 - Train Loss: 0.0691, Val Loss: 0.0685\n",
      "Epoch 30/300 - Train Loss: 0.0689, Val Loss: 0.0706\n",
      "Epoch 31/300 - Train Loss: 0.0691, Val Loss: 0.0729\n",
      "Epoch 32/300 - Train Loss: 0.0684, Val Loss: 0.0715\n",
      "Epoch 33/300 - Train Loss: 0.0684, Val Loss: 0.0701\n",
      "Epoch 34/300 - Train Loss: 0.0674, Val Loss: 0.0730\n",
      "Epoch 35/300 - Train Loss: 0.0678, Val Loss: 0.0770\n",
      "Epoch 36/300 - Train Loss: 0.0677, Val Loss: 0.0745\n",
      "Epoch 37/300 - Train Loss: 0.0683, Val Loss: 0.0735\n",
      "Epoch 38/300 - Train Loss: 0.0668, Val Loss: 0.0718\n",
      "Epoch 39/300 - Train Loss: 0.0676, Val Loss: 0.0780\n",
      "Epoch 40/300 - Train Loss: 0.0662, Val Loss: 0.0701\n",
      "Epoch 41/300 - Train Loss: 0.0654, Val Loss: 0.0734\n",
      "Epoch 42/300 - Train Loss: 0.0669, Val Loss: 0.0738\n",
      "Epoch 43/300 - Train Loss: 0.0660, Val Loss: 0.0723\n",
      "Epoch 44/300 - Train Loss: 0.0665, Val Loss: 0.0732\n",
      "Epoch 45/300 - Train Loss: 0.0662, Val Loss: 0.0737\n",
      "Epoch 46/300 - Train Loss: 0.0654, Val Loss: 0.0729\n",
      "Epoch 47/300 - Train Loss: 0.0651, Val Loss: 0.0709\n",
      "Epoch 48/300 - Train Loss: 0.0635, Val Loss: 0.0743\n",
      "Epoch 49/300 - Train Loss: 0.0634, Val Loss: 0.0707\n",
      "Epoch 50/300 - Train Loss: 0.0640, Val Loss: 0.0729\n",
      "Epoch 51/300 - Train Loss: 0.0636, Val Loss: 0.0732\n",
      "Epoch 52/300 - Train Loss: 0.0638, Val Loss: 0.0751\n",
      "Epoch 53/300 - Train Loss: 0.0646, Val Loss: 0.0727\n",
      "Epoch 54/300 - Train Loss: 0.0634, Val Loss: 0.0749\n",
      "Epoch 55/300 - Train Loss: 0.0634, Val Loss: 0.0755\n",
      "Epoch 56/300 - Train Loss: 0.0640, Val Loss: 0.0754\n",
      "Epoch 57/300 - Train Loss: 0.0621, Val Loss: 0.0761\n",
      "Epoch 58/300 - Train Loss: 0.0633, Val Loss: 0.0704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 00:57:43,869] Trial 414 finished with value: 0.9623333836793434 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.18923954480517846, 'learning_rate': 0.0003421582166428454, 'batch_size': 128, 'weight_decay': 5.174549762331336e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300 - Train Loss: 0.0630, Val Loss: 0.0726\n",
      "Early stopping at epoch 59\n",
      "Macro F1 Score: 0.9623, Macro Precision: 0.9544, Macro Recall: 0.9710\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 416\n",
      "Training with F1=32, F2=8, D=2, dropout=0.2269062103637029, LR=0.0007735606804240876, BS=32, WD=3.224024629188703e-05\n",
      "Epoch 1/300 - Train Loss: 0.1624, Val Loss: 0.0877\n",
      "Epoch 2/300 - Train Loss: 0.1014, Val Loss: 0.0700\n",
      "Epoch 3/300 - Train Loss: 0.0959, Val Loss: 0.0911\n",
      "Epoch 4/300 - Train Loss: 0.0919, Val Loss: 0.0769\n",
      "Epoch 5/300 - Train Loss: 0.0892, Val Loss: 0.0724\n",
      "Epoch 6/300 - Train Loss: 0.0887, Val Loss: 0.0898\n",
      "Epoch 7/300 - Train Loss: 0.0847, Val Loss: 0.0699\n",
      "Epoch 8/300 - Train Loss: 0.0874, Val Loss: 0.0821\n",
      "Epoch 9/300 - Train Loss: 0.0886, Val Loss: 0.0832\n",
      "Epoch 10/300 - Train Loss: 0.0839, Val Loss: 0.0722\n",
      "Epoch 11/300 - Train Loss: 0.0851, Val Loss: 0.0737\n",
      "Epoch 12/300 - Train Loss: 0.0831, Val Loss: 0.0680\n",
      "Epoch 13/300 - Train Loss: 0.0839, Val Loss: 0.0875\n",
      "Epoch 14/300 - Train Loss: 0.0851, Val Loss: 0.0663\n",
      "Epoch 15/300 - Train Loss: 0.0808, Val Loss: 0.0653\n",
      "Epoch 16/300 - Train Loss: 0.0828, Val Loss: 0.0655\n",
      "Epoch 17/300 - Train Loss: 0.0794, Val Loss: 0.0693\n",
      "Epoch 18/300 - Train Loss: 0.0774, Val Loss: 0.0702\n",
      "Epoch 19/300 - Train Loss: 0.0789, Val Loss: 0.0806\n",
      "Epoch 20/300 - Train Loss: 0.0777, Val Loss: 0.0707\n",
      "Epoch 21/300 - Train Loss: 0.0782, Val Loss: 0.0682\n",
      "Epoch 22/300 - Train Loss: 0.0793, Val Loss: 0.0662\n",
      "Epoch 23/300 - Train Loss: 0.0780, Val Loss: 0.0705\n",
      "Epoch 24/300 - Train Loss: 0.0771, Val Loss: 0.0659\n",
      "Epoch 25/300 - Train Loss: 0.0757, Val Loss: 0.0730\n",
      "Epoch 26/300 - Train Loss: 0.0791, Val Loss: 0.0703\n",
      "Epoch 27/300 - Train Loss: 0.0751, Val Loss: 0.0626\n",
      "Epoch 28/300 - Train Loss: 0.0751, Val Loss: 0.0641\n",
      "Epoch 29/300 - Train Loss: 0.0779, Val Loss: 0.0689\n",
      "Epoch 30/300 - Train Loss: 0.0784, Val Loss: 0.0661\n",
      "Epoch 31/300 - Train Loss: 0.0755, Val Loss: 0.0766\n",
      "Epoch 32/300 - Train Loss: 0.0757, Val Loss: 0.0709\n",
      "Epoch 33/300 - Train Loss: 0.0764, Val Loss: 0.0637\n",
      "Epoch 34/300 - Train Loss: 0.0761, Val Loss: 0.0693\n",
      "Epoch 35/300 - Train Loss: 0.0735, Val Loss: 0.0657\n",
      "Epoch 36/300 - Train Loss: 0.0743, Val Loss: 0.0665\n",
      "Epoch 37/300 - Train Loss: 0.0744, Val Loss: 0.0673\n",
      "Epoch 38/300 - Train Loss: 0.0717, Val Loss: 0.0684\n",
      "Epoch 39/300 - Train Loss: 0.0736, Val Loss: 0.0674\n",
      "Epoch 40/300 - Train Loss: 0.0737, Val Loss: 0.0664\n",
      "Epoch 41/300 - Train Loss: 0.0728, Val Loss: 0.0695\n",
      "Epoch 42/300 - Train Loss: 0.0735, Val Loss: 0.0708\n",
      "Epoch 43/300 - Train Loss: 0.0717, Val Loss: 0.0656\n",
      "Epoch 44/300 - Train Loss: 0.0728, Val Loss: 0.0637\n",
      "Epoch 45/300 - Train Loss: 0.0701, Val Loss: 0.0690\n",
      "Epoch 46/300 - Train Loss: 0.0727, Val Loss: 0.0695\n",
      "Epoch 47/300 - Train Loss: 0.0716, Val Loss: 0.0745\n",
      "Epoch 48/300 - Train Loss: 0.0739, Val Loss: 0.0731\n",
      "Epoch 49/300 - Train Loss: 0.0709, Val Loss: 0.0701\n",
      "Epoch 50/300 - Train Loss: 0.0713, Val Loss: 0.0685\n",
      "Epoch 51/300 - Train Loss: 0.0722, Val Loss: 0.0633\n",
      "Epoch 52/300 - Train Loss: 0.0689, Val Loss: 0.0705\n",
      "Epoch 53/300 - Train Loss: 0.0695, Val Loss: 0.0713\n",
      "Epoch 54/300 - Train Loss: 0.0693, Val Loss: 0.0645\n",
      "Epoch 55/300 - Train Loss: 0.0685, Val Loss: 0.0711\n",
      "Epoch 56/300 - Train Loss: 0.0694, Val Loss: 0.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:00:06,448] Trial 415 finished with value: 0.9740700587607184 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.2269062103637029, 'learning_rate': 0.0007735606804240876, 'batch_size': 32, 'weight_decay': 3.224024629188703e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/300 - Train Loss: 0.0689, Val Loss: 0.0694\n",
      "Early stopping at epoch 57\n",
      "Macro F1 Score: 0.9741, Macro Precision: 0.9703, Macro Recall: 0.9781\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 417\n",
      "Training with F1=4, F2=8, D=2, dropout=0.17014086809992815, LR=0.0006634621094113959, BS=32, WD=1.9156770331125617e-05\n",
      "Epoch 1/300 - Train Loss: 0.2921, Val Loss: 0.1219\n",
      "Epoch 2/300 - Train Loss: 0.1326, Val Loss: 0.1098\n",
      "Epoch 3/300 - Train Loss: 0.1186, Val Loss: 0.0856\n",
      "Epoch 4/300 - Train Loss: 0.1080, Val Loss: 0.0991\n",
      "Epoch 5/300 - Train Loss: 0.1056, Val Loss: 0.0875\n",
      "Epoch 6/300 - Train Loss: 0.1039, Val Loss: 0.0751\n",
      "Epoch 7/300 - Train Loss: 0.1029, Val Loss: 0.0782\n",
      "Epoch 8/300 - Train Loss: 0.1017, Val Loss: 0.0862\n",
      "Epoch 9/300 - Train Loss: 0.0988, Val Loss: 0.0797\n",
      "Epoch 10/300 - Train Loss: 0.0982, Val Loss: 0.0747\n",
      "Epoch 11/300 - Train Loss: 0.0978, Val Loss: 0.0698\n",
      "Epoch 12/300 - Train Loss: 0.0961, Val Loss: 0.0753\n",
      "Epoch 13/300 - Train Loss: 0.0969, Val Loss: 0.0687\n",
      "Epoch 14/300 - Train Loss: 0.0955, Val Loss: 0.0696\n",
      "Epoch 15/300 - Train Loss: 0.0969, Val Loss: 0.0734\n",
      "Epoch 16/300 - Train Loss: 0.0948, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.0927, Val Loss: 0.0733\n",
      "Epoch 18/300 - Train Loss: 0.0911, Val Loss: 0.0741\n",
      "Epoch 19/300 - Train Loss: 0.0930, Val Loss: 0.0739\n",
      "Epoch 20/300 - Train Loss: 0.0908, Val Loss: 0.0715\n",
      "Epoch 21/300 - Train Loss: 0.0896, Val Loss: 0.0732\n",
      "Epoch 22/300 - Train Loss: 0.0898, Val Loss: 0.0675\n",
      "Epoch 23/300 - Train Loss: 0.0916, Val Loss: 0.0706\n",
      "Epoch 24/300 - Train Loss: 0.0916, Val Loss: 0.0699\n",
      "Epoch 25/300 - Train Loss: 0.0901, Val Loss: 0.0696\n",
      "Epoch 26/300 - Train Loss: 0.0907, Val Loss: 0.0681\n",
      "Epoch 27/300 - Train Loss: 0.0902, Val Loss: 0.0741\n",
      "Epoch 28/300 - Train Loss: 0.0880, Val Loss: 0.0723\n",
      "Epoch 29/300 - Train Loss: 0.0881, Val Loss: 0.0730\n",
      "Epoch 30/300 - Train Loss: 0.0886, Val Loss: 0.0672\n",
      "Epoch 31/300 - Train Loss: 0.0850, Val Loss: 0.0693\n",
      "Epoch 32/300 - Train Loss: 0.0883, Val Loss: 0.0721\n",
      "Epoch 33/300 - Train Loss: 0.0862, Val Loss: 0.0793\n",
      "Epoch 34/300 - Train Loss: 0.0888, Val Loss: 0.0685\n",
      "Epoch 35/300 - Train Loss: 0.0862, Val Loss: 0.0699\n",
      "Epoch 36/300 - Train Loss: 0.0859, Val Loss: 0.0681\n",
      "Epoch 37/300 - Train Loss: 0.0857, Val Loss: 0.0698\n",
      "Epoch 38/300 - Train Loss: 0.0854, Val Loss: 0.0693\n",
      "Epoch 39/300 - Train Loss: 0.0840, Val Loss: 0.0692\n",
      "Epoch 40/300 - Train Loss: 0.0860, Val Loss: 0.0705\n",
      "Epoch 41/300 - Train Loss: 0.0870, Val Loss: 0.0704\n",
      "Epoch 42/300 - Train Loss: 0.0826, Val Loss: 0.0698\n",
      "Epoch 43/300 - Train Loss: 0.0871, Val Loss: 0.0655\n",
      "Epoch 44/300 - Train Loss: 0.0855, Val Loss: 0.0864\n",
      "Epoch 45/300 - Train Loss: 0.0859, Val Loss: 0.0650\n",
      "Epoch 46/300 - Train Loss: 0.0837, Val Loss: 0.0709\n",
      "Epoch 47/300 - Train Loss: 0.0857, Val Loss: 0.0634\n",
      "Epoch 48/300 - Train Loss: 0.0816, Val Loss: 0.0669\n",
      "Epoch 49/300 - Train Loss: 0.0841, Val Loss: 0.0703\n",
      "Epoch 50/300 - Train Loss: 0.0803, Val Loss: 0.0656\n",
      "Epoch 51/300 - Train Loss: 0.0838, Val Loss: 0.0653\n",
      "Epoch 52/300 - Train Loss: 0.0825, Val Loss: 0.0834\n",
      "Epoch 53/300 - Train Loss: 0.0846, Val Loss: 0.0687\n",
      "Epoch 54/300 - Train Loss: 0.0834, Val Loss: 0.0779\n",
      "Epoch 55/300 - Train Loss: 0.0810, Val Loss: 0.0725\n",
      "Epoch 56/300 - Train Loss: 0.0825, Val Loss: 0.0666\n",
      "Epoch 57/300 - Train Loss: 0.0814, Val Loss: 0.0637\n",
      "Epoch 58/300 - Train Loss: 0.0828, Val Loss: 0.0822\n",
      "Epoch 59/300 - Train Loss: 0.0797, Val Loss: 0.0771\n",
      "Epoch 60/300 - Train Loss: 0.0817, Val Loss: 0.0682\n",
      "Epoch 61/300 - Train Loss: 0.0792, Val Loss: 0.0696\n",
      "Epoch 62/300 - Train Loss: 0.0812, Val Loss: 0.0701\n",
      "Epoch 63/300 - Train Loss: 0.0810, Val Loss: 0.0707\n",
      "Epoch 64/300 - Train Loss: 0.0813, Val Loss: 0.0660\n",
      "Epoch 65/300 - Train Loss: 0.0824, Val Loss: 0.0702\n",
      "Epoch 66/300 - Train Loss: 0.0802, Val Loss: 0.0719\n",
      "Epoch 67/300 - Train Loss: 0.0786, Val Loss: 0.0695\n",
      "Epoch 68/300 - Train Loss: 0.0795, Val Loss: 0.0685\n",
      "Epoch 69/300 - Train Loss: 0.0800, Val Loss: 0.0659\n",
      "Epoch 70/300 - Train Loss: 0.0789, Val Loss: 0.0659\n",
      "Epoch 71/300 - Train Loss: 0.0791, Val Loss: 0.0662\n",
      "Epoch 72/300 - Train Loss: 0.0767, Val Loss: 0.0693\n",
      "Epoch 73/300 - Train Loss: 0.0781, Val Loss: 0.0767\n",
      "Epoch 74/300 - Train Loss: 0.0792, Val Loss: 0.0753\n",
      "Epoch 75/300 - Train Loss: 0.0797, Val Loss: 0.0793\n",
      "Epoch 76/300 - Train Loss: 0.0794, Val Loss: 0.0753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:02:32,644] Trial 416 finished with value: 0.9656605282691254 and parameters: {'F1': 4, 'F2': 8, 'D': 2, 'dropout': 0.17014086809992815, 'learning_rate': 0.0006634621094113959, 'batch_size': 32, 'weight_decay': 1.9156770331125617e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/300 - Train Loss: 0.0787, Val Loss: 0.0711\n",
      "Early stopping at epoch 77\n",
      "Macro F1 Score: 0.9657, Macro Precision: 0.9619, Macro Recall: 0.9697\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 418\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1833793517441769, LR=0.0009038406601338769, BS=32, WD=5.8177116661082455e-05\n",
      "Epoch 1/300 - Train Loss: 0.1615, Val Loss: 0.0812\n",
      "Epoch 2/300 - Train Loss: 0.1001, Val Loss: 0.0794\n",
      "Epoch 3/300 - Train Loss: 0.0927, Val Loss: 0.0797\n",
      "Epoch 4/300 - Train Loss: 0.0912, Val Loss: 0.0678\n",
      "Epoch 5/300 - Train Loss: 0.0879, Val Loss: 0.0834\n",
      "Epoch 6/300 - Train Loss: 0.0877, Val Loss: 0.0750\n",
      "Epoch 7/300 - Train Loss: 0.0865, Val Loss: 0.0831\n",
      "Epoch 8/300 - Train Loss: 0.0856, Val Loss: 0.0727\n",
      "Epoch 9/300 - Train Loss: 0.0832, Val Loss: 0.0803\n",
      "Epoch 10/300 - Train Loss: 0.0800, Val Loss: 0.0665\n",
      "Epoch 11/300 - Train Loss: 0.0812, Val Loss: 0.0776\n",
      "Epoch 12/300 - Train Loss: 0.0799, Val Loss: 0.0678\n",
      "Epoch 13/300 - Train Loss: 0.0833, Val Loss: 0.0742\n",
      "Epoch 14/300 - Train Loss: 0.0806, Val Loss: 0.0675\n",
      "Epoch 15/300 - Train Loss: 0.0776, Val Loss: 0.0692\n",
      "Epoch 16/300 - Train Loss: 0.0800, Val Loss: 0.0787\n",
      "Epoch 17/300 - Train Loss: 0.0777, Val Loss: 0.0723\n",
      "Epoch 18/300 - Train Loss: 0.0791, Val Loss: 0.0696\n",
      "Epoch 19/300 - Train Loss: 0.0810, Val Loss: 0.0793\n",
      "Epoch 20/300 - Train Loss: 0.0789, Val Loss: 0.0657\n",
      "Epoch 21/300 - Train Loss: 0.0744, Val Loss: 0.0651\n",
      "Epoch 22/300 - Train Loss: 0.0779, Val Loss: 0.0675\n",
      "Epoch 23/300 - Train Loss: 0.0743, Val Loss: 0.0701\n",
      "Epoch 24/300 - Train Loss: 0.0761, Val Loss: 0.0709\n",
      "Epoch 25/300 - Train Loss: 0.0757, Val Loss: 0.0864\n",
      "Epoch 26/300 - Train Loss: 0.0749, Val Loss: 0.0765\n",
      "Epoch 27/300 - Train Loss: 0.0765, Val Loss: 0.0721\n",
      "Epoch 28/300 - Train Loss: 0.0765, Val Loss: 0.0769\n",
      "Epoch 29/300 - Train Loss: 0.0740, Val Loss: 0.0715\n",
      "Epoch 30/300 - Train Loss: 0.0732, Val Loss: 0.0718\n",
      "Epoch 31/300 - Train Loss: 0.0746, Val Loss: 0.0690\n",
      "Epoch 32/300 - Train Loss: 0.0736, Val Loss: 0.0732\n",
      "Epoch 33/300 - Train Loss: 0.0729, Val Loss: 0.0796\n",
      "Epoch 34/300 - Train Loss: 0.0723, Val Loss: 0.0751\n",
      "Epoch 35/300 - Train Loss: 0.0708, Val Loss: 0.0640\n",
      "Epoch 36/300 - Train Loss: 0.0722, Val Loss: 0.0717\n",
      "Epoch 37/300 - Train Loss: 0.0698, Val Loss: 0.0648\n",
      "Epoch 38/300 - Train Loss: 0.0764, Val Loss: 0.0720\n",
      "Epoch 39/300 - Train Loss: 0.0699, Val Loss: 0.0715\n",
      "Epoch 40/300 - Train Loss: 0.0735, Val Loss: 0.0722\n",
      "Epoch 41/300 - Train Loss: 0.0693, Val Loss: 0.0807\n",
      "Epoch 42/300 - Train Loss: 0.0718, Val Loss: 0.0727\n",
      "Epoch 43/300 - Train Loss: 0.0705, Val Loss: 0.0834\n",
      "Epoch 44/300 - Train Loss: 0.0738, Val Loss: 0.0742\n",
      "Epoch 45/300 - Train Loss: 0.0738, Val Loss: 0.0772\n",
      "Epoch 46/300 - Train Loss: 0.0706, Val Loss: 0.0693\n",
      "Epoch 47/300 - Train Loss: 0.0695, Val Loss: 0.0741\n",
      "Epoch 48/300 - Train Loss: 0.0685, Val Loss: 0.0791\n",
      "Epoch 49/300 - Train Loss: 0.0688, Val Loss: 0.0761\n",
      "Epoch 50/300 - Train Loss: 0.0702, Val Loss: 0.0733\n",
      "Epoch 51/300 - Train Loss: 0.0698, Val Loss: 0.0748\n",
      "Epoch 52/300 - Train Loss: 0.0684, Val Loss: 0.0709\n",
      "Epoch 53/300 - Train Loss: 0.0671, Val Loss: 0.0770\n",
      "Epoch 54/300 - Train Loss: 0.0677, Val Loss: 0.0806\n",
      "Epoch 55/300 - Train Loss: 0.0659, Val Loss: 0.0750\n",
      "Epoch 56/300 - Train Loss: 0.0652, Val Loss: 0.0701\n",
      "Epoch 57/300 - Train Loss: 0.0688, Val Loss: 0.0689\n",
      "Epoch 58/300 - Train Loss: 0.0707, Val Loss: 0.0804\n",
      "Epoch 59/300 - Train Loss: 0.0713, Val Loss: 0.0690\n",
      "Epoch 60/300 - Train Loss: 0.0669, Val Loss: 0.0720\n",
      "Epoch 61/300 - Train Loss: 0.0661, Val Loss: 0.0692\n",
      "Epoch 62/300 - Train Loss: 0.0660, Val Loss: 0.0747\n",
      "Epoch 63/300 - Train Loss: 0.0655, Val Loss: 0.0739\n",
      "Epoch 64/300 - Train Loss: 0.0660, Val Loss: 0.0816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:05:15,586] Trial 417 finished with value: 0.9665236752936733 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1833793517441769, 'learning_rate': 0.0009038406601338769, 'batch_size': 32, 'weight_decay': 5.8177116661082455e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300 - Train Loss: 0.0678, Val Loss: 0.0762\n",
      "Early stopping at epoch 65\n",
      "Macro F1 Score: 0.9665, Macro Precision: 0.9672, Macro Recall: 0.9659\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 419\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1985402373062839, LR=0.0008333764116421338, BS=32, WD=2.4314713557842548e-05\n",
      "Epoch 1/300 - Train Loss: 0.1563, Val Loss: 0.0776\n",
      "Epoch 2/300 - Train Loss: 0.0978, Val Loss: 0.0756\n",
      "Epoch 3/300 - Train Loss: 0.0935, Val Loss: 0.0847\n",
      "Epoch 4/300 - Train Loss: 0.0923, Val Loss: 0.0728\n",
      "Epoch 5/300 - Train Loss: 0.0893, Val Loss: 0.0739\n",
      "Epoch 6/300 - Train Loss: 0.0866, Val Loss: 0.0752\n",
      "Epoch 7/300 - Train Loss: 0.0892, Val Loss: 0.0738\n",
      "Epoch 8/300 - Train Loss: 0.0838, Val Loss: 0.0708\n",
      "Epoch 9/300 - Train Loss: 0.0836, Val Loss: 0.0768\n",
      "Epoch 10/300 - Train Loss: 0.0843, Val Loss: 0.0705\n",
      "Epoch 11/300 - Train Loss: 0.0816, Val Loss: 0.0762\n",
      "Epoch 12/300 - Train Loss: 0.0804, Val Loss: 0.0780\n",
      "Epoch 13/300 - Train Loss: 0.0814, Val Loss: 0.0732\n",
      "Epoch 14/300 - Train Loss: 0.0788, Val Loss: 0.0733\n",
      "Epoch 15/300 - Train Loss: 0.0792, Val Loss: 0.0709\n",
      "Epoch 16/300 - Train Loss: 0.0778, Val Loss: 0.0763\n",
      "Epoch 17/300 - Train Loss: 0.0750, Val Loss: 0.0747\n",
      "Epoch 18/300 - Train Loss: 0.0776, Val Loss: 0.0715\n",
      "Epoch 19/300 - Train Loss: 0.0757, Val Loss: 0.0804\n",
      "Epoch 20/300 - Train Loss: 0.0766, Val Loss: 0.0718\n",
      "Epoch 21/300 - Train Loss: 0.0727, Val Loss: 0.0773\n",
      "Epoch 22/300 - Train Loss: 0.0735, Val Loss: 0.0784\n",
      "Epoch 23/300 - Train Loss: 0.0734, Val Loss: 0.0689\n",
      "Epoch 24/300 - Train Loss: 0.0737, Val Loss: 0.0705\n",
      "Epoch 25/300 - Train Loss: 0.0720, Val Loss: 0.0768\n",
      "Epoch 26/300 - Train Loss: 0.0753, Val Loss: 0.0748\n",
      "Epoch 27/300 - Train Loss: 0.0751, Val Loss: 0.0702\n",
      "Epoch 28/300 - Train Loss: 0.0735, Val Loss: 0.0718\n",
      "Epoch 29/300 - Train Loss: 0.0715, Val Loss: 0.0688\n",
      "Epoch 30/300 - Train Loss: 0.0713, Val Loss: 0.0667\n",
      "Epoch 31/300 - Train Loss: 0.0723, Val Loss: 0.0686\n",
      "Epoch 32/300 - Train Loss: 0.0725, Val Loss: 0.0747\n",
      "Epoch 33/300 - Train Loss: 0.0708, Val Loss: 0.0704\n",
      "Epoch 34/300 - Train Loss: 0.0718, Val Loss: 0.0702\n",
      "Epoch 35/300 - Train Loss: 0.0721, Val Loss: 0.0730\n",
      "Epoch 36/300 - Train Loss: 0.0704, Val Loss: 0.0736\n",
      "Epoch 37/300 - Train Loss: 0.0690, Val Loss: 0.0679\n",
      "Epoch 38/300 - Train Loss: 0.0671, Val Loss: 0.0790\n",
      "Epoch 39/300 - Train Loss: 0.0696, Val Loss: 0.0733\n",
      "Epoch 40/300 - Train Loss: 0.0704, Val Loss: 0.0788\n",
      "Epoch 41/300 - Train Loss: 0.0699, Val Loss: 0.0727\n",
      "Epoch 42/300 - Train Loss: 0.0668, Val Loss: 0.0731\n",
      "Epoch 43/300 - Train Loss: 0.0657, Val Loss: 0.0683\n",
      "Epoch 44/300 - Train Loss: 0.0672, Val Loss: 0.0778\n",
      "Epoch 45/300 - Train Loss: 0.0667, Val Loss: 0.0749\n",
      "Epoch 46/300 - Train Loss: 0.0663, Val Loss: 0.0720\n",
      "Epoch 47/300 - Train Loss: 0.0687, Val Loss: 0.0688\n",
      "Epoch 48/300 - Train Loss: 0.0665, Val Loss: 0.0687\n",
      "Epoch 49/300 - Train Loss: 0.0653, Val Loss: 0.0732\n",
      "Epoch 50/300 - Train Loss: 0.0656, Val Loss: 0.0772\n",
      "Epoch 51/300 - Train Loss: 0.0670, Val Loss: 0.0759\n",
      "Epoch 52/300 - Train Loss: 0.0662, Val Loss: 0.0743\n",
      "Epoch 53/300 - Train Loss: 0.0645, Val Loss: 0.0695\n",
      "Epoch 54/300 - Train Loss: 0.0624, Val Loss: 0.0887\n",
      "Epoch 55/300 - Train Loss: 0.0628, Val Loss: 0.0720\n",
      "Epoch 56/300 - Train Loss: 0.0655, Val Loss: 0.0817\n",
      "Epoch 57/300 - Train Loss: 0.0649, Val Loss: 0.0810\n",
      "Epoch 58/300 - Train Loss: 0.0664, Val Loss: 0.0708\n",
      "Epoch 59/300 - Train Loss: 0.0624, Val Loss: 0.0755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:07:47,385] Trial 418 finished with value: 0.968123988352733 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1985402373062839, 'learning_rate': 0.0008333764116421338, 'batch_size': 32, 'weight_decay': 2.4314713557842548e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/300 - Train Loss: 0.0622, Val Loss: 0.0679\n",
      "Early stopping at epoch 60\n",
      "Macro F1 Score: 0.9681, Macro Precision: 0.9671, Macro Recall: 0.9694\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 420\n",
      "Training with F1=32, F2=8, D=2, dropout=0.2145412846680681, LR=0.0006865748555413618, BS=256, WD=4.361129165899205e-05\n",
      "Epoch 1/300 - Train Loss: 0.3650, Val Loss: 0.1547\n",
      "Epoch 2/300 - Train Loss: 0.1177, Val Loss: 0.0955\n",
      "Epoch 3/300 - Train Loss: 0.0976, Val Loss: 0.0762\n",
      "Epoch 4/300 - Train Loss: 0.0925, Val Loss: 0.0826\n",
      "Epoch 5/300 - Train Loss: 0.0859, Val Loss: 0.0724\n",
      "Epoch 6/300 - Train Loss: 0.0834, Val Loss: 0.0748\n",
      "Epoch 7/300 - Train Loss: 0.0814, Val Loss: 0.0700\n",
      "Epoch 8/300 - Train Loss: 0.0798, Val Loss: 0.0674\n",
      "Epoch 9/300 - Train Loss: 0.0788, Val Loss: 0.0746\n",
      "Epoch 10/300 - Train Loss: 0.0783, Val Loss: 0.0737\n",
      "Epoch 11/300 - Train Loss: 0.0767, Val Loss: 0.0725\n",
      "Epoch 12/300 - Train Loss: 0.0764, Val Loss: 0.0860\n",
      "Epoch 13/300 - Train Loss: 0.0760, Val Loss: 0.0699\n",
      "Epoch 14/300 - Train Loss: 0.0744, Val Loss: 0.0697\n",
      "Epoch 15/300 - Train Loss: 0.0734, Val Loss: 0.0665\n",
      "Epoch 16/300 - Train Loss: 0.0753, Val Loss: 0.0693\n",
      "Epoch 17/300 - Train Loss: 0.0725, Val Loss: 0.0704\n",
      "Epoch 18/300 - Train Loss: 0.0723, Val Loss: 0.0766\n",
      "Epoch 19/300 - Train Loss: 0.0724, Val Loss: 0.0695\n",
      "Epoch 20/300 - Train Loss: 0.0732, Val Loss: 0.0695\n",
      "Epoch 21/300 - Train Loss: 0.0722, Val Loss: 0.0674\n",
      "Epoch 22/300 - Train Loss: 0.0752, Val Loss: 0.0750\n",
      "Epoch 23/300 - Train Loss: 0.0714, Val Loss: 0.0767\n",
      "Epoch 24/300 - Train Loss: 0.0706, Val Loss: 0.0714\n",
      "Epoch 25/300 - Train Loss: 0.0697, Val Loss: 0.0707\n",
      "Epoch 26/300 - Train Loss: 0.0703, Val Loss: 0.0676\n",
      "Epoch 27/300 - Train Loss: 0.0696, Val Loss: 0.0722\n",
      "Epoch 28/300 - Train Loss: 0.0713, Val Loss: 0.0684\n",
      "Epoch 29/300 - Train Loss: 0.0703, Val Loss: 0.0717\n",
      "Epoch 30/300 - Train Loss: 0.0698, Val Loss: 0.0682\n",
      "Epoch 31/300 - Train Loss: 0.0680, Val Loss: 0.0685\n",
      "Epoch 32/300 - Train Loss: 0.0680, Val Loss: 0.0658\n",
      "Epoch 33/300 - Train Loss: 0.0670, Val Loss: 0.0670\n",
      "Epoch 34/300 - Train Loss: 0.0664, Val Loss: 0.0691\n",
      "Epoch 35/300 - Train Loss: 0.0652, Val Loss: 0.0698\n",
      "Epoch 36/300 - Train Loss: 0.0674, Val Loss: 0.0673\n",
      "Epoch 37/300 - Train Loss: 0.0664, Val Loss: 0.0700\n",
      "Epoch 38/300 - Train Loss: 0.0656, Val Loss: 0.0698\n",
      "Epoch 39/300 - Train Loss: 0.0664, Val Loss: 0.0703\n",
      "Epoch 40/300 - Train Loss: 0.0687, Val Loss: 0.0653\n",
      "Epoch 41/300 - Train Loss: 0.0658, Val Loss: 0.0699\n",
      "Epoch 42/300 - Train Loss: 0.0649, Val Loss: 0.0687\n",
      "Epoch 43/300 - Train Loss: 0.0655, Val Loss: 0.0731\n",
      "Epoch 44/300 - Train Loss: 0.0647, Val Loss: 0.0665\n",
      "Epoch 45/300 - Train Loss: 0.0653, Val Loss: 0.0665\n",
      "Epoch 46/300 - Train Loss: 0.0652, Val Loss: 0.0728\n",
      "Epoch 47/300 - Train Loss: 0.0639, Val Loss: 0.0666\n",
      "Epoch 48/300 - Train Loss: 0.0638, Val Loss: 0.0689\n",
      "Epoch 49/300 - Train Loss: 0.0638, Val Loss: 0.0718\n",
      "Epoch 50/300 - Train Loss: 0.0630, Val Loss: 0.0682\n",
      "Epoch 51/300 - Train Loss: 0.0639, Val Loss: 0.0697\n",
      "Epoch 52/300 - Train Loss: 0.0622, Val Loss: 0.0741\n",
      "Epoch 53/300 - Train Loss: 0.0625, Val Loss: 0.0711\n",
      "Epoch 54/300 - Train Loss: 0.0627, Val Loss: 0.0690\n",
      "Epoch 55/300 - Train Loss: 0.0616, Val Loss: 0.0687\n",
      "Epoch 56/300 - Train Loss: 0.0612, Val Loss: 0.0678\n",
      "Epoch 57/300 - Train Loss: 0.0629, Val Loss: 0.0721\n",
      "Epoch 58/300 - Train Loss: 0.0613, Val Loss: 0.0656\n",
      "Epoch 59/300 - Train Loss: 0.0609, Val Loss: 0.0682\n",
      "Epoch 60/300 - Train Loss: 0.0624, Val Loss: 0.0661\n",
      "Epoch 61/300 - Train Loss: 0.0626, Val Loss: 0.0700\n",
      "Epoch 62/300 - Train Loss: 0.0609, Val Loss: 0.0682\n",
      "Epoch 63/300 - Train Loss: 0.0614, Val Loss: 0.0640\n",
      "Epoch 64/300 - Train Loss: 0.0597, Val Loss: 0.0699\n",
      "Epoch 65/300 - Train Loss: 0.0613, Val Loss: 0.0680\n",
      "Epoch 66/300 - Train Loss: 0.0606, Val Loss: 0.0668\n",
      "Epoch 67/300 - Train Loss: 0.0617, Val Loss: 0.0659\n",
      "Epoch 68/300 - Train Loss: 0.0616, Val Loss: 0.0675\n",
      "Epoch 69/300 - Train Loss: 0.0601, Val Loss: 0.0676\n",
      "Epoch 70/300 - Train Loss: 0.0619, Val Loss: 0.0665\n",
      "Epoch 71/300 - Train Loss: 0.0596, Val Loss: 0.0671\n",
      "Epoch 72/300 - Train Loss: 0.0601, Val Loss: 0.0685\n",
      "Epoch 73/300 - Train Loss: 0.0584, Val Loss: 0.0707\n",
      "Epoch 74/300 - Train Loss: 0.0576, Val Loss: 0.0666\n",
      "Epoch 75/300 - Train Loss: 0.0607, Val Loss: 0.0638\n",
      "Epoch 76/300 - Train Loss: 0.0603, Val Loss: 0.0704\n",
      "Epoch 77/300 - Train Loss: 0.0591, Val Loss: 0.0659\n",
      "Epoch 78/300 - Train Loss: 0.0590, Val Loss: 0.0662\n",
      "Epoch 79/300 - Train Loss: 0.0600, Val Loss: 0.0650\n",
      "Epoch 80/300 - Train Loss: 0.0597, Val Loss: 0.0670\n",
      "Epoch 81/300 - Train Loss: 0.0595, Val Loss: 0.0688\n",
      "Epoch 82/300 - Train Loss: 0.0582, Val Loss: 0.0666\n",
      "Epoch 83/300 - Train Loss: 0.0593, Val Loss: 0.0678\n",
      "Epoch 84/300 - Train Loss: 0.0574, Val Loss: 0.0663\n",
      "Epoch 85/300 - Train Loss: 0.0586, Val Loss: 0.0680\n",
      "Epoch 86/300 - Train Loss: 0.0572, Val Loss: 0.0677\n",
      "Epoch 87/300 - Train Loss: 0.0577, Val Loss: 0.0653\n",
      "Epoch 88/300 - Train Loss: 0.0577, Val Loss: 0.0647\n",
      "Epoch 89/300 - Train Loss: 0.0584, Val Loss: 0.0677\n",
      "Epoch 90/300 - Train Loss: 0.0575, Val Loss: 0.0675\n",
      "Epoch 91/300 - Train Loss: 0.0570, Val Loss: 0.0656\n",
      "Epoch 92/300 - Train Loss: 0.0577, Val Loss: 0.0677\n",
      "Epoch 93/300 - Train Loss: 0.0559, Val Loss: 0.0650\n",
      "Epoch 94/300 - Train Loss: 0.0575, Val Loss: 0.0674\n",
      "Epoch 95/300 - Train Loss: 0.0559, Val Loss: 0.0668\n",
      "Epoch 96/300 - Train Loss: 0.0557, Val Loss: 0.0663\n",
      "Epoch 97/300 - Train Loss: 0.0563, Val Loss: 0.0701\n",
      "Epoch 98/300 - Train Loss: 0.0563, Val Loss: 0.0698\n",
      "Epoch 99/300 - Train Loss: 0.0558, Val Loss: 0.0654\n",
      "Epoch 100/300 - Train Loss: 0.0541, Val Loss: 0.0695\n",
      "Epoch 101/300 - Train Loss: 0.0555, Val Loss: 0.0679\n",
      "Epoch 102/300 - Train Loss: 0.0556, Val Loss: 0.0689\n",
      "Epoch 103/300 - Train Loss: 0.0561, Val Loss: 0.0663\n",
      "Epoch 104/300 - Train Loss: 0.0559, Val Loss: 0.0691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:11:19,563] Trial 419 finished with value: 0.968116539545111 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.2145412846680681, 'learning_rate': 0.0006865748555413618, 'batch_size': 256, 'weight_decay': 4.361129165899205e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 105/300 - Train Loss: 0.0553, Val Loss: 0.0696\n",
      "Early stopping at epoch 105\n",
      "Macro F1 Score: 0.9681, Macro Precision: 0.9601, Macro Recall: 0.9768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 421\n",
      "Training with F1=32, F2=8, D=2, dropout=0.16985263295627995, LR=0.0007506221510548171, BS=32, WD=3.941027686926418e-05\n",
      "Epoch 1/300 - Train Loss: 0.1594, Val Loss: 0.0814\n",
      "Epoch 2/300 - Train Loss: 0.0974, Val Loss: 0.0755\n",
      "Epoch 3/300 - Train Loss: 0.0938, Val Loss: 0.0699\n",
      "Epoch 4/300 - Train Loss: 0.0914, Val Loss: 0.0776\n",
      "Epoch 5/300 - Train Loss: 0.0881, Val Loss: 0.0765\n",
      "Epoch 6/300 - Train Loss: 0.0877, Val Loss: 0.0753\n",
      "Epoch 7/300 - Train Loss: 0.0855, Val Loss: 0.0709\n",
      "Epoch 8/300 - Train Loss: 0.0851, Val Loss: 0.0726\n",
      "Epoch 9/300 - Train Loss: 0.0851, Val Loss: 0.0707\n",
      "Epoch 10/300 - Train Loss: 0.0818, Val Loss: 0.0763\n",
      "Epoch 11/300 - Train Loss: 0.0812, Val Loss: 0.0834\n",
      "Epoch 12/300 - Train Loss: 0.0814, Val Loss: 0.0786\n",
      "Epoch 13/300 - Train Loss: 0.0792, Val Loss: 0.0706\n",
      "Epoch 14/300 - Train Loss: 0.0811, Val Loss: 0.0686\n",
      "Epoch 15/300 - Train Loss: 0.0798, Val Loss: 0.0780\n",
      "Epoch 16/300 - Train Loss: 0.0806, Val Loss: 0.0712\n",
      "Epoch 17/300 - Train Loss: 0.0797, Val Loss: 0.0728\n",
      "Epoch 18/300 - Train Loss: 0.0761, Val Loss: 0.0732\n",
      "Epoch 19/300 - Train Loss: 0.0772, Val Loss: 0.0721\n",
      "Epoch 20/300 - Train Loss: 0.0787, Val Loss: 0.0690\n",
      "Epoch 21/300 - Train Loss: 0.0776, Val Loss: 0.0718\n",
      "Epoch 22/300 - Train Loss: 0.0753, Val Loss: 0.0741\n",
      "Epoch 23/300 - Train Loss: 0.0757, Val Loss: 0.0734\n",
      "Epoch 24/300 - Train Loss: 0.0763, Val Loss: 0.0731\n",
      "Epoch 25/300 - Train Loss: 0.0770, Val Loss: 0.0708\n",
      "Epoch 26/300 - Train Loss: 0.0752, Val Loss: 0.0780\n",
      "Epoch 27/300 - Train Loss: 0.0756, Val Loss: 0.0690\n",
      "Epoch 28/300 - Train Loss: 0.0719, Val Loss: 0.0688\n",
      "Epoch 29/300 - Train Loss: 0.0741, Val Loss: 0.0751\n",
      "Epoch 30/300 - Train Loss: 0.0742, Val Loss: 0.0718\n",
      "Epoch 31/300 - Train Loss: 0.0732, Val Loss: 0.0748\n",
      "Epoch 32/300 - Train Loss: 0.0747, Val Loss: 0.0943\n",
      "Epoch 33/300 - Train Loss: 0.0720, Val Loss: 0.0695\n",
      "Epoch 34/300 - Train Loss: 0.0733, Val Loss: 0.0694\n",
      "Epoch 35/300 - Train Loss: 0.0718, Val Loss: 0.0744\n",
      "Epoch 36/300 - Train Loss: 0.0749, Val Loss: 0.0717\n",
      "Epoch 37/300 - Train Loss: 0.0741, Val Loss: 0.0717\n",
      "Epoch 38/300 - Train Loss: 0.0731, Val Loss: 0.0802\n",
      "Epoch 39/300 - Train Loss: 0.0708, Val Loss: 0.0739\n",
      "Epoch 40/300 - Train Loss: 0.0724, Val Loss: 0.0685\n",
      "Epoch 41/300 - Train Loss: 0.0716, Val Loss: 0.0715\n",
      "Epoch 42/300 - Train Loss: 0.0711, Val Loss: 0.0791\n",
      "Epoch 43/300 - Train Loss: 0.0713, Val Loss: 0.0774\n",
      "Epoch 44/300 - Train Loss: 0.0691, Val Loss: 0.0739\n",
      "Epoch 45/300 - Train Loss: 0.0672, Val Loss: 0.0742\n",
      "Epoch 46/300 - Train Loss: 0.0692, Val Loss: 0.0722\n",
      "Epoch 47/300 - Train Loss: 0.0684, Val Loss: 0.0757\n",
      "Epoch 48/300 - Train Loss: 0.0722, Val Loss: 0.0737\n",
      "Epoch 49/300 - Train Loss: 0.0679, Val Loss: 0.0684\n",
      "Epoch 50/300 - Train Loss: 0.0668, Val Loss: 0.0746\n",
      "Epoch 51/300 - Train Loss: 0.0678, Val Loss: 0.0726\n",
      "Epoch 52/300 - Train Loss: 0.0679, Val Loss: 0.0755\n",
      "Epoch 53/300 - Train Loss: 0.0683, Val Loss: 0.0816\n",
      "Epoch 54/300 - Train Loss: 0.0686, Val Loss: 0.0737\n",
      "Epoch 55/300 - Train Loss: 0.0671, Val Loss: 0.0704\n",
      "Epoch 56/300 - Train Loss: 0.0671, Val Loss: 0.0689\n",
      "Epoch 57/300 - Train Loss: 0.0685, Val Loss: 0.0814\n",
      "Epoch 58/300 - Train Loss: 0.0684, Val Loss: 0.0676\n",
      "Epoch 59/300 - Train Loss: 0.0683, Val Loss: 0.0737\n",
      "Epoch 60/300 - Train Loss: 0.0661, Val Loss: 0.0777\n",
      "Epoch 61/300 - Train Loss: 0.0662, Val Loss: 0.0739\n",
      "Epoch 62/300 - Train Loss: 0.0660, Val Loss: 0.0732\n",
      "Epoch 63/300 - Train Loss: 0.0664, Val Loss: 0.0732\n",
      "Epoch 64/300 - Train Loss: 0.0674, Val Loss: 0.0757\n",
      "Epoch 65/300 - Train Loss: 0.0662, Val Loss: 0.0805\n",
      "Epoch 66/300 - Train Loss: 0.0651, Val Loss: 0.0738\n",
      "Epoch 67/300 - Train Loss: 0.0650, Val Loss: 0.0717\n",
      "Epoch 68/300 - Train Loss: 0.0650, Val Loss: 0.0761\n",
      "Epoch 69/300 - Train Loss: 0.0665, Val Loss: 0.0752\n",
      "Epoch 70/300 - Train Loss: 0.0659, Val Loss: 0.0718\n",
      "Epoch 71/300 - Train Loss: 0.0660, Val Loss: 0.0771\n",
      "Epoch 72/300 - Train Loss: 0.0645, Val Loss: 0.0748\n",
      "Epoch 73/300 - Train Loss: 0.0624, Val Loss: 0.0790\n",
      "Epoch 74/300 - Train Loss: 0.0652, Val Loss: 0.0755\n",
      "Epoch 75/300 - Train Loss: 0.0654, Val Loss: 0.0761\n",
      "Epoch 76/300 - Train Loss: 0.0637, Val Loss: 0.0774\n",
      "Epoch 77/300 - Train Loss: 0.0611, Val Loss: 0.0796\n",
      "Epoch 78/300 - Train Loss: 0.0628, Val Loss: 0.0719\n",
      "Epoch 79/300 - Train Loss: 0.0629, Val Loss: 0.0724\n",
      "Epoch 80/300 - Train Loss: 0.0635, Val Loss: 0.0799\n",
      "Epoch 81/300 - Train Loss: 0.0644, Val Loss: 0.0834\n",
      "Epoch 82/300 - Train Loss: 0.0627, Val Loss: 0.0711\n",
      "Epoch 83/300 - Train Loss: 0.0620, Val Loss: 0.0764\n",
      "Epoch 84/300 - Train Loss: 0.0634, Val Loss: 0.0732\n",
      "Epoch 85/300 - Train Loss: 0.0609, Val Loss: 0.0772\n",
      "Epoch 86/300 - Train Loss: 0.0635, Val Loss: 0.0818\n",
      "Epoch 87/300 - Train Loss: 0.0631, Val Loss: 0.0738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:15:01,381] Trial 420 finished with value: 0.9734690425775622 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.16985263295627995, 'learning_rate': 0.0007506221510548171, 'batch_size': 32, 'weight_decay': 3.941027686926418e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/300 - Train Loss: 0.0623, Val Loss: 0.0783\n",
      "Early stopping at epoch 88\n",
      "Macro F1 Score: 0.9735, Macro Precision: 0.9741, Macro Recall: 0.9729\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 422\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1839071393183021, LR=0.0009353931170559927, BS=32, WD=5.0559393273129924e-05\n",
      "Epoch 1/300 - Train Loss: 0.1652, Val Loss: 0.0837\n",
      "Epoch 2/300 - Train Loss: 0.1024, Val Loss: 0.0762\n",
      "Epoch 3/300 - Train Loss: 0.0939, Val Loss: 0.0771\n",
      "Epoch 4/300 - Train Loss: 0.0921, Val Loss: 0.0727\n",
      "Epoch 5/300 - Train Loss: 0.0891, Val Loss: 0.0750\n",
      "Epoch 6/300 - Train Loss: 0.0890, Val Loss: 0.0724\n",
      "Epoch 7/300 - Train Loss: 0.0877, Val Loss: 0.0770\n",
      "Epoch 8/300 - Train Loss: 0.0841, Val Loss: 0.0780\n",
      "Epoch 9/300 - Train Loss: 0.0842, Val Loss: 0.0785\n",
      "Epoch 10/300 - Train Loss: 0.0843, Val Loss: 0.0930\n",
      "Epoch 11/300 - Train Loss: 0.0848, Val Loss: 0.0761\n",
      "Epoch 12/300 - Train Loss: 0.0828, Val Loss: 0.0738\n",
      "Epoch 13/300 - Train Loss: 0.0816, Val Loss: 0.1355\n",
      "Epoch 14/300 - Train Loss: 0.0817, Val Loss: 0.0862\n",
      "Epoch 15/300 - Train Loss: 0.0789, Val Loss: 0.0760\n",
      "Epoch 16/300 - Train Loss: 0.0792, Val Loss: 0.0691\n",
      "Epoch 17/300 - Train Loss: 0.0788, Val Loss: 0.0706\n",
      "Epoch 18/300 - Train Loss: 0.0777, Val Loss: 0.0730\n",
      "Epoch 19/300 - Train Loss: 0.0775, Val Loss: 0.0743\n",
      "Epoch 20/300 - Train Loss: 0.0782, Val Loss: 0.0844\n",
      "Epoch 21/300 - Train Loss: 0.0777, Val Loss: 0.0689\n",
      "Epoch 22/300 - Train Loss: 0.0761, Val Loss: 0.0739\n",
      "Epoch 23/300 - Train Loss: 0.0763, Val Loss: 0.0747\n",
      "Epoch 24/300 - Train Loss: 0.0777, Val Loss: 0.0704\n",
      "Epoch 25/300 - Train Loss: 0.0744, Val Loss: 0.0864\n",
      "Epoch 26/300 - Train Loss: 0.0729, Val Loss: 0.0788\n",
      "Epoch 27/300 - Train Loss: 0.0760, Val Loss: 0.0826\n",
      "Epoch 28/300 - Train Loss: 0.0771, Val Loss: 0.0838\n",
      "Epoch 29/300 - Train Loss: 0.0747, Val Loss: 0.0713\n",
      "Epoch 30/300 - Train Loss: 0.0748, Val Loss: 0.0824\n",
      "Epoch 31/300 - Train Loss: 0.0747, Val Loss: 0.0816\n",
      "Epoch 32/300 - Train Loss: 0.0716, Val Loss: 0.0752\n",
      "Epoch 33/300 - Train Loss: 0.0726, Val Loss: 0.0746\n",
      "Epoch 34/300 - Train Loss: 0.0740, Val Loss: 0.0695\n",
      "Epoch 35/300 - Train Loss: 0.0711, Val Loss: 0.0743\n",
      "Epoch 36/300 - Train Loss: 0.0730, Val Loss: 0.0690\n",
      "Epoch 37/300 - Train Loss: 0.0716, Val Loss: 0.0886\n",
      "Epoch 38/300 - Train Loss: 0.0705, Val Loss: 0.0791\n",
      "Epoch 39/300 - Train Loss: 0.0682, Val Loss: 0.0743\n",
      "Epoch 40/300 - Train Loss: 0.0715, Val Loss: 0.0775\n",
      "Epoch 41/300 - Train Loss: 0.0715, Val Loss: 0.0708\n",
      "Epoch 42/300 - Train Loss: 0.0710, Val Loss: 0.0728\n",
      "Epoch 43/300 - Train Loss: 0.0699, Val Loss: 0.0808\n",
      "Epoch 44/300 - Train Loss: 0.0698, Val Loss: 0.0695\n",
      "Epoch 45/300 - Train Loss: 0.0738, Val Loss: 0.0904\n",
      "Epoch 46/300 - Train Loss: 0.0701, Val Loss: 0.0688\n",
      "Epoch 47/300 - Train Loss: 0.0696, Val Loss: 0.0714\n",
      "Epoch 48/300 - Train Loss: 0.0703, Val Loss: 0.0719\n",
      "Epoch 49/300 - Train Loss: 0.0686, Val Loss: 0.0716\n",
      "Epoch 50/300 - Train Loss: 0.0698, Val Loss: 0.0751\n",
      "Epoch 51/300 - Train Loss: 0.0713, Val Loss: 0.0828\n",
      "Epoch 52/300 - Train Loss: 0.0714, Val Loss: 0.0868\n",
      "Epoch 53/300 - Train Loss: 0.0704, Val Loss: 0.0719\n",
      "Epoch 54/300 - Train Loss: 0.0677, Val Loss: 0.0708\n",
      "Epoch 55/300 - Train Loss: 0.0739, Val Loss: 0.0772\n",
      "Epoch 56/300 - Train Loss: 0.0676, Val Loss: 0.0768\n",
      "Epoch 57/300 - Train Loss: 0.0664, Val Loss: 0.0724\n",
      "Epoch 58/300 - Train Loss: 0.0695, Val Loss: 0.0753\n",
      "Epoch 59/300 - Train Loss: 0.0665, Val Loss: 0.0780\n",
      "Epoch 60/300 - Train Loss: 0.0676, Val Loss: 0.0701\n",
      "Epoch 61/300 - Train Loss: 0.0673, Val Loss: 0.0752\n",
      "Epoch 62/300 - Train Loss: 0.0662, Val Loss: 0.0698\n",
      "Epoch 63/300 - Train Loss: 0.0680, Val Loss: 0.0756\n",
      "Epoch 64/300 - Train Loss: 0.0693, Val Loss: 0.0713\n",
      "Epoch 65/300 - Train Loss: 0.0667, Val Loss: 0.0698\n",
      "Epoch 66/300 - Train Loss: 0.0664, Val Loss: 0.0770\n",
      "Epoch 67/300 - Train Loss: 0.0642, Val Loss: 0.0773\n",
      "Epoch 68/300 - Train Loss: 0.0667, Val Loss: 0.0758\n",
      "Epoch 69/300 - Train Loss: 0.0652, Val Loss: 0.0735\n",
      "Epoch 70/300 - Train Loss: 0.0646, Val Loss: 0.0742\n",
      "Epoch 71/300 - Train Loss: 0.0667, Val Loss: 0.0776\n",
      "Epoch 72/300 - Train Loss: 0.0651, Val Loss: 0.0731\n",
      "Epoch 73/300 - Train Loss: 0.0653, Val Loss: 0.0854\n",
      "Epoch 74/300 - Train Loss: 0.0675, Val Loss: 0.0727\n",
      "Epoch 75/300 - Train Loss: 0.0626, Val Loss: 0.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:18:12,384] Trial 421 finished with value: 0.9632394448039322 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1839071393183021, 'learning_rate': 0.0009353931170559927, 'batch_size': 32, 'weight_decay': 5.0559393273129924e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300 - Train Loss: 0.0650, Val Loss: 0.0771\n",
      "Early stopping at epoch 76\n",
      "Macro F1 Score: 0.9632, Macro Precision: 0.9661, Macro Recall: 0.9604\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.92      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 423\n",
      "Training with F1=32, F2=16, D=2, dropout=0.19890681093631116, LR=0.0008262860234942411, BS=32, WD=6.905515479937155e-05\n",
      "Epoch 1/300 - Train Loss: 0.1415, Val Loss: 0.1505\n",
      "Epoch 2/300 - Train Loss: 0.0984, Val Loss: 0.0865\n",
      "Epoch 3/300 - Train Loss: 0.0930, Val Loss: 0.0731\n",
      "Epoch 4/300 - Train Loss: 0.0935, Val Loss: 0.0921\n",
      "Epoch 5/300 - Train Loss: 0.0888, Val Loss: 0.1060\n",
      "Epoch 6/300 - Train Loss: 0.0868, Val Loss: 0.0769\n",
      "Epoch 7/300 - Train Loss: 0.0839, Val Loss: 0.0807\n",
      "Epoch 8/300 - Train Loss: 0.0840, Val Loss: 0.0778\n",
      "Epoch 9/300 - Train Loss: 0.0832, Val Loss: 0.0700\n",
      "Epoch 10/300 - Train Loss: 0.0814, Val Loss: 0.0739\n",
      "Epoch 11/300 - Train Loss: 0.0812, Val Loss: 0.0920\n",
      "Epoch 12/300 - Train Loss: 0.0797, Val Loss: 0.0769\n",
      "Epoch 13/300 - Train Loss: 0.0787, Val Loss: 0.0706\n",
      "Epoch 14/300 - Train Loss: 0.0813, Val Loss: 0.0699\n",
      "Epoch 15/300 - Train Loss: 0.0781, Val Loss: 0.0821\n",
      "Epoch 16/300 - Train Loss: 0.0759, Val Loss: 0.0741\n",
      "Epoch 17/300 - Train Loss: 0.0784, Val Loss: 0.0691\n",
      "Epoch 18/300 - Train Loss: 0.0751, Val Loss: 0.0698\n",
      "Epoch 19/300 - Train Loss: 0.0771, Val Loss: 0.0694\n",
      "Epoch 20/300 - Train Loss: 0.0772, Val Loss: 0.0654\n",
      "Epoch 21/300 - Train Loss: 0.0740, Val Loss: 0.0707\n",
      "Epoch 22/300 - Train Loss: 0.0737, Val Loss: 0.0680\n",
      "Epoch 23/300 - Train Loss: 0.0746, Val Loss: 0.0690\n",
      "Epoch 24/300 - Train Loss: 0.0740, Val Loss: 0.0652\n",
      "Epoch 25/300 - Train Loss: 0.0729, Val Loss: 0.0855\n",
      "Epoch 26/300 - Train Loss: 0.0740, Val Loss: 0.0747\n",
      "Epoch 27/300 - Train Loss: 0.0725, Val Loss: 0.0696\n",
      "Epoch 28/300 - Train Loss: 0.0701, Val Loss: 0.0728\n",
      "Epoch 29/300 - Train Loss: 0.0689, Val Loss: 0.0752\n",
      "Epoch 30/300 - Train Loss: 0.0717, Val Loss: 0.0768\n",
      "Epoch 31/300 - Train Loss: 0.0715, Val Loss: 0.0662\n",
      "Epoch 32/300 - Train Loss: 0.0723, Val Loss: 0.0730\n",
      "Epoch 33/300 - Train Loss: 0.0689, Val Loss: 0.0718\n",
      "Epoch 34/300 - Train Loss: 0.0710, Val Loss: 0.0720\n",
      "Epoch 35/300 - Train Loss: 0.0668, Val Loss: 0.0690\n",
      "Epoch 36/300 - Train Loss: 0.0656, Val Loss: 0.0672\n",
      "Epoch 37/300 - Train Loss: 0.0653, Val Loss: 0.0643\n",
      "Epoch 38/300 - Train Loss: 0.0663, Val Loss: 0.0796\n",
      "Epoch 39/300 - Train Loss: 0.0681, Val Loss: 0.0752\n",
      "Epoch 40/300 - Train Loss: 0.0656, Val Loss: 0.0707\n",
      "Epoch 41/300 - Train Loss: 0.0659, Val Loss: 0.0683\n",
      "Epoch 42/300 - Train Loss: 0.0633, Val Loss: 0.0683\n",
      "Epoch 43/300 - Train Loss: 0.0665, Val Loss: 0.0710\n",
      "Epoch 44/300 - Train Loss: 0.0638, Val Loss: 0.0636\n",
      "Epoch 45/300 - Train Loss: 0.0614, Val Loss: 0.0816\n",
      "Epoch 46/300 - Train Loss: 0.0621, Val Loss: 0.0758\n",
      "Epoch 47/300 - Train Loss: 0.0618, Val Loss: 0.0688\n",
      "Epoch 48/300 - Train Loss: 0.0626, Val Loss: 0.0716\n",
      "Epoch 49/300 - Train Loss: 0.0608, Val Loss: 0.0694\n",
      "Epoch 50/300 - Train Loss: 0.0624, Val Loss: 0.0713\n",
      "Epoch 51/300 - Train Loss: 0.0640, Val Loss: 0.0724\n",
      "Epoch 52/300 - Train Loss: 0.0612, Val Loss: 0.0680\n",
      "Epoch 53/300 - Train Loss: 0.0604, Val Loss: 0.0666\n",
      "Epoch 54/300 - Train Loss: 0.0613, Val Loss: 0.0729\n",
      "Epoch 55/300 - Train Loss: 0.0665, Val Loss: 0.0729\n",
      "Epoch 56/300 - Train Loss: 0.0626, Val Loss: 0.0644\n",
      "Epoch 57/300 - Train Loss: 0.0590, Val Loss: 0.0778\n",
      "Epoch 58/300 - Train Loss: 0.0599, Val Loss: 0.0864\n",
      "Epoch 59/300 - Train Loss: 0.0597, Val Loss: 0.0760\n",
      "Epoch 60/300 - Train Loss: 0.0593, Val Loss: 0.0750\n",
      "Epoch 61/300 - Train Loss: 0.0598, Val Loss: 0.0733\n",
      "Epoch 62/300 - Train Loss: 0.0616, Val Loss: 0.0751\n",
      "Epoch 63/300 - Train Loss: 0.0615, Val Loss: 0.0744\n",
      "Epoch 64/300 - Train Loss: 0.0574, Val Loss: 0.0785\n",
      "Epoch 65/300 - Train Loss: 0.0594, Val Loss: 0.0734\n",
      "Epoch 66/300 - Train Loss: 0.0611, Val Loss: 0.0735\n",
      "Epoch 67/300 - Train Loss: 0.0566, Val Loss: 0.0747\n",
      "Epoch 68/300 - Train Loss: 0.0582, Val Loss: 0.0692\n",
      "Epoch 69/300 - Train Loss: 0.0631, Val Loss: 0.0691\n",
      "Epoch 70/300 - Train Loss: 0.0604, Val Loss: 0.0740\n",
      "Epoch 71/300 - Train Loss: 0.0566, Val Loss: 0.0773\n",
      "Epoch 72/300 - Train Loss: 0.0571, Val Loss: 0.0727\n",
      "Epoch 73/300 - Train Loss: 0.0566, Val Loss: 0.0819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:21:29,642] Trial 422 finished with value: 0.9750380599199949 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.19890681093631116, 'learning_rate': 0.0008262860234942411, 'batch_size': 32, 'weight_decay': 6.905515479937155e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300 - Train Loss: 0.0549, Val Loss: 0.0804\n",
      "Early stopping at epoch 74\n",
      "Macro F1 Score: 0.9750, Macro Precision: 0.9733, Macro Recall: 0.9768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.97      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.98      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 424\n",
      "Training with F1=32, F2=16, D=2, dropout=0.2062435965668426, LR=0.0007600617438038617, BS=32, WD=6.582538726652609e-05\n",
      "Epoch 1/300 - Train Loss: 0.1515, Val Loss: 0.0907\n",
      "Epoch 2/300 - Train Loss: 0.0976, Val Loss: 0.0761\n",
      "Epoch 3/300 - Train Loss: 0.0934, Val Loss: 0.0757\n",
      "Epoch 4/300 - Train Loss: 0.0920, Val Loss: 0.0759\n",
      "Epoch 5/300 - Train Loss: 0.0885, Val Loss: 0.0986\n",
      "Epoch 6/300 - Train Loss: 0.0892, Val Loss: 0.0708\n",
      "Epoch 7/300 - Train Loss: 0.0862, Val Loss: 0.0753\n",
      "Epoch 8/300 - Train Loss: 0.0873, Val Loss: 0.0788\n",
      "Epoch 9/300 - Train Loss: 0.0843, Val Loss: 0.1358\n",
      "Epoch 10/300 - Train Loss: 0.0846, Val Loss: 0.0766\n",
      "Epoch 11/300 - Train Loss: 0.0813, Val Loss: 0.0764\n",
      "Epoch 12/300 - Train Loss: 0.0803, Val Loss: 0.0722\n",
      "Epoch 13/300 - Train Loss: 0.0806, Val Loss: 0.0754\n",
      "Epoch 14/300 - Train Loss: 0.0786, Val Loss: 0.0718\n",
      "Epoch 15/300 - Train Loss: 0.0778, Val Loss: 0.0706\n",
      "Epoch 16/300 - Train Loss: 0.0780, Val Loss: 0.0793\n",
      "Epoch 17/300 - Train Loss: 0.0768, Val Loss: 0.0709\n",
      "Epoch 18/300 - Train Loss: 0.0779, Val Loss: 0.0665\n",
      "Epoch 19/300 - Train Loss: 0.0729, Val Loss: 0.0719\n",
      "Epoch 20/300 - Train Loss: 0.0744, Val Loss: 0.0672\n",
      "Epoch 21/300 - Train Loss: 0.0741, Val Loss: 0.0761\n",
      "Epoch 22/300 - Train Loss: 0.0748, Val Loss: 0.0848\n",
      "Epoch 23/300 - Train Loss: 0.0731, Val Loss: 0.0725\n",
      "Epoch 24/300 - Train Loss: 0.0739, Val Loss: 0.0734\n",
      "Epoch 25/300 - Train Loss: 0.0721, Val Loss: 0.0753\n",
      "Epoch 26/300 - Train Loss: 0.0712, Val Loss: 0.0742\n",
      "Epoch 27/300 - Train Loss: 0.0733, Val Loss: 0.0696\n",
      "Epoch 28/300 - Train Loss: 0.0709, Val Loss: 0.0764\n",
      "Epoch 29/300 - Train Loss: 0.0719, Val Loss: 0.0747\n",
      "Epoch 30/300 - Train Loss: 0.0694, Val Loss: 0.0769\n",
      "Epoch 31/300 - Train Loss: 0.0701, Val Loss: 0.0673\n",
      "Epoch 32/300 - Train Loss: 0.0803, Val Loss: 0.0725\n",
      "Epoch 33/300 - Train Loss: 0.0680, Val Loss: 0.0730\n",
      "Epoch 34/300 - Train Loss: 0.0656, Val Loss: 0.0738\n",
      "Epoch 35/300 - Train Loss: 0.0668, Val Loss: 0.0719\n",
      "Epoch 36/300 - Train Loss: 0.0665, Val Loss: 0.0806\n",
      "Epoch 37/300 - Train Loss: 0.0682, Val Loss: 0.0809\n",
      "Epoch 38/300 - Train Loss: 0.0663, Val Loss: 0.0734\n",
      "Epoch 39/300 - Train Loss: 0.0665, Val Loss: 0.0724\n",
      "Epoch 40/300 - Train Loss: 0.0657, Val Loss: 0.0733\n",
      "Epoch 41/300 - Train Loss: 0.0658, Val Loss: 0.0720\n",
      "Epoch 42/300 - Train Loss: 0.0678, Val Loss: 0.0710\n",
      "Epoch 43/300 - Train Loss: 0.0645, Val Loss: 0.0743\n",
      "Epoch 44/300 - Train Loss: 0.0671, Val Loss: 0.0708\n",
      "Epoch 45/300 - Train Loss: 0.0634, Val Loss: 0.0790\n",
      "Epoch 46/300 - Train Loss: 0.0632, Val Loss: 0.0734\n",
      "Epoch 47/300 - Train Loss: 0.0640, Val Loss: 0.0686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:23:37,697] Trial 423 finished with value: 0.9684276876244264 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.2062435965668426, 'learning_rate': 0.0007600617438038617, 'batch_size': 32, 'weight_decay': 6.582538726652609e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300 - Train Loss: 0.0666, Val Loss: 0.0677\n",
      "Early stopping at epoch 48\n",
      "Macro F1 Score: 0.9684, Macro Precision: 0.9766, Macro Recall: 0.9607\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.92      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 425\n",
      "Training with F1=32, F2=16, D=2, dropout=0.19875907824481917, LR=0.0009068899226794098, BS=32, WD=1.354910832736828e-05\n",
      "Epoch 1/300 - Train Loss: 0.1459, Val Loss: 0.0912\n",
      "Epoch 2/300 - Train Loss: 0.0993, Val Loss: 0.0822\n",
      "Epoch 3/300 - Train Loss: 0.0936, Val Loss: 0.0815\n",
      "Epoch 4/300 - Train Loss: 0.0912, Val Loss: 0.0742\n",
      "Epoch 5/300 - Train Loss: 0.0868, Val Loss: 0.0748\n",
      "Epoch 6/300 - Train Loss: 0.0872, Val Loss: 0.0704\n",
      "Epoch 7/300 - Train Loss: 0.0838, Val Loss: 0.0851\n",
      "Epoch 8/300 - Train Loss: 0.0819, Val Loss: 0.0794\n",
      "Epoch 9/300 - Train Loss: 0.0812, Val Loss: 0.0730\n",
      "Epoch 10/300 - Train Loss: 0.0799, Val Loss: 0.0731\n",
      "Epoch 11/300 - Train Loss: 0.0802, Val Loss: 0.0745\n",
      "Epoch 12/300 - Train Loss: 0.0766, Val Loss: 0.0744\n",
      "Epoch 13/300 - Train Loss: 0.0773, Val Loss: 0.0829\n",
      "Epoch 14/300 - Train Loss: 0.0750, Val Loss: 0.0775\n",
      "Epoch 15/300 - Train Loss: 0.0746, Val Loss: 0.0739\n",
      "Epoch 16/300 - Train Loss: 0.0742, Val Loss: 0.0752\n",
      "Epoch 17/300 - Train Loss: 0.0766, Val Loss: 0.0718\n",
      "Epoch 18/300 - Train Loss: 0.0737, Val Loss: 0.0753\n",
      "Epoch 19/300 - Train Loss: 0.0741, Val Loss: 0.0713\n",
      "Epoch 20/300 - Train Loss: 0.0724, Val Loss: 0.0722\n",
      "Epoch 21/300 - Train Loss: 0.0730, Val Loss: 0.0724\n",
      "Epoch 22/300 - Train Loss: 0.0705, Val Loss: 0.0731\n",
      "Epoch 23/300 - Train Loss: 0.0691, Val Loss: 0.0776\n",
      "Epoch 24/300 - Train Loss: 0.0704, Val Loss: 0.0725\n",
      "Epoch 25/300 - Train Loss: 0.0692, Val Loss: 0.0735\n",
      "Epoch 26/300 - Train Loss: 0.0665, Val Loss: 0.0827\n",
      "Epoch 27/300 - Train Loss: 0.0660, Val Loss: 0.0756\n",
      "Epoch 28/300 - Train Loss: 0.0673, Val Loss: 0.0834\n",
      "Epoch 29/300 - Train Loss: 0.0682, Val Loss: 0.0748\n",
      "Epoch 30/300 - Train Loss: 0.0668, Val Loss: 0.0804\n",
      "Epoch 31/300 - Train Loss: 0.0667, Val Loss: 0.0738\n",
      "Epoch 32/300 - Train Loss: 0.0671, Val Loss: 0.0748\n",
      "Epoch 33/300 - Train Loss: 0.0664, Val Loss: 0.0750\n",
      "Epoch 34/300 - Train Loss: 0.0622, Val Loss: 0.0778\n",
      "Epoch 35/300 - Train Loss: 0.0651, Val Loss: 0.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:25:13,716] Trial 424 finished with value: 0.9646542057229036 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.19875907824481917, 'learning_rate': 0.0009068899226794098, 'batch_size': 32, 'weight_decay': 1.354910832736828e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300 - Train Loss: 0.0622, Val Loss: 0.0772\n",
      "Early stopping at epoch 36\n",
      "Macro F1 Score: 0.9647, Macro Precision: 0.9758, Macro Recall: 0.9544\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.96      0.90      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.95      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 426\n",
      "Training with F1=32, F2=16, D=2, dropout=0.21764394359456848, LR=4.1366500796149806e-05, BS=32, WD=6.913508326356424e-05\n",
      "Epoch 1/300 - Train Loss: 0.4844, Val Loss: 0.2238\n",
      "Epoch 2/300 - Train Loss: 0.2133, Val Loss: 0.1514\n",
      "Epoch 3/300 - Train Loss: 0.1506, Val Loss: 0.1111\n",
      "Epoch 4/300 - Train Loss: 0.1203, Val Loss: 0.0945\n",
      "Epoch 5/300 - Train Loss: 0.1109, Val Loss: 0.0871\n",
      "Epoch 6/300 - Train Loss: 0.1055, Val Loss: 0.0947\n",
      "Epoch 7/300 - Train Loss: 0.1037, Val Loss: 0.0850\n",
      "Epoch 8/300 - Train Loss: 0.0977, Val Loss: 0.0762\n",
      "Epoch 9/300 - Train Loss: 0.0962, Val Loss: 0.0798\n",
      "Epoch 10/300 - Train Loss: 0.0960, Val Loss: 0.0757\n",
      "Epoch 11/300 - Train Loss: 0.0977, Val Loss: 0.0816\n",
      "Epoch 12/300 - Train Loss: 0.0927, Val Loss: 0.0742\n",
      "Epoch 13/300 - Train Loss: 0.0909, Val Loss: 0.0749\n",
      "Epoch 14/300 - Train Loss: 0.0911, Val Loss: 0.0733\n",
      "Epoch 15/300 - Train Loss: 0.0901, Val Loss: 0.0768\n",
      "Epoch 16/300 - Train Loss: 0.0888, Val Loss: 0.0800\n",
      "Epoch 17/300 - Train Loss: 0.0886, Val Loss: 0.0740\n",
      "Epoch 18/300 - Train Loss: 0.0876, Val Loss: 0.0724\n",
      "Epoch 19/300 - Train Loss: 0.0871, Val Loss: 0.0721\n",
      "Epoch 20/300 - Train Loss: 0.0866, Val Loss: 0.0762\n",
      "Epoch 21/300 - Train Loss: 0.0877, Val Loss: 0.0728\n",
      "Epoch 22/300 - Train Loss: 0.0861, Val Loss: 0.0767\n",
      "Epoch 23/300 - Train Loss: 0.0828, Val Loss: 0.0727\n",
      "Epoch 24/300 - Train Loss: 0.0834, Val Loss: 0.0724\n",
      "Epoch 25/300 - Train Loss: 0.0825, Val Loss: 0.0747\n",
      "Epoch 26/300 - Train Loss: 0.0830, Val Loss: 0.0715\n",
      "Epoch 27/300 - Train Loss: 0.0838, Val Loss: 0.0690\n",
      "Epoch 28/300 - Train Loss: 0.0848, Val Loss: 0.0808\n",
      "Epoch 29/300 - Train Loss: 0.0832, Val Loss: 0.0739\n",
      "Epoch 30/300 - Train Loss: 0.0811, Val Loss: 0.0758\n",
      "Epoch 31/300 - Train Loss: 0.0819, Val Loss: 0.0719\n",
      "Epoch 32/300 - Train Loss: 0.0809, Val Loss: 0.0720\n",
      "Epoch 33/300 - Train Loss: 0.0837, Val Loss: 0.0710\n",
      "Epoch 34/300 - Train Loss: 0.0807, Val Loss: 0.0770\n",
      "Epoch 35/300 - Train Loss: 0.0800, Val Loss: 0.0735\n",
      "Epoch 36/300 - Train Loss: 0.0774, Val Loss: 0.0736\n",
      "Epoch 37/300 - Train Loss: 0.0806, Val Loss: 0.0704\n",
      "Epoch 38/300 - Train Loss: 0.0809, Val Loss: 0.0765\n",
      "Epoch 39/300 - Train Loss: 0.0808, Val Loss: 0.0687\n",
      "Epoch 40/300 - Train Loss: 0.0812, Val Loss: 0.0722\n",
      "Epoch 41/300 - Train Loss: 0.0803, Val Loss: 0.0761\n",
      "Epoch 42/300 - Train Loss: 0.0799, Val Loss: 0.0684\n",
      "Epoch 43/300 - Train Loss: 0.0788, Val Loss: 0.0692\n",
      "Epoch 44/300 - Train Loss: 0.0778, Val Loss: 0.0688\n",
      "Epoch 45/300 - Train Loss: 0.0787, Val Loss: 0.0729\n",
      "Epoch 46/300 - Train Loss: 0.0766, Val Loss: 0.0716\n",
      "Epoch 47/300 - Train Loss: 0.0792, Val Loss: 0.0705\n",
      "Epoch 48/300 - Train Loss: 0.0766, Val Loss: 0.0701\n",
      "Epoch 49/300 - Train Loss: 0.0793, Val Loss: 0.0695\n",
      "Epoch 50/300 - Train Loss: 0.0784, Val Loss: 0.0709\n",
      "Epoch 51/300 - Train Loss: 0.0773, Val Loss: 0.0718\n",
      "Epoch 52/300 - Train Loss: 0.0767, Val Loss: 0.0745\n",
      "Epoch 53/300 - Train Loss: 0.0769, Val Loss: 0.0673\n",
      "Epoch 54/300 - Train Loss: 0.0770, Val Loss: 0.0724\n",
      "Epoch 55/300 - Train Loss: 0.0743, Val Loss: 0.0768\n",
      "Epoch 56/300 - Train Loss: 0.0766, Val Loss: 0.0681\n",
      "Epoch 57/300 - Train Loss: 0.0746, Val Loss: 0.0693\n",
      "Epoch 58/300 - Train Loss: 0.0764, Val Loss: 0.0688\n",
      "Epoch 59/300 - Train Loss: 0.0771, Val Loss: 0.0721\n",
      "Epoch 60/300 - Train Loss: 0.0754, Val Loss: 0.0695\n",
      "Epoch 61/300 - Train Loss: 0.0749, Val Loss: 0.0702\n",
      "Epoch 62/300 - Train Loss: 0.0766, Val Loss: 0.0675\n",
      "Epoch 63/300 - Train Loss: 0.0755, Val Loss: 0.0714\n",
      "Epoch 64/300 - Train Loss: 0.0735, Val Loss: 0.0705\n",
      "Epoch 65/300 - Train Loss: 0.0731, Val Loss: 0.0702\n",
      "Epoch 66/300 - Train Loss: 0.0733, Val Loss: 0.0682\n",
      "Epoch 67/300 - Train Loss: 0.0728, Val Loss: 0.0671\n",
      "Epoch 68/300 - Train Loss: 0.0719, Val Loss: 0.0659\n",
      "Epoch 69/300 - Train Loss: 0.0727, Val Loss: 0.0679\n",
      "Epoch 70/300 - Train Loss: 0.0730, Val Loss: 0.0689\n",
      "Epoch 71/300 - Train Loss: 0.0718, Val Loss: 0.0666\n",
      "Epoch 72/300 - Train Loss: 0.0742, Val Loss: 0.0697\n",
      "Epoch 73/300 - Train Loss: 0.0752, Val Loss: 0.0696\n",
      "Epoch 74/300 - Train Loss: 0.0734, Val Loss: 0.0663\n",
      "Epoch 75/300 - Train Loss: 0.0742, Val Loss: 0.0679\n",
      "Epoch 76/300 - Train Loss: 0.0749, Val Loss: 0.0678\n",
      "Epoch 77/300 - Train Loss: 0.0739, Val Loss: 0.0691\n",
      "Epoch 78/300 - Train Loss: 0.0715, Val Loss: 0.0675\n",
      "Epoch 79/300 - Train Loss: 0.0715, Val Loss: 0.0676\n",
      "Epoch 80/300 - Train Loss: 0.0714, Val Loss: 0.0724\n",
      "Epoch 81/300 - Train Loss: 0.0734, Val Loss: 0.0678\n",
      "Epoch 82/300 - Train Loss: 0.0717, Val Loss: 0.0704\n",
      "Epoch 83/300 - Train Loss: 0.0699, Val Loss: 0.0719\n",
      "Epoch 84/300 - Train Loss: 0.0714, Val Loss: 0.0686\n",
      "Epoch 85/300 - Train Loss: 0.0703, Val Loss: 0.0740\n",
      "Epoch 86/300 - Train Loss: 0.0709, Val Loss: 0.0720\n",
      "Epoch 87/300 - Train Loss: 0.0717, Val Loss: 0.0651\n",
      "Epoch 88/300 - Train Loss: 0.0700, Val Loss: 0.0679\n",
      "Epoch 89/300 - Train Loss: 0.0709, Val Loss: 0.0672\n",
      "Epoch 90/300 - Train Loss: 0.0723, Val Loss: 0.0719\n",
      "Epoch 91/300 - Train Loss: 0.0709, Val Loss: 0.0689\n",
      "Epoch 92/300 - Train Loss: 0.0703, Val Loss: 0.0689\n",
      "Epoch 93/300 - Train Loss: 0.0715, Val Loss: 0.0728\n",
      "Epoch 94/300 - Train Loss: 0.0719, Val Loss: 0.0780\n",
      "Epoch 95/300 - Train Loss: 0.0704, Val Loss: 0.0688\n",
      "Epoch 96/300 - Train Loss: 0.0686, Val Loss: 0.0696\n",
      "Epoch 97/300 - Train Loss: 0.0691, Val Loss: 0.0706\n",
      "Epoch 98/300 - Train Loss: 0.0676, Val Loss: 0.0706\n",
      "Epoch 99/300 - Train Loss: 0.0691, Val Loss: 0.0684\n",
      "Epoch 100/300 - Train Loss: 0.0681, Val Loss: 0.0673\n",
      "Epoch 101/300 - Train Loss: 0.0685, Val Loss: 0.0659\n",
      "Epoch 102/300 - Train Loss: 0.0678, Val Loss: 0.0689\n",
      "Epoch 103/300 - Train Loss: 0.0702, Val Loss: 0.0689\n",
      "Epoch 104/300 - Train Loss: 0.0694, Val Loss: 0.0704\n",
      "Epoch 105/300 - Train Loss: 0.0694, Val Loss: 0.0680\n",
      "Epoch 106/300 - Train Loss: 0.0693, Val Loss: 0.0684\n",
      "Epoch 107/300 - Train Loss: 0.0684, Val Loss: 0.0698\n",
      "Epoch 108/300 - Train Loss: 0.0699, Val Loss: 0.0668\n",
      "Epoch 109/300 - Train Loss: 0.0696, Val Loss: 0.0714\n",
      "Epoch 110/300 - Train Loss: 0.0682, Val Loss: 0.0660\n",
      "Epoch 111/300 - Train Loss: 0.0668, Val Loss: 0.0694\n",
      "Epoch 112/300 - Train Loss: 0.0677, Val Loss: 0.0687\n",
      "Epoch 113/300 - Train Loss: 0.0668, Val Loss: 0.0697\n",
      "Epoch 114/300 - Train Loss: 0.0686, Val Loss: 0.0658\n",
      "Epoch 115/300 - Train Loss: 0.0677, Val Loss: 0.0733\n",
      "Epoch 116/300 - Train Loss: 0.0657, Val Loss: 0.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:30:25,596] Trial 425 finished with value: 0.9658658936471493 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.21764394359456848, 'learning_rate': 4.1366500796149806e-05, 'batch_size': 32, 'weight_decay': 6.913508326356424e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 117/300 - Train Loss: 0.0675, Val Loss: 0.0686\n",
      "Early stopping at epoch 117\n",
      "Macro F1 Score: 0.9659, Macro Precision: 0.9559, Macro Recall: 0.9768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 427\n",
      "Training with F1=32, F2=16, D=2, dropout=0.19307359056949, LR=0.0009934582980703588, BS=32, WD=5.765871258160414e-05\n",
      "Epoch 1/300 - Train Loss: 0.1470, Val Loss: 0.0820\n",
      "Epoch 2/300 - Train Loss: 0.0987, Val Loss: 0.0763\n",
      "Epoch 3/300 - Train Loss: 0.0919, Val Loss: 0.0762\n",
      "Epoch 4/300 - Train Loss: 0.0897, Val Loss: 0.0705\n",
      "Epoch 5/300 - Train Loss: 0.0843, Val Loss: 0.0766\n",
      "Epoch 6/300 - Train Loss: 0.0842, Val Loss: 0.0710\n",
      "Epoch 7/300 - Train Loss: 0.0845, Val Loss: 0.0748\n",
      "Epoch 8/300 - Train Loss: 0.0819, Val Loss: 0.0693\n",
      "Epoch 9/300 - Train Loss: 0.0809, Val Loss: 0.0707\n",
      "Epoch 10/300 - Train Loss: 0.0808, Val Loss: 0.0708\n",
      "Epoch 11/300 - Train Loss: 0.0810, Val Loss: 0.0761\n",
      "Epoch 12/300 - Train Loss: 0.0810, Val Loss: 0.0808\n",
      "Epoch 13/300 - Train Loss: 0.0790, Val Loss: 0.0723\n",
      "Epoch 14/300 - Train Loss: 0.0769, Val Loss: 0.0768\n",
      "Epoch 15/300 - Train Loss: 0.0767, Val Loss: 0.0748\n",
      "Epoch 16/300 - Train Loss: 0.0777, Val Loss: 0.0771\n",
      "Epoch 17/300 - Train Loss: 0.0754, Val Loss: 0.0792\n",
      "Epoch 18/300 - Train Loss: 0.0739, Val Loss: 0.0746\n",
      "Epoch 19/300 - Train Loss: 0.0744, Val Loss: 0.0765\n",
      "Epoch 20/300 - Train Loss: 0.0759, Val Loss: 0.0793\n",
      "Epoch 21/300 - Train Loss: 0.0750, Val Loss: 0.0733\n",
      "Epoch 22/300 - Train Loss: 0.0729, Val Loss: 0.0758\n",
      "Epoch 23/300 - Train Loss: 0.0732, Val Loss: 0.0804\n",
      "Epoch 24/300 - Train Loss: 0.0727, Val Loss: 0.0754\n",
      "Epoch 25/300 - Train Loss: 0.0715, Val Loss: 0.0765\n",
      "Epoch 26/300 - Train Loss: 0.0706, Val Loss: 0.0771\n",
      "Epoch 27/300 - Train Loss: 0.0700, Val Loss: 0.0829\n",
      "Epoch 28/300 - Train Loss: 0.0700, Val Loss: 0.0814\n",
      "Epoch 29/300 - Train Loss: 0.0722, Val Loss: 0.0788\n",
      "Epoch 30/300 - Train Loss: 0.0681, Val Loss: 0.0776\n",
      "Epoch 31/300 - Train Loss: 0.0692, Val Loss: 0.0805\n",
      "Epoch 32/300 - Train Loss: 0.0697, Val Loss: 0.0921\n",
      "Epoch 33/300 - Train Loss: 0.0690, Val Loss: 0.0735\n",
      "Epoch 34/300 - Train Loss: 0.0701, Val Loss: 0.0748\n",
      "Epoch 35/300 - Train Loss: 0.0683, Val Loss: 0.0839\n",
      "Epoch 36/300 - Train Loss: 0.0671, Val Loss: 0.0782\n",
      "Epoch 37/300 - Train Loss: 0.0680, Val Loss: 0.0722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:32:06,897] Trial 426 finished with value: 0.9602583963133045 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.19307359056949, 'learning_rate': 0.0009934582980703588, 'batch_size': 32, 'weight_decay': 5.765871258160414e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300 - Train Loss: 0.0679, Val Loss: 0.0718\n",
      "Early stopping at epoch 38\n",
      "Macro F1 Score: 0.9603, Macro Precision: 0.9488, Macro Recall: 0.9732\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 428\n",
      "Training with F1=32, F2=16, D=2, dropout=0.22848329506318976, LR=0.0008567378773696577, BS=32, WD=8.084465216230354e-05\n",
      "Epoch 1/300 - Train Loss: 0.1466, Val Loss: 0.0756\n",
      "Epoch 2/300 - Train Loss: 0.0982, Val Loss: 0.1379\n",
      "Epoch 3/300 - Train Loss: 0.0943, Val Loss: 0.0752\n",
      "Epoch 4/300 - Train Loss: 0.0920, Val Loss: 0.1700\n",
      "Epoch 5/300 - Train Loss: 0.0911, Val Loss: 0.0772\n",
      "Epoch 6/300 - Train Loss: 0.0884, Val Loss: 0.0729\n",
      "Epoch 7/300 - Train Loss: 0.0864, Val Loss: 0.0768\n",
      "Epoch 8/300 - Train Loss: 0.0868, Val Loss: 0.0712\n",
      "Epoch 9/300 - Train Loss: 0.0832, Val Loss: 0.0851\n",
      "Epoch 10/300 - Train Loss: 0.0814, Val Loss: 0.0721\n",
      "Epoch 11/300 - Train Loss: 0.0817, Val Loss: 0.0737\n",
      "Epoch 12/300 - Train Loss: 0.0805, Val Loss: 0.0761\n",
      "Epoch 13/300 - Train Loss: 0.0824, Val Loss: 0.0678\n",
      "Epoch 14/300 - Train Loss: 0.0776, Val Loss: 0.0709\n",
      "Epoch 15/300 - Train Loss: 0.0785, Val Loss: 0.0800\n",
      "Epoch 16/300 - Train Loss: 0.0757, Val Loss: 0.0860\n",
      "Epoch 17/300 - Train Loss: 0.0782, Val Loss: 0.0725\n",
      "Epoch 18/300 - Train Loss: 0.0754, Val Loss: 0.0713\n",
      "Epoch 19/300 - Train Loss: 0.0752, Val Loss: 0.0739\n",
      "Epoch 20/300 - Train Loss: 0.0753, Val Loss: 0.0774\n",
      "Epoch 21/300 - Train Loss: 0.0753, Val Loss: 0.0785\n",
      "Epoch 22/300 - Train Loss: 0.0744, Val Loss: 0.0685\n",
      "Epoch 23/300 - Train Loss: 0.0744, Val Loss: 0.0780\n",
      "Epoch 24/300 - Train Loss: 0.0723, Val Loss: 0.0729\n",
      "Epoch 25/300 - Train Loss: 0.0715, Val Loss: 0.0685\n",
      "Epoch 26/300 - Train Loss: 0.0740, Val Loss: 0.0755\n",
      "Epoch 27/300 - Train Loss: 0.0728, Val Loss: 0.0718\n",
      "Epoch 28/300 - Train Loss: 0.0747, Val Loss: 0.0776\n",
      "Epoch 29/300 - Train Loss: 0.0720, Val Loss: 0.0681\n",
      "Epoch 30/300 - Train Loss: 0.0693, Val Loss: 0.0758\n",
      "Epoch 31/300 - Train Loss: 0.0704, Val Loss: 0.0712\n",
      "Epoch 32/300 - Train Loss: 0.0724, Val Loss: 0.0668\n",
      "Epoch 33/300 - Train Loss: 0.0716, Val Loss: 0.0788\n",
      "Epoch 34/300 - Train Loss: 0.0693, Val Loss: 0.0651\n",
      "Epoch 35/300 - Train Loss: 0.0708, Val Loss: 0.0627\n",
      "Epoch 36/300 - Train Loss: 0.0715, Val Loss: 0.0729\n",
      "Epoch 37/300 - Train Loss: 0.0663, Val Loss: 0.0739\n",
      "Epoch 38/300 - Train Loss: 0.0688, Val Loss: 0.0707\n",
      "Epoch 39/300 - Train Loss: 0.0677, Val Loss: 0.0753\n",
      "Epoch 40/300 - Train Loss: 0.0678, Val Loss: 0.0802\n",
      "Epoch 41/300 - Train Loss: 0.0674, Val Loss: 0.0819\n",
      "Epoch 42/300 - Train Loss: 0.0687, Val Loss: 0.0700\n",
      "Epoch 43/300 - Train Loss: 0.0672, Val Loss: 0.0675\n",
      "Epoch 44/300 - Train Loss: 0.0668, Val Loss: 0.0755\n",
      "Epoch 45/300 - Train Loss: 0.0670, Val Loss: 0.0695\n",
      "Epoch 46/300 - Train Loss: 0.0677, Val Loss: 0.0768\n",
      "Epoch 47/300 - Train Loss: 0.0662, Val Loss: 0.0690\n",
      "Epoch 48/300 - Train Loss: 0.0635, Val Loss: 0.0733\n",
      "Epoch 49/300 - Train Loss: 0.0647, Val Loss: 0.0802\n",
      "Epoch 50/300 - Train Loss: 0.0636, Val Loss: 0.0755\n",
      "Epoch 51/300 - Train Loss: 0.0660, Val Loss: 0.0655\n",
      "Epoch 52/300 - Train Loss: 0.0642, Val Loss: 0.0719\n",
      "Epoch 53/300 - Train Loss: 0.0660, Val Loss: 0.0663\n",
      "Epoch 54/300 - Train Loss: 0.0639, Val Loss: 0.0709\n",
      "Epoch 55/300 - Train Loss: 0.0649, Val Loss: 0.0707\n",
      "Epoch 56/300 - Train Loss: 0.0625, Val Loss: 0.0743\n",
      "Epoch 57/300 - Train Loss: 0.0628, Val Loss: 0.0724\n",
      "Epoch 58/300 - Train Loss: 0.0654, Val Loss: 0.0750\n",
      "Epoch 59/300 - Train Loss: 0.0648, Val Loss: 0.0735\n",
      "Epoch 60/300 - Train Loss: 0.0653, Val Loss: 0.0731\n",
      "Epoch 61/300 - Train Loss: 0.0597, Val Loss: 0.0719\n",
      "Epoch 62/300 - Train Loss: 0.0622, Val Loss: 0.0734\n",
      "Epoch 63/300 - Train Loss: 0.0626, Val Loss: 0.0702\n",
      "Epoch 64/300 - Train Loss: 0.0619, Val Loss: 0.0785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:35:00,250] Trial 427 finished with value: 0.9677815532238739 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.22848329506318976, 'learning_rate': 0.0008567378773696577, 'batch_size': 32, 'weight_decay': 8.084465216230354e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300 - Train Loss: 0.0624, Val Loss: 0.0754\n",
      "Early stopping at epoch 65\n",
      "Macro F1 Score: 0.9678, Macro Precision: 0.9640, Macro Recall: 0.9718\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 429\n",
      "Training with F1=32, F2=16, D=2, dropout=0.21040197503355682, LR=0.0007208052410972903, BS=32, WD=0.0005956423815882356\n",
      "Epoch 1/300 - Train Loss: 0.1470, Val Loss: 0.0767\n",
      "Epoch 2/300 - Train Loss: 0.1026, Val Loss: 0.0792\n",
      "Epoch 3/300 - Train Loss: 0.0957, Val Loss: 0.0729\n",
      "Epoch 4/300 - Train Loss: 0.0926, Val Loss: 0.0779\n",
      "Epoch 5/300 - Train Loss: 0.0933, Val Loss: 0.0783\n",
      "Epoch 6/300 - Train Loss: 0.0919, Val Loss: 0.0833\n",
      "Epoch 7/300 - Train Loss: 0.0868, Val Loss: 0.0728\n",
      "Epoch 8/300 - Train Loss: 0.0875, Val Loss: 0.0773\n",
      "Epoch 9/300 - Train Loss: 0.0891, Val Loss: 0.0817\n",
      "Epoch 10/300 - Train Loss: 0.0882, Val Loss: 0.0721\n",
      "Epoch 11/300 - Train Loss: 0.0876, Val Loss: 0.0794\n",
      "Epoch 12/300 - Train Loss: 0.0860, Val Loss: 0.0721\n",
      "Epoch 13/300 - Train Loss: 0.0859, Val Loss: 0.0709\n",
      "Epoch 14/300 - Train Loss: 0.0855, Val Loss: 0.0737\n",
      "Epoch 15/300 - Train Loss: 0.0874, Val Loss: 0.0899\n",
      "Epoch 16/300 - Train Loss: 0.0860, Val Loss: 0.0718\n",
      "Epoch 17/300 - Train Loss: 0.0864, Val Loss: 0.0709\n",
      "Epoch 18/300 - Train Loss: 0.0851, Val Loss: 0.0707\n",
      "Epoch 19/300 - Train Loss: 0.0877, Val Loss: 0.0760\n",
      "Epoch 20/300 - Train Loss: 0.0848, Val Loss: 0.0652\n",
      "Epoch 21/300 - Train Loss: 0.0828, Val Loss: 0.0819\n",
      "Epoch 22/300 - Train Loss: 0.0822, Val Loss: 0.0740\n",
      "Epoch 23/300 - Train Loss: 0.0877, Val Loss: 0.0844\n",
      "Epoch 24/300 - Train Loss: 0.0854, Val Loss: 0.0790\n",
      "Epoch 25/300 - Train Loss: 0.0838, Val Loss: 0.0765\n",
      "Epoch 26/300 - Train Loss: 0.0841, Val Loss: 0.0751\n",
      "Epoch 27/300 - Train Loss: 0.0861, Val Loss: 0.0696\n",
      "Epoch 28/300 - Train Loss: 0.0866, Val Loss: 0.0731\n",
      "Epoch 29/300 - Train Loss: 0.0835, Val Loss: 0.0741\n",
      "Epoch 30/300 - Train Loss: 0.0816, Val Loss: 0.0729\n",
      "Epoch 31/300 - Train Loss: 0.0854, Val Loss: 0.0647\n",
      "Epoch 32/300 - Train Loss: 0.0841, Val Loss: 0.0750\n",
      "Epoch 33/300 - Train Loss: 0.0842, Val Loss: 0.0718\n",
      "Epoch 34/300 - Train Loss: 0.0873, Val Loss: 0.0716\n",
      "Epoch 35/300 - Train Loss: 0.0860, Val Loss: 0.0719\n",
      "Epoch 36/300 - Train Loss: 0.0828, Val Loss: 0.0677\n",
      "Epoch 37/300 - Train Loss: 0.0826, Val Loss: 0.0758\n",
      "Epoch 38/300 - Train Loss: 0.0823, Val Loss: 0.0690\n",
      "Epoch 39/300 - Train Loss: 0.0827, Val Loss: 0.0778\n",
      "Epoch 40/300 - Train Loss: 0.0831, Val Loss: 0.0740\n",
      "Epoch 41/300 - Train Loss: 0.0825, Val Loss: 0.0700\n",
      "Epoch 42/300 - Train Loss: 0.0817, Val Loss: 0.0714\n",
      "Epoch 43/300 - Train Loss: 0.0831, Val Loss: 0.0671\n",
      "Epoch 44/300 - Train Loss: 0.0818, Val Loss: 0.0694\n",
      "Epoch 45/300 - Train Loss: 0.0831, Val Loss: 0.0687\n",
      "Epoch 46/300 - Train Loss: 0.0803, Val Loss: 0.0921\n",
      "Epoch 47/300 - Train Loss: 0.0821, Val Loss: 0.0711\n",
      "Epoch 48/300 - Train Loss: 0.0827, Val Loss: 0.0723\n",
      "Epoch 49/300 - Train Loss: 0.0846, Val Loss: 0.0840\n",
      "Epoch 50/300 - Train Loss: 0.0835, Val Loss: 0.0701\n",
      "Epoch 51/300 - Train Loss: 0.0817, Val Loss: 0.0825\n",
      "Epoch 52/300 - Train Loss: 0.0819, Val Loss: 0.0726\n",
      "Epoch 53/300 - Train Loss: 0.0824, Val Loss: 0.0732\n",
      "Epoch 54/300 - Train Loss: 0.0829, Val Loss: 0.0711\n",
      "Epoch 55/300 - Train Loss: 0.0821, Val Loss: 0.0730\n",
      "Epoch 56/300 - Train Loss: 0.0811, Val Loss: 0.0732\n",
      "Epoch 57/300 - Train Loss: 0.0827, Val Loss: 0.0688\n",
      "Epoch 58/300 - Train Loss: 0.0828, Val Loss: 0.0703\n",
      "Epoch 59/300 - Train Loss: 0.0809, Val Loss: 0.0807\n",
      "Epoch 60/300 - Train Loss: 0.0828, Val Loss: 0.0708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:37:42,894] Trial 428 finished with value: 0.9727430818652468 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.21040197503355682, 'learning_rate': 0.0007208052410972903, 'batch_size': 32, 'weight_decay': 0.0005956423815882356}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/300 - Train Loss: 0.0826, Val Loss: 0.0734\n",
      "Early stopping at epoch 61\n",
      "Macro F1 Score: 0.9727, Macro Precision: 0.9738, Macro Recall: 0.9718\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 430\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1768672306403899, LR=0.0008003379896445558, BS=32, WD=4.691697618307483e-05\n",
      "Epoch 1/300 - Train Loss: 0.1629, Val Loss: 0.0790\n",
      "Epoch 2/300 - Train Loss: 0.0972, Val Loss: 0.0819\n",
      "Epoch 3/300 - Train Loss: 0.0924, Val Loss: 0.0769\n",
      "Epoch 4/300 - Train Loss: 0.0899, Val Loss: 0.0753\n",
      "Epoch 5/300 - Train Loss: 0.0884, Val Loss: 0.0738\n",
      "Epoch 6/300 - Train Loss: 0.0881, Val Loss: 0.0794\n",
      "Epoch 7/300 - Train Loss: 0.0866, Val Loss: 0.1260\n",
      "Epoch 8/300 - Train Loss: 0.0850, Val Loss: 0.0660\n",
      "Epoch 9/300 - Train Loss: 0.0817, Val Loss: 0.0786\n",
      "Epoch 10/300 - Train Loss: 0.0844, Val Loss: 0.0715\n",
      "Epoch 11/300 - Train Loss: 0.0844, Val Loss: 0.0707\n",
      "Epoch 12/300 - Train Loss: 0.0812, Val Loss: 0.0719\n",
      "Epoch 13/300 - Train Loss: 0.0817, Val Loss: 0.0741\n",
      "Epoch 14/300 - Train Loss: 0.0787, Val Loss: 0.0807\n",
      "Epoch 15/300 - Train Loss: 0.0791, Val Loss: 0.0741\n",
      "Epoch 16/300 - Train Loss: 0.0784, Val Loss: 0.0773\n",
      "Epoch 17/300 - Train Loss: 0.0762, Val Loss: 0.0720\n",
      "Epoch 18/300 - Train Loss: 0.0770, Val Loss: 0.0832\n",
      "Epoch 19/300 - Train Loss: 0.0748, Val Loss: 0.0676\n",
      "Epoch 20/300 - Train Loss: 0.0776, Val Loss: 0.0772\n",
      "Epoch 21/300 - Train Loss: 0.0755, Val Loss: 0.0676\n",
      "Epoch 22/300 - Train Loss: 0.0764, Val Loss: 0.0781\n",
      "Epoch 23/300 - Train Loss: 0.0765, Val Loss: 0.0772\n",
      "Epoch 24/300 - Train Loss: 0.0762, Val Loss: 0.0667\n",
      "Epoch 25/300 - Train Loss: 0.0777, Val Loss: 0.0767\n",
      "Epoch 26/300 - Train Loss: 0.0745, Val Loss: 0.0773\n",
      "Epoch 27/300 - Train Loss: 0.0740, Val Loss: 0.0676\n",
      "Epoch 28/300 - Train Loss: 0.0757, Val Loss: 0.0730\n",
      "Epoch 29/300 - Train Loss: 0.0706, Val Loss: 0.0674\n",
      "Epoch 30/300 - Train Loss: 0.0723, Val Loss: 0.0697\n",
      "Epoch 31/300 - Train Loss: 0.0746, Val Loss: 0.0698\n",
      "Epoch 32/300 - Train Loss: 0.0767, Val Loss: 0.0749\n",
      "Epoch 33/300 - Train Loss: 0.0718, Val Loss: 0.0792\n",
      "Epoch 34/300 - Train Loss: 0.0721, Val Loss: 0.0669\n",
      "Epoch 35/300 - Train Loss: 0.0707, Val Loss: 0.0703\n",
      "Epoch 36/300 - Train Loss: 0.0692, Val Loss: 0.0934\n",
      "Epoch 37/300 - Train Loss: 0.0726, Val Loss: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:39:18,580] Trial 429 finished with value: 0.9725824108483815 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1768672306403899, 'learning_rate': 0.0008003379896445558, 'batch_size': 32, 'weight_decay': 4.691697618307483e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300 - Train Loss: 0.0679, Val Loss: 0.0736\n",
      "Early stopping at epoch 38\n",
      "Macro F1 Score: 0.9726, Macro Precision: 0.9690, Macro Recall: 0.9764\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 431\n",
      "Training with F1=32, F2=16, D=2, dropout=0.1916254988136284, LR=0.0009108863067695202, BS=32, WD=3.064128012604271e-05\n",
      "Epoch 1/300 - Train Loss: 0.1494, Val Loss: 0.0949\n",
      "Epoch 2/300 - Train Loss: 0.0992, Val Loss: 0.0770\n",
      "Epoch 3/300 - Train Loss: 0.0928, Val Loss: 0.0747\n",
      "Epoch 4/300 - Train Loss: 0.0906, Val Loss: 0.0763\n",
      "Epoch 5/300 - Train Loss: 0.0893, Val Loss: 0.0786\n",
      "Epoch 6/300 - Train Loss: 0.0868, Val Loss: 0.0712\n",
      "Epoch 7/300 - Train Loss: 0.0834, Val Loss: 0.0724\n",
      "Epoch 8/300 - Train Loss: 0.0823, Val Loss: 0.0761\n",
      "Epoch 9/300 - Train Loss: 0.0828, Val Loss: 0.0803\n",
      "Epoch 10/300 - Train Loss: 0.0826, Val Loss: 0.0741\n",
      "Epoch 11/300 - Train Loss: 0.0788, Val Loss: 0.0865\n",
      "Epoch 12/300 - Train Loss: 0.0793, Val Loss: 0.0799\n",
      "Epoch 13/300 - Train Loss: 0.0778, Val Loss: 0.0727\n",
      "Epoch 14/300 - Train Loss: 0.0768, Val Loss: 0.0782\n",
      "Epoch 15/300 - Train Loss: 0.0756, Val Loss: 0.0718\n",
      "Epoch 16/300 - Train Loss: 0.0738, Val Loss: 0.0737\n",
      "Epoch 17/300 - Train Loss: 0.0710, Val Loss: 0.0751\n",
      "Epoch 18/300 - Train Loss: 0.0722, Val Loss: 0.0750\n",
      "Epoch 19/300 - Train Loss: 0.0753, Val Loss: 0.0736\n",
      "Epoch 20/300 - Train Loss: 0.0697, Val Loss: 0.0742\n",
      "Epoch 21/300 - Train Loss: 0.0713, Val Loss: 0.0817\n",
      "Epoch 22/300 - Train Loss: 0.0698, Val Loss: 0.0742\n",
      "Epoch 23/300 - Train Loss: 0.0684, Val Loss: 0.0773\n",
      "Epoch 24/300 - Train Loss: 0.0684, Val Loss: 0.0858\n",
      "Epoch 25/300 - Train Loss: 0.0697, Val Loss: 0.0796\n",
      "Epoch 26/300 - Train Loss: 0.0714, Val Loss: 0.0771\n",
      "Epoch 27/300 - Train Loss: 0.0642, Val Loss: 0.0745\n",
      "Epoch 28/300 - Train Loss: 0.0671, Val Loss: 0.0718\n",
      "Epoch 29/300 - Train Loss: 0.0667, Val Loss: 0.0734\n",
      "Epoch 30/300 - Train Loss: 0.0660, Val Loss: 0.0743\n",
      "Epoch 31/300 - Train Loss: 0.0646, Val Loss: 0.0733\n",
      "Epoch 32/300 - Train Loss: 0.0645, Val Loss: 0.0728\n",
      "Epoch 33/300 - Train Loss: 0.0626, Val Loss: 0.0749\n",
      "Epoch 34/300 - Train Loss: 0.0646, Val Loss: 0.0820\n",
      "Epoch 35/300 - Train Loss: 0.0617, Val Loss: 0.0716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:40:54,675] Trial 430 finished with value: 0.9638039293910877 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.1916254988136284, 'learning_rate': 0.0009108863067695202, 'batch_size': 32, 'weight_decay': 3.064128012604271e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300 - Train Loss: 0.0607, Val Loss: 0.0722\n",
      "Early stopping at epoch 36\n",
      "Macro F1 Score: 0.9638, Macro Precision: 0.9543, Macro Recall: 0.9744\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 432\n",
      "Training with F1=32, F2=16, D=2, dropout=0.17941800424871923, LR=0.0007107440681585216, BS=32, WD=5.7292940359700514e-05\n",
      "Epoch 1/300 - Train Loss: 0.1541, Val Loss: 0.1092\n",
      "Epoch 2/300 - Train Loss: 0.0997, Val Loss: 0.0747\n",
      "Epoch 3/300 - Train Loss: 0.0907, Val Loss: 0.0962\n",
      "Epoch 4/300 - Train Loss: 0.0879, Val Loss: 0.0814\n",
      "Epoch 5/300 - Train Loss: 0.0870, Val Loss: 0.0784\n",
      "Epoch 6/300 - Train Loss: 0.0861, Val Loss: 0.0685\n",
      "Epoch 7/300 - Train Loss: 0.0811, Val Loss: 0.0660\n",
      "Epoch 8/300 - Train Loss: 0.0830, Val Loss: 0.0753\n",
      "Epoch 9/300 - Train Loss: 0.0815, Val Loss: 0.0751\n",
      "Epoch 10/300 - Train Loss: 0.0782, Val Loss: 0.0738\n",
      "Epoch 11/300 - Train Loss: 0.0777, Val Loss: 0.0681\n",
      "Epoch 12/300 - Train Loss: 0.0763, Val Loss: 0.0668\n",
      "Epoch 13/300 - Train Loss: 0.0750, Val Loss: 0.0737\n",
      "Epoch 14/300 - Train Loss: 0.0731, Val Loss: 0.0793\n",
      "Epoch 15/300 - Train Loss: 0.0764, Val Loss: 0.0684\n",
      "Epoch 16/300 - Train Loss: 0.0751, Val Loss: 0.0743\n",
      "Epoch 17/300 - Train Loss: 0.0717, Val Loss: 0.0678\n",
      "Epoch 18/300 - Train Loss: 0.0737, Val Loss: 0.0720\n",
      "Epoch 19/300 - Train Loss: 0.0697, Val Loss: 0.0715\n",
      "Epoch 20/300 - Train Loss: 0.0691, Val Loss: 0.0686\n",
      "Epoch 21/300 - Train Loss: 0.0702, Val Loss: 0.0701\n",
      "Epoch 22/300 - Train Loss: 0.0667, Val Loss: 0.0747\n",
      "Epoch 23/300 - Train Loss: 0.0678, Val Loss: 0.0730\n",
      "Epoch 24/300 - Train Loss: 0.0653, Val Loss: 0.0729\n",
      "Epoch 25/300 - Train Loss: 0.0656, Val Loss: 0.0734\n",
      "Epoch 26/300 - Train Loss: 0.0677, Val Loss: 0.0753\n",
      "Epoch 27/300 - Train Loss: 0.0651, Val Loss: 0.0746\n",
      "Epoch 28/300 - Train Loss: 0.0666, Val Loss: 0.0747\n",
      "Epoch 29/300 - Train Loss: 0.0652, Val Loss: 0.0836\n",
      "Epoch 30/300 - Train Loss: 0.0651, Val Loss: 0.0749\n",
      "Epoch 31/300 - Train Loss: 0.0633, Val Loss: 0.0727\n",
      "Epoch 32/300 - Train Loss: 0.0651, Val Loss: 0.0761\n",
      "Epoch 33/300 - Train Loss: 0.0623, Val Loss: 0.0682\n",
      "Epoch 34/300 - Train Loss: 0.0635, Val Loss: 0.0762\n",
      "Epoch 35/300 - Train Loss: 0.0626, Val Loss: 0.0753\n",
      "Epoch 36/300 - Train Loss: 0.0639, Val Loss: 0.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:42:33,511] Trial 431 finished with value: 0.9720323096544233 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.17941800424871923, 'learning_rate': 0.0007107440681585216, 'batch_size': 32, 'weight_decay': 5.7292940359700514e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/300 - Train Loss: 0.0644, Val Loss: 0.0707\n",
      "Early stopping at epoch 37\n",
      "Macro F1 Score: 0.9720, Macro Precision: 0.9618, Macro Recall: 0.9833\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 433\n",
      "Training with F1=16, F2=16, D=2, dropout=0.16523097956831534, LR=0.000618927829411173, BS=256, WD=3.413174504302921e-05\n",
      "Epoch 1/300 - Train Loss: 0.3282, Val Loss: 0.1442\n",
      "Epoch 2/300 - Train Loss: 0.1256, Val Loss: 0.1010\n",
      "Epoch 3/300 - Train Loss: 0.1045, Val Loss: 0.0800\n",
      "Epoch 4/300 - Train Loss: 0.0925, Val Loss: 0.0770\n",
      "Epoch 5/300 - Train Loss: 0.0870, Val Loss: 0.0707\n",
      "Epoch 6/300 - Train Loss: 0.0832, Val Loss: 0.0756\n",
      "Epoch 7/300 - Train Loss: 0.0821, Val Loss: 0.0702\n",
      "Epoch 8/300 - Train Loss: 0.0798, Val Loss: 0.0718\n",
      "Epoch 9/300 - Train Loss: 0.0786, Val Loss: 0.0666\n",
      "Epoch 10/300 - Train Loss: 0.0760, Val Loss: 0.0725\n",
      "Epoch 11/300 - Train Loss: 0.0754, Val Loss: 0.0673\n",
      "Epoch 12/300 - Train Loss: 0.0741, Val Loss: 0.0768\n",
      "Epoch 13/300 - Train Loss: 0.0756, Val Loss: 0.0720\n",
      "Epoch 14/300 - Train Loss: 0.0747, Val Loss: 0.0684\n",
      "Epoch 15/300 - Train Loss: 0.0728, Val Loss: 0.0693\n",
      "Epoch 16/300 - Train Loss: 0.0717, Val Loss: 0.0659\n",
      "Epoch 17/300 - Train Loss: 0.0724, Val Loss: 0.0661\n",
      "Epoch 18/300 - Train Loss: 0.0707, Val Loss: 0.0678\n",
      "Epoch 19/300 - Train Loss: 0.0686, Val Loss: 0.0662\n",
      "Epoch 20/300 - Train Loss: 0.0691, Val Loss: 0.0685\n",
      "Epoch 21/300 - Train Loss: 0.0693, Val Loss: 0.0652\n",
      "Epoch 22/300 - Train Loss: 0.0689, Val Loss: 0.0692\n",
      "Epoch 23/300 - Train Loss: 0.0710, Val Loss: 0.0667\n",
      "Epoch 24/300 - Train Loss: 0.0670, Val Loss: 0.0695\n",
      "Epoch 25/300 - Train Loss: 0.0665, Val Loss: 0.0746\n",
      "Epoch 26/300 - Train Loss: 0.0685, Val Loss: 0.0779\n",
      "Epoch 27/300 - Train Loss: 0.0689, Val Loss: 0.0681\n",
      "Epoch 28/300 - Train Loss: 0.0680, Val Loss: 0.0700\n",
      "Epoch 29/300 - Train Loss: 0.0690, Val Loss: 0.0699\n",
      "Epoch 30/300 - Train Loss: 0.0662, Val Loss: 0.0710\n",
      "Epoch 31/300 - Train Loss: 0.0651, Val Loss: 0.0730\n",
      "Epoch 32/300 - Train Loss: 0.0624, Val Loss: 0.0712\n",
      "Epoch 33/300 - Train Loss: 0.0634, Val Loss: 0.0700\n",
      "Epoch 34/300 - Train Loss: 0.0641, Val Loss: 0.0678\n",
      "Epoch 35/300 - Train Loss: 0.0651, Val Loss: 0.0702\n",
      "Epoch 36/300 - Train Loss: 0.0654, Val Loss: 0.0674\n",
      "Epoch 37/300 - Train Loss: 0.0650, Val Loss: 0.0674\n",
      "Epoch 38/300 - Train Loss: 0.0635, Val Loss: 0.0747\n",
      "Epoch 39/300 - Train Loss: 0.0639, Val Loss: 0.0718\n",
      "Epoch 40/300 - Train Loss: 0.0630, Val Loss: 0.0718\n",
      "Epoch 41/300 - Train Loss: 0.0637, Val Loss: 0.0679\n",
      "Epoch 42/300 - Train Loss: 0.0631, Val Loss: 0.0680\n",
      "Epoch 43/300 - Train Loss: 0.0615, Val Loss: 0.0673\n",
      "Epoch 44/300 - Train Loss: 0.0592, Val Loss: 0.0740\n",
      "Epoch 45/300 - Train Loss: 0.0612, Val Loss: 0.0752\n",
      "Epoch 46/300 - Train Loss: 0.0626, Val Loss: 0.0726\n",
      "Epoch 47/300 - Train Loss: 0.0616, Val Loss: 0.0726\n",
      "Epoch 48/300 - Train Loss: 0.0614, Val Loss: 0.0727\n",
      "Epoch 49/300 - Train Loss: 0.0603, Val Loss: 0.0761\n",
      "Epoch 50/300 - Train Loss: 0.0596, Val Loss: 0.0761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:43:38,239] Trial 432 finished with value: 0.9720284714571865 and parameters: {'F1': 16, 'F2': 16, 'D': 2, 'dropout': 0.16523097956831534, 'learning_rate': 0.000618927829411173, 'batch_size': 256, 'weight_decay': 3.413174504302921e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/300 - Train Loss: 0.0621, Val Loss: 0.0722\n",
      "Early stopping at epoch 51\n",
      "Macro F1 Score: 0.9720, Macro Precision: 0.9620, Macro Recall: 0.9831\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 434\n",
      "Training with F1=32, F2=8, D=2, dropout=0.2031271556250016, LR=0.000809144559079944, BS=64, WD=7.317880771482929e-05\n",
      "Epoch 1/300 - Train Loss: 0.1879, Val Loss: 0.0845\n",
      "Epoch 2/300 - Train Loss: 0.0967, Val Loss: 0.0772\n",
      "Epoch 3/300 - Train Loss: 0.0918, Val Loss: 0.0895\n",
      "Epoch 4/300 - Train Loss: 0.0879, Val Loss: 0.0723\n",
      "Epoch 5/300 - Train Loss: 0.0851, Val Loss: 0.1020\n",
      "Epoch 6/300 - Train Loss: 0.0839, Val Loss: 0.0763\n",
      "Epoch 7/300 - Train Loss: 0.0797, Val Loss: 0.0766\n",
      "Epoch 8/300 - Train Loss: 0.0795, Val Loss: 0.0719\n",
      "Epoch 9/300 - Train Loss: 0.0796, Val Loss: 0.0717\n",
      "Epoch 10/300 - Train Loss: 0.0818, Val Loss: 0.0749\n",
      "Epoch 11/300 - Train Loss: 0.0797, Val Loss: 0.0739\n",
      "Epoch 12/300 - Train Loss: 0.0782, Val Loss: 0.0746\n",
      "Epoch 13/300 - Train Loss: 0.0767, Val Loss: 0.0765\n",
      "Epoch 14/300 - Train Loss: 0.0760, Val Loss: 0.0681\n",
      "Epoch 15/300 - Train Loss: 0.0747, Val Loss: 0.0696\n",
      "Epoch 16/300 - Train Loss: 0.0753, Val Loss: 0.0804\n",
      "Epoch 17/300 - Train Loss: 0.0740, Val Loss: 0.0739\n",
      "Epoch 18/300 - Train Loss: 0.0740, Val Loss: 0.0691\n",
      "Epoch 19/300 - Train Loss: 0.0720, Val Loss: 0.0771\n",
      "Epoch 20/300 - Train Loss: 0.0749, Val Loss: 0.0768\n",
      "Epoch 21/300 - Train Loss: 0.0731, Val Loss: 0.0721\n",
      "Epoch 22/300 - Train Loss: 0.0702, Val Loss: 0.0717\n",
      "Epoch 23/300 - Train Loss: 0.0708, Val Loss: 0.0734\n",
      "Epoch 24/300 - Train Loss: 0.0715, Val Loss: 0.0726\n",
      "Epoch 25/300 - Train Loss: 0.0698, Val Loss: 0.0688\n",
      "Epoch 26/300 - Train Loss: 0.0707, Val Loss: 0.0684\n",
      "Epoch 27/300 - Train Loss: 0.0708, Val Loss: 0.0758\n",
      "Epoch 28/300 - Train Loss: 0.0698, Val Loss: 0.0705\n",
      "Epoch 29/300 - Train Loss: 0.0705, Val Loss: 0.0723\n",
      "Epoch 30/300 - Train Loss: 0.0686, Val Loss: 0.0726\n",
      "Epoch 31/300 - Train Loss: 0.0705, Val Loss: 0.0713\n",
      "Epoch 32/300 - Train Loss: 0.0689, Val Loss: 0.0689\n",
      "Epoch 33/300 - Train Loss: 0.0695, Val Loss: 0.0721\n",
      "Epoch 34/300 - Train Loss: 0.0694, Val Loss: 0.0721\n",
      "Epoch 35/300 - Train Loss: 0.0675, Val Loss: 0.0742\n",
      "Epoch 36/300 - Train Loss: 0.0694, Val Loss: 0.0705\n",
      "Epoch 37/300 - Train Loss: 0.0691, Val Loss: 0.0706\n",
      "Epoch 38/300 - Train Loss: 0.0672, Val Loss: 0.0696\n",
      "Epoch 39/300 - Train Loss: 0.0670, Val Loss: 0.0698\n",
      "Epoch 40/300 - Train Loss: 0.0670, Val Loss: 0.0718\n",
      "Epoch 41/300 - Train Loss: 0.0647, Val Loss: 0.0693\n",
      "Epoch 42/300 - Train Loss: 0.0653, Val Loss: 0.0747\n",
      "Epoch 43/300 - Train Loss: 0.0654, Val Loss: 0.0745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:45:15,452] Trial 433 finished with value: 0.9735820285480727 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.2031271556250016, 'learning_rate': 0.000809144559079944, 'batch_size': 64, 'weight_decay': 7.317880771482929e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/300 - Train Loss: 0.0665, Val Loss: 0.0725\n",
      "Early stopping at epoch 44\n",
      "Macro F1 Score: 0.9736, Macro Precision: 0.9695, Macro Recall: 0.9778\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 435\n",
      "Training with F1=32, F2=8, D=2, dropout=0.10812195188681314, LR=0.0008611566905680785, BS=32, WD=9.844975589961197e-05\n",
      "Epoch 1/300 - Train Loss: 0.1483, Val Loss: 0.0763\n",
      "Epoch 2/300 - Train Loss: 0.1001, Val Loss: 0.0831\n",
      "Epoch 3/300 - Train Loss: 0.0934, Val Loss: 0.1250\n",
      "Epoch 4/300 - Train Loss: 0.0908, Val Loss: 0.0755\n",
      "Epoch 5/300 - Train Loss: 0.0874, Val Loss: 0.0785\n",
      "Epoch 6/300 - Train Loss: 0.0854, Val Loss: 0.0705\n",
      "Epoch 7/300 - Train Loss: 0.0857, Val Loss: 0.0722\n",
      "Epoch 8/300 - Train Loss: 0.0841, Val Loss: 0.0806\n",
      "Epoch 9/300 - Train Loss: 0.0827, Val Loss: 0.0748\n",
      "Epoch 10/300 - Train Loss: 0.0854, Val Loss: 0.0818\n",
      "Epoch 11/300 - Train Loss: 0.0809, Val Loss: 0.0849\n",
      "Epoch 12/300 - Train Loss: 0.0811, Val Loss: 0.0677\n",
      "Epoch 13/300 - Train Loss: 0.0770, Val Loss: 0.0718\n",
      "Epoch 14/300 - Train Loss: 0.0762, Val Loss: 0.0751\n",
      "Epoch 15/300 - Train Loss: 0.0753, Val Loss: 0.0674\n",
      "Epoch 16/300 - Train Loss: 0.0759, Val Loss: 0.0735\n",
      "Epoch 17/300 - Train Loss: 0.0764, Val Loss: 0.0786\n",
      "Epoch 18/300 - Train Loss: 0.0752, Val Loss: 0.0717\n",
      "Epoch 19/300 - Train Loss: 0.0791, Val Loss: 0.0704\n",
      "Epoch 20/300 - Train Loss: 0.0752, Val Loss: 0.0689\n",
      "Epoch 21/300 - Train Loss: 0.0749, Val Loss: 0.0688\n",
      "Epoch 22/300 - Train Loss: 0.0755, Val Loss: 0.0756\n",
      "Epoch 23/300 - Train Loss: 0.0738, Val Loss: 0.0726\n",
      "Epoch 24/300 - Train Loss: 0.0771, Val Loss: 0.0704\n",
      "Epoch 25/300 - Train Loss: 0.0730, Val Loss: 0.0765\n",
      "Epoch 26/300 - Train Loss: 0.0720, Val Loss: 0.0803\n",
      "Epoch 27/300 - Train Loss: 0.0735, Val Loss: 0.0698\n",
      "Epoch 28/300 - Train Loss: 0.0727, Val Loss: 0.0721\n",
      "Epoch 29/300 - Train Loss: 0.0714, Val Loss: 0.0714\n",
      "Epoch 30/300 - Train Loss: 0.0738, Val Loss: 0.0837\n",
      "Epoch 31/300 - Train Loss: 0.0717, Val Loss: 0.0780\n",
      "Epoch 32/300 - Train Loss: 0.0716, Val Loss: 0.0714\n",
      "Epoch 33/300 - Train Loss: 0.0705, Val Loss: 0.0761\n",
      "Epoch 34/300 - Train Loss: 0.0713, Val Loss: 0.0742\n",
      "Epoch 35/300 - Train Loss: 0.0693, Val Loss: 0.0732\n",
      "Epoch 36/300 - Train Loss: 0.0695, Val Loss: 0.0705\n",
      "Epoch 37/300 - Train Loss: 0.0692, Val Loss: 0.0703\n",
      "Epoch 38/300 - Train Loss: 0.0704, Val Loss: 0.0744\n",
      "Epoch 39/300 - Train Loss: 0.0678, Val Loss: 0.0776\n",
      "Epoch 40/300 - Train Loss: 0.0695, Val Loss: 0.0691\n",
      "Epoch 41/300 - Train Loss: 0.0698, Val Loss: 0.0719\n",
      "Epoch 42/300 - Train Loss: 0.0676, Val Loss: 0.0720\n",
      "Epoch 43/300 - Train Loss: 0.0678, Val Loss: 0.0694\n",
      "Epoch 44/300 - Train Loss: 0.0683, Val Loss: 0.0755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:47:09,771] Trial 434 finished with value: 0.9692778956949564 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.10812195188681314, 'learning_rate': 0.0008611566905680785, 'batch_size': 32, 'weight_decay': 9.844975589961197e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/300 - Train Loss: 0.0666, Val Loss: 0.0770\n",
      "Early stopping at epoch 45\n",
      "Macro F1 Score: 0.9693, Macro Precision: 0.9631, Macro Recall: 0.9759\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 436\n",
      "Training with F1=16, F2=16, D=2, dropout=0.19007133151484365, LR=1.934393937090599e-05, BS=32, WD=3.8578000984132584e-05\n",
      "Epoch 1/300 - Train Loss: 0.7104, Val Loss: 0.4090\n",
      "Epoch 2/300 - Train Loss: 0.3204, Val Loss: 0.2324\n",
      "Epoch 3/300 - Train Loss: 0.2301, Val Loss: 0.1827\n",
      "Epoch 4/300 - Train Loss: 0.1856, Val Loss: 0.1430\n",
      "Epoch 5/300 - Train Loss: 0.1583, Val Loss: 0.1320\n",
      "Epoch 6/300 - Train Loss: 0.1431, Val Loss: 0.1128\n",
      "Epoch 7/300 - Train Loss: 0.1338, Val Loss: 0.1066\n",
      "Epoch 8/300 - Train Loss: 0.1250, Val Loss: 0.0974\n",
      "Epoch 9/300 - Train Loss: 0.1190, Val Loss: 0.0978\n",
      "Epoch 10/300 - Train Loss: 0.1178, Val Loss: 0.0861\n",
      "Epoch 11/300 - Train Loss: 0.1125, Val Loss: 0.0921\n",
      "Epoch 12/300 - Train Loss: 0.1112, Val Loss: 0.0885\n",
      "Epoch 13/300 - Train Loss: 0.1089, Val Loss: 0.0921\n",
      "Epoch 14/300 - Train Loss: 0.1072, Val Loss: 0.0843\n",
      "Epoch 15/300 - Train Loss: 0.1058, Val Loss: 0.0852\n",
      "Epoch 16/300 - Train Loss: 0.1045, Val Loss: 0.0828\n",
      "Epoch 17/300 - Train Loss: 0.1013, Val Loss: 0.0856\n",
      "Epoch 18/300 - Train Loss: 0.1015, Val Loss: 0.0789\n",
      "Epoch 19/300 - Train Loss: 0.1036, Val Loss: 0.0833\n",
      "Epoch 20/300 - Train Loss: 0.1021, Val Loss: 0.0855\n",
      "Epoch 21/300 - Train Loss: 0.0992, Val Loss: 0.0755\n",
      "Epoch 22/300 - Train Loss: 0.0997, Val Loss: 0.0801\n",
      "Epoch 23/300 - Train Loss: 0.0976, Val Loss: 0.0841\n",
      "Epoch 24/300 - Train Loss: 0.0981, Val Loss: 0.0784\n",
      "Epoch 25/300 - Train Loss: 0.0978, Val Loss: 0.0787\n",
      "Epoch 26/300 - Train Loss: 0.0975, Val Loss: 0.0763\n",
      "Epoch 27/300 - Train Loss: 0.0955, Val Loss: 0.0791\n",
      "Epoch 28/300 - Train Loss: 0.0961, Val Loss: 0.0751\n",
      "Epoch 29/300 - Train Loss: 0.0968, Val Loss: 0.0749\n",
      "Epoch 30/300 - Train Loss: 0.0936, Val Loss: 0.0775\n",
      "Epoch 31/300 - Train Loss: 0.0922, Val Loss: 0.0744\n",
      "Epoch 32/300 - Train Loss: 0.0928, Val Loss: 0.0793\n",
      "Epoch 33/300 - Train Loss: 0.0927, Val Loss: 0.0776\n",
      "Epoch 34/300 - Train Loss: 0.0934, Val Loss: 0.0750\n",
      "Epoch 35/300 - Train Loss: 0.0923, Val Loss: 0.0755\n",
      "Epoch 36/300 - Train Loss: 0.0918, Val Loss: 0.0763\n",
      "Epoch 37/300 - Train Loss: 0.0943, Val Loss: 0.0775\n",
      "Epoch 38/300 - Train Loss: 0.0878, Val Loss: 0.0730\n",
      "Epoch 39/300 - Train Loss: 0.0896, Val Loss: 0.0773\n",
      "Epoch 40/300 - Train Loss: 0.0892, Val Loss: 0.0797\n",
      "Epoch 41/300 - Train Loss: 0.0900, Val Loss: 0.0744\n",
      "Epoch 42/300 - Train Loss: 0.0897, Val Loss: 0.0757\n",
      "Epoch 43/300 - Train Loss: 0.0901, Val Loss: 0.0715\n",
      "Epoch 44/300 - Train Loss: 0.0899, Val Loss: 0.0716\n",
      "Epoch 45/300 - Train Loss: 0.0911, Val Loss: 0.0708\n",
      "Epoch 46/300 - Train Loss: 0.0879, Val Loss: 0.0764\n",
      "Epoch 47/300 - Train Loss: 0.0872, Val Loss: 0.0737\n",
      "Epoch 48/300 - Train Loss: 0.0890, Val Loss: 0.0716\n",
      "Epoch 49/300 - Train Loss: 0.0893, Val Loss: 0.0742\n",
      "Epoch 50/300 - Train Loss: 0.0855, Val Loss: 0.0748\n",
      "Epoch 51/300 - Train Loss: 0.0860, Val Loss: 0.0751\n",
      "Epoch 52/300 - Train Loss: 0.0875, Val Loss: 0.0750\n",
      "Epoch 53/300 - Train Loss: 0.0862, Val Loss: 0.0717\n",
      "Epoch 54/300 - Train Loss: 0.0869, Val Loss: 0.0747\n",
      "Epoch 55/300 - Train Loss: 0.0900, Val Loss: 0.0712\n",
      "Epoch 56/300 - Train Loss: 0.0869, Val Loss: 0.0757\n",
      "Epoch 57/300 - Train Loss: 0.0865, Val Loss: 0.0701\n",
      "Epoch 58/300 - Train Loss: 0.0862, Val Loss: 0.0714\n",
      "Epoch 59/300 - Train Loss: 0.0867, Val Loss: 0.0736\n",
      "Epoch 60/300 - Train Loss: 0.0856, Val Loss: 0.0718\n",
      "Epoch 61/300 - Train Loss: 0.0831, Val Loss: 0.0716\n",
      "Epoch 62/300 - Train Loss: 0.0849, Val Loss: 0.0739\n",
      "Epoch 63/300 - Train Loss: 0.0848, Val Loss: 0.0747\n",
      "Epoch 64/300 - Train Loss: 0.0839, Val Loss: 0.0740\n",
      "Epoch 65/300 - Train Loss: 0.0836, Val Loss: 0.0732\n",
      "Epoch 66/300 - Train Loss: 0.0834, Val Loss: 0.0775\n",
      "Epoch 67/300 - Train Loss: 0.0844, Val Loss: 0.0718\n",
      "Epoch 68/300 - Train Loss: 0.0863, Val Loss: 0.0772\n",
      "Epoch 69/300 - Train Loss: 0.0837, Val Loss: 0.0740\n",
      "Epoch 70/300 - Train Loss: 0.0807, Val Loss: 0.0731\n",
      "Epoch 71/300 - Train Loss: 0.0806, Val Loss: 0.0749\n",
      "Epoch 72/300 - Train Loss: 0.0823, Val Loss: 0.0753\n",
      "Epoch 73/300 - Train Loss: 0.0835, Val Loss: 0.0734\n",
      "Epoch 74/300 - Train Loss: 0.0830, Val Loss: 0.0752\n",
      "Epoch 75/300 - Train Loss: 0.0823, Val Loss: 0.0757\n",
      "Epoch 76/300 - Train Loss: 0.0832, Val Loss: 0.0712\n",
      "Epoch 77/300 - Train Loss: 0.0844, Val Loss: 0.0764\n",
      "Epoch 78/300 - Train Loss: 0.0818, Val Loss: 0.0744\n",
      "Epoch 79/300 - Train Loss: 0.0814, Val Loss: 0.0742\n",
      "Epoch 80/300 - Train Loss: 0.0828, Val Loss: 0.0733\n",
      "Epoch 81/300 - Train Loss: 0.0821, Val Loss: 0.0703\n",
      "Epoch 82/300 - Train Loss: 0.0819, Val Loss: 0.0747\n",
      "Epoch 83/300 - Train Loss: 0.0828, Val Loss: 0.0736\n",
      "Epoch 84/300 - Train Loss: 0.0846, Val Loss: 0.0782\n",
      "Epoch 85/300 - Train Loss: 0.0808, Val Loss: 0.0715\n",
      "Epoch 86/300 - Train Loss: 0.0797, Val Loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:50:13,738] Trial 435 finished with value: 0.9660454444124 and parameters: {'F1': 16, 'F2': 16, 'D': 2, 'dropout': 0.19007133151484365, 'learning_rate': 1.934393937090599e-05, 'batch_size': 32, 'weight_decay': 3.8578000984132584e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300 - Train Loss: 0.0800, Val Loss: 0.0756\n",
      "Early stopping at epoch 87\n",
      "Macro F1 Score: 0.9660, Macro Precision: 0.9588, Macro Recall: 0.9742\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 437\n",
      "Training with F1=32, F2=8, D=2, dropout=0.16289149639984835, LR=0.0009989928036935383, BS=32, WD=5.1962257359194314e-05\n",
      "Epoch 1/300 - Train Loss: 0.1468, Val Loss: 0.0976\n",
      "Epoch 2/300 - Train Loss: 0.0966, Val Loss: 0.0721\n",
      "Epoch 3/300 - Train Loss: 0.0923, Val Loss: 0.1035\n",
      "Epoch 4/300 - Train Loss: 0.0933, Val Loss: 0.0775\n",
      "Epoch 5/300 - Train Loss: 0.0868, Val Loss: 0.0664\n",
      "Epoch 6/300 - Train Loss: 0.0870, Val Loss: 0.0748\n",
      "Epoch 7/300 - Train Loss: 0.0889, Val Loss: 0.0848\n",
      "Epoch 8/300 - Train Loss: 0.0895, Val Loss: 0.0716\n",
      "Epoch 9/300 - Train Loss: 0.0836, Val Loss: 0.0731\n",
      "Epoch 10/300 - Train Loss: 0.0831, Val Loss: 0.0730\n",
      "Epoch 11/300 - Train Loss: 0.0830, Val Loss: 0.0683\n",
      "Epoch 12/300 - Train Loss: 0.0833, Val Loss: 0.0738\n",
      "Epoch 13/300 - Train Loss: 0.0803, Val Loss: 0.0662\n",
      "Epoch 14/300 - Train Loss: 0.0818, Val Loss: 0.0777\n",
      "Epoch 15/300 - Train Loss: 0.0780, Val Loss: 0.0762\n",
      "Epoch 16/300 - Train Loss: 0.0772, Val Loss: 0.0738\n",
      "Epoch 17/300 - Train Loss: 0.0796, Val Loss: 0.0722\n",
      "Epoch 18/300 - Train Loss: 0.0783, Val Loss: 0.0832\n",
      "Epoch 19/300 - Train Loss: 0.0789, Val Loss: 0.0699\n",
      "Epoch 20/300 - Train Loss: 0.0808, Val Loss: 0.0834\n",
      "Epoch 21/300 - Train Loss: 0.0767, Val Loss: 0.0712\n",
      "Epoch 22/300 - Train Loss: 0.0769, Val Loss: 0.0711\n",
      "Epoch 23/300 - Train Loss: 0.0780, Val Loss: 0.0679\n",
      "Epoch 24/300 - Train Loss: 0.0774, Val Loss: 0.0727\n",
      "Epoch 25/300 - Train Loss: 0.0763, Val Loss: 0.0656\n",
      "Epoch 26/300 - Train Loss: 0.0737, Val Loss: 0.0713\n",
      "Epoch 27/300 - Train Loss: 0.0755, Val Loss: 0.0711\n",
      "Epoch 28/300 - Train Loss: 0.0748, Val Loss: 0.0753\n",
      "Epoch 29/300 - Train Loss: 0.0738, Val Loss: 0.0746\n",
      "Epoch 30/300 - Train Loss: 0.0733, Val Loss: 0.0733\n",
      "Epoch 31/300 - Train Loss: 0.0762, Val Loss: 0.0717\n",
      "Epoch 32/300 - Train Loss: 0.0749, Val Loss: 0.0702\n",
      "Epoch 33/300 - Train Loss: 0.0732, Val Loss: 0.0766\n",
      "Epoch 34/300 - Train Loss: 0.0741, Val Loss: 0.0729\n",
      "Epoch 35/300 - Train Loss: 0.0736, Val Loss: 0.0719\n",
      "Epoch 36/300 - Train Loss: 0.0747, Val Loss: 0.0799\n",
      "Epoch 37/300 - Train Loss: 0.0728, Val Loss: 0.0766\n",
      "Epoch 38/300 - Train Loss: 0.0720, Val Loss: 0.0769\n",
      "Epoch 39/300 - Train Loss: 0.0746, Val Loss: 0.0773\n",
      "Epoch 40/300 - Train Loss: 0.0712, Val Loss: 0.0709\n",
      "Epoch 41/300 - Train Loss: 0.0744, Val Loss: 0.0709\n",
      "Epoch 42/300 - Train Loss: 0.0729, Val Loss: 0.0731\n",
      "Epoch 43/300 - Train Loss: 0.0716, Val Loss: 0.0764\n",
      "Epoch 44/300 - Train Loss: 0.0715, Val Loss: 0.0702\n",
      "Epoch 45/300 - Train Loss: 0.0719, Val Loss: 0.0692\n",
      "Epoch 46/300 - Train Loss: 0.0701, Val Loss: 0.0772\n",
      "Epoch 47/300 - Train Loss: 0.0703, Val Loss: 0.0692\n",
      "Epoch 48/300 - Train Loss: 0.0708, Val Loss: 0.0682\n",
      "Epoch 49/300 - Train Loss: 0.0683, Val Loss: 0.0742\n",
      "Epoch 50/300 - Train Loss: 0.0702, Val Loss: 0.0724\n",
      "Epoch 51/300 - Train Loss: 0.0704, Val Loss: 0.0791\n",
      "Epoch 52/300 - Train Loss: 0.0696, Val Loss: 0.0731\n",
      "Epoch 53/300 - Train Loss: 0.0703, Val Loss: 0.0697\n",
      "Epoch 54/300 - Train Loss: 0.0709, Val Loss: 0.0722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:52:32,637] Trial 436 finished with value: 0.9742417289599746 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.16289149639984835, 'learning_rate': 0.0009989928036935383, 'batch_size': 32, 'weight_decay': 5.1962257359194314e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300 - Train Loss: 0.0679, Val Loss: 0.0749\n",
      "Early stopping at epoch 55\n",
      "Macro F1 Score: 0.9742, Macro Precision: 0.9779, Macro Recall: 0.9708\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.95      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 438\n",
      "Training with F1=16, F2=8, D=2, dropout=0.12534626765485526, LR=5.1410317484802964e-05, BS=32, WD=4.2345600193690865e-05\n",
      "Epoch 1/300 - Train Loss: 0.5538, Val Loss: 0.2537\n",
      "Epoch 2/300 - Train Loss: 0.2229, Val Loss: 0.1499\n",
      "Epoch 3/300 - Train Loss: 0.1588, Val Loss: 0.1208\n",
      "Epoch 4/300 - Train Loss: 0.1329, Val Loss: 0.1035\n",
      "Epoch 5/300 - Train Loss: 0.1231, Val Loss: 0.0889\n",
      "Epoch 6/300 - Train Loss: 0.1096, Val Loss: 0.0848\n",
      "Epoch 7/300 - Train Loss: 0.1089, Val Loss: 0.0817\n",
      "Epoch 8/300 - Train Loss: 0.1054, Val Loss: 0.0863\n",
      "Epoch 9/300 - Train Loss: 0.1045, Val Loss: 0.0835\n",
      "Epoch 10/300 - Train Loss: 0.1008, Val Loss: 0.0793\n",
      "Epoch 11/300 - Train Loss: 0.0988, Val Loss: 0.0754\n",
      "Epoch 12/300 - Train Loss: 0.0946, Val Loss: 0.0774\n",
      "Epoch 13/300 - Train Loss: 0.0956, Val Loss: 0.0799\n",
      "Epoch 14/300 - Train Loss: 0.0934, Val Loss: 0.0716\n",
      "Epoch 15/300 - Train Loss: 0.0921, Val Loss: 0.0730\n",
      "Epoch 16/300 - Train Loss: 0.0905, Val Loss: 0.0788\n",
      "Epoch 17/300 - Train Loss: 0.0889, Val Loss: 0.0766\n",
      "Epoch 18/300 - Train Loss: 0.0928, Val Loss: 0.0775\n",
      "Epoch 19/300 - Train Loss: 0.0878, Val Loss: 0.0704\n",
      "Epoch 20/300 - Train Loss: 0.0869, Val Loss: 0.0788\n",
      "Epoch 21/300 - Train Loss: 0.0858, Val Loss: 0.0715\n",
      "Epoch 22/300 - Train Loss: 0.0874, Val Loss: 0.0748\n",
      "Epoch 23/300 - Train Loss: 0.0859, Val Loss: 0.0775\n",
      "Epoch 24/300 - Train Loss: 0.0850, Val Loss: 0.0709\n",
      "Epoch 25/300 - Train Loss: 0.0840, Val Loss: 0.0721\n",
      "Epoch 26/300 - Train Loss: 0.0873, Val Loss: 0.0697\n",
      "Epoch 27/300 - Train Loss: 0.0845, Val Loss: 0.0714\n",
      "Epoch 28/300 - Train Loss: 0.0825, Val Loss: 0.0796\n",
      "Epoch 29/300 - Train Loss: 0.0839, Val Loss: 0.0704\n",
      "Epoch 30/300 - Train Loss: 0.0819, Val Loss: 0.0710\n",
      "Epoch 31/300 - Train Loss: 0.0843, Val Loss: 0.0769\n",
      "Epoch 32/300 - Train Loss: 0.0819, Val Loss: 0.0695\n",
      "Epoch 33/300 - Train Loss: 0.0818, Val Loss: 0.0729\n",
      "Epoch 34/300 - Train Loss: 0.0828, Val Loss: 0.0695\n",
      "Epoch 35/300 - Train Loss: 0.0804, Val Loss: 0.0735\n",
      "Epoch 36/300 - Train Loss: 0.0816, Val Loss: 0.0705\n",
      "Epoch 37/300 - Train Loss: 0.0813, Val Loss: 0.0737\n",
      "Epoch 38/300 - Train Loss: 0.0777, Val Loss: 0.0729\n",
      "Epoch 39/300 - Train Loss: 0.0784, Val Loss: 0.0697\n",
      "Epoch 40/300 - Train Loss: 0.0802, Val Loss: 0.0692\n",
      "Epoch 41/300 - Train Loss: 0.0806, Val Loss: 0.0690\n",
      "Epoch 42/300 - Train Loss: 0.0777, Val Loss: 0.0669\n",
      "Epoch 43/300 - Train Loss: 0.0807, Val Loss: 0.0706\n",
      "Epoch 44/300 - Train Loss: 0.0802, Val Loss: 0.0728\n",
      "Epoch 45/300 - Train Loss: 0.0791, Val Loss: 0.0692\n",
      "Epoch 46/300 - Train Loss: 0.0794, Val Loss: 0.0700\n",
      "Epoch 47/300 - Train Loss: 0.0811, Val Loss: 0.0699\n",
      "Epoch 48/300 - Train Loss: 0.0792, Val Loss: 0.0666\n",
      "Epoch 49/300 - Train Loss: 0.0786, Val Loss: 0.0713\n",
      "Epoch 50/300 - Train Loss: 0.0772, Val Loss: 0.0708\n",
      "Epoch 51/300 - Train Loss: 0.0792, Val Loss: 0.0738\n",
      "Epoch 52/300 - Train Loss: 0.0780, Val Loss: 0.0675\n",
      "Epoch 53/300 - Train Loss: 0.0788, Val Loss: 0.0710\n",
      "Epoch 54/300 - Train Loss: 0.0797, Val Loss: 0.0675\n",
      "Epoch 55/300 - Train Loss: 0.0755, Val Loss: 0.0676\n",
      "Epoch 56/300 - Train Loss: 0.0779, Val Loss: 0.0705\n",
      "Epoch 57/300 - Train Loss: 0.0772, Val Loss: 0.0767\n",
      "Epoch 58/300 - Train Loss: 0.0769, Val Loss: 0.0700\n",
      "Epoch 59/300 - Train Loss: 0.0766, Val Loss: 0.0654\n",
      "Epoch 60/300 - Train Loss: 0.0762, Val Loss: 0.0681\n",
      "Epoch 61/300 - Train Loss: 0.0782, Val Loss: 0.0669\n",
      "Epoch 62/300 - Train Loss: 0.0790, Val Loss: 0.0665\n",
      "Epoch 63/300 - Train Loss: 0.0773, Val Loss: 0.0712\n",
      "Epoch 64/300 - Train Loss: 0.0748, Val Loss: 0.0681\n",
      "Epoch 65/300 - Train Loss: 0.0756, Val Loss: 0.0692\n",
      "Epoch 66/300 - Train Loss: 0.0748, Val Loss: 0.0685\n",
      "Epoch 67/300 - Train Loss: 0.0745, Val Loss: 0.0678\n",
      "Epoch 68/300 - Train Loss: 0.0779, Val Loss: 0.0666\n",
      "Epoch 69/300 - Train Loss: 0.0754, Val Loss: 0.0667\n",
      "Epoch 70/300 - Train Loss: 0.0745, Val Loss: 0.0690\n",
      "Epoch 71/300 - Train Loss: 0.0768, Val Loss: 0.0714\n",
      "Epoch 72/300 - Train Loss: 0.0751, Val Loss: 0.0678\n",
      "Epoch 73/300 - Train Loss: 0.0757, Val Loss: 0.0688\n",
      "Epoch 74/300 - Train Loss: 0.0732, Val Loss: 0.0676\n",
      "Epoch 75/300 - Train Loss: 0.0745, Val Loss: 0.0681\n",
      "Epoch 76/300 - Train Loss: 0.0744, Val Loss: 0.0697\n",
      "Epoch 77/300 - Train Loss: 0.0741, Val Loss: 0.0681\n",
      "Epoch 78/300 - Train Loss: 0.0756, Val Loss: 0.0703\n",
      "Epoch 79/300 - Train Loss: 0.0745, Val Loss: 0.0677\n",
      "Epoch 80/300 - Train Loss: 0.0738, Val Loss: 0.0687\n",
      "Epoch 81/300 - Train Loss: 0.0733, Val Loss: 0.0716\n",
      "Epoch 82/300 - Train Loss: 0.0733, Val Loss: 0.0676\n",
      "Epoch 83/300 - Train Loss: 0.0735, Val Loss: 0.0692\n",
      "Epoch 84/300 - Train Loss: 0.0715, Val Loss: 0.0675\n",
      "Epoch 85/300 - Train Loss: 0.0717, Val Loss: 0.0707\n",
      "Epoch 86/300 - Train Loss: 0.0743, Val Loss: 0.0682\n",
      "Epoch 87/300 - Train Loss: 0.0737, Val Loss: 0.0722\n",
      "Epoch 88/300 - Train Loss: 0.0703, Val Loss: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:55:40,793] Trial 437 finished with value: 0.9675496461210747 and parameters: {'F1': 16, 'F2': 8, 'D': 2, 'dropout': 0.12534626765485526, 'learning_rate': 5.1410317484802964e-05, 'batch_size': 32, 'weight_decay': 4.2345600193690865e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/300 - Train Loss: 0.0726, Val Loss: 0.0692\n",
      "Early stopping at epoch 89\n",
      "Macro F1 Score: 0.9675, Macro Precision: 0.9595, Macro Recall: 0.9763\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 439\n",
      "Training with F1=32, F2=8, D=2, dropout=0.17549758156752587, LR=0.0007372774866463662, BS=128, WD=2.6332944973881037e-05\n",
      "Epoch 1/300 - Train Loss: 0.2425, Val Loss: 0.1106\n",
      "Epoch 2/300 - Train Loss: 0.0996, Val Loss: 0.1078\n",
      "Epoch 3/300 - Train Loss: 0.0904, Val Loss: 0.0810\n",
      "Epoch 4/300 - Train Loss: 0.0846, Val Loss: 0.0972\n",
      "Epoch 5/300 - Train Loss: 0.0815, Val Loss: 0.0756\n",
      "Epoch 6/300 - Train Loss: 0.0793, Val Loss: 0.0840\n",
      "Epoch 7/300 - Train Loss: 0.0781, Val Loss: 0.0843\n",
      "Epoch 8/300 - Train Loss: 0.0778, Val Loss: 0.0740\n",
      "Epoch 9/300 - Train Loss: 0.0749, Val Loss: 0.0759\n",
      "Epoch 10/300 - Train Loss: 0.0744, Val Loss: 0.0881\n",
      "Epoch 11/300 - Train Loss: 0.0732, Val Loss: 0.0770\n",
      "Epoch 12/300 - Train Loss: 0.0745, Val Loss: 0.0743\n",
      "Epoch 13/300 - Train Loss: 0.0725, Val Loss: 0.0725\n",
      "Epoch 14/300 - Train Loss: 0.0711, Val Loss: 0.0780\n",
      "Epoch 15/300 - Train Loss: 0.0728, Val Loss: 0.0709\n",
      "Epoch 16/300 - Train Loss: 0.0730, Val Loss: 0.0722\n",
      "Epoch 17/300 - Train Loss: 0.0703, Val Loss: 0.0710\n",
      "Epoch 18/300 - Train Loss: 0.0695, Val Loss: 0.0734\n",
      "Epoch 19/300 - Train Loss: 0.0708, Val Loss: 0.0768\n",
      "Epoch 20/300 - Train Loss: 0.0699, Val Loss: 0.0784\n",
      "Epoch 21/300 - Train Loss: 0.0703, Val Loss: 0.0712\n",
      "Epoch 22/300 - Train Loss: 0.0685, Val Loss: 0.0704\n",
      "Epoch 23/300 - Train Loss: 0.0689, Val Loss: 0.0678\n",
      "Epoch 24/300 - Train Loss: 0.0683, Val Loss: 0.0710\n",
      "Epoch 25/300 - Train Loss: 0.0680, Val Loss: 0.0779\n",
      "Epoch 26/300 - Train Loss: 0.0668, Val Loss: 0.0753\n",
      "Epoch 27/300 - Train Loss: 0.0672, Val Loss: 0.0722\n",
      "Epoch 28/300 - Train Loss: 0.0667, Val Loss: 0.0817\n",
      "Epoch 29/300 - Train Loss: 0.0669, Val Loss: 0.0701\n",
      "Epoch 30/300 - Train Loss: 0.0642, Val Loss: 0.0708\n",
      "Epoch 31/300 - Train Loss: 0.0651, Val Loss: 0.0696\n",
      "Epoch 32/300 - Train Loss: 0.0654, Val Loss: 0.0724\n",
      "Epoch 33/300 - Train Loss: 0.0657, Val Loss: 0.0775\n",
      "Epoch 34/300 - Train Loss: 0.0643, Val Loss: 0.0753\n",
      "Epoch 35/300 - Train Loss: 0.0653, Val Loss: 0.0666\n",
      "Epoch 36/300 - Train Loss: 0.0630, Val Loss: 0.0692\n",
      "Epoch 37/300 - Train Loss: 0.0623, Val Loss: 0.0739\n",
      "Epoch 38/300 - Train Loss: 0.0629, Val Loss: 0.0770\n",
      "Epoch 39/300 - Train Loss: 0.0632, Val Loss: 0.0699\n",
      "Epoch 40/300 - Train Loss: 0.0619, Val Loss: 0.0760\n",
      "Epoch 41/300 - Train Loss: 0.0627, Val Loss: 0.0715\n",
      "Epoch 42/300 - Train Loss: 0.0612, Val Loss: 0.0730\n",
      "Epoch 43/300 - Train Loss: 0.0609, Val Loss: 0.0751\n",
      "Epoch 44/300 - Train Loss: 0.0609, Val Loss: 0.0766\n",
      "Epoch 45/300 - Train Loss: 0.0626, Val Loss: 0.0720\n",
      "Epoch 46/300 - Train Loss: 0.0594, Val Loss: 0.0704\n",
      "Epoch 47/300 - Train Loss: 0.0610, Val Loss: 0.0730\n",
      "Epoch 48/300 - Train Loss: 0.0598, Val Loss: 0.0690\n",
      "Epoch 49/300 - Train Loss: 0.0588, Val Loss: 0.0741\n",
      "Epoch 50/300 - Train Loss: 0.0595, Val Loss: 0.0670\n",
      "Epoch 51/300 - Train Loss: 0.0604, Val Loss: 0.0746\n",
      "Epoch 52/300 - Train Loss: 0.0604, Val Loss: 0.0711\n",
      "Epoch 53/300 - Train Loss: 0.0589, Val Loss: 0.0821\n",
      "Epoch 54/300 - Train Loss: 0.0583, Val Loss: 0.0744\n",
      "Epoch 55/300 - Train Loss: 0.0598, Val Loss: 0.0721\n",
      "Epoch 56/300 - Train Loss: 0.0604, Val Loss: 0.0766\n",
      "Epoch 57/300 - Train Loss: 0.0597, Val Loss: 0.0705\n",
      "Epoch 58/300 - Train Loss: 0.0576, Val Loss: 0.0682\n",
      "Epoch 59/300 - Train Loss: 0.0576, Val Loss: 0.0701\n",
      "Epoch 60/300 - Train Loss: 0.0582, Val Loss: 0.0721\n",
      "Epoch 61/300 - Train Loss: 0.0563, Val Loss: 0.0706\n",
      "Epoch 62/300 - Train Loss: 0.0559, Val Loss: 0.0710\n",
      "Epoch 63/300 - Train Loss: 0.0574, Val Loss: 0.0700\n",
      "Epoch 64/300 - Train Loss: 0.0556, Val Loss: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 01:57:53,332] Trial 438 finished with value: 0.9662188902378963 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.17549758156752587, 'learning_rate': 0.0007372774866463662, 'batch_size': 128, 'weight_decay': 2.6332944973881037e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/300 - Train Loss: 0.0571, Val Loss: 0.0710\n",
      "Early stopping at epoch 65\n",
      "Macro F1 Score: 0.9662, Macro Precision: 0.9528, Macro Recall: 0.9816\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.88      0.98      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 440\n",
      "Training with F1=32, F2=8, D=2, dropout=0.20077987393430086, LR=6.603345661879434e-05, BS=32, WD=6.53646735405678e-05\n",
      "Epoch 1/300 - Train Loss: 0.4855, Val Loss: 0.2004\n",
      "Epoch 2/300 - Train Loss: 0.1802, Val Loss: 0.1172\n",
      "Epoch 3/300 - Train Loss: 0.1343, Val Loss: 0.1124\n",
      "Epoch 4/300 - Train Loss: 0.1146, Val Loss: 0.0867\n",
      "Epoch 5/300 - Train Loss: 0.1073, Val Loss: 0.0844\n",
      "Epoch 6/300 - Train Loss: 0.1019, Val Loss: 0.0764\n",
      "Epoch 7/300 - Train Loss: 0.0988, Val Loss: 0.0889\n",
      "Epoch 8/300 - Train Loss: 0.0981, Val Loss: 0.0780\n",
      "Epoch 9/300 - Train Loss: 0.0956, Val Loss: 0.0797\n",
      "Epoch 10/300 - Train Loss: 0.0919, Val Loss: 0.0776\n",
      "Epoch 11/300 - Train Loss: 0.0925, Val Loss: 0.0796\n",
      "Epoch 12/300 - Train Loss: 0.0892, Val Loss: 0.0731\n",
      "Epoch 13/300 - Train Loss: 0.0885, Val Loss: 0.0719\n",
      "Epoch 14/300 - Train Loss: 0.0888, Val Loss: 0.0763\n",
      "Epoch 15/300 - Train Loss: 0.0875, Val Loss: 0.0751\n",
      "Epoch 16/300 - Train Loss: 0.0880, Val Loss: 0.0727\n",
      "Epoch 17/300 - Train Loss: 0.0864, Val Loss: 0.0719\n",
      "Epoch 18/300 - Train Loss: 0.0859, Val Loss: 0.0697\n",
      "Epoch 19/300 - Train Loss: 0.0859, Val Loss: 0.0701\n",
      "Epoch 20/300 - Train Loss: 0.0848, Val Loss: 0.0698\n",
      "Epoch 21/300 - Train Loss: 0.0837, Val Loss: 0.0773\n",
      "Epoch 22/300 - Train Loss: 0.0857, Val Loss: 0.0756\n",
      "Epoch 23/300 - Train Loss: 0.0871, Val Loss: 0.0756\n",
      "Epoch 24/300 - Train Loss: 0.0825, Val Loss: 0.0685\n",
      "Epoch 25/300 - Train Loss: 0.0841, Val Loss: 0.0753\n",
      "Epoch 26/300 - Train Loss: 0.0845, Val Loss: 0.0753\n",
      "Epoch 27/300 - Train Loss: 0.0827, Val Loss: 0.0773\n",
      "Epoch 28/300 - Train Loss: 0.0846, Val Loss: 0.0738\n",
      "Epoch 29/300 - Train Loss: 0.0804, Val Loss: 0.0701\n",
      "Epoch 30/300 - Train Loss: 0.0840, Val Loss: 0.0663\n",
      "Epoch 31/300 - Train Loss: 0.0815, Val Loss: 0.0723\n",
      "Epoch 32/300 - Train Loss: 0.0833, Val Loss: 0.0742\n",
      "Epoch 33/300 - Train Loss: 0.0836, Val Loss: 0.0734\n",
      "Epoch 34/300 - Train Loss: 0.0820, Val Loss: 0.0662\n",
      "Epoch 35/300 - Train Loss: 0.0804, Val Loss: 0.0671\n",
      "Epoch 36/300 - Train Loss: 0.0801, Val Loss: 0.0676\n",
      "Epoch 37/300 - Train Loss: 0.0792, Val Loss: 0.0740\n",
      "Epoch 38/300 - Train Loss: 0.0797, Val Loss: 0.0727\n",
      "Epoch 39/300 - Train Loss: 0.0794, Val Loss: 0.0734\n",
      "Epoch 40/300 - Train Loss: 0.0789, Val Loss: 0.0685\n",
      "Epoch 41/300 - Train Loss: 0.0799, Val Loss: 0.0697\n",
      "Epoch 42/300 - Train Loss: 0.0787, Val Loss: 0.0701\n",
      "Epoch 43/300 - Train Loss: 0.0791, Val Loss: 0.0678\n",
      "Epoch 44/300 - Train Loss: 0.0807, Val Loss: 0.0705\n",
      "Epoch 45/300 - Train Loss: 0.0795, Val Loss: 0.0701\n",
      "Epoch 46/300 - Train Loss: 0.0795, Val Loss: 0.0752\n",
      "Epoch 47/300 - Train Loss: 0.0790, Val Loss: 0.0699\n",
      "Epoch 48/300 - Train Loss: 0.0765, Val Loss: 0.0697\n",
      "Epoch 49/300 - Train Loss: 0.0771, Val Loss: 0.0672\n",
      "Epoch 50/300 - Train Loss: 0.0769, Val Loss: 0.0695\n",
      "Epoch 51/300 - Train Loss: 0.0768, Val Loss: 0.0677\n",
      "Epoch 52/300 - Train Loss: 0.0759, Val Loss: 0.0671\n",
      "Epoch 53/300 - Train Loss: 0.0780, Val Loss: 0.0704\n",
      "Epoch 54/300 - Train Loss: 0.0746, Val Loss: 0.0676\n",
      "Epoch 55/300 - Train Loss: 0.0770, Val Loss: 0.0759\n",
      "Epoch 56/300 - Train Loss: 0.0745, Val Loss: 0.0704\n",
      "Epoch 57/300 - Train Loss: 0.0751, Val Loss: 0.0695\n",
      "Epoch 58/300 - Train Loss: 0.0757, Val Loss: 0.0686\n",
      "Epoch 59/300 - Train Loss: 0.0760, Val Loss: 0.0708\n",
      "Epoch 60/300 - Train Loss: 0.0776, Val Loss: 0.0656\n",
      "Epoch 61/300 - Train Loss: 0.0766, Val Loss: 0.0691\n",
      "Epoch 62/300 - Train Loss: 0.0748, Val Loss: 0.0727\n",
      "Epoch 63/300 - Train Loss: 0.0745, Val Loss: 0.0731\n",
      "Epoch 64/300 - Train Loss: 0.0730, Val Loss: 0.0717\n",
      "Epoch 65/300 - Train Loss: 0.0747, Val Loss: 0.0740\n",
      "Epoch 66/300 - Train Loss: 0.0744, Val Loss: 0.0706\n",
      "Epoch 67/300 - Train Loss: 0.0744, Val Loss: 0.0675\n",
      "Epoch 68/300 - Train Loss: 0.0744, Val Loss: 0.0691\n",
      "Epoch 69/300 - Train Loss: 0.0751, Val Loss: 0.0694\n",
      "Epoch 70/300 - Train Loss: 0.0742, Val Loss: 0.0722\n",
      "Epoch 71/300 - Train Loss: 0.0733, Val Loss: 0.0697\n",
      "Epoch 72/300 - Train Loss: 0.0736, Val Loss: 0.0705\n",
      "Epoch 73/300 - Train Loss: 0.0752, Val Loss: 0.0780\n",
      "Epoch 74/300 - Train Loss: 0.0719, Val Loss: 0.0661\n",
      "Epoch 75/300 - Train Loss: 0.0729, Val Loss: 0.0686\n",
      "Epoch 76/300 - Train Loss: 0.0731, Val Loss: 0.0711\n",
      "Epoch 77/300 - Train Loss: 0.0729, Val Loss: 0.0732\n",
      "Epoch 78/300 - Train Loss: 0.0733, Val Loss: 0.0727\n",
      "Epoch 79/300 - Train Loss: 0.0716, Val Loss: 0.0680\n",
      "Epoch 80/300 - Train Loss: 0.0719, Val Loss: 0.0694\n",
      "Epoch 81/300 - Train Loss: 0.0728, Val Loss: 0.0681\n",
      "Epoch 82/300 - Train Loss: 0.0723, Val Loss: 0.0688\n",
      "Epoch 83/300 - Train Loss: 0.0752, Val Loss: 0.0740\n",
      "Epoch 84/300 - Train Loss: 0.0729, Val Loss: 0.0709\n",
      "Epoch 85/300 - Train Loss: 0.0738, Val Loss: 0.0800\n",
      "Epoch 86/300 - Train Loss: 0.0707, Val Loss: 0.0730\n",
      "Epoch 87/300 - Train Loss: 0.0726, Val Loss: 0.0674\n",
      "Epoch 88/300 - Train Loss: 0.0711, Val Loss: 0.0688\n",
      "Epoch 89/300 - Train Loss: 0.0714, Val Loss: 0.0723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:01:39,293] Trial 439 finished with value: 0.9678739489896252 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.20077987393430086, 'learning_rate': 6.603345661879434e-05, 'batch_size': 32, 'weight_decay': 6.53646735405678e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/300 - Train Loss: 0.0687, Val Loss: 0.0667\n",
      "Early stopping at epoch 90\n",
      "Macro F1 Score: 0.9679, Macro Precision: 0.9559, Macro Recall: 0.9813\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 441\n",
      "Training with F1=16, F2=32, D=2, dropout=0.22126529266875908, LR=0.0009024711476191355, BS=32, WD=4.6303831740872614e-05\n",
      "Epoch 1/300 - Train Loss: 0.1534, Val Loss: 0.0858\n",
      "Epoch 2/300 - Train Loss: 0.1055, Val Loss: 0.0795\n",
      "Epoch 3/300 - Train Loss: 0.1007, Val Loss: 0.0772\n",
      "Epoch 4/300 - Train Loss: 0.0961, Val Loss: 0.0677\n",
      "Epoch 5/300 - Train Loss: 0.0913, Val Loss: 0.0715\n",
      "Epoch 6/300 - Train Loss: 0.0889, Val Loss: 0.0784\n",
      "Epoch 7/300 - Train Loss: 0.0863, Val Loss: 0.0695\n",
      "Epoch 8/300 - Train Loss: 0.0843, Val Loss: 0.0689\n",
      "Epoch 9/300 - Train Loss: 0.0857, Val Loss: 0.0792\n",
      "Epoch 10/300 - Train Loss: 0.0835, Val Loss: 0.0826\n",
      "Epoch 11/300 - Train Loss: 0.0813, Val Loss: 0.0731\n",
      "Epoch 12/300 - Train Loss: 0.0808, Val Loss: 0.0779\n",
      "Epoch 13/300 - Train Loss: 0.0800, Val Loss: 0.0756\n",
      "Epoch 14/300 - Train Loss: 0.0789, Val Loss: 0.0733\n",
      "Epoch 15/300 - Train Loss: 0.0803, Val Loss: 0.0889\n",
      "Epoch 16/300 - Train Loss: 0.0757, Val Loss: 0.0736\n",
      "Epoch 17/300 - Train Loss: 0.0785, Val Loss: 0.0852\n",
      "Epoch 18/300 - Train Loss: 0.0788, Val Loss: 0.0894\n",
      "Epoch 19/300 - Train Loss: 0.0799, Val Loss: 0.0764\n",
      "Epoch 20/300 - Train Loss: 0.0766, Val Loss: 0.0770\n",
      "Epoch 21/300 - Train Loss: 0.0741, Val Loss: 0.0850\n",
      "Epoch 22/300 - Train Loss: 0.0723, Val Loss: 0.0739\n",
      "Epoch 23/300 - Train Loss: 0.0733, Val Loss: 0.0820\n",
      "Epoch 24/300 - Train Loss: 0.0742, Val Loss: 0.0835\n",
      "Epoch 25/300 - Train Loss: 0.0721, Val Loss: 0.0770\n",
      "Epoch 26/300 - Train Loss: 0.0714, Val Loss: 0.0868\n",
      "Epoch 27/300 - Train Loss: 0.0723, Val Loss: 0.0852\n",
      "Epoch 28/300 - Train Loss: 0.0698, Val Loss: 0.0812\n",
      "Epoch 29/300 - Train Loss: 0.0706, Val Loss: 0.0806\n",
      "Epoch 30/300 - Train Loss: 0.0729, Val Loss: 0.0772\n",
      "Epoch 31/300 - Train Loss: 0.0704, Val Loss: 0.0821\n",
      "Epoch 32/300 - Train Loss: 0.0678, Val Loss: 0.0831\n",
      "Epoch 33/300 - Train Loss: 0.0677, Val Loss: 0.0828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:02:51,548] Trial 440 finished with value: 0.970642032859551 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.22126529266875908, 'learning_rate': 0.0009024711476191355, 'batch_size': 32, 'weight_decay': 4.6303831740872614e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300 - Train Loss: 0.0674, Val Loss: 0.0791\n",
      "Early stopping at epoch 34\n",
      "Macro F1 Score: 0.9706, Macro Precision: 0.9738, Macro Recall: 0.9676\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 442\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1863699200404065, LR=0.000781130829997743, BS=32, WD=0.0007346145197793535\n",
      "Epoch 1/300 - Train Loss: 0.1671, Val Loss: 0.0755\n",
      "Epoch 2/300 - Train Loss: 0.0996, Val Loss: 0.0881\n",
      "Epoch 3/300 - Train Loss: 0.0936, Val Loss: 0.0833\n",
      "Epoch 4/300 - Train Loss: 0.0921, Val Loss: 0.0715\n",
      "Epoch 5/300 - Train Loss: 0.0920, Val Loss: 0.0742\n",
      "Epoch 6/300 - Train Loss: 0.0907, Val Loss: 0.0785\n",
      "Epoch 7/300 - Train Loss: 0.0916, Val Loss: 0.0708\n",
      "Epoch 8/300 - Train Loss: 0.0897, Val Loss: 0.0845\n",
      "Epoch 9/300 - Train Loss: 0.0893, Val Loss: 0.0741\n",
      "Epoch 10/300 - Train Loss: 0.0896, Val Loss: 0.0892\n",
      "Epoch 11/300 - Train Loss: 0.0839, Val Loss: 0.0733\n",
      "Epoch 12/300 - Train Loss: 0.0881, Val Loss: 0.0728\n",
      "Epoch 13/300 - Train Loss: 0.0876, Val Loss: 0.0759\n",
      "Epoch 14/300 - Train Loss: 0.0862, Val Loss: 0.0739\n",
      "Epoch 15/300 - Train Loss: 0.0854, Val Loss: 0.0670\n",
      "Epoch 16/300 - Train Loss: 0.0869, Val Loss: 0.0683\n",
      "Epoch 17/300 - Train Loss: 0.0871, Val Loss: 0.0796\n",
      "Epoch 18/300 - Train Loss: 0.0838, Val Loss: 0.0752\n",
      "Epoch 19/300 - Train Loss: 0.0868, Val Loss: 0.0778\n",
      "Epoch 20/300 - Train Loss: 0.0885, Val Loss: 0.0778\n",
      "Epoch 21/300 - Train Loss: 0.0833, Val Loss: 0.0742\n",
      "Epoch 22/300 - Train Loss: 0.0863, Val Loss: 0.0707\n",
      "Epoch 23/300 - Train Loss: 0.0844, Val Loss: 0.0692\n",
      "Epoch 24/300 - Train Loss: 0.0851, Val Loss: 0.0650\n",
      "Epoch 25/300 - Train Loss: 0.0864, Val Loss: 0.0683\n",
      "Epoch 26/300 - Train Loss: 0.0827, Val Loss: 0.0773\n",
      "Epoch 27/300 - Train Loss: 0.0849, Val Loss: 0.0709\n",
      "Epoch 28/300 - Train Loss: 0.0858, Val Loss: 0.0746\n",
      "Epoch 29/300 - Train Loss: 0.0839, Val Loss: 0.0744\n",
      "Epoch 30/300 - Train Loss: 0.0830, Val Loss: 0.0712\n",
      "Epoch 31/300 - Train Loss: 0.0840, Val Loss: 0.0745\n",
      "Epoch 32/300 - Train Loss: 0.0812, Val Loss: 0.0737\n",
      "Epoch 33/300 - Train Loss: 0.0855, Val Loss: 0.0764\n",
      "Epoch 34/300 - Train Loss: 0.0865, Val Loss: 0.0688\n",
      "Epoch 35/300 - Train Loss: 0.0832, Val Loss: 0.0690\n",
      "Epoch 36/300 - Train Loss: 0.0836, Val Loss: 0.0731\n",
      "Epoch 37/300 - Train Loss: 0.0833, Val Loss: 0.0695\n",
      "Epoch 38/300 - Train Loss: 0.0848, Val Loss: 0.0790\n",
      "Epoch 39/300 - Train Loss: 0.0863, Val Loss: 0.0648\n",
      "Epoch 40/300 - Train Loss: 0.0848, Val Loss: 0.0787\n",
      "Epoch 41/300 - Train Loss: 0.0827, Val Loss: 0.0746\n",
      "Epoch 42/300 - Train Loss: 0.0848, Val Loss: 0.0785\n",
      "Epoch 43/300 - Train Loss: 0.0835, Val Loss: 0.0785\n",
      "Epoch 44/300 - Train Loss: 0.0850, Val Loss: 0.0717\n",
      "Epoch 45/300 - Train Loss: 0.0834, Val Loss: 0.0698\n",
      "Epoch 46/300 - Train Loss: 0.0867, Val Loss: 0.0722\n",
      "Epoch 47/300 - Train Loss: 0.0852, Val Loss: 0.0704\n",
      "Epoch 48/300 - Train Loss: 0.0860, Val Loss: 0.0700\n",
      "Epoch 49/300 - Train Loss: 0.0863, Val Loss: 0.0774\n",
      "Epoch 50/300 - Train Loss: 0.0847, Val Loss: 0.0659\n",
      "Epoch 51/300 - Train Loss: 0.0840, Val Loss: 0.0717\n",
      "Epoch 52/300 - Train Loss: 0.0849, Val Loss: 0.0659\n",
      "Epoch 53/300 - Train Loss: 0.0839, Val Loss: 0.0681\n",
      "Epoch 54/300 - Train Loss: 0.0815, Val Loss: 0.0782\n",
      "Epoch 55/300 - Train Loss: 0.0857, Val Loss: 0.0752\n",
      "Epoch 56/300 - Train Loss: 0.0849, Val Loss: 0.0779\n",
      "Epoch 57/300 - Train Loss: 0.0828, Val Loss: 0.0710\n",
      "Epoch 58/300 - Train Loss: 0.0847, Val Loss: 0.0692\n",
      "Epoch 59/300 - Train Loss: 0.0844, Val Loss: 0.0689\n",
      "Epoch 60/300 - Train Loss: 0.0853, Val Loss: 0.0792\n",
      "Epoch 61/300 - Train Loss: 0.0830, Val Loss: 0.0651\n",
      "Epoch 62/300 - Train Loss: 0.0855, Val Loss: 0.0657\n",
      "Epoch 63/300 - Train Loss: 0.0846, Val Loss: 0.0738\n",
      "Epoch 64/300 - Train Loss: 0.0831, Val Loss: 0.0653\n",
      "Epoch 65/300 - Train Loss: 0.0840, Val Loss: 0.0701\n",
      "Epoch 66/300 - Train Loss: 0.0865, Val Loss: 0.0759\n",
      "Epoch 67/300 - Train Loss: 0.0864, Val Loss: 0.0700\n",
      "Epoch 68/300 - Train Loss: 0.0839, Val Loss: 0.0699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:05:44,916] Trial 441 finished with value: 0.9679235850673521 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1863699200404065, 'learning_rate': 0.000781130829997743, 'batch_size': 32, 'weight_decay': 0.0007346145197793535}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69/300 - Train Loss: 0.0849, Val Loss: 0.0651\n",
      "Early stopping at epoch 69\n",
      "Macro F1 Score: 0.9679, Macro Precision: 0.9765, Macro Recall: 0.9599\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.97      0.92      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 443\n",
      "Training with F1=32, F2=16, D=2, dropout=0.17240633106966383, LR=0.0006500781553942419, BS=32, WD=5.6746106340982435e-05\n",
      "Epoch 1/300 - Train Loss: 0.1584, Val Loss: 0.1012\n",
      "Epoch 2/300 - Train Loss: 0.0981, Val Loss: 0.0844\n",
      "Epoch 3/300 - Train Loss: 0.0918, Val Loss: 0.0704\n",
      "Epoch 4/300 - Train Loss: 0.0859, Val Loss: 0.0782\n",
      "Epoch 5/300 - Train Loss: 0.0890, Val Loss: 0.0716\n",
      "Epoch 6/300 - Train Loss: 0.0862, Val Loss: 0.0728\n",
      "Epoch 7/300 - Train Loss: 0.0854, Val Loss: 0.0888\n",
      "Epoch 8/300 - Train Loss: 0.0832, Val Loss: 0.0777\n",
      "Epoch 9/300 - Train Loss: 0.0817, Val Loss: 0.0705\n",
      "Epoch 10/300 - Train Loss: 0.0819, Val Loss: 0.0809\n",
      "Epoch 11/300 - Train Loss: 0.0800, Val Loss: 0.0711\n",
      "Epoch 12/300 - Train Loss: 0.0782, Val Loss: 0.0765\n",
      "Epoch 13/300 - Train Loss: 0.0816, Val Loss: 0.0828\n",
      "Epoch 14/300 - Train Loss: 0.0773, Val Loss: 0.0762\n",
      "Epoch 15/300 - Train Loss: 0.0766, Val Loss: 0.0763\n",
      "Epoch 16/300 - Train Loss: 0.0777, Val Loss: 0.0705\n",
      "Epoch 17/300 - Train Loss: 0.0771, Val Loss: 0.0747\n",
      "Epoch 18/300 - Train Loss: 0.0736, Val Loss: 0.0745\n",
      "Epoch 19/300 - Train Loss: 0.0747, Val Loss: 0.0726\n",
      "Epoch 20/300 - Train Loss: 0.0731, Val Loss: 0.0721\n",
      "Epoch 21/300 - Train Loss: 0.0712, Val Loss: 0.0728\n",
      "Epoch 22/300 - Train Loss: 0.0708, Val Loss: 0.0788\n",
      "Epoch 23/300 - Train Loss: 0.0738, Val Loss: 0.0849\n",
      "Epoch 24/300 - Train Loss: 0.0701, Val Loss: 0.0747\n",
      "Epoch 25/300 - Train Loss: 0.0748, Val Loss: 0.0706\n",
      "Epoch 26/300 - Train Loss: 0.0697, Val Loss: 0.0812\n",
      "Epoch 27/300 - Train Loss: 0.0696, Val Loss: 0.0779\n",
      "Epoch 28/300 - Train Loss: 0.0700, Val Loss: 0.0765\n",
      "Epoch 29/300 - Train Loss: 0.0696, Val Loss: 0.0717\n",
      "Epoch 30/300 - Train Loss: 0.0670, Val Loss: 0.0788\n",
      "Epoch 31/300 - Train Loss: 0.0670, Val Loss: 0.0809\n",
      "Epoch 32/300 - Train Loss: 0.0664, Val Loss: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:07:13,124] Trial 442 finished with value: 0.9631635810923309 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.17240633106966383, 'learning_rate': 0.0006500781553942419, 'batch_size': 32, 'weight_decay': 5.6746106340982435e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300 - Train Loss: 0.0695, Val Loss: 0.0868\n",
      "Early stopping at epoch 33\n",
      "Macro F1 Score: 0.9632, Macro Precision: 0.9577, Macro Recall: 0.9691\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 444\n",
      "Training with F1=16, F2=32, D=4, dropout=0.15994037629423333, LR=0.00027199138190850896, BS=32, WD=3.487475494618072e-05\n",
      "Epoch 1/300 - Train Loss: 0.1834, Val Loss: 0.0858\n",
      "Epoch 2/300 - Train Loss: 0.1057, Val Loss: 0.0774\n",
      "Epoch 3/300 - Train Loss: 0.1008, Val Loss: 0.0813\n",
      "Epoch 4/300 - Train Loss: 0.0936, Val Loss: 0.0941\n",
      "Epoch 5/300 - Train Loss: 0.0886, Val Loss: 0.0740\n",
      "Epoch 6/300 - Train Loss: 0.0856, Val Loss: 0.0772\n",
      "Epoch 7/300 - Train Loss: 0.0862, Val Loss: 0.0714\n",
      "Epoch 8/300 - Train Loss: 0.0867, Val Loss: 0.0811\n",
      "Epoch 9/300 - Train Loss: 0.0815, Val Loss: 0.0725\n",
      "Epoch 10/300 - Train Loss: 0.0815, Val Loss: 0.0806\n",
      "Epoch 11/300 - Train Loss: 0.0790, Val Loss: 0.0746\n",
      "Epoch 12/300 - Train Loss: 0.0786, Val Loss: 0.0732\n",
      "Epoch 13/300 - Train Loss: 0.0761, Val Loss: 0.0727\n",
      "Epoch 14/300 - Train Loss: 0.0758, Val Loss: 0.0683\n",
      "Epoch 15/300 - Train Loss: 0.0737, Val Loss: 0.0682\n",
      "Epoch 16/300 - Train Loss: 0.0730, Val Loss: 0.0676\n",
      "Epoch 17/300 - Train Loss: 0.0735, Val Loss: 0.0736\n",
      "Epoch 18/300 - Train Loss: 0.0705, Val Loss: 0.0651\n",
      "Epoch 19/300 - Train Loss: 0.0721, Val Loss: 0.0694\n",
      "Epoch 20/300 - Train Loss: 0.0718, Val Loss: 0.0673\n",
      "Epoch 21/300 - Train Loss: 0.0721, Val Loss: 0.0709\n",
      "Epoch 22/300 - Train Loss: 0.0666, Val Loss: 0.0690\n",
      "Epoch 23/300 - Train Loss: 0.0673, Val Loss: 0.0662\n",
      "Epoch 24/300 - Train Loss: 0.0676, Val Loss: 0.0697\n",
      "Epoch 25/300 - Train Loss: 0.0658, Val Loss: 0.0758\n",
      "Epoch 26/300 - Train Loss: 0.0624, Val Loss: 0.0704\n",
      "Epoch 27/300 - Train Loss: 0.0652, Val Loss: 0.0715\n",
      "Epoch 28/300 - Train Loss: 0.0668, Val Loss: 0.0698\n",
      "Epoch 29/300 - Train Loss: 0.0645, Val Loss: 0.0701\n",
      "Epoch 30/300 - Train Loss: 0.0607, Val Loss: 0.0720\n",
      "Epoch 31/300 - Train Loss: 0.0611, Val Loss: 0.0679\n",
      "Epoch 32/300 - Train Loss: 0.0616, Val Loss: 0.0693\n",
      "Epoch 33/300 - Train Loss: 0.0583, Val Loss: 0.0731\n",
      "Epoch 34/300 - Train Loss: 0.0608, Val Loss: 0.0751\n",
      "Epoch 35/300 - Train Loss: 0.0590, Val Loss: 0.0696\n",
      "Epoch 36/300 - Train Loss: 0.0558, Val Loss: 0.0717\n",
      "Epoch 37/300 - Train Loss: 0.0560, Val Loss: 0.0719\n",
      "Epoch 38/300 - Train Loss: 0.0564, Val Loss: 0.0707\n",
      "Epoch 39/300 - Train Loss: 0.0555, Val Loss: 0.0664\n",
      "Epoch 40/300 - Train Loss: 0.0527, Val Loss: 0.0684\n",
      "Epoch 41/300 - Train Loss: 0.0571, Val Loss: 0.0726\n",
      "Epoch 42/300 - Train Loss: 0.0547, Val Loss: 0.0790\n",
      "Epoch 43/300 - Train Loss: 0.0513, Val Loss: 0.0712\n",
      "Epoch 44/300 - Train Loss: 0.0502, Val Loss: 0.0791\n",
      "Epoch 45/300 - Train Loss: 0.0529, Val Loss: 0.0779\n",
      "Epoch 46/300 - Train Loss: 0.0511, Val Loss: 0.0690\n",
      "Epoch 47/300 - Train Loss: 0.0503, Val Loss: 0.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:09:13,973] Trial 443 finished with value: 0.9636356897341817 and parameters: {'F1': 16, 'F2': 32, 'D': 4, 'dropout': 0.15994037629423333, 'learning_rate': 0.00027199138190850896, 'batch_size': 32, 'weight_decay': 3.487475494618072e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/300 - Train Loss: 0.0499, Val Loss: 0.0775\n",
      "Early stopping at epoch 48\n",
      "Macro F1 Score: 0.9636, Macro Precision: 0.9619, Macro Recall: 0.9655\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 445\n",
      "Training with F1=32, F2=32, D=2, dropout=0.11438948460208097, LR=5.9568802445558294e-05, BS=32, WD=5.014576305732263e-05\n",
      "Epoch 1/300 - Train Loss: 0.3043, Val Loss: 0.1513\n",
      "Epoch 2/300 - Train Loss: 0.1372, Val Loss: 0.0976\n",
      "Epoch 3/300 - Train Loss: 0.1081, Val Loss: 0.0896\n",
      "Epoch 4/300 - Train Loss: 0.1013, Val Loss: 0.0784\n",
      "Epoch 5/300 - Train Loss: 0.0980, Val Loss: 0.0740\n",
      "Epoch 6/300 - Train Loss: 0.0934, Val Loss: 0.0744\n",
      "Epoch 7/300 - Train Loss: 0.0917, Val Loss: 0.0883\n",
      "Epoch 8/300 - Train Loss: 0.0917, Val Loss: 0.0792\n",
      "Epoch 9/300 - Train Loss: 0.0886, Val Loss: 0.0788\n",
      "Epoch 10/300 - Train Loss: 0.0895, Val Loss: 0.0898\n",
      "Epoch 11/300 - Train Loss: 0.0856, Val Loss: 0.0755\n",
      "Epoch 12/300 - Train Loss: 0.0843, Val Loss: 0.0733\n",
      "Epoch 13/300 - Train Loss: 0.0842, Val Loss: 0.0766\n",
      "Epoch 14/300 - Train Loss: 0.0817, Val Loss: 0.0722\n",
      "Epoch 15/300 - Train Loss: 0.0806, Val Loss: 0.0700\n",
      "Epoch 16/300 - Train Loss: 0.0813, Val Loss: 0.0728\n",
      "Epoch 17/300 - Train Loss: 0.0806, Val Loss: 0.0754\n",
      "Epoch 18/300 - Train Loss: 0.0762, Val Loss: 0.0782\n",
      "Epoch 19/300 - Train Loss: 0.0795, Val Loss: 0.0743\n",
      "Epoch 20/300 - Train Loss: 0.0774, Val Loss: 0.0692\n",
      "Epoch 21/300 - Train Loss: 0.0756, Val Loss: 0.0728\n",
      "Epoch 22/300 - Train Loss: 0.0745, Val Loss: 0.0747\n",
      "Epoch 23/300 - Train Loss: 0.0733, Val Loss: 0.0733\n",
      "Epoch 24/300 - Train Loss: 0.0770, Val Loss: 0.0786\n",
      "Epoch 25/300 - Train Loss: 0.0726, Val Loss: 0.0703\n",
      "Epoch 26/300 - Train Loss: 0.0760, Val Loss: 0.0862\n",
      "Epoch 27/300 - Train Loss: 0.0751, Val Loss: 0.0693\n",
      "Epoch 28/300 - Train Loss: 0.0721, Val Loss: 0.0748\n",
      "Epoch 29/300 - Train Loss: 0.0723, Val Loss: 0.0773\n",
      "Epoch 30/300 - Train Loss: 0.0695, Val Loss: 0.0703\n",
      "Epoch 31/300 - Train Loss: 0.0710, Val Loss: 0.0887\n",
      "Epoch 32/300 - Train Loss: 0.0714, Val Loss: 0.0673\n",
      "Epoch 33/300 - Train Loss: 0.0716, Val Loss: 0.0724\n",
      "Epoch 34/300 - Train Loss: 0.0725, Val Loss: 0.0686\n",
      "Epoch 35/300 - Train Loss: 0.0706, Val Loss: 0.0730\n",
      "Epoch 36/300 - Train Loss: 0.0686, Val Loss: 0.0722\n",
      "Epoch 37/300 - Train Loss: 0.0688, Val Loss: 0.0693\n",
      "Epoch 38/300 - Train Loss: 0.0690, Val Loss: 0.0774\n",
      "Epoch 39/300 - Train Loss: 0.0687, Val Loss: 0.0678\n",
      "Epoch 40/300 - Train Loss: 0.0680, Val Loss: 0.0710\n",
      "Epoch 41/300 - Train Loss: 0.0683, Val Loss: 0.0665\n",
      "Epoch 42/300 - Train Loss: 0.0676, Val Loss: 0.0688\n",
      "Epoch 43/300 - Train Loss: 0.0653, Val Loss: 0.0669\n",
      "Epoch 44/300 - Train Loss: 0.0654, Val Loss: 0.0738\n",
      "Epoch 45/300 - Train Loss: 0.0670, Val Loss: 0.0728\n",
      "Epoch 46/300 - Train Loss: 0.0668, Val Loss: 0.0684\n",
      "Epoch 47/300 - Train Loss: 0.0639, Val Loss: 0.0690\n",
      "Epoch 48/300 - Train Loss: 0.0647, Val Loss: 0.0723\n",
      "Epoch 49/300 - Train Loss: 0.0683, Val Loss: 0.0738\n",
      "Epoch 50/300 - Train Loss: 0.0641, Val Loss: 0.0724\n",
      "Epoch 51/300 - Train Loss: 0.0633, Val Loss: 0.0692\n",
      "Epoch 52/300 - Train Loss: 0.0626, Val Loss: 0.0730\n",
      "Epoch 53/300 - Train Loss: 0.0610, Val Loss: 0.0713\n",
      "Epoch 54/300 - Train Loss: 0.0645, Val Loss: 0.0727\n",
      "Epoch 55/300 - Train Loss: 0.0629, Val Loss: 0.0690\n",
      "Epoch 56/300 - Train Loss: 0.0609, Val Loss: 0.0723\n",
      "Epoch 57/300 - Train Loss: 0.0615, Val Loss: 0.0699\n",
      "Epoch 58/300 - Train Loss: 0.0626, Val Loss: 0.0737\n",
      "Epoch 59/300 - Train Loss: 0.0640, Val Loss: 0.0756\n",
      "Epoch 60/300 - Train Loss: 0.0605, Val Loss: 0.0682\n",
      "Epoch 61/300 - Train Loss: 0.0611, Val Loss: 0.0677\n",
      "Epoch 62/300 - Train Loss: 0.0610, Val Loss: 0.0713\n",
      "Epoch 63/300 - Train Loss: 0.0593, Val Loss: 0.0685\n",
      "Epoch 64/300 - Train Loss: 0.0599, Val Loss: 0.0667\n",
      "Epoch 65/300 - Train Loss: 0.0582, Val Loss: 0.0730\n",
      "Epoch 66/300 - Train Loss: 0.0575, Val Loss: 0.0698\n",
      "Epoch 67/300 - Train Loss: 0.0590, Val Loss: 0.0684\n",
      "Epoch 68/300 - Train Loss: 0.0563, Val Loss: 0.0756\n",
      "Epoch 69/300 - Train Loss: 0.0552, Val Loss: 0.0684\n",
      "Epoch 70/300 - Train Loss: 0.0579, Val Loss: 0.0699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:12:28,899] Trial 444 finished with value: 0.968041518357647 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.11438948460208097, 'learning_rate': 5.9568802445558294e-05, 'batch_size': 32, 'weight_decay': 5.014576305732263e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300 - Train Loss: 0.0574, Val Loss: 0.0673\n",
      "Early stopping at epoch 71\n",
      "Macro F1 Score: 0.9680, Macro Precision: 0.9601, Macro Recall: 0.9767\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 446\n",
      "Training with F1=16, F2=8, D=2, dropout=0.2078407174811111, LR=0.0008382221248054739, BS=32, WD=7.771315666960754e-05\n",
      "Epoch 1/300 - Train Loss: 0.1685, Val Loss: 0.0804\n",
      "Epoch 2/300 - Train Loss: 0.1077, Val Loss: 0.0763\n",
      "Epoch 3/300 - Train Loss: 0.0985, Val Loss: 0.0764\n",
      "Epoch 4/300 - Train Loss: 0.0922, Val Loss: 0.0870\n",
      "Epoch 5/300 - Train Loss: 0.0965, Val Loss: 0.0728\n",
      "Epoch 6/300 - Train Loss: 0.0912, Val Loss: 0.0722\n",
      "Epoch 7/300 - Train Loss: 0.0908, Val Loss: 0.0736\n",
      "Epoch 8/300 - Train Loss: 0.0892, Val Loss: 0.0724\n",
      "Epoch 9/300 - Train Loss: 0.0873, Val Loss: 0.0707\n",
      "Epoch 10/300 - Train Loss: 0.0875, Val Loss: 0.0721\n",
      "Epoch 11/300 - Train Loss: 0.0868, Val Loss: 0.0703\n",
      "Epoch 12/300 - Train Loss: 0.0876, Val Loss: 0.0767\n",
      "Epoch 13/300 - Train Loss: 0.0877, Val Loss: 0.0874\n",
      "Epoch 14/300 - Train Loss: 0.0846, Val Loss: 0.0751\n",
      "Epoch 15/300 - Train Loss: 0.0834, Val Loss: 0.0721\n",
      "Epoch 16/300 - Train Loss: 0.0855, Val Loss: 0.0710\n",
      "Epoch 17/300 - Train Loss: 0.0829, Val Loss: 0.0724\n",
      "Epoch 18/300 - Train Loss: 0.0875, Val Loss: 0.0698\n",
      "Epoch 19/300 - Train Loss: 0.0820, Val Loss: 0.0755\n",
      "Epoch 20/300 - Train Loss: 0.0839, Val Loss: 0.0795\n",
      "Epoch 21/300 - Train Loss: 0.0817, Val Loss: 0.0750\n",
      "Epoch 22/300 - Train Loss: 0.0817, Val Loss: 0.0672\n",
      "Epoch 23/300 - Train Loss: 0.0812, Val Loss: 0.0685\n",
      "Epoch 24/300 - Train Loss: 0.0802, Val Loss: 0.0840\n",
      "Epoch 25/300 - Train Loss: 0.0799, Val Loss: 0.0751\n",
      "Epoch 26/300 - Train Loss: 0.0832, Val Loss: 0.0688\n",
      "Epoch 27/300 - Train Loss: 0.0799, Val Loss: 0.0698\n",
      "Epoch 28/300 - Train Loss: 0.0781, Val Loss: 0.0782\n",
      "Epoch 29/300 - Train Loss: 0.0804, Val Loss: 0.0660\n",
      "Epoch 30/300 - Train Loss: 0.0798, Val Loss: 0.0664\n",
      "Epoch 31/300 - Train Loss: 0.0806, Val Loss: 0.0716\n",
      "Epoch 32/300 - Train Loss: 0.0800, Val Loss: 0.0691\n",
      "Epoch 33/300 - Train Loss: 0.0802, Val Loss: 0.0709\n",
      "Epoch 34/300 - Train Loss: 0.0780, Val Loss: 0.0704\n",
      "Epoch 35/300 - Train Loss: 0.0794, Val Loss: 0.0668\n",
      "Epoch 36/300 - Train Loss: 0.0759, Val Loss: 0.0715\n",
      "Epoch 37/300 - Train Loss: 0.0757, Val Loss: 0.0709\n",
      "Epoch 38/300 - Train Loss: 0.0764, Val Loss: 0.0692\n",
      "Epoch 39/300 - Train Loss: 0.0773, Val Loss: 0.0690\n",
      "Epoch 40/300 - Train Loss: 0.0816, Val Loss: 0.0757\n",
      "Epoch 41/300 - Train Loss: 0.0774, Val Loss: 0.0739\n",
      "Epoch 42/300 - Train Loss: 0.0775, Val Loss: 0.0731\n",
      "Epoch 43/300 - Train Loss: 0.0787, Val Loss: 0.0753\n",
      "Epoch 44/300 - Train Loss: 0.0773, Val Loss: 0.0715\n",
      "Epoch 45/300 - Train Loss: 0.0778, Val Loss: 0.0767\n",
      "Epoch 46/300 - Train Loss: 0.0753, Val Loss: 0.0775\n",
      "Epoch 47/300 - Train Loss: 0.0782, Val Loss: 0.0713\n",
      "Epoch 48/300 - Train Loss: 0.0779, Val Loss: 0.0751\n",
      "Epoch 49/300 - Train Loss: 0.0807, Val Loss: 0.0718\n",
      "Epoch 50/300 - Train Loss: 0.0754, Val Loss: 0.0770\n",
      "Epoch 51/300 - Train Loss: 0.0738, Val Loss: 0.0689\n",
      "Epoch 52/300 - Train Loss: 0.0758, Val Loss: 0.0718\n",
      "Epoch 53/300 - Train Loss: 0.0772, Val Loss: 0.0801\n",
      "Epoch 54/300 - Train Loss: 0.0760, Val Loss: 0.0730\n",
      "Epoch 55/300 - Train Loss: 0.0770, Val Loss: 0.0711\n",
      "Epoch 56/300 - Train Loss: 0.0768, Val Loss: 0.0674\n",
      "Epoch 57/300 - Train Loss: 0.0769, Val Loss: 0.0710\n",
      "Epoch 58/300 - Train Loss: 0.0755, Val Loss: 0.0721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:14:34,190] Trial 445 finished with value: 0.9687008560826862 and parameters: {'F1': 16, 'F2': 8, 'D': 2, 'dropout': 0.2078407174811111, 'learning_rate': 0.0008382221248054739, 'batch_size': 32, 'weight_decay': 7.771315666960754e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300 - Train Loss: 0.0728, Val Loss: 0.0720\n",
      "Early stopping at epoch 59\n",
      "Macro F1 Score: 0.9687, Macro Precision: 0.9647, Macro Recall: 0.9729\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 447\n",
      "Training with F1=32, F2=32, D=2, dropout=0.19343418616130958, LR=7.252259501925036e-05, BS=256, WD=4.308981873951814e-05\n",
      "Epoch 1/300 - Train Loss: 0.6896, Val Loss: 0.3512\n",
      "Epoch 2/300 - Train Loss: 0.2914, Val Loss: 0.2300\n",
      "Epoch 3/300 - Train Loss: 0.2116, Val Loss: 0.1756\n",
      "Epoch 4/300 - Train Loss: 0.1638, Val Loss: 0.1445\n",
      "Epoch 5/300 - Train Loss: 0.1351, Val Loss: 0.1148\n",
      "Epoch 6/300 - Train Loss: 0.1174, Val Loss: 0.1062\n",
      "Epoch 7/300 - Train Loss: 0.1081, Val Loss: 0.0999\n",
      "Epoch 8/300 - Train Loss: 0.1008, Val Loss: 0.1007\n",
      "Epoch 9/300 - Train Loss: 0.0984, Val Loss: 0.0894\n",
      "Epoch 10/300 - Train Loss: 0.0936, Val Loss: 0.0912\n",
      "Epoch 11/300 - Train Loss: 0.0918, Val Loss: 0.0822\n",
      "Epoch 12/300 - Train Loss: 0.0906, Val Loss: 0.0851\n",
      "Epoch 13/300 - Train Loss: 0.0878, Val Loss: 0.0864\n",
      "Epoch 14/300 - Train Loss: 0.0863, Val Loss: 0.0811\n",
      "Epoch 15/300 - Train Loss: 0.0866, Val Loss: 0.0853\n",
      "Epoch 16/300 - Train Loss: 0.0852, Val Loss: 0.0780\n",
      "Epoch 17/300 - Train Loss: 0.0837, Val Loss: 0.0816\n",
      "Epoch 18/300 - Train Loss: 0.0828, Val Loss: 0.0771\n",
      "Epoch 19/300 - Train Loss: 0.0825, Val Loss: 0.0783\n",
      "Epoch 20/300 - Train Loss: 0.0808, Val Loss: 0.0794\n",
      "Epoch 21/300 - Train Loss: 0.0817, Val Loss: 0.0772\n",
      "Epoch 22/300 - Train Loss: 0.0803, Val Loss: 0.0793\n",
      "Epoch 23/300 - Train Loss: 0.0798, Val Loss: 0.0771\n",
      "Epoch 24/300 - Train Loss: 0.0807, Val Loss: 0.0751\n",
      "Epoch 25/300 - Train Loss: 0.0793, Val Loss: 0.0754\n",
      "Epoch 26/300 - Train Loss: 0.0779, Val Loss: 0.0760\n",
      "Epoch 27/300 - Train Loss: 0.0772, Val Loss: 0.0762\n",
      "Epoch 28/300 - Train Loss: 0.0775, Val Loss: 0.0762\n",
      "Epoch 29/300 - Train Loss: 0.0778, Val Loss: 0.0732\n",
      "Epoch 30/300 - Train Loss: 0.0768, Val Loss: 0.0762\n",
      "Epoch 31/300 - Train Loss: 0.0751, Val Loss: 0.0810\n",
      "Epoch 32/300 - Train Loss: 0.0767, Val Loss: 0.0743\n",
      "Epoch 33/300 - Train Loss: 0.0751, Val Loss: 0.0746\n",
      "Epoch 34/300 - Train Loss: 0.0756, Val Loss: 0.0751\n",
      "Epoch 35/300 - Train Loss: 0.0737, Val Loss: 0.0744\n",
      "Epoch 36/300 - Train Loss: 0.0737, Val Loss: 0.0748\n",
      "Epoch 37/300 - Train Loss: 0.0744, Val Loss: 0.0776\n",
      "Epoch 38/300 - Train Loss: 0.0734, Val Loss: 0.0755\n",
      "Epoch 39/300 - Train Loss: 0.0728, Val Loss: 0.0760\n",
      "Epoch 40/300 - Train Loss: 0.0726, Val Loss: 0.0771\n",
      "Epoch 41/300 - Train Loss: 0.0726, Val Loss: 0.0733\n",
      "Epoch 42/300 - Train Loss: 0.0718, Val Loss: 0.0729\n",
      "Epoch 43/300 - Train Loss: 0.0714, Val Loss: 0.0748\n",
      "Epoch 44/300 - Train Loss: 0.0702, Val Loss: 0.0726\n",
      "Epoch 45/300 - Train Loss: 0.0741, Val Loss: 0.0732\n",
      "Epoch 46/300 - Train Loss: 0.0730, Val Loss: 0.0752\n",
      "Epoch 47/300 - Train Loss: 0.0706, Val Loss: 0.0722\n",
      "Epoch 48/300 - Train Loss: 0.0699, Val Loss: 0.0725\n",
      "Epoch 49/300 - Train Loss: 0.0710, Val Loss: 0.0750\n",
      "Epoch 50/300 - Train Loss: 0.0693, Val Loss: 0.0732\n",
      "Epoch 51/300 - Train Loss: 0.0688, Val Loss: 0.0729\n",
      "Epoch 52/300 - Train Loss: 0.0684, Val Loss: 0.0753\n",
      "Epoch 53/300 - Train Loss: 0.0686, Val Loss: 0.0744\n",
      "Epoch 54/300 - Train Loss: 0.0683, Val Loss: 0.0736\n",
      "Epoch 55/300 - Train Loss: 0.0685, Val Loss: 0.0755\n",
      "Epoch 56/300 - Train Loss: 0.0689, Val Loss: 0.0733\n",
      "Epoch 57/300 - Train Loss: 0.0672, Val Loss: 0.0738\n",
      "Epoch 58/300 - Train Loss: 0.0671, Val Loss: 0.0772\n",
      "Epoch 59/300 - Train Loss: 0.0688, Val Loss: 0.0754\n",
      "Epoch 60/300 - Train Loss: 0.0690, Val Loss: 0.0754\n",
      "Epoch 61/300 - Train Loss: 0.0671, Val Loss: 0.0728\n",
      "Epoch 62/300 - Train Loss: 0.0660, Val Loss: 0.0740\n",
      "Epoch 63/300 - Train Loss: 0.0671, Val Loss: 0.0712\n",
      "Epoch 64/300 - Train Loss: 0.0673, Val Loss: 0.0741\n",
      "Epoch 65/300 - Train Loss: 0.0656, Val Loss: 0.0767\n",
      "Epoch 66/300 - Train Loss: 0.0661, Val Loss: 0.0731\n",
      "Epoch 67/300 - Train Loss: 0.0650, Val Loss: 0.0750\n",
      "Epoch 68/300 - Train Loss: 0.0669, Val Loss: 0.0765\n",
      "Epoch 69/300 - Train Loss: 0.0662, Val Loss: 0.0727\n",
      "Epoch 70/300 - Train Loss: 0.0645, Val Loss: 0.0745\n",
      "Epoch 71/300 - Train Loss: 0.0647, Val Loss: 0.0736\n",
      "Epoch 72/300 - Train Loss: 0.0657, Val Loss: 0.0723\n",
      "Epoch 73/300 - Train Loss: 0.0645, Val Loss: 0.0763\n",
      "Epoch 74/300 - Train Loss: 0.0654, Val Loss: 0.0759\n",
      "Epoch 75/300 - Train Loss: 0.0643, Val Loss: 0.0758\n",
      "Epoch 76/300 - Train Loss: 0.0628, Val Loss: 0.0749\n",
      "Epoch 77/300 - Train Loss: 0.0651, Val Loss: 0.0775\n",
      "Epoch 78/300 - Train Loss: 0.0631, Val Loss: 0.0741\n",
      "Epoch 79/300 - Train Loss: 0.0628, Val Loss: 0.0737\n",
      "Epoch 80/300 - Train Loss: 0.0631, Val Loss: 0.0707\n",
      "Epoch 81/300 - Train Loss: 0.0631, Val Loss: 0.0750\n",
      "Epoch 82/300 - Train Loss: 0.0636, Val Loss: 0.0714\n",
      "Epoch 83/300 - Train Loss: 0.0626, Val Loss: 0.0728\n",
      "Epoch 84/300 - Train Loss: 0.0621, Val Loss: 0.0712\n",
      "Epoch 85/300 - Train Loss: 0.0626, Val Loss: 0.0726\n",
      "Epoch 86/300 - Train Loss: 0.0636, Val Loss: 0.0717\n",
      "Epoch 87/300 - Train Loss: 0.0640, Val Loss: 0.0732\n",
      "Epoch 88/300 - Train Loss: 0.0633, Val Loss: 0.0714\n",
      "Epoch 89/300 - Train Loss: 0.0612, Val Loss: 0.0735\n",
      "Epoch 90/300 - Train Loss: 0.0622, Val Loss: 0.0735\n",
      "Epoch 91/300 - Train Loss: 0.0640, Val Loss: 0.0772\n",
      "Epoch 92/300 - Train Loss: 0.0613, Val Loss: 0.0733\n",
      "Epoch 93/300 - Train Loss: 0.0611, Val Loss: 0.0748\n",
      "Epoch 94/300 - Train Loss: 0.0601, Val Loss: 0.0729\n",
      "Epoch 95/300 - Train Loss: 0.0598, Val Loss: 0.0738\n",
      "Epoch 96/300 - Train Loss: 0.0600, Val Loss: 0.0755\n",
      "Epoch 97/300 - Train Loss: 0.0614, Val Loss: 0.0732\n",
      "Epoch 98/300 - Train Loss: 0.0598, Val Loss: 0.0730\n",
      "Epoch 99/300 - Train Loss: 0.0633, Val Loss: 0.0732\n",
      "Epoch 100/300 - Train Loss: 0.0609, Val Loss: 0.0751\n",
      "Epoch 101/300 - Train Loss: 0.0600, Val Loss: 0.0728\n",
      "Epoch 102/300 - Train Loss: 0.0592, Val Loss: 0.0724\n",
      "Epoch 103/300 - Train Loss: 0.0584, Val Loss: 0.0732\n",
      "Epoch 104/300 - Train Loss: 0.0582, Val Loss: 0.0708\n",
      "Epoch 105/300 - Train Loss: 0.0592, Val Loss: 0.0722\n",
      "Epoch 106/300 - Train Loss: 0.0578, Val Loss: 0.0753\n",
      "Epoch 107/300 - Train Loss: 0.0588, Val Loss: 0.0732\n",
      "Epoch 108/300 - Train Loss: 0.0602, Val Loss: 0.0742\n",
      "Epoch 109/300 - Train Loss: 0.0596, Val Loss: 0.0712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:18:30,672] Trial 446 finished with value: 0.9653846508606123 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.19343418616130958, 'learning_rate': 7.252259501925036e-05, 'batch_size': 256, 'weight_decay': 4.308981873951814e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 110/300 - Train Loss: 0.0586, Val Loss: 0.0742\n",
      "Early stopping at epoch 110\n",
      "Macro F1 Score: 0.9654, Macro Precision: 0.9549, Macro Recall: 0.9768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 448\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1814940713461006, LR=0.000988791100285624, BS=32, WD=2.9584568995458077e-05\n",
      "Epoch 1/300 - Train Loss: 0.1427, Val Loss: 0.0961\n",
      "Epoch 2/300 - Train Loss: 0.0991, Val Loss: 0.0865\n",
      "Epoch 3/300 - Train Loss: 0.0936, Val Loss: 0.0701\n",
      "Epoch 4/300 - Train Loss: 0.0898, Val Loss: 0.0733\n",
      "Epoch 5/300 - Train Loss: 0.0899, Val Loss: 0.0894\n",
      "Epoch 6/300 - Train Loss: 0.0889, Val Loss: 0.0721\n",
      "Epoch 7/300 - Train Loss: 0.0808, Val Loss: 0.0785\n",
      "Epoch 8/300 - Train Loss: 0.0803, Val Loss: 0.0800\n",
      "Epoch 9/300 - Train Loss: 0.0785, Val Loss: 0.0728\n",
      "Epoch 10/300 - Train Loss: 0.0794, Val Loss: 0.0752\n",
      "Epoch 11/300 - Train Loss: 0.0769, Val Loss: 0.0810\n",
      "Epoch 12/300 - Train Loss: 0.0753, Val Loss: 0.0727\n",
      "Epoch 13/300 - Train Loss: 0.0749, Val Loss: 0.0790\n",
      "Epoch 14/300 - Train Loss: 0.0738, Val Loss: 0.0864\n",
      "Epoch 15/300 - Train Loss: 0.0729, Val Loss: 0.0769\n",
      "Epoch 16/300 - Train Loss: 0.0715, Val Loss: 0.0763\n",
      "Epoch 17/300 - Train Loss: 0.0709, Val Loss: 0.0822\n",
      "Epoch 18/300 - Train Loss: 0.0702, Val Loss: 0.0812\n",
      "Epoch 19/300 - Train Loss: 0.0664, Val Loss: 0.0827\n",
      "Epoch 20/300 - Train Loss: 0.0664, Val Loss: 0.0924\n",
      "Epoch 21/300 - Train Loss: 0.0652, Val Loss: 0.0728\n",
      "Epoch 22/300 - Train Loss: 0.0663, Val Loss: 0.0864\n",
      "Epoch 23/300 - Train Loss: 0.0665, Val Loss: 0.0776\n",
      "Epoch 24/300 - Train Loss: 0.0646, Val Loss: 0.0852\n",
      "Epoch 25/300 - Train Loss: 0.0644, Val Loss: 0.0799\n",
      "Epoch 26/300 - Train Loss: 0.0622, Val Loss: 0.0773\n",
      "Epoch 27/300 - Train Loss: 0.0608, Val Loss: 0.0792\n",
      "Epoch 28/300 - Train Loss: 0.0626, Val Loss: 0.0817\n",
      "Epoch 29/300 - Train Loss: 0.0597, Val Loss: 0.0847\n",
      "Epoch 30/300 - Train Loss: 0.0578, Val Loss: 0.0852\n",
      "Epoch 31/300 - Train Loss: 0.0590, Val Loss: 0.0860\n",
      "Epoch 32/300 - Train Loss: 0.0556, Val Loss: 0.0841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:20:01,004] Trial 447 finished with value: 0.9628153310867745 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1814940713461006, 'learning_rate': 0.000988791100285624, 'batch_size': 32, 'weight_decay': 2.9584568995458077e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/300 - Train Loss: 0.0541, Val Loss: 0.0815\n",
      "Early stopping at epoch 33\n",
      "Macro F1 Score: 0.9628, Macro Precision: 0.9764, Macro Recall: 0.9505\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.96      0.89      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.95      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 449\n",
      "Training with F1=16, F2=8, D=2, dropout=0.1267082597003396, LR=0.0009033571892881018, BS=32, WD=6.383434092023769e-05\n",
      "Epoch 1/300 - Train Loss: 0.1579, Val Loss: 0.0780\n",
      "Epoch 2/300 - Train Loss: 0.1011, Val Loss: 0.0769\n",
      "Epoch 3/300 - Train Loss: 0.0956, Val Loss: 0.0772\n",
      "Epoch 4/300 - Train Loss: 0.0907, Val Loss: 0.0695\n",
      "Epoch 5/300 - Train Loss: 0.0893, Val Loss: 0.0666\n",
      "Epoch 6/300 - Train Loss: 0.0886, Val Loss: 0.0683\n",
      "Epoch 7/300 - Train Loss: 0.0873, Val Loss: 0.0756\n",
      "Epoch 8/300 - Train Loss: 0.0858, Val Loss: 0.0811\n",
      "Epoch 9/300 - Train Loss: 0.0846, Val Loss: 0.0697\n",
      "Epoch 10/300 - Train Loss: 0.0815, Val Loss: 0.0719\n",
      "Epoch 11/300 - Train Loss: 0.0834, Val Loss: 0.0763\n",
      "Epoch 12/300 - Train Loss: 0.0810, Val Loss: 0.0777\n",
      "Epoch 13/300 - Train Loss: 0.0794, Val Loss: 0.0667\n",
      "Epoch 14/300 - Train Loss: 0.0786, Val Loss: 0.0739\n",
      "Epoch 15/300 - Train Loss: 0.0797, Val Loss: 0.0696\n",
      "Epoch 16/300 - Train Loss: 0.0795, Val Loss: 0.0717\n",
      "Epoch 17/300 - Train Loss: 0.0811, Val Loss: 0.0857\n",
      "Epoch 18/300 - Train Loss: 0.0808, Val Loss: 0.0754\n",
      "Epoch 19/300 - Train Loss: 0.0806, Val Loss: 0.0784\n",
      "Epoch 20/300 - Train Loss: 0.0768, Val Loss: 0.0729\n",
      "Epoch 21/300 - Train Loss: 0.0781, Val Loss: 0.0824\n",
      "Epoch 22/300 - Train Loss: 0.0760, Val Loss: 0.0741\n",
      "Epoch 23/300 - Train Loss: 0.0735, Val Loss: 0.0744\n",
      "Epoch 24/300 - Train Loss: 0.0766, Val Loss: 0.0828\n",
      "Epoch 25/300 - Train Loss: 0.0751, Val Loss: 0.0761\n",
      "Epoch 26/300 - Train Loss: 0.0741, Val Loss: 0.0739\n",
      "Epoch 27/300 - Train Loss: 0.0758, Val Loss: 0.0693\n",
      "Epoch 28/300 - Train Loss: 0.0747, Val Loss: 0.0717\n",
      "Epoch 29/300 - Train Loss: 0.0750, Val Loss: 0.0700\n",
      "Epoch 30/300 - Train Loss: 0.0714, Val Loss: 0.0742\n",
      "Epoch 31/300 - Train Loss: 0.0742, Val Loss: 0.0744\n",
      "Epoch 32/300 - Train Loss: 0.0736, Val Loss: 0.0691\n",
      "Epoch 33/300 - Train Loss: 0.0737, Val Loss: 0.0710\n",
      "Epoch 34/300 - Train Loss: 0.0712, Val Loss: 0.0735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:21:14,544] Trial 448 finished with value: 0.9645384677934229 and parameters: {'F1': 16, 'F2': 8, 'D': 2, 'dropout': 0.1267082597003396, 'learning_rate': 0.0009033571892881018, 'batch_size': 32, 'weight_decay': 6.383434092023769e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/300 - Train Loss: 0.0717, Val Loss: 0.0729\n",
      "Early stopping at epoch 35\n",
      "Macro F1 Score: 0.9645, Macro Precision: 0.9632, Macro Recall: 0.9661\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.93      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 450\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1585281758685721, LR=1.6930449572938558e-05, BS=32, WD=3.747366094550786e-05\n",
      "Epoch 1/300 - Train Loss: 0.5644, Val Loss: 0.3092\n",
      "Epoch 2/300 - Train Loss: 0.2682, Val Loss: 0.1997\n",
      "Epoch 3/300 - Train Loss: 0.1998, Val Loss: 0.1661\n",
      "Epoch 4/300 - Train Loss: 0.1720, Val Loss: 0.1384\n",
      "Epoch 5/300 - Train Loss: 0.1519, Val Loss: 0.1225\n",
      "Epoch 6/300 - Train Loss: 0.1370, Val Loss: 0.1157\n",
      "Epoch 7/300 - Train Loss: 0.1272, Val Loss: 0.1046\n",
      "Epoch 8/300 - Train Loss: 0.1185, Val Loss: 0.1020\n",
      "Epoch 9/300 - Train Loss: 0.1161, Val Loss: 0.0944\n",
      "Epoch 10/300 - Train Loss: 0.1091, Val Loss: 0.0922\n",
      "Epoch 11/300 - Train Loss: 0.1082, Val Loss: 0.0902\n",
      "Epoch 12/300 - Train Loss: 0.1073, Val Loss: 0.0899\n",
      "Epoch 13/300 - Train Loss: 0.1010, Val Loss: 0.0865\n",
      "Epoch 14/300 - Train Loss: 0.0986, Val Loss: 0.0869\n",
      "Epoch 15/300 - Train Loss: 0.0990, Val Loss: 0.0847\n",
      "Epoch 16/300 - Train Loss: 0.0996, Val Loss: 0.0866\n",
      "Epoch 17/300 - Train Loss: 0.0959, Val Loss: 0.0873\n",
      "Epoch 18/300 - Train Loss: 0.0946, Val Loss: 0.0832\n",
      "Epoch 19/300 - Train Loss: 0.0958, Val Loss: 0.0819\n",
      "Epoch 20/300 - Train Loss: 0.0932, Val Loss: 0.0826\n",
      "Epoch 21/300 - Train Loss: 0.0943, Val Loss: 0.0821\n",
      "Epoch 22/300 - Train Loss: 0.0908, Val Loss: 0.0776\n",
      "Epoch 23/300 - Train Loss: 0.0933, Val Loss: 0.0788\n",
      "Epoch 24/300 - Train Loss: 0.0902, Val Loss: 0.0789\n",
      "Epoch 25/300 - Train Loss: 0.0901, Val Loss: 0.0745\n",
      "Epoch 26/300 - Train Loss: 0.0902, Val Loss: 0.0856\n",
      "Epoch 27/300 - Train Loss: 0.0918, Val Loss: 0.0806\n",
      "Epoch 28/300 - Train Loss: 0.0894, Val Loss: 0.0777\n",
      "Epoch 29/300 - Train Loss: 0.0887, Val Loss: 0.0783\n",
      "Epoch 30/300 - Train Loss: 0.0881, Val Loss: 0.0758\n",
      "Epoch 31/300 - Train Loss: 0.0891, Val Loss: 0.0782\n",
      "Epoch 32/300 - Train Loss: 0.0872, Val Loss: 0.0786\n",
      "Epoch 33/300 - Train Loss: 0.0880, Val Loss: 0.0763\n",
      "Epoch 34/300 - Train Loss: 0.0872, Val Loss: 0.0769\n",
      "Epoch 35/300 - Train Loss: 0.0862, Val Loss: 0.0754\n",
      "Epoch 36/300 - Train Loss: 0.0861, Val Loss: 0.0736\n",
      "Epoch 37/300 - Train Loss: 0.0828, Val Loss: 0.0768\n",
      "Epoch 38/300 - Train Loss: 0.0865, Val Loss: 0.0736\n",
      "Epoch 39/300 - Train Loss: 0.0836, Val Loss: 0.0777\n",
      "Epoch 40/300 - Train Loss: 0.0830, Val Loss: 0.0789\n",
      "Epoch 41/300 - Train Loss: 0.0840, Val Loss: 0.0780\n",
      "Epoch 42/300 - Train Loss: 0.0830, Val Loss: 0.0783\n",
      "Epoch 43/300 - Train Loss: 0.0811, Val Loss: 0.0728\n",
      "Epoch 44/300 - Train Loss: 0.0819, Val Loss: 0.0782\n",
      "Epoch 45/300 - Train Loss: 0.0827, Val Loss: 0.0749\n",
      "Epoch 46/300 - Train Loss: 0.0818, Val Loss: 0.0761\n",
      "Epoch 47/300 - Train Loss: 0.0817, Val Loss: 0.0731\n",
      "Epoch 48/300 - Train Loss: 0.0811, Val Loss: 0.0778\n",
      "Epoch 49/300 - Train Loss: 0.0813, Val Loss: 0.0797\n",
      "Epoch 50/300 - Train Loss: 0.0804, Val Loss: 0.0742\n",
      "Epoch 51/300 - Train Loss: 0.0807, Val Loss: 0.0721\n",
      "Epoch 52/300 - Train Loss: 0.0802, Val Loss: 0.0718\n",
      "Epoch 53/300 - Train Loss: 0.0814, Val Loss: 0.0738\n",
      "Epoch 54/300 - Train Loss: 0.0789, Val Loss: 0.0742\n",
      "Epoch 55/300 - Train Loss: 0.0799, Val Loss: 0.0721\n",
      "Epoch 56/300 - Train Loss: 0.0806, Val Loss: 0.0709\n",
      "Epoch 57/300 - Train Loss: 0.0796, Val Loss: 0.0770\n",
      "Epoch 58/300 - Train Loss: 0.0778, Val Loss: 0.0752\n",
      "Epoch 59/300 - Train Loss: 0.0779, Val Loss: 0.0787\n",
      "Epoch 60/300 - Train Loss: 0.0771, Val Loss: 0.0768\n",
      "Epoch 61/300 - Train Loss: 0.0789, Val Loss: 0.0701\n",
      "Epoch 62/300 - Train Loss: 0.0783, Val Loss: 0.0765\n",
      "Epoch 63/300 - Train Loss: 0.0779, Val Loss: 0.0693\n",
      "Epoch 64/300 - Train Loss: 0.0769, Val Loss: 0.0737\n",
      "Epoch 65/300 - Train Loss: 0.0774, Val Loss: 0.0708\n",
      "Epoch 66/300 - Train Loss: 0.0782, Val Loss: 0.0694\n",
      "Epoch 67/300 - Train Loss: 0.0758, Val Loss: 0.0697\n",
      "Epoch 68/300 - Train Loss: 0.0752, Val Loss: 0.0750\n",
      "Epoch 69/300 - Train Loss: 0.0766, Val Loss: 0.0747\n",
      "Epoch 70/300 - Train Loss: 0.0763, Val Loss: 0.0768\n",
      "Epoch 71/300 - Train Loss: 0.0784, Val Loss: 0.0756\n",
      "Epoch 72/300 - Train Loss: 0.0762, Val Loss: 0.0739\n",
      "Epoch 73/300 - Train Loss: 0.0774, Val Loss: 0.0708\n",
      "Epoch 74/300 - Train Loss: 0.0760, Val Loss: 0.0701\n",
      "Epoch 75/300 - Train Loss: 0.0743, Val Loss: 0.0713\n",
      "Epoch 76/300 - Train Loss: 0.0762, Val Loss: 0.0752\n",
      "Epoch 77/300 - Train Loss: 0.0734, Val Loss: 0.0703\n",
      "Epoch 78/300 - Train Loss: 0.0757, Val Loss: 0.0730\n",
      "Epoch 79/300 - Train Loss: 0.0753, Val Loss: 0.0734\n",
      "Epoch 80/300 - Train Loss: 0.0757, Val Loss: 0.0761\n",
      "Epoch 81/300 - Train Loss: 0.0752, Val Loss: 0.0696\n",
      "Epoch 82/300 - Train Loss: 0.0740, Val Loss: 0.0698\n",
      "Epoch 83/300 - Train Loss: 0.0730, Val Loss: 0.0694\n",
      "Epoch 84/300 - Train Loss: 0.0747, Val Loss: 0.0689\n",
      "Epoch 85/300 - Train Loss: 0.0761, Val Loss: 0.0655\n",
      "Epoch 86/300 - Train Loss: 0.0742, Val Loss: 0.0689\n",
      "Epoch 87/300 - Train Loss: 0.0741, Val Loss: 0.0709\n",
      "Epoch 88/300 - Train Loss: 0.0720, Val Loss: 0.0699\n",
      "Epoch 89/300 - Train Loss: 0.0736, Val Loss: 0.0712\n",
      "Epoch 90/300 - Train Loss: 0.0730, Val Loss: 0.0694\n",
      "Epoch 91/300 - Train Loss: 0.0715, Val Loss: 0.0713\n",
      "Epoch 92/300 - Train Loss: 0.0760, Val Loss: 0.0687\n",
      "Epoch 93/300 - Train Loss: 0.0714, Val Loss: 0.0699\n",
      "Epoch 94/300 - Train Loss: 0.0717, Val Loss: 0.0701\n",
      "Epoch 95/300 - Train Loss: 0.0738, Val Loss: 0.0669\n",
      "Epoch 96/300 - Train Loss: 0.0726, Val Loss: 0.0717\n",
      "Epoch 97/300 - Train Loss: 0.0722, Val Loss: 0.0715\n",
      "Epoch 98/300 - Train Loss: 0.0711, Val Loss: 0.0721\n",
      "Epoch 99/300 - Train Loss: 0.0721, Val Loss: 0.0718\n",
      "Epoch 100/300 - Train Loss: 0.0730, Val Loss: 0.0691\n",
      "Epoch 101/300 - Train Loss: 0.0699, Val Loss: 0.0685\n",
      "Epoch 102/300 - Train Loss: 0.0720, Val Loss: 0.0680\n",
      "Epoch 103/300 - Train Loss: 0.0710, Val Loss: 0.0805\n",
      "Epoch 104/300 - Train Loss: 0.0713, Val Loss: 0.0683\n",
      "Epoch 105/300 - Train Loss: 0.0710, Val Loss: 0.0663\n",
      "Epoch 106/300 - Train Loss: 0.0710, Val Loss: 0.0723\n",
      "Epoch 107/300 - Train Loss: 0.0705, Val Loss: 0.0721\n",
      "Epoch 108/300 - Train Loss: 0.0676, Val Loss: 0.0684\n",
      "Epoch 109/300 - Train Loss: 0.0705, Val Loss: 0.0712\n",
      "Epoch 110/300 - Train Loss: 0.0703, Val Loss: 0.0703\n",
      "Epoch 111/300 - Train Loss: 0.0718, Val Loss: 0.0698\n",
      "Epoch 112/300 - Train Loss: 0.0691, Val Loss: 0.0690\n",
      "Epoch 113/300 - Train Loss: 0.0675, Val Loss: 0.0700\n",
      "Epoch 114/300 - Train Loss: 0.0694, Val Loss: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:26:30,584] Trial 449 finished with value: 0.968041518357647 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1585281758685721, 'learning_rate': 1.6930449572938558e-05, 'batch_size': 32, 'weight_decay': 3.747366094550786e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 115/300 - Train Loss: 0.0721, Val Loss: 0.0673\n",
      "Early stopping at epoch 115\n",
      "Macro F1 Score: 0.9680, Macro Precision: 0.9601, Macro Recall: 0.9767\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 451\n",
      "Training with F1=16, F2=32, D=2, dropout=0.1726523530679045, LR=4.9955456479080255e-05, BS=32, WD=1.6230998670388643e-05\n",
      "Epoch 1/300 - Train Loss: 0.4038, Val Loss: 0.1893\n",
      "Epoch 2/300 - Train Loss: 0.1793, Val Loss: 0.1173\n",
      "Epoch 3/300 - Train Loss: 0.1381, Val Loss: 0.1081\n",
      "Epoch 4/300 - Train Loss: 0.1220, Val Loss: 0.0920\n",
      "Epoch 5/300 - Train Loss: 0.1112, Val Loss: 0.0889\n",
      "Epoch 6/300 - Train Loss: 0.1043, Val Loss: 0.0844\n",
      "Epoch 7/300 - Train Loss: 0.1015, Val Loss: 0.0830\n",
      "Epoch 8/300 - Train Loss: 0.0979, Val Loss: 0.0826\n",
      "Epoch 9/300 - Train Loss: 0.0938, Val Loss: 0.0829\n",
      "Epoch 10/300 - Train Loss: 0.0943, Val Loss: 0.0778\n",
      "Epoch 11/300 - Train Loss: 0.0926, Val Loss: 0.0793\n",
      "Epoch 12/300 - Train Loss: 0.0918, Val Loss: 0.0735\n",
      "Epoch 13/300 - Train Loss: 0.0889, Val Loss: 0.0829\n",
      "Epoch 14/300 - Train Loss: 0.0886, Val Loss: 0.0775\n",
      "Epoch 15/300 - Train Loss: 0.0870, Val Loss: 0.0815\n",
      "Epoch 16/300 - Train Loss: 0.0864, Val Loss: 0.0749\n",
      "Epoch 17/300 - Train Loss: 0.0875, Val Loss: 0.0758\n",
      "Epoch 18/300 - Train Loss: 0.0856, Val Loss: 0.0805\n",
      "Epoch 19/300 - Train Loss: 0.0877, Val Loss: 0.0757\n",
      "Epoch 20/300 - Train Loss: 0.0846, Val Loss: 0.0786\n",
      "Epoch 21/300 - Train Loss: 0.0846, Val Loss: 0.0763\n",
      "Epoch 22/300 - Train Loss: 0.0850, Val Loss: 0.0809\n",
      "Epoch 23/300 - Train Loss: 0.0805, Val Loss: 0.0772\n",
      "Epoch 24/300 - Train Loss: 0.0802, Val Loss: 0.0760\n",
      "Epoch 25/300 - Train Loss: 0.0819, Val Loss: 0.0788\n",
      "Epoch 26/300 - Train Loss: 0.0792, Val Loss: 0.0788\n",
      "Epoch 27/300 - Train Loss: 0.0813, Val Loss: 0.0715\n",
      "Epoch 28/300 - Train Loss: 0.0796, Val Loss: 0.0754\n",
      "Epoch 29/300 - Train Loss: 0.0771, Val Loss: 0.0744\n",
      "Epoch 30/300 - Train Loss: 0.0776, Val Loss: 0.0800\n",
      "Epoch 31/300 - Train Loss: 0.0817, Val Loss: 0.0755\n",
      "Epoch 32/300 - Train Loss: 0.0786, Val Loss: 0.0744\n",
      "Epoch 33/300 - Train Loss: 0.0775, Val Loss: 0.0692\n",
      "Epoch 34/300 - Train Loss: 0.0759, Val Loss: 0.0781\n",
      "Epoch 35/300 - Train Loss: 0.0764, Val Loss: 0.0717\n",
      "Epoch 36/300 - Train Loss: 0.0760, Val Loss: 0.0735\n",
      "Epoch 37/300 - Train Loss: 0.0753, Val Loss: 0.0735\n",
      "Epoch 38/300 - Train Loss: 0.0748, Val Loss: 0.0756\n",
      "Epoch 39/300 - Train Loss: 0.0788, Val Loss: 0.0775\n",
      "Epoch 40/300 - Train Loss: 0.0764, Val Loss: 0.0708\n",
      "Epoch 41/300 - Train Loss: 0.0754, Val Loss: 0.0749\n",
      "Epoch 42/300 - Train Loss: 0.0745, Val Loss: 0.0737\n",
      "Epoch 43/300 - Train Loss: 0.0763, Val Loss: 0.0733\n",
      "Epoch 44/300 - Train Loss: 0.0729, Val Loss: 0.0707\n",
      "Epoch 45/300 - Train Loss: 0.0761, Val Loss: 0.0726\n",
      "Epoch 46/300 - Train Loss: 0.0751, Val Loss: 0.0736\n",
      "Epoch 47/300 - Train Loss: 0.0747, Val Loss: 0.0691\n",
      "Epoch 48/300 - Train Loss: 0.0726, Val Loss: 0.0734\n",
      "Epoch 49/300 - Train Loss: 0.0744, Val Loss: 0.0749\n",
      "Epoch 50/300 - Train Loss: 0.0735, Val Loss: 0.0701\n",
      "Epoch 51/300 - Train Loss: 0.0726, Val Loss: 0.0745\n",
      "Epoch 52/300 - Train Loss: 0.0707, Val Loss: 0.0675\n",
      "Epoch 53/300 - Train Loss: 0.0706, Val Loss: 0.0747\n",
      "Epoch 54/300 - Train Loss: 0.0730, Val Loss: 0.0798\n",
      "Epoch 55/300 - Train Loss: 0.0726, Val Loss: 0.0695\n",
      "Epoch 56/300 - Train Loss: 0.0737, Val Loss: 0.0729\n",
      "Epoch 57/300 - Train Loss: 0.0710, Val Loss: 0.0697\n",
      "Epoch 58/300 - Train Loss: 0.0700, Val Loss: 0.0722\n",
      "Epoch 59/300 - Train Loss: 0.0715, Val Loss: 0.0731\n",
      "Epoch 60/300 - Train Loss: 0.0709, Val Loss: 0.0726\n",
      "Epoch 61/300 - Train Loss: 0.0699, Val Loss: 0.0696\n",
      "Epoch 62/300 - Train Loss: 0.0703, Val Loss: 0.0753\n",
      "Epoch 63/300 - Train Loss: 0.0682, Val Loss: 0.0694\n",
      "Epoch 64/300 - Train Loss: 0.0717, Val Loss: 0.0745\n",
      "Epoch 65/300 - Train Loss: 0.0681, Val Loss: 0.0711\n",
      "Epoch 66/300 - Train Loss: 0.0704, Val Loss: 0.0711\n",
      "Epoch 67/300 - Train Loss: 0.0702, Val Loss: 0.0722\n",
      "Epoch 68/300 - Train Loss: 0.0693, Val Loss: 0.0725\n",
      "Epoch 69/300 - Train Loss: 0.0681, Val Loss: 0.0706\n",
      "Epoch 70/300 - Train Loss: 0.0675, Val Loss: 0.0735\n",
      "Epoch 71/300 - Train Loss: 0.0696, Val Loss: 0.0714\n",
      "Epoch 72/300 - Train Loss: 0.0694, Val Loss: 0.0681\n",
      "Epoch 73/300 - Train Loss: 0.0704, Val Loss: 0.0796\n",
      "Epoch 74/300 - Train Loss: 0.0674, Val Loss: 0.0704\n",
      "Epoch 75/300 - Train Loss: 0.0681, Val Loss: 0.0707\n",
      "Epoch 76/300 - Train Loss: 0.0672, Val Loss: 0.0712\n",
      "Epoch 77/300 - Train Loss: 0.0684, Val Loss: 0.0710\n",
      "Epoch 78/300 - Train Loss: 0.0693, Val Loss: 0.0695\n",
      "Epoch 79/300 - Train Loss: 0.0665, Val Loss: 0.0748\n",
      "Epoch 80/300 - Train Loss: 0.0662, Val Loss: 0.0697\n",
      "Epoch 81/300 - Train Loss: 0.0692, Val Loss: 0.0775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:29:26,467] Trial 450 finished with value: 0.9659894584474024 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.1726523530679045, 'learning_rate': 4.9955456479080255e-05, 'batch_size': 32, 'weight_decay': 1.6230998670388643e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/300 - Train Loss: 0.0652, Val Loss: 0.0700\n",
      "Early stopping at epoch 82\n",
      "Macro F1 Score: 0.9660, Macro Precision: 0.9600, Macro Recall: 0.9724\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 452\n",
      "Training with F1=32, F2=8, D=8, dropout=0.21499446672329184, LR=0.0008235647186241445, BS=32, WD=5.307383881347495e-05\n",
      "Epoch 1/300 - Train Loss: 0.1570, Val Loss: 0.1206\n",
      "Epoch 2/300 - Train Loss: 0.0972, Val Loss: 0.0698\n",
      "Epoch 3/300 - Train Loss: 0.0908, Val Loss: 0.0704\n",
      "Epoch 4/300 - Train Loss: 0.0907, Val Loss: 0.0730\n",
      "Epoch 5/300 - Train Loss: 0.0868, Val Loss: 0.0721\n",
      "Epoch 6/300 - Train Loss: 0.0859, Val Loss: 0.0689\n",
      "Epoch 7/300 - Train Loss: 0.0824, Val Loss: 0.0873\n",
      "Epoch 8/300 - Train Loss: 0.0840, Val Loss: 0.0698\n",
      "Epoch 9/300 - Train Loss: 0.0826, Val Loss: 0.0751\n",
      "Epoch 10/300 - Train Loss: 0.0798, Val Loss: 0.0652\n",
      "Epoch 11/300 - Train Loss: 0.0783, Val Loss: 0.0681\n",
      "Epoch 12/300 - Train Loss: 0.0791, Val Loss: 0.0803\n",
      "Epoch 13/300 - Train Loss: 0.0774, Val Loss: 0.0681\n",
      "Epoch 14/300 - Train Loss: 0.0778, Val Loss: 0.0682\n",
      "Epoch 15/300 - Train Loss: 0.0756, Val Loss: 0.0707\n",
      "Epoch 16/300 - Train Loss: 0.0755, Val Loss: 0.0672\n",
      "Epoch 17/300 - Train Loss: 0.0772, Val Loss: 0.0687\n",
      "Epoch 18/300 - Train Loss: 0.0758, Val Loss: 0.0649\n",
      "Epoch 19/300 - Train Loss: 0.0752, Val Loss: 0.0674\n",
      "Epoch 20/300 - Train Loss: 0.0740, Val Loss: 0.0643\n",
      "Epoch 21/300 - Train Loss: 0.0745, Val Loss: 0.0749\n",
      "Epoch 22/300 - Train Loss: 0.0728, Val Loss: 0.0642\n",
      "Epoch 23/300 - Train Loss: 0.0706, Val Loss: 0.0644\n",
      "Epoch 24/300 - Train Loss: 0.0721, Val Loss: 0.0690\n",
      "Epoch 25/300 - Train Loss: 0.0728, Val Loss: 0.0665\n",
      "Epoch 26/300 - Train Loss: 0.0722, Val Loss: 0.0633\n",
      "Epoch 27/300 - Train Loss: 0.0706, Val Loss: 0.0704\n",
      "Epoch 28/300 - Train Loss: 0.0712, Val Loss: 0.0673\n",
      "Epoch 29/300 - Train Loss: 0.0694, Val Loss: 0.0718\n",
      "Epoch 30/300 - Train Loss: 0.0711, Val Loss: 0.0651\n",
      "Epoch 31/300 - Train Loss: 0.0724, Val Loss: 0.0668\n",
      "Epoch 32/300 - Train Loss: 0.0689, Val Loss: 0.0754\n",
      "Epoch 33/300 - Train Loss: 0.0684, Val Loss: 0.0694\n",
      "Epoch 34/300 - Train Loss: 0.0684, Val Loss: 0.0697\n",
      "Epoch 35/300 - Train Loss: 0.0691, Val Loss: 0.0741\n",
      "Epoch 36/300 - Train Loss: 0.0692, Val Loss: 0.0649\n",
      "Epoch 37/300 - Train Loss: 0.0690, Val Loss: 0.0657\n",
      "Epoch 38/300 - Train Loss: 0.0675, Val Loss: 0.0611\n",
      "Epoch 39/300 - Train Loss: 0.0654, Val Loss: 0.0752\n",
      "Epoch 40/300 - Train Loss: 0.0688, Val Loss: 0.0626\n",
      "Epoch 41/300 - Train Loss: 0.0670, Val Loss: 0.0674\n",
      "Epoch 42/300 - Train Loss: 0.0673, Val Loss: 0.0670\n",
      "Epoch 43/300 - Train Loss: 0.0690, Val Loss: 0.0750\n",
      "Epoch 44/300 - Train Loss: 0.0669, Val Loss: 0.0762\n",
      "Epoch 45/300 - Train Loss: 0.0648, Val Loss: 0.0747\n",
      "Epoch 46/300 - Train Loss: 0.0656, Val Loss: 0.0692\n",
      "Epoch 47/300 - Train Loss: 0.0660, Val Loss: 0.0671\n",
      "Epoch 48/300 - Train Loss: 0.0661, Val Loss: 0.0701\n",
      "Epoch 49/300 - Train Loss: 0.0660, Val Loss: 0.0722\n",
      "Epoch 50/300 - Train Loss: 0.0689, Val Loss: 0.0661\n",
      "Epoch 51/300 - Train Loss: 0.0661, Val Loss: 0.0675\n",
      "Epoch 52/300 - Train Loss: 0.0651, Val Loss: 0.0657\n",
      "Epoch 53/300 - Train Loss: 0.0642, Val Loss: 0.0768\n",
      "Epoch 54/300 - Train Loss: 0.0640, Val Loss: 0.0754\n",
      "Epoch 55/300 - Train Loss: 0.0668, Val Loss: 0.0694\n",
      "Epoch 56/300 - Train Loss: 0.0667, Val Loss: 0.0694\n",
      "Epoch 57/300 - Train Loss: 0.0649, Val Loss: 0.0710\n",
      "Epoch 58/300 - Train Loss: 0.0643, Val Loss: 0.0730\n",
      "Epoch 59/300 - Train Loss: 0.0617, Val Loss: 0.0705\n",
      "Epoch 60/300 - Train Loss: 0.0637, Val Loss: 0.0694\n",
      "Epoch 61/300 - Train Loss: 0.0626, Val Loss: 0.0719\n",
      "Epoch 62/300 - Train Loss: 0.0644, Val Loss: 0.0698\n",
      "Epoch 63/300 - Train Loss: 0.0639, Val Loss: 0.0715\n",
      "Epoch 64/300 - Train Loss: 0.0626, Val Loss: 0.0702\n",
      "Epoch 65/300 - Train Loss: 0.0640, Val Loss: 0.0718\n",
      "Epoch 66/300 - Train Loss: 0.0632, Val Loss: 0.0751\n",
      "Epoch 67/300 - Train Loss: 0.0636, Val Loss: 0.0667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:35:56,407] Trial 451 finished with value: 0.9668016368766814 and parameters: {'F1': 32, 'F2': 8, 'D': 8, 'dropout': 0.21499446672329184, 'learning_rate': 0.0008235647186241445, 'batch_size': 32, 'weight_decay': 5.307383881347495e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/300 - Train Loss: 0.0653, Val Loss: 0.0705\n",
      "Early stopping at epoch 68\n",
      "Macro F1 Score: 0.9668, Macro Precision: 0.9627, Macro Recall: 0.9711\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 453\n",
      "Training with F1=32, F2=32, D=4, dropout=0.13711074064261353, LR=7.851920756317113e-05, BS=32, WD=4.531703369232466e-05\n",
      "Epoch 1/300 - Train Loss: 0.2711, Val Loss: 0.1209\n",
      "Epoch 2/300 - Train Loss: 0.1197, Val Loss: 0.0902\n",
      "Epoch 3/300 - Train Loss: 0.1011, Val Loss: 0.0805\n",
      "Epoch 4/300 - Train Loss: 0.0953, Val Loss: 0.0754\n",
      "Epoch 5/300 - Train Loss: 0.0909, Val Loss: 0.0749\n",
      "Epoch 6/300 - Train Loss: 0.0879, Val Loss: 0.0881\n",
      "Epoch 7/300 - Train Loss: 0.0878, Val Loss: 0.0827\n",
      "Epoch 8/300 - Train Loss: 0.0861, Val Loss: 0.0737\n",
      "Epoch 9/300 - Train Loss: 0.0888, Val Loss: 0.0707\n",
      "Epoch 10/300 - Train Loss: 0.0811, Val Loss: 0.0766\n",
      "Epoch 11/300 - Train Loss: 0.0812, Val Loss: 0.0735\n",
      "Epoch 12/300 - Train Loss: 0.0796, Val Loss: 0.0732\n",
      "Epoch 13/300 - Train Loss: 0.0784, Val Loss: 0.0685\n",
      "Epoch 14/300 - Train Loss: 0.0770, Val Loss: 0.0735\n",
      "Epoch 15/300 - Train Loss: 0.0765, Val Loss: 0.0666\n",
      "Epoch 16/300 - Train Loss: 0.0750, Val Loss: 0.0683\n",
      "Epoch 17/300 - Train Loss: 0.0742, Val Loss: 0.0673\n",
      "Epoch 18/300 - Train Loss: 0.0735, Val Loss: 0.0761\n",
      "Epoch 19/300 - Train Loss: 0.0714, Val Loss: 0.0668\n",
      "Epoch 20/300 - Train Loss: 0.0721, Val Loss: 0.0683\n",
      "Epoch 21/300 - Train Loss: 0.0720, Val Loss: 0.0667\n",
      "Epoch 22/300 - Train Loss: 0.0705, Val Loss: 0.0750\n",
      "Epoch 23/300 - Train Loss: 0.0728, Val Loss: 0.0853\n",
      "Epoch 24/300 - Train Loss: 0.0693, Val Loss: 0.0685\n",
      "Epoch 25/300 - Train Loss: 0.0677, Val Loss: 0.0662\n",
      "Epoch 26/300 - Train Loss: 0.0686, Val Loss: 0.0657\n",
      "Epoch 27/300 - Train Loss: 0.0656, Val Loss: 0.0658\n",
      "Epoch 28/300 - Train Loss: 0.0664, Val Loss: 0.0691\n",
      "Epoch 29/300 - Train Loss: 0.0662, Val Loss: 0.0743\n",
      "Epoch 30/300 - Train Loss: 0.0659, Val Loss: 0.0655\n",
      "Epoch 31/300 - Train Loss: 0.0650, Val Loss: 0.0693\n",
      "Epoch 32/300 - Train Loss: 0.0666, Val Loss: 0.0671\n",
      "Epoch 33/300 - Train Loss: 0.0641, Val Loss: 0.0689\n",
      "Epoch 34/300 - Train Loss: 0.0648, Val Loss: 0.0680\n",
      "Epoch 35/300 - Train Loss: 0.0624, Val Loss: 0.0672\n",
      "Epoch 36/300 - Train Loss: 0.0636, Val Loss: 0.0697\n",
      "Epoch 37/300 - Train Loss: 0.0604, Val Loss: 0.0704\n",
      "Epoch 38/300 - Train Loss: 0.0612, Val Loss: 0.0666\n",
      "Epoch 39/300 - Train Loss: 0.0591, Val Loss: 0.0654\n",
      "Epoch 40/300 - Train Loss: 0.0598, Val Loss: 0.0697\n",
      "Epoch 41/300 - Train Loss: 0.0599, Val Loss: 0.0737\n",
      "Epoch 42/300 - Train Loss: 0.0601, Val Loss: 0.0654\n",
      "Epoch 43/300 - Train Loss: 0.0588, Val Loss: 0.0670\n",
      "Epoch 44/300 - Train Loss: 0.0587, Val Loss: 0.0681\n",
      "Epoch 45/300 - Train Loss: 0.0582, Val Loss: 0.0684\n",
      "Epoch 46/300 - Train Loss: 0.0585, Val Loss: 0.0694\n",
      "Epoch 47/300 - Train Loss: 0.0595, Val Loss: 0.0676\n",
      "Epoch 48/300 - Train Loss: 0.0558, Val Loss: 0.0735\n",
      "Epoch 49/300 - Train Loss: 0.0572, Val Loss: 0.0697\n",
      "Epoch 50/300 - Train Loss: 0.0547, Val Loss: 0.0688\n",
      "Epoch 51/300 - Train Loss: 0.0539, Val Loss: 0.0672\n",
      "Epoch 52/300 - Train Loss: 0.0535, Val Loss: 0.0676\n",
      "Epoch 53/300 - Train Loss: 0.0559, Val Loss: 0.0682\n",
      "Epoch 54/300 - Train Loss: 0.0541, Val Loss: 0.0648\n",
      "Epoch 55/300 - Train Loss: 0.0553, Val Loss: 0.0677\n",
      "Epoch 56/300 - Train Loss: 0.0558, Val Loss: 0.0668\n",
      "Epoch 57/300 - Train Loss: 0.0524, Val Loss: 0.0684\n",
      "Epoch 58/300 - Train Loss: 0.0532, Val Loss: 0.0696\n",
      "Epoch 59/300 - Train Loss: 0.0517, Val Loss: 0.0667\n",
      "Epoch 60/300 - Train Loss: 0.0515, Val Loss: 0.0656\n",
      "Epoch 61/300 - Train Loss: 0.0500, Val Loss: 0.0710\n",
      "Epoch 62/300 - Train Loss: 0.0512, Val Loss: 0.0684\n",
      "Epoch 63/300 - Train Loss: 0.0508, Val Loss: 0.0663\n",
      "Epoch 64/300 - Train Loss: 0.0483, Val Loss: 0.0752\n",
      "Epoch 65/300 - Train Loss: 0.0501, Val Loss: 0.0750\n",
      "Epoch 66/300 - Train Loss: 0.0503, Val Loss: 0.0723\n",
      "Epoch 67/300 - Train Loss: 0.0489, Val Loss: 0.0713\n",
      "Epoch 68/300 - Train Loss: 0.0519, Val Loss: 0.0785\n",
      "Epoch 69/300 - Train Loss: 0.0483, Val Loss: 0.0754\n",
      "Epoch 70/300 - Train Loss: 0.0485, Val Loss: 0.0707\n",
      "Epoch 71/300 - Train Loss: 0.0485, Val Loss: 0.0817\n",
      "Epoch 72/300 - Train Loss: 0.0491, Val Loss: 0.0723\n",
      "Epoch 73/300 - Train Loss: 0.0470, Val Loss: 0.0752\n",
      "Epoch 74/300 - Train Loss: 0.0471, Val Loss: 0.0702\n",
      "Epoch 75/300 - Train Loss: 0.0474, Val Loss: 0.0705\n",
      "Epoch 76/300 - Train Loss: 0.0470, Val Loss: 0.0771\n",
      "Epoch 77/300 - Train Loss: 0.0463, Val Loss: 0.0725\n",
      "Epoch 78/300 - Train Loss: 0.0457, Val Loss: 0.0784\n",
      "Epoch 79/300 - Train Loss: 0.0439, Val Loss: 0.0752\n",
      "Epoch 80/300 - Train Loss: 0.0473, Val Loss: 0.0743\n",
      "Epoch 81/300 - Train Loss: 0.0450, Val Loss: 0.0770\n",
      "Epoch 82/300 - Train Loss: 0.0443, Val Loss: 0.0727\n",
      "Epoch 83/300 - Train Loss: 0.0436, Val Loss: 0.0783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:41:39,355] Trial 452 finished with value: 0.9682826851389178 and parameters: {'F1': 32, 'F2': 32, 'D': 4, 'dropout': 0.13711074064261353, 'learning_rate': 7.851920756317113e-05, 'batch_size': 32, 'weight_decay': 4.531703369232466e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300 - Train Loss: 0.0432, Val Loss: 0.0754\n",
      "Early stopping at epoch 84\n",
      "Macro F1 Score: 0.9683, Macro Precision: 0.9641, Macro Recall: 0.9726\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 454\n",
      "Training with F1=16, F2=32, D=2, dropout=0.19654513882867863, LR=0.0007000319496878079, BS=32, WD=6.000932886695463e-05\n",
      "Epoch 1/300 - Train Loss: 0.1540, Val Loss: 0.0898\n",
      "Epoch 2/300 - Train Loss: 0.1024, Val Loss: 0.0805\n",
      "Epoch 3/300 - Train Loss: 0.0973, Val Loss: 0.0786\n",
      "Epoch 4/300 - Train Loss: 0.0953, Val Loss: 0.0735\n",
      "Epoch 5/300 - Train Loss: 0.0916, Val Loss: 0.0909\n",
      "Epoch 6/300 - Train Loss: 0.0886, Val Loss: 0.0828\n",
      "Epoch 7/300 - Train Loss: 0.0889, Val Loss: 0.0718\n",
      "Epoch 8/300 - Train Loss: 0.0870, Val Loss: 0.0730\n",
      "Epoch 9/300 - Train Loss: 0.0834, Val Loss: 0.0701\n",
      "Epoch 10/300 - Train Loss: 0.0847, Val Loss: 0.0770\n",
      "Epoch 11/300 - Train Loss: 0.0833, Val Loss: 0.0732\n",
      "Epoch 12/300 - Train Loss: 0.0811, Val Loss: 0.0752\n",
      "Epoch 13/300 - Train Loss: 0.0792, Val Loss: 0.0774\n",
      "Epoch 14/300 - Train Loss: 0.0800, Val Loss: 0.0760\n",
      "Epoch 15/300 - Train Loss: 0.0782, Val Loss: 0.0784\n",
      "Epoch 16/300 - Train Loss: 0.0756, Val Loss: 0.0768\n",
      "Epoch 17/300 - Train Loss: 0.0766, Val Loss: 0.0828\n",
      "Epoch 18/300 - Train Loss: 0.0770, Val Loss: 0.0797\n",
      "Epoch 19/300 - Train Loss: 0.0772, Val Loss: 0.0869\n",
      "Epoch 20/300 - Train Loss: 0.0730, Val Loss: 0.0867\n",
      "Epoch 21/300 - Train Loss: 0.0738, Val Loss: 0.0735\n",
      "Epoch 22/300 - Train Loss: 0.0711, Val Loss: 0.0786\n",
      "Epoch 23/300 - Train Loss: 0.0713, Val Loss: 0.0776\n",
      "Epoch 24/300 - Train Loss: 0.0710, Val Loss: 0.0754\n",
      "Epoch 25/300 - Train Loss: 0.0691, Val Loss: 0.0757\n",
      "Epoch 26/300 - Train Loss: 0.0672, Val Loss: 0.0772\n",
      "Epoch 27/300 - Train Loss: 0.0701, Val Loss: 0.0882\n",
      "Epoch 28/300 - Train Loss: 0.0659, Val Loss: 0.0753\n",
      "Epoch 29/300 - Train Loss: 0.0660, Val Loss: 0.0917\n",
      "Epoch 30/300 - Train Loss: 0.0651, Val Loss: 0.0785\n",
      "Epoch 31/300 - Train Loss: 0.0683, Val Loss: 0.0798\n",
      "Epoch 32/300 - Train Loss: 0.0665, Val Loss: 0.0820\n",
      "Epoch 33/300 - Train Loss: 0.0659, Val Loss: 0.0841\n",
      "Epoch 34/300 - Train Loss: 0.0649, Val Loss: 0.0898\n",
      "Epoch 35/300 - Train Loss: 0.0656, Val Loss: 0.0801\n",
      "Epoch 36/300 - Train Loss: 0.0638, Val Loss: 0.0806\n",
      "Epoch 37/300 - Train Loss: 0.0656, Val Loss: 0.0827\n",
      "Epoch 38/300 - Train Loss: 0.0643, Val Loss: 0.0806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:43:02,397] Trial 453 finished with value: 0.9543501634940856 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.19654513882867863, 'learning_rate': 0.0007000319496878079, 'batch_size': 32, 'weight_decay': 6.000932886695463e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300 - Train Loss: 0.0592, Val Loss: 0.0831\n",
      "Early stopping at epoch 39\n",
      "Macro F1 Score: 0.9544, Macro Precision: 0.9647, Macro Recall: 0.9448\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.93      0.87      0.90        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.94      0.95      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 455\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1580727831375942, LR=6.654772556859842e-05, BS=32, WD=3.243704683069958e-05\n",
      "Epoch 1/300 - Train Loss: 0.4544, Val Loss: 0.2094\n",
      "Epoch 2/300 - Train Loss: 0.1789, Val Loss: 0.1227\n",
      "Epoch 3/300 - Train Loss: 0.1291, Val Loss: 0.0956\n",
      "Epoch 4/300 - Train Loss: 0.1132, Val Loss: 0.0925\n",
      "Epoch 5/300 - Train Loss: 0.1067, Val Loss: 0.0893\n",
      "Epoch 6/300 - Train Loss: 0.1040, Val Loss: 0.0923\n",
      "Epoch 7/300 - Train Loss: 0.1022, Val Loss: 0.0748\n",
      "Epoch 8/300 - Train Loss: 0.0974, Val Loss: 0.0703\n",
      "Epoch 9/300 - Train Loss: 0.0979, Val Loss: 0.0729\n",
      "Epoch 10/300 - Train Loss: 0.0943, Val Loss: 0.0707\n",
      "Epoch 11/300 - Train Loss: 0.0937, Val Loss: 0.0685\n",
      "Epoch 12/300 - Train Loss: 0.0924, Val Loss: 0.0685\n",
      "Epoch 13/300 - Train Loss: 0.0904, Val Loss: 0.0701\n",
      "Epoch 14/300 - Train Loss: 0.0908, Val Loss: 0.0711\n",
      "Epoch 15/300 - Train Loss: 0.0877, Val Loss: 0.0716\n",
      "Epoch 16/300 - Train Loss: 0.0895, Val Loss: 0.0740\n",
      "Epoch 17/300 - Train Loss: 0.0875, Val Loss: 0.0743\n",
      "Epoch 18/300 - Train Loss: 0.0874, Val Loss: 0.0678\n",
      "Epoch 19/300 - Train Loss: 0.0862, Val Loss: 0.0728\n",
      "Epoch 20/300 - Train Loss: 0.0855, Val Loss: 0.0725\n",
      "Epoch 21/300 - Train Loss: 0.0849, Val Loss: 0.0681\n",
      "Epoch 22/300 - Train Loss: 0.0841, Val Loss: 0.0687\n",
      "Epoch 23/300 - Train Loss: 0.0828, Val Loss: 0.0703\n",
      "Epoch 24/300 - Train Loss: 0.0833, Val Loss: 0.0711\n",
      "Epoch 25/300 - Train Loss: 0.0844, Val Loss: 0.0717\n",
      "Epoch 26/300 - Train Loss: 0.0830, Val Loss: 0.0669\n",
      "Epoch 27/300 - Train Loss: 0.0820, Val Loss: 0.0689\n",
      "Epoch 28/300 - Train Loss: 0.0830, Val Loss: 0.0702\n",
      "Epoch 29/300 - Train Loss: 0.0824, Val Loss: 0.0704\n",
      "Epoch 30/300 - Train Loss: 0.0821, Val Loss: 0.0687\n",
      "Epoch 31/300 - Train Loss: 0.0837, Val Loss: 0.0674\n",
      "Epoch 32/300 - Train Loss: 0.0819, Val Loss: 0.0681\n",
      "Epoch 33/300 - Train Loss: 0.0818, Val Loss: 0.0697\n",
      "Epoch 34/300 - Train Loss: 0.0813, Val Loss: 0.0721\n",
      "Epoch 35/300 - Train Loss: 0.0804, Val Loss: 0.0667\n",
      "Epoch 36/300 - Train Loss: 0.0790, Val Loss: 0.0669\n",
      "Epoch 37/300 - Train Loss: 0.0800, Val Loss: 0.0674\n",
      "Epoch 38/300 - Train Loss: 0.0792, Val Loss: 0.0683\n",
      "Epoch 39/300 - Train Loss: 0.0774, Val Loss: 0.0684\n",
      "Epoch 40/300 - Train Loss: 0.0808, Val Loss: 0.0670\n",
      "Epoch 41/300 - Train Loss: 0.0798, Val Loss: 0.0675\n",
      "Epoch 42/300 - Train Loss: 0.0809, Val Loss: 0.0695\n",
      "Epoch 43/300 - Train Loss: 0.0807, Val Loss: 0.0688\n",
      "Epoch 44/300 - Train Loss: 0.0810, Val Loss: 0.0726\n",
      "Epoch 45/300 - Train Loss: 0.0766, Val Loss: 0.0695\n",
      "Epoch 46/300 - Train Loss: 0.0765, Val Loss: 0.0715\n",
      "Epoch 47/300 - Train Loss: 0.0769, Val Loss: 0.0705\n",
      "Epoch 48/300 - Train Loss: 0.0776, Val Loss: 0.0682\n",
      "Epoch 49/300 - Train Loss: 0.0756, Val Loss: 0.0708\n",
      "Epoch 50/300 - Train Loss: 0.0790, Val Loss: 0.0691\n",
      "Epoch 51/300 - Train Loss: 0.0760, Val Loss: 0.0674\n",
      "Epoch 52/300 - Train Loss: 0.0770, Val Loss: 0.0656\n",
      "Epoch 53/300 - Train Loss: 0.0756, Val Loss: 0.0686\n",
      "Epoch 54/300 - Train Loss: 0.0770, Val Loss: 0.0739\n",
      "Epoch 55/300 - Train Loss: 0.0758, Val Loss: 0.0689\n",
      "Epoch 56/300 - Train Loss: 0.0767, Val Loss: 0.0682\n",
      "Epoch 57/300 - Train Loss: 0.0753, Val Loss: 0.0688\n",
      "Epoch 58/300 - Train Loss: 0.0771, Val Loss: 0.0679\n",
      "Epoch 59/300 - Train Loss: 0.0763, Val Loss: 0.0654\n",
      "Epoch 60/300 - Train Loss: 0.0745, Val Loss: 0.0670\n",
      "Epoch 61/300 - Train Loss: 0.0764, Val Loss: 0.0665\n",
      "Epoch 62/300 - Train Loss: 0.0756, Val Loss: 0.0694\n",
      "Epoch 63/300 - Train Loss: 0.0757, Val Loss: 0.0661\n",
      "Epoch 64/300 - Train Loss: 0.0733, Val Loss: 0.0652\n",
      "Epoch 65/300 - Train Loss: 0.0742, Val Loss: 0.0689\n",
      "Epoch 66/300 - Train Loss: 0.0743, Val Loss: 0.0675\n",
      "Epoch 67/300 - Train Loss: 0.0740, Val Loss: 0.0669\n",
      "Epoch 68/300 - Train Loss: 0.0743, Val Loss: 0.0715\n",
      "Epoch 69/300 - Train Loss: 0.0749, Val Loss: 0.0713\n",
      "Epoch 70/300 - Train Loss: 0.0730, Val Loss: 0.0656\n",
      "Epoch 71/300 - Train Loss: 0.0740, Val Loss: 0.0637\n",
      "Epoch 72/300 - Train Loss: 0.0727, Val Loss: 0.0683\n",
      "Epoch 73/300 - Train Loss: 0.0733, Val Loss: 0.0677\n",
      "Epoch 74/300 - Train Loss: 0.0750, Val Loss: 0.0803\n",
      "Epoch 75/300 - Train Loss: 0.0713, Val Loss: 0.0659\n",
      "Epoch 76/300 - Train Loss: 0.0711, Val Loss: 0.0658\n",
      "Epoch 77/300 - Train Loss: 0.0721, Val Loss: 0.0689\n",
      "Epoch 78/300 - Train Loss: 0.0744, Val Loss: 0.0665\n",
      "Epoch 79/300 - Train Loss: 0.0711, Val Loss: 0.0657\n",
      "Epoch 80/300 - Train Loss: 0.0745, Val Loss: 0.0717\n",
      "Epoch 81/300 - Train Loss: 0.0715, Val Loss: 0.0646\n",
      "Epoch 82/300 - Train Loss: 0.0750, Val Loss: 0.0699\n",
      "Epoch 83/300 - Train Loss: 0.0714, Val Loss: 0.0675\n",
      "Epoch 84/300 - Train Loss: 0.0716, Val Loss: 0.0655\n",
      "Epoch 85/300 - Train Loss: 0.0711, Val Loss: 0.0641\n",
      "Epoch 86/300 - Train Loss: 0.0710, Val Loss: 0.0679\n",
      "Epoch 87/300 - Train Loss: 0.0707, Val Loss: 0.0653\n",
      "Epoch 88/300 - Train Loss: 0.0710, Val Loss: 0.0681\n",
      "Epoch 89/300 - Train Loss: 0.0709, Val Loss: 0.0639\n",
      "Epoch 90/300 - Train Loss: 0.0696, Val Loss: 0.0679\n",
      "Epoch 91/300 - Train Loss: 0.0696, Val Loss: 0.0666\n",
      "Epoch 92/300 - Train Loss: 0.0707, Val Loss: 0.0724\n",
      "Epoch 93/300 - Train Loss: 0.0699, Val Loss: 0.0655\n",
      "Epoch 94/300 - Train Loss: 0.0688, Val Loss: 0.0694\n",
      "Epoch 95/300 - Train Loss: 0.0712, Val Loss: 0.0681\n",
      "Epoch 96/300 - Train Loss: 0.0690, Val Loss: 0.0703\n",
      "Epoch 97/300 - Train Loss: 0.0707, Val Loss: 0.0664\n",
      "Epoch 98/300 - Train Loss: 0.0706, Val Loss: 0.0686\n",
      "Epoch 99/300 - Train Loss: 0.0693, Val Loss: 0.0689\n",
      "Epoch 100/300 - Train Loss: 0.0702, Val Loss: 0.0676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:47:15,763] Trial 454 finished with value: 0.968126338788172 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1580727831375942, 'learning_rate': 6.654772556859842e-05, 'batch_size': 32, 'weight_decay': 3.243704683069958e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/300 - Train Loss: 0.0690, Val Loss: 0.0704\n",
      "Early stopping at epoch 101\n",
      "Macro F1 Score: 0.9681, Macro Precision: 0.9644, Macro Recall: 0.9721\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 456\n",
      "Training with F1=32, F2=32, D=8, dropout=0.10301530244011761, LR=3.021794525442352e-05, BS=32, WD=4.0057668023285024e-05\n",
      "Epoch 1/300 - Train Loss: 0.3327, Val Loss: 0.1869\n",
      "Epoch 2/300 - Train Loss: 0.1516, Val Loss: 0.0982\n",
      "Epoch 3/300 - Train Loss: 0.1185, Val Loss: 0.0986\n",
      "Epoch 4/300 - Train Loss: 0.1068, Val Loss: 0.0873\n",
      "Epoch 5/300 - Train Loss: 0.1008, Val Loss: 0.0865\n",
      "Epoch 6/300 - Train Loss: 0.0960, Val Loss: 0.0787\n",
      "Epoch 7/300 - Train Loss: 0.0963, Val Loss: 0.0735\n",
      "Epoch 8/300 - Train Loss: 0.0918, Val Loss: 0.0817\n",
      "Epoch 9/300 - Train Loss: 0.0902, Val Loss: 0.0758\n",
      "Epoch 10/300 - Train Loss: 0.0871, Val Loss: 0.0706\n",
      "Epoch 11/300 - Train Loss: 0.0873, Val Loss: 0.0734\n",
      "Epoch 12/300 - Train Loss: 0.0834, Val Loss: 0.0759\n",
      "Epoch 13/300 - Train Loss: 0.0830, Val Loss: 0.0709\n",
      "Epoch 14/300 - Train Loss: 0.0810, Val Loss: 0.0668\n",
      "Epoch 15/300 - Train Loss: 0.0790, Val Loss: 0.0669\n",
      "Epoch 16/300 - Train Loss: 0.0770, Val Loss: 0.0716\n",
      "Epoch 17/300 - Train Loss: 0.0787, Val Loss: 0.0672\n",
      "Epoch 18/300 - Train Loss: 0.0776, Val Loss: 0.0636\n",
      "Epoch 19/300 - Train Loss: 0.0785, Val Loss: 0.0698\n",
      "Epoch 20/300 - Train Loss: 0.0763, Val Loss: 0.0725\n",
      "Epoch 21/300 - Train Loss: 0.0753, Val Loss: 0.0692\n",
      "Epoch 22/300 - Train Loss: 0.0763, Val Loss: 0.0743\n",
      "Epoch 23/300 - Train Loss: 0.0746, Val Loss: 0.0658\n",
      "Epoch 24/300 - Train Loss: 0.0737, Val Loss: 0.0639\n",
      "Epoch 25/300 - Train Loss: 0.0726, Val Loss: 0.0738\n",
      "Epoch 26/300 - Train Loss: 0.0758, Val Loss: 0.0666\n",
      "Epoch 27/300 - Train Loss: 0.0705, Val Loss: 0.0685\n",
      "Epoch 28/300 - Train Loss: 0.0718, Val Loss: 0.0708\n",
      "Epoch 29/300 - Train Loss: 0.0728, Val Loss: 0.0682\n",
      "Epoch 30/300 - Train Loss: 0.0685, Val Loss: 0.0674\n",
      "Epoch 31/300 - Train Loss: 0.0693, Val Loss: 0.0722\n",
      "Epoch 32/300 - Train Loss: 0.0686, Val Loss: 0.0816\n",
      "Epoch 33/300 - Train Loss: 0.0687, Val Loss: 0.0679\n",
      "Epoch 34/300 - Train Loss: 0.0670, Val Loss: 0.0675\n",
      "Epoch 35/300 - Train Loss: 0.0677, Val Loss: 0.0719\n",
      "Epoch 36/300 - Train Loss: 0.0674, Val Loss: 0.0628\n",
      "Epoch 37/300 - Train Loss: 0.0672, Val Loss: 0.0638\n",
      "Epoch 38/300 - Train Loss: 0.0640, Val Loss: 0.0708\n",
      "Epoch 39/300 - Train Loss: 0.0653, Val Loss: 0.0634\n",
      "Epoch 40/300 - Train Loss: 0.0655, Val Loss: 0.0662\n",
      "Epoch 41/300 - Train Loss: 0.0615, Val Loss: 0.0728\n",
      "Epoch 42/300 - Train Loss: 0.0650, Val Loss: 0.0810\n",
      "Epoch 43/300 - Train Loss: 0.0653, Val Loss: 0.0641\n",
      "Epoch 44/300 - Train Loss: 0.0618, Val Loss: 0.0648\n",
      "Epoch 45/300 - Train Loss: 0.0643, Val Loss: 0.0658\n",
      "Epoch 46/300 - Train Loss: 0.0631, Val Loss: 0.0639\n",
      "Epoch 47/300 - Train Loss: 0.0605, Val Loss: 0.0663\n",
      "Epoch 48/300 - Train Loss: 0.0603, Val Loss: 0.0633\n",
      "Epoch 49/300 - Train Loss: 0.0613, Val Loss: 0.0630\n",
      "Epoch 50/300 - Train Loss: 0.0605, Val Loss: 0.0641\n",
      "Epoch 51/300 - Train Loss: 0.0610, Val Loss: 0.0669\n",
      "Epoch 52/300 - Train Loss: 0.0578, Val Loss: 0.0643\n",
      "Epoch 53/300 - Train Loss: 0.0614, Val Loss: 0.0648\n",
      "Epoch 54/300 - Train Loss: 0.0605, Val Loss: 0.0806\n",
      "Epoch 55/300 - Train Loss: 0.0590, Val Loss: 0.0659\n",
      "Epoch 56/300 - Train Loss: 0.0583, Val Loss: 0.0638\n",
      "Epoch 57/300 - Train Loss: 0.0580, Val Loss: 0.0636\n",
      "Epoch 58/300 - Train Loss: 0.0567, Val Loss: 0.0708\n",
      "Epoch 59/300 - Train Loss: 0.0560, Val Loss: 0.0636\n",
      "Epoch 60/300 - Train Loss: 0.0570, Val Loss: 0.0632\n",
      "Epoch 61/300 - Train Loss: 0.0558, Val Loss: 0.0708\n",
      "Epoch 62/300 - Train Loss: 0.0565, Val Loss: 0.0638\n",
      "Epoch 63/300 - Train Loss: 0.0553, Val Loss: 0.0666\n",
      "Epoch 64/300 - Train Loss: 0.0558, Val Loss: 0.0648\n",
      "Epoch 65/300 - Train Loss: 0.0553, Val Loss: 0.0695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:54:25,405] Trial 455 finished with value: 0.9683779348420019 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.10301530244011761, 'learning_rate': 3.021794525442352e-05, 'batch_size': 32, 'weight_decay': 4.0057668023285024e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/300 - Train Loss: 0.0534, Val Loss: 0.0696\n",
      "Early stopping at epoch 66\n",
      "Macro F1 Score: 0.9684, Macro Precision: 0.9560, Macro Recall: 0.9822\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.98      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 457\n",
      "Training with F1=16, F2=32, D=2, dropout=0.1817045571617448, LR=2.620391387822242e-05, BS=32, WD=6.959845172937338e-05\n",
      "Epoch 1/300 - Train Loss: 0.5224, Val Loss: 0.2717\n",
      "Epoch 2/300 - Train Loss: 0.2397, Val Loss: 0.1821\n",
      "Epoch 3/300 - Train Loss: 0.1889, Val Loss: 0.1435\n",
      "Epoch 4/300 - Train Loss: 0.1628, Val Loss: 0.1241\n",
      "Epoch 5/300 - Train Loss: 0.1416, Val Loss: 0.1187\n",
      "Epoch 6/300 - Train Loss: 0.1316, Val Loss: 0.1070\n",
      "Epoch 7/300 - Train Loss: 0.1251, Val Loss: 0.0980\n",
      "Epoch 8/300 - Train Loss: 0.1176, Val Loss: 0.0985\n",
      "Epoch 9/300 - Train Loss: 0.1125, Val Loss: 0.0920\n",
      "Epoch 10/300 - Train Loss: 0.1099, Val Loss: 0.0908\n",
      "Epoch 11/300 - Train Loss: 0.1079, Val Loss: 0.0871\n",
      "Epoch 12/300 - Train Loss: 0.1064, Val Loss: 0.0859\n",
      "Epoch 13/300 - Train Loss: 0.1047, Val Loss: 0.0861\n",
      "Epoch 14/300 - Train Loss: 0.1021, Val Loss: 0.0846\n",
      "Epoch 15/300 - Train Loss: 0.1019, Val Loss: 0.0837\n",
      "Epoch 16/300 - Train Loss: 0.0981, Val Loss: 0.0826\n",
      "Epoch 17/300 - Train Loss: 0.0989, Val Loss: 0.0797\n",
      "Epoch 18/300 - Train Loss: 0.0961, Val Loss: 0.0805\n",
      "Epoch 19/300 - Train Loss: 0.0956, Val Loss: 0.0822\n",
      "Epoch 20/300 - Train Loss: 0.0931, Val Loss: 0.0824\n",
      "Epoch 21/300 - Train Loss: 0.0941, Val Loss: 0.0857\n",
      "Epoch 22/300 - Train Loss: 0.0949, Val Loss: 0.0891\n",
      "Epoch 23/300 - Train Loss: 0.0928, Val Loss: 0.0775\n",
      "Epoch 24/300 - Train Loss: 0.0939, Val Loss: 0.0745\n",
      "Epoch 25/300 - Train Loss: 0.0924, Val Loss: 0.0732\n",
      "Epoch 26/300 - Train Loss: 0.0891, Val Loss: 0.0766\n",
      "Epoch 27/300 - Train Loss: 0.0889, Val Loss: 0.0766\n",
      "Epoch 28/300 - Train Loss: 0.0889, Val Loss: 0.0773\n",
      "Epoch 29/300 - Train Loss: 0.0878, Val Loss: 0.0787\n",
      "Epoch 30/300 - Train Loss: 0.0880, Val Loss: 0.0825\n",
      "Epoch 31/300 - Train Loss: 0.0885, Val Loss: 0.0809\n",
      "Epoch 32/300 - Train Loss: 0.0854, Val Loss: 0.0772\n",
      "Epoch 33/300 - Train Loss: 0.0863, Val Loss: 0.0784\n",
      "Epoch 34/300 - Train Loss: 0.0843, Val Loss: 0.0849\n",
      "Epoch 35/300 - Train Loss: 0.0847, Val Loss: 0.0775\n",
      "Epoch 36/300 - Train Loss: 0.0841, Val Loss: 0.0810\n",
      "Epoch 37/300 - Train Loss: 0.0847, Val Loss: 0.0755\n",
      "Epoch 38/300 - Train Loss: 0.0825, Val Loss: 0.0795\n",
      "Epoch 39/300 - Train Loss: 0.0836, Val Loss: 0.0773\n",
      "Epoch 40/300 - Train Loss: 0.0837, Val Loss: 0.0850\n",
      "Epoch 41/300 - Train Loss: 0.0814, Val Loss: 0.0765\n",
      "Epoch 42/300 - Train Loss: 0.0823, Val Loss: 0.0804\n",
      "Epoch 43/300 - Train Loss: 0.0817, Val Loss: 0.0807\n",
      "Epoch 44/300 - Train Loss: 0.0839, Val Loss: 0.0843\n",
      "Epoch 45/300 - Train Loss: 0.0820, Val Loss: 0.0795\n",
      "Epoch 46/300 - Train Loss: 0.0837, Val Loss: 0.0731\n",
      "Epoch 47/300 - Train Loss: 0.0806, Val Loss: 0.0759\n",
      "Epoch 48/300 - Train Loss: 0.0809, Val Loss: 0.0755\n",
      "Epoch 49/300 - Train Loss: 0.0797, Val Loss: 0.0800\n",
      "Epoch 50/300 - Train Loss: 0.0803, Val Loss: 0.0770\n",
      "Epoch 51/300 - Train Loss: 0.0800, Val Loss: 0.0791\n",
      "Epoch 52/300 - Train Loss: 0.0811, Val Loss: 0.0746\n",
      "Epoch 53/300 - Train Loss: 0.0791, Val Loss: 0.0754\n",
      "Epoch 54/300 - Train Loss: 0.0805, Val Loss: 0.0752\n",
      "Epoch 55/300 - Train Loss: 0.0816, Val Loss: 0.0788\n",
      "Epoch 56/300 - Train Loss: 0.0799, Val Loss: 0.0741\n",
      "Epoch 57/300 - Train Loss: 0.0776, Val Loss: 0.0774\n",
      "Epoch 58/300 - Train Loss: 0.0785, Val Loss: 0.0776\n",
      "Epoch 59/300 - Train Loss: 0.0808, Val Loss: 0.0756\n",
      "Epoch 60/300 - Train Loss: 0.0800, Val Loss: 0.0775\n",
      "Epoch 61/300 - Train Loss: 0.0787, Val Loss: 0.0754\n",
      "Epoch 62/300 - Train Loss: 0.0772, Val Loss: 0.0799\n",
      "Epoch 63/300 - Train Loss: 0.0787, Val Loss: 0.0750\n",
      "Epoch 64/300 - Train Loss: 0.0773, Val Loss: 0.0733\n",
      "Epoch 65/300 - Train Loss: 0.0784, Val Loss: 0.0804\n",
      "Epoch 66/300 - Train Loss: 0.0764, Val Loss: 0.0765\n",
      "Epoch 67/300 - Train Loss: 0.0778, Val Loss: 0.0754\n",
      "Epoch 68/300 - Train Loss: 0.0756, Val Loss: 0.0758\n",
      "Epoch 69/300 - Train Loss: 0.0764, Val Loss: 0.0755\n",
      "Epoch 70/300 - Train Loss: 0.0764, Val Loss: 0.0768\n",
      "Epoch 71/300 - Train Loss: 0.0753, Val Loss: 0.0813\n",
      "Epoch 72/300 - Train Loss: 0.0762, Val Loss: 0.0789\n",
      "Epoch 73/300 - Train Loss: 0.0749, Val Loss: 0.0760\n",
      "Epoch 74/300 - Train Loss: 0.0743, Val Loss: 0.0788\n",
      "Epoch 75/300 - Train Loss: 0.0764, Val Loss: 0.0752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 02:57:08,262] Trial 456 finished with value: 0.9606333695196275 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.1817045571617448, 'learning_rate': 2.620391387822242e-05, 'batch_size': 32, 'weight_decay': 6.959845172937338e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300 - Train Loss: 0.0743, Val Loss: 0.0845\n",
      "Early stopping at epoch 76\n",
      "Macro F1 Score: 0.9606, Macro Precision: 0.9473, Macro Recall: 0.9759\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.87      0.97      0.91        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 458\n",
      "Training with F1=32, F2=8, D=2, dropout=0.22732962614064883, LR=4.548502851643997e-05, BS=32, WD=2.25122529699582e-05\n",
      "Epoch 1/300 - Train Loss: 0.5899, Val Loss: 0.2849\n",
      "Epoch 2/300 - Train Loss: 0.2471, Val Loss: 0.1889\n",
      "Epoch 3/300 - Train Loss: 0.1805, Val Loss: 0.1303\n",
      "Epoch 4/300 - Train Loss: 0.1412, Val Loss: 0.1018\n",
      "Epoch 5/300 - Train Loss: 0.1238, Val Loss: 0.0991\n",
      "Epoch 6/300 - Train Loss: 0.1145, Val Loss: 0.0889\n",
      "Epoch 7/300 - Train Loss: 0.1089, Val Loss: 0.0834\n",
      "Epoch 8/300 - Train Loss: 0.1061, Val Loss: 0.0879\n",
      "Epoch 9/300 - Train Loss: 0.1042, Val Loss: 0.0798\n",
      "Epoch 10/300 - Train Loss: 0.1019, Val Loss: 0.0889\n",
      "Epoch 11/300 - Train Loss: 0.1005, Val Loss: 0.0794\n",
      "Epoch 12/300 - Train Loss: 0.0958, Val Loss: 0.0822\n",
      "Epoch 13/300 - Train Loss: 0.0974, Val Loss: 0.0839\n",
      "Epoch 14/300 - Train Loss: 0.0960, Val Loss: 0.0750\n",
      "Epoch 15/300 - Train Loss: 0.0957, Val Loss: 0.0751\n",
      "Epoch 16/300 - Train Loss: 0.0952, Val Loss: 0.0757\n",
      "Epoch 17/300 - Train Loss: 0.0921, Val Loss: 0.0720\n",
      "Epoch 18/300 - Train Loss: 0.0921, Val Loss: 0.0758\n",
      "Epoch 19/300 - Train Loss: 0.0896, Val Loss: 0.0742\n",
      "Epoch 20/300 - Train Loss: 0.0914, Val Loss: 0.0723\n",
      "Epoch 21/300 - Train Loss: 0.0904, Val Loss: 0.0736\n",
      "Epoch 22/300 - Train Loss: 0.0891, Val Loss: 0.0769\n",
      "Epoch 23/300 - Train Loss: 0.0873, Val Loss: 0.0712\n",
      "Epoch 24/300 - Train Loss: 0.0875, Val Loss: 0.0721\n",
      "Epoch 25/300 - Train Loss: 0.0872, Val Loss: 0.0714\n",
      "Epoch 26/300 - Train Loss: 0.0868, Val Loss: 0.0755\n",
      "Epoch 27/300 - Train Loss: 0.0855, Val Loss: 0.0735\n",
      "Epoch 28/300 - Train Loss: 0.0853, Val Loss: 0.0788\n",
      "Epoch 29/300 - Train Loss: 0.0831, Val Loss: 0.0795\n",
      "Epoch 30/300 - Train Loss: 0.0877, Val Loss: 0.0730\n",
      "Epoch 31/300 - Train Loss: 0.0818, Val Loss: 0.0686\n",
      "Epoch 32/300 - Train Loss: 0.0849, Val Loss: 0.0694\n",
      "Epoch 33/300 - Train Loss: 0.0850, Val Loss: 0.0750\n",
      "Epoch 34/300 - Train Loss: 0.0862, Val Loss: 0.0695\n",
      "Epoch 35/300 - Train Loss: 0.0847, Val Loss: 0.0707\n",
      "Epoch 36/300 - Train Loss: 0.0847, Val Loss: 0.0723\n",
      "Epoch 37/300 - Train Loss: 0.0857, Val Loss: 0.0705\n",
      "Epoch 38/300 - Train Loss: 0.0808, Val Loss: 0.0707\n",
      "Epoch 39/300 - Train Loss: 0.0853, Val Loss: 0.0874\n",
      "Epoch 40/300 - Train Loss: 0.0814, Val Loss: 0.0700\n",
      "Epoch 41/300 - Train Loss: 0.0825, Val Loss: 0.0700\n",
      "Epoch 42/300 - Train Loss: 0.0815, Val Loss: 0.0703\n",
      "Epoch 43/300 - Train Loss: 0.0824, Val Loss: 0.0711\n",
      "Epoch 44/300 - Train Loss: 0.0816, Val Loss: 0.0666\n",
      "Epoch 45/300 - Train Loss: 0.0815, Val Loss: 0.0744\n",
      "Epoch 46/300 - Train Loss: 0.0813, Val Loss: 0.0706\n",
      "Epoch 47/300 - Train Loss: 0.0834, Val Loss: 0.0745\n",
      "Epoch 48/300 - Train Loss: 0.0831, Val Loss: 0.0707\n",
      "Epoch 49/300 - Train Loss: 0.0814, Val Loss: 0.0693\n",
      "Epoch 50/300 - Train Loss: 0.0795, Val Loss: 0.0739\n",
      "Epoch 51/300 - Train Loss: 0.0788, Val Loss: 0.0717\n",
      "Epoch 52/300 - Train Loss: 0.0813, Val Loss: 0.0732\n",
      "Epoch 53/300 - Train Loss: 0.0799, Val Loss: 0.0724\n",
      "Epoch 54/300 - Train Loss: 0.0778, Val Loss: 0.0673\n",
      "Epoch 55/300 - Train Loss: 0.0784, Val Loss: 0.0670\n",
      "Epoch 56/300 - Train Loss: 0.0790, Val Loss: 0.0706\n",
      "Epoch 57/300 - Train Loss: 0.0799, Val Loss: 0.0700\n",
      "Epoch 58/300 - Train Loss: 0.0781, Val Loss: 0.0703\n",
      "Epoch 59/300 - Train Loss: 0.0784, Val Loss: 0.0678\n",
      "Epoch 60/300 - Train Loss: 0.0826, Val Loss: 0.0702\n",
      "Epoch 61/300 - Train Loss: 0.0804, Val Loss: 0.0672\n",
      "Epoch 62/300 - Train Loss: 0.0801, Val Loss: 0.0713\n",
      "Epoch 63/300 - Train Loss: 0.0770, Val Loss: 0.0672\n",
      "Epoch 64/300 - Train Loss: 0.0802, Val Loss: 0.0727\n",
      "Epoch 65/300 - Train Loss: 0.0787, Val Loss: 0.0695\n",
      "Epoch 66/300 - Train Loss: 0.0803, Val Loss: 0.0694\n",
      "Epoch 67/300 - Train Loss: 0.0796, Val Loss: 0.0733\n",
      "Epoch 68/300 - Train Loss: 0.0770, Val Loss: 0.0666\n",
      "Epoch 69/300 - Train Loss: 0.0770, Val Loss: 0.0669\n",
      "Epoch 70/300 - Train Loss: 0.0770, Val Loss: 0.0728\n",
      "Epoch 71/300 - Train Loss: 0.0769, Val Loss: 0.0717\n",
      "Epoch 72/300 - Train Loss: 0.0759, Val Loss: 0.0697\n",
      "Epoch 73/300 - Train Loss: 0.0760, Val Loss: 0.0676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:00:16,526] Trial 457 finished with value: 0.9638898242132692 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.22732962614064883, 'learning_rate': 4.548502851643997e-05, 'batch_size': 32, 'weight_decay': 2.25122529699582e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74/300 - Train Loss: 0.0781, Val Loss: 0.0689\n",
      "Early stopping at epoch 74\n",
      "Macro F1 Score: 0.9639, Macro Precision: 0.9539, Macro Recall: 0.9749\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.89      0.97      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 459\n",
      "Training with F1=16, F2=16, D=8, dropout=0.11878122216277695, LR=0.00013520813649692338, BS=64, WD=4.8070903274319164e-05\n",
      "Epoch 1/300 - Train Loss: 0.2802, Val Loss: 0.1569\n",
      "Epoch 2/300 - Train Loss: 0.1128, Val Loss: 0.0840\n",
      "Epoch 3/300 - Train Loss: 0.0954, Val Loss: 0.0907\n",
      "Epoch 4/300 - Train Loss: 0.0925, Val Loss: 0.0749\n",
      "Epoch 5/300 - Train Loss: 0.0889, Val Loss: 0.0745\n",
      "Epoch 6/300 - Train Loss: 0.0857, Val Loss: 0.0786\n",
      "Epoch 7/300 - Train Loss: 0.0832, Val Loss: 0.0731\n",
      "Epoch 8/300 - Train Loss: 0.0807, Val Loss: 0.0790\n",
      "Epoch 9/300 - Train Loss: 0.0806, Val Loss: 0.0715\n",
      "Epoch 10/300 - Train Loss: 0.0778, Val Loss: 0.0720\n",
      "Epoch 11/300 - Train Loss: 0.0773, Val Loss: 0.0689\n",
      "Epoch 12/300 - Train Loss: 0.0763, Val Loss: 0.0710\n",
      "Epoch 13/300 - Train Loss: 0.0747, Val Loss: 0.0851\n",
      "Epoch 14/300 - Train Loss: 0.0733, Val Loss: 0.0704\n",
      "Epoch 15/300 - Train Loss: 0.0738, Val Loss: 0.0704\n",
      "Epoch 16/300 - Train Loss: 0.0718, Val Loss: 0.0741\n",
      "Epoch 17/300 - Train Loss: 0.0724, Val Loss: 0.0804\n",
      "Epoch 18/300 - Train Loss: 0.0711, Val Loss: 0.0701\n",
      "Epoch 19/300 - Train Loss: 0.0703, Val Loss: 0.0699\n",
      "Epoch 20/300 - Train Loss: 0.0702, Val Loss: 0.0685\n",
      "Epoch 21/300 - Train Loss: 0.0689, Val Loss: 0.0734\n",
      "Epoch 22/300 - Train Loss: 0.0685, Val Loss: 0.0685\n",
      "Epoch 23/300 - Train Loss: 0.0678, Val Loss: 0.0710\n",
      "Epoch 24/300 - Train Loss: 0.0670, Val Loss: 0.0767\n",
      "Epoch 25/300 - Train Loss: 0.0670, Val Loss: 0.0761\n",
      "Epoch 26/300 - Train Loss: 0.0671, Val Loss: 0.0727\n",
      "Epoch 27/300 - Train Loss: 0.0665, Val Loss: 0.0680\n",
      "Epoch 28/300 - Train Loss: 0.0647, Val Loss: 0.0688\n",
      "Epoch 29/300 - Train Loss: 0.0648, Val Loss: 0.0743\n",
      "Epoch 30/300 - Train Loss: 0.0640, Val Loss: 0.0639\n",
      "Epoch 31/300 - Train Loss: 0.0649, Val Loss: 0.0670\n",
      "Epoch 32/300 - Train Loss: 0.0629, Val Loss: 0.0675\n",
      "Epoch 33/300 - Train Loss: 0.0640, Val Loss: 0.0698\n",
      "Epoch 34/300 - Train Loss: 0.0637, Val Loss: 0.0690\n",
      "Epoch 35/300 - Train Loss: 0.0615, Val Loss: 0.0673\n",
      "Epoch 36/300 - Train Loss: 0.0618, Val Loss: 0.0687\n",
      "Epoch 37/300 - Train Loss: 0.0613, Val Loss: 0.0689\n",
      "Epoch 38/300 - Train Loss: 0.0608, Val Loss: 0.0660\n",
      "Epoch 39/300 - Train Loss: 0.0600, Val Loss: 0.0657\n",
      "Epoch 40/300 - Train Loss: 0.0586, Val Loss: 0.0639\n",
      "Epoch 41/300 - Train Loss: 0.0599, Val Loss: 0.0638\n",
      "Epoch 42/300 - Train Loss: 0.0601, Val Loss: 0.0639\n",
      "Epoch 43/300 - Train Loss: 0.0592, Val Loss: 0.0645\n",
      "Epoch 44/300 - Train Loss: 0.0587, Val Loss: 0.0714\n",
      "Epoch 45/300 - Train Loss: 0.0573, Val Loss: 0.0659\n",
      "Epoch 46/300 - Train Loss: 0.0567, Val Loss: 0.0638\n",
      "Epoch 47/300 - Train Loss: 0.0579, Val Loss: 0.0683\n",
      "Epoch 48/300 - Train Loss: 0.0571, Val Loss: 0.0644\n",
      "Epoch 49/300 - Train Loss: 0.0572, Val Loss: 0.0879\n",
      "Epoch 50/300 - Train Loss: 0.0583, Val Loss: 0.0655\n",
      "Epoch 51/300 - Train Loss: 0.0570, Val Loss: 0.0668\n",
      "Epoch 52/300 - Train Loss: 0.0558, Val Loss: 0.0707\n",
      "Epoch 53/300 - Train Loss: 0.0547, Val Loss: 0.0706\n",
      "Epoch 54/300 - Train Loss: 0.0556, Val Loss: 0.0670\n",
      "Epoch 55/300 - Train Loss: 0.0545, Val Loss: 0.0690\n",
      "Epoch 56/300 - Train Loss: 0.0538, Val Loss: 0.0660\n",
      "Epoch 57/300 - Train Loss: 0.0542, Val Loss: 0.0638\n",
      "Epoch 58/300 - Train Loss: 0.0538, Val Loss: 0.0662\n",
      "Epoch 59/300 - Train Loss: 0.0531, Val Loss: 0.0726\n",
      "Epoch 60/300 - Train Loss: 0.0536, Val Loss: 0.0674\n",
      "Epoch 61/300 - Train Loss: 0.0524, Val Loss: 0.0716\n",
      "Epoch 62/300 - Train Loss: 0.0531, Val Loss: 0.0658\n",
      "Epoch 63/300 - Train Loss: 0.0515, Val Loss: 0.0678\n",
      "Epoch 64/300 - Train Loss: 0.0527, Val Loss: 0.0718\n",
      "Epoch 65/300 - Train Loss: 0.0525, Val Loss: 0.0665\n",
      "Epoch 66/300 - Train Loss: 0.0511, Val Loss: 0.0696\n",
      "Epoch 67/300 - Train Loss: 0.0495, Val Loss: 0.0673\n",
      "Epoch 68/300 - Train Loss: 0.0505, Val Loss: 0.0685\n",
      "Epoch 69/300 - Train Loss: 0.0507, Val Loss: 0.0670\n",
      "Epoch 70/300 - Train Loss: 0.0508, Val Loss: 0.0688\n",
      "Epoch 71/300 - Train Loss: 0.0493, Val Loss: 0.0671\n",
      "Epoch 72/300 - Train Loss: 0.0488, Val Loss: 0.0682\n",
      "Epoch 73/300 - Train Loss: 0.0494, Val Loss: 0.0702\n",
      "Epoch 74/300 - Train Loss: 0.0488, Val Loss: 0.0722\n",
      "Epoch 75/300 - Train Loss: 0.0488, Val Loss: 0.0711\n",
      "Epoch 76/300 - Train Loss: 0.0476, Val Loss: 0.0665\n",
      "Epoch 77/300 - Train Loss: 0.0477, Val Loss: 0.0695\n",
      "Epoch 78/300 - Train Loss: 0.0477, Val Loss: 0.0707\n",
      "Epoch 79/300 - Train Loss: 0.0470, Val Loss: 0.0649\n",
      "Epoch 80/300 - Train Loss: 0.0464, Val Loss: 0.0707\n",
      "Epoch 81/300 - Train Loss: 0.0461, Val Loss: 0.0676\n",
      "Epoch 82/300 - Train Loss: 0.0451, Val Loss: 0.0651\n",
      "Epoch 83/300 - Train Loss: 0.0453, Val Loss: 0.0722\n",
      "Epoch 84/300 - Train Loss: 0.0470, Val Loss: 0.0693\n",
      "Epoch 85/300 - Train Loss: 0.0467, Val Loss: 0.0699\n",
      "Epoch 86/300 - Train Loss: 0.0450, Val Loss: 0.0693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:04:49,240] Trial 458 finished with value: 0.9655637981176791 and parameters: {'F1': 16, 'F2': 16, 'D': 8, 'dropout': 0.11878122216277695, 'learning_rate': 0.00013520813649692338, 'batch_size': 64, 'weight_decay': 4.8070903274319164e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300 - Train Loss: 0.0449, Val Loss: 0.0761\n",
      "Early stopping at epoch 87\n",
      "Macro F1 Score: 0.9656, Macro Precision: 0.9578, Macro Recall: 0.9740\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 460\n",
      "Training with F1=32, F2=32, D=2, dropout=0.16746562948774113, LR=0.00010609916441265377, BS=256, WD=8.482788041803328e-05\n",
      "Epoch 1/300 - Train Loss: 0.5640, Val Loss: 0.2814\n",
      "Epoch 2/300 - Train Loss: 0.2344, Val Loss: 0.2052\n",
      "Epoch 3/300 - Train Loss: 0.1848, Val Loss: 0.1660\n",
      "Epoch 4/300 - Train Loss: 0.1495, Val Loss: 0.1347\n",
      "Epoch 5/300 - Train Loss: 0.1258, Val Loss: 0.1098\n",
      "Epoch 6/300 - Train Loss: 0.1087, Val Loss: 0.1061\n",
      "Epoch 7/300 - Train Loss: 0.0999, Val Loss: 0.0890\n",
      "Epoch 8/300 - Train Loss: 0.0920, Val Loss: 0.0822\n",
      "Epoch 9/300 - Train Loss: 0.0890, Val Loss: 0.0771\n",
      "Epoch 10/300 - Train Loss: 0.0865, Val Loss: 0.0805\n",
      "Epoch 11/300 - Train Loss: 0.0825, Val Loss: 0.0792\n",
      "Epoch 12/300 - Train Loss: 0.0834, Val Loss: 0.0773\n",
      "Epoch 13/300 - Train Loss: 0.0807, Val Loss: 0.0774\n",
      "Epoch 14/300 - Train Loss: 0.0800, Val Loss: 0.0766\n",
      "Epoch 15/300 - Train Loss: 0.0785, Val Loss: 0.0735\n",
      "Epoch 16/300 - Train Loss: 0.0777, Val Loss: 0.0745\n",
      "Epoch 17/300 - Train Loss: 0.0768, Val Loss: 0.0786\n",
      "Epoch 18/300 - Train Loss: 0.0747, Val Loss: 0.0752\n",
      "Epoch 19/300 - Train Loss: 0.0754, Val Loss: 0.0722\n",
      "Epoch 20/300 - Train Loss: 0.0738, Val Loss: 0.0740\n",
      "Epoch 21/300 - Train Loss: 0.0744, Val Loss: 0.0724\n",
      "Epoch 22/300 - Train Loss: 0.0754, Val Loss: 0.0782\n",
      "Epoch 23/300 - Train Loss: 0.0736, Val Loss: 0.0723\n",
      "Epoch 24/300 - Train Loss: 0.0714, Val Loss: 0.0716\n",
      "Epoch 25/300 - Train Loss: 0.0704, Val Loss: 0.0715\n",
      "Epoch 26/300 - Train Loss: 0.0701, Val Loss: 0.0723\n",
      "Epoch 27/300 - Train Loss: 0.0706, Val Loss: 0.0720\n",
      "Epoch 28/300 - Train Loss: 0.0694, Val Loss: 0.0719\n",
      "Epoch 29/300 - Train Loss: 0.0690, Val Loss: 0.0717\n",
      "Epoch 30/300 - Train Loss: 0.0699, Val Loss: 0.0721\n",
      "Epoch 31/300 - Train Loss: 0.0709, Val Loss: 0.0755\n",
      "Epoch 32/300 - Train Loss: 0.0694, Val Loss: 0.0732\n",
      "Epoch 33/300 - Train Loss: 0.0697, Val Loss: 0.0744\n",
      "Epoch 34/300 - Train Loss: 0.0701, Val Loss: 0.0724\n",
      "Epoch 35/300 - Train Loss: 0.0689, Val Loss: 0.0720\n",
      "Epoch 36/300 - Train Loss: 0.0678, Val Loss: 0.0704\n",
      "Epoch 37/300 - Train Loss: 0.0668, Val Loss: 0.0722\n",
      "Epoch 38/300 - Train Loss: 0.0666, Val Loss: 0.0725\n",
      "Epoch 39/300 - Train Loss: 0.0658, Val Loss: 0.0709\n",
      "Epoch 40/300 - Train Loss: 0.0664, Val Loss: 0.0721\n",
      "Epoch 41/300 - Train Loss: 0.0656, Val Loss: 0.0712\n",
      "Epoch 42/300 - Train Loss: 0.0659, Val Loss: 0.0708\n",
      "Epoch 43/300 - Train Loss: 0.0647, Val Loss: 0.0712\n",
      "Epoch 44/300 - Train Loss: 0.0655, Val Loss: 0.0740\n",
      "Epoch 45/300 - Train Loss: 0.0645, Val Loss: 0.0723\n",
      "Epoch 46/300 - Train Loss: 0.0647, Val Loss: 0.0693\n",
      "Epoch 47/300 - Train Loss: 0.0683, Val Loss: 0.0723\n",
      "Epoch 48/300 - Train Loss: 0.0642, Val Loss: 0.0694\n",
      "Epoch 49/300 - Train Loss: 0.0632, Val Loss: 0.0717\n",
      "Epoch 50/300 - Train Loss: 0.0637, Val Loss: 0.0778\n",
      "Epoch 51/300 - Train Loss: 0.0630, Val Loss: 0.0727\n",
      "Epoch 52/300 - Train Loss: 0.0644, Val Loss: 0.0689\n",
      "Epoch 53/300 - Train Loss: 0.0626, Val Loss: 0.0714\n",
      "Epoch 54/300 - Train Loss: 0.0624, Val Loss: 0.0729\n",
      "Epoch 55/300 - Train Loss: 0.0635, Val Loss: 0.0721\n",
      "Epoch 56/300 - Train Loss: 0.0635, Val Loss: 0.0755\n",
      "Epoch 57/300 - Train Loss: 0.0616, Val Loss: 0.0703\n",
      "Epoch 58/300 - Train Loss: 0.0619, Val Loss: 0.0731\n",
      "Epoch 59/300 - Train Loss: 0.0611, Val Loss: 0.0715\n",
      "Epoch 60/300 - Train Loss: 0.0624, Val Loss: 0.0712\n",
      "Epoch 61/300 - Train Loss: 0.0605, Val Loss: 0.0701\n",
      "Epoch 62/300 - Train Loss: 0.0607, Val Loss: 0.0723\n",
      "Epoch 63/300 - Train Loss: 0.0599, Val Loss: 0.0726\n",
      "Epoch 64/300 - Train Loss: 0.0617, Val Loss: 0.0720\n",
      "Epoch 65/300 - Train Loss: 0.0589, Val Loss: 0.0702\n",
      "Epoch 66/300 - Train Loss: 0.0580, Val Loss: 0.0698\n",
      "Epoch 67/300 - Train Loss: 0.0581, Val Loss: 0.0737\n",
      "Epoch 68/300 - Train Loss: 0.0593, Val Loss: 0.0693\n",
      "Epoch 69/300 - Train Loss: 0.0590, Val Loss: 0.0691\n",
      "Epoch 70/300 - Train Loss: 0.0568, Val Loss: 0.0714\n",
      "Epoch 71/300 - Train Loss: 0.0595, Val Loss: 0.0727\n",
      "Epoch 72/300 - Train Loss: 0.0577, Val Loss: 0.0718\n",
      "Epoch 73/300 - Train Loss: 0.0579, Val Loss: 0.0685\n",
      "Epoch 74/300 - Train Loss: 0.0574, Val Loss: 0.0719\n",
      "Epoch 75/300 - Train Loss: 0.0570, Val Loss: 0.0713\n",
      "Epoch 76/300 - Train Loss: 0.0572, Val Loss: 0.0744\n",
      "Epoch 77/300 - Train Loss: 0.0569, Val Loss: 0.0716\n",
      "Epoch 78/300 - Train Loss: 0.0554, Val Loss: 0.0694\n",
      "Epoch 79/300 - Train Loss: 0.0555, Val Loss: 0.0706\n",
      "Epoch 80/300 - Train Loss: 0.0558, Val Loss: 0.0705\n",
      "Epoch 81/300 - Train Loss: 0.0571, Val Loss: 0.0717\n",
      "Epoch 82/300 - Train Loss: 0.0562, Val Loss: 0.0721\n",
      "Epoch 83/300 - Train Loss: 0.0552, Val Loss: 0.0722\n",
      "Epoch 84/300 - Train Loss: 0.0548, Val Loss: 0.0691\n",
      "Epoch 85/300 - Train Loss: 0.0555, Val Loss: 0.0739\n",
      "Epoch 86/300 - Train Loss: 0.0575, Val Loss: 0.0725\n",
      "Epoch 87/300 - Train Loss: 0.0550, Val Loss: 0.0709\n",
      "Epoch 88/300 - Train Loss: 0.0539, Val Loss: 0.0749\n",
      "Epoch 89/300 - Train Loss: 0.0547, Val Loss: 0.0698\n",
      "Epoch 90/300 - Train Loss: 0.0554, Val Loss: 0.0703\n",
      "Epoch 91/300 - Train Loss: 0.0532, Val Loss: 0.0701\n",
      "Epoch 92/300 - Train Loss: 0.0544, Val Loss: 0.0699\n",
      "Epoch 93/300 - Train Loss: 0.0544, Val Loss: 0.0724\n",
      "Epoch 94/300 - Train Loss: 0.0542, Val Loss: 0.0710\n",
      "Epoch 95/300 - Train Loss: 0.0520, Val Loss: 0.0716\n",
      "Epoch 96/300 - Train Loss: 0.0529, Val Loss: 0.0705\n",
      "Epoch 97/300 - Train Loss: 0.0548, Val Loss: 0.0718\n",
      "Epoch 98/300 - Train Loss: 0.0530, Val Loss: 0.0724\n",
      "Epoch 99/300 - Train Loss: 0.0509, Val Loss: 0.0716\n",
      "Epoch 100/300 - Train Loss: 0.0538, Val Loss: 0.0719\n",
      "Epoch 101/300 - Train Loss: 0.0525, Val Loss: 0.0697\n",
      "Epoch 102/300 - Train Loss: 0.0511, Val Loss: 0.0727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:08:30,999] Trial 459 finished with value: 0.9627468579026655 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.16746562948774113, 'learning_rate': 0.00010609916441265377, 'batch_size': 256, 'weight_decay': 8.482788041803328e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/300 - Train Loss: 0.0505, Val Loss: 0.0726\n",
      "Early stopping at epoch 103\n",
      "Macro F1 Score: 0.9627, Macro Precision: 0.9506, Macro Recall: 0.9763\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 461\n",
      "Training with F1=32, F2=32, D=2, dropout=0.2041928331840449, LR=1.0956690251658671e-05, BS=32, WD=5.4602197875888794e-05\n",
      "Epoch 1/300 - Train Loss: 0.7721, Val Loss: 0.4809\n",
      "Epoch 2/300 - Train Loss: 0.3779, Val Loss: 0.2961\n",
      "Epoch 3/300 - Train Loss: 0.2704, Val Loss: 0.2351\n",
      "Epoch 4/300 - Train Loss: 0.2194, Val Loss: 0.1892\n",
      "Epoch 5/300 - Train Loss: 0.1901, Val Loss: 0.1585\n",
      "Epoch 6/300 - Train Loss: 0.1699, Val Loss: 0.1352\n",
      "Epoch 7/300 - Train Loss: 0.1551, Val Loss: 0.1267\n",
      "Epoch 8/300 - Train Loss: 0.1430, Val Loss: 0.1161\n",
      "Epoch 9/300 - Train Loss: 0.1309, Val Loss: 0.1121\n",
      "Epoch 10/300 - Train Loss: 0.1233, Val Loss: 0.1151\n",
      "Epoch 11/300 - Train Loss: 0.1206, Val Loss: 0.0971\n",
      "Epoch 12/300 - Train Loss: 0.1139, Val Loss: 0.1041\n",
      "Epoch 13/300 - Train Loss: 0.1121, Val Loss: 0.1009\n",
      "Epoch 14/300 - Train Loss: 0.1111, Val Loss: 0.1004\n",
      "Epoch 15/300 - Train Loss: 0.1069, Val Loss: 0.0858\n",
      "Epoch 16/300 - Train Loss: 0.1079, Val Loss: 0.0852\n",
      "Epoch 17/300 - Train Loss: 0.1039, Val Loss: 0.1052\n",
      "Epoch 18/300 - Train Loss: 0.1055, Val Loss: 0.0950\n",
      "Epoch 19/300 - Train Loss: 0.1037, Val Loss: 0.0931\n",
      "Epoch 20/300 - Train Loss: 0.1006, Val Loss: 0.0848\n",
      "Epoch 21/300 - Train Loss: 0.0997, Val Loss: 0.0866\n",
      "Epoch 22/300 - Train Loss: 0.0977, Val Loss: 0.0859\n",
      "Epoch 23/300 - Train Loss: 0.0974, Val Loss: 0.0847\n",
      "Epoch 24/300 - Train Loss: 0.0999, Val Loss: 0.0877\n",
      "Epoch 25/300 - Train Loss: 0.0970, Val Loss: 0.0803\n",
      "Epoch 26/300 - Train Loss: 0.0969, Val Loss: 0.0827\n",
      "Epoch 27/300 - Train Loss: 0.0957, Val Loss: 0.0859\n",
      "Epoch 28/300 - Train Loss: 0.0952, Val Loss: 0.0796\n",
      "Epoch 29/300 - Train Loss: 0.0976, Val Loss: 0.0800\n",
      "Epoch 30/300 - Train Loss: 0.0958, Val Loss: 0.0836\n",
      "Epoch 31/300 - Train Loss: 0.0958, Val Loss: 0.0860\n",
      "Epoch 32/300 - Train Loss: 0.0950, Val Loss: 0.0798\n",
      "Epoch 33/300 - Train Loss: 0.0939, Val Loss: 0.0770\n",
      "Epoch 34/300 - Train Loss: 0.0934, Val Loss: 0.0798\n",
      "Epoch 35/300 - Train Loss: 0.0935, Val Loss: 0.0838\n",
      "Epoch 36/300 - Train Loss: 0.0909, Val Loss: 0.0776\n",
      "Epoch 37/300 - Train Loss: 0.0957, Val Loss: 0.0751\n",
      "Epoch 38/300 - Train Loss: 0.0926, Val Loss: 0.0809\n",
      "Epoch 39/300 - Train Loss: 0.0915, Val Loss: 0.0801\n",
      "Epoch 40/300 - Train Loss: 0.0922, Val Loss: 0.0744\n",
      "Epoch 41/300 - Train Loss: 0.0920, Val Loss: 0.0770\n",
      "Epoch 42/300 - Train Loss: 0.0913, Val Loss: 0.0761\n",
      "Epoch 43/300 - Train Loss: 0.0888, Val Loss: 0.0772\n",
      "Epoch 44/300 - Train Loss: 0.0906, Val Loss: 0.0757\n",
      "Epoch 45/300 - Train Loss: 0.0882, Val Loss: 0.0772\n",
      "Epoch 46/300 - Train Loss: 0.0870, Val Loss: 0.0845\n",
      "Epoch 47/300 - Train Loss: 0.0877, Val Loss: 0.0819\n",
      "Epoch 48/300 - Train Loss: 0.0870, Val Loss: 0.0747\n",
      "Epoch 49/300 - Train Loss: 0.0884, Val Loss: 0.0804\n",
      "Epoch 50/300 - Train Loss: 0.0878, Val Loss: 0.0761\n",
      "Epoch 51/300 - Train Loss: 0.0868, Val Loss: 0.0784\n",
      "Epoch 52/300 - Train Loss: 0.0871, Val Loss: 0.0717\n",
      "Epoch 53/300 - Train Loss: 0.0880, Val Loss: 0.0723\n",
      "Epoch 54/300 - Train Loss: 0.0873, Val Loss: 0.0833\n",
      "Epoch 55/300 - Train Loss: 0.0848, Val Loss: 0.0794\n",
      "Epoch 56/300 - Train Loss: 0.0853, Val Loss: 0.0761\n",
      "Epoch 57/300 - Train Loss: 0.0884, Val Loss: 0.0705\n",
      "Epoch 58/300 - Train Loss: 0.0873, Val Loss: 0.0783\n",
      "Epoch 59/300 - Train Loss: 0.0847, Val Loss: 0.0759\n",
      "Epoch 60/300 - Train Loss: 0.0824, Val Loss: 0.0811\n",
      "Epoch 61/300 - Train Loss: 0.0832, Val Loss: 0.0756\n",
      "Epoch 62/300 - Train Loss: 0.0854, Val Loss: 0.0770\n",
      "Epoch 63/300 - Train Loss: 0.0852, Val Loss: 0.0779\n",
      "Epoch 64/300 - Train Loss: 0.0854, Val Loss: 0.0812\n",
      "Epoch 65/300 - Train Loss: 0.0833, Val Loss: 0.0763\n",
      "Epoch 66/300 - Train Loss: 0.0818, Val Loss: 0.0763\n",
      "Epoch 67/300 - Train Loss: 0.0824, Val Loss: 0.0789\n",
      "Epoch 68/300 - Train Loss: 0.0837, Val Loss: 0.0742\n",
      "Epoch 69/300 - Train Loss: 0.0839, Val Loss: 0.0711\n",
      "Epoch 70/300 - Train Loss: 0.0827, Val Loss: 0.0742\n",
      "Epoch 71/300 - Train Loss: 0.0842, Val Loss: 0.0719\n",
      "Epoch 72/300 - Train Loss: 0.0829, Val Loss: 0.0756\n",
      "Epoch 73/300 - Train Loss: 0.0828, Val Loss: 0.0760\n",
      "Epoch 74/300 - Train Loss: 0.0815, Val Loss: 0.0719\n",
      "Epoch 75/300 - Train Loss: 0.0841, Val Loss: 0.0771\n",
      "Epoch 76/300 - Train Loss: 0.0816, Val Loss: 0.0731\n",
      "Epoch 77/300 - Train Loss: 0.0835, Val Loss: 0.0749\n",
      "Epoch 78/300 - Train Loss: 0.0800, Val Loss: 0.0724\n",
      "Epoch 79/300 - Train Loss: 0.0833, Val Loss: 0.0740\n",
      "Epoch 80/300 - Train Loss: 0.0798, Val Loss: 0.0745\n",
      "Epoch 81/300 - Train Loss: 0.0815, Val Loss: 0.0726\n",
      "Epoch 82/300 - Train Loss: 0.0813, Val Loss: 0.0743\n",
      "Epoch 83/300 - Train Loss: 0.0814, Val Loss: 0.0751\n",
      "Epoch 84/300 - Train Loss: 0.0801, Val Loss: 0.0725\n",
      "Epoch 85/300 - Train Loss: 0.0794, Val Loss: 0.0737\n",
      "Epoch 86/300 - Train Loss: 0.0829, Val Loss: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:12:29,647] Trial 460 finished with value: 0.9613556802725279 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.2041928331840449, 'learning_rate': 1.0956690251658671e-05, 'batch_size': 32, 'weight_decay': 5.4602197875888794e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/300 - Train Loss: 0.0808, Val Loss: 0.0768\n",
      "Early stopping at epoch 87\n",
      "Macro F1 Score: 0.9614, Macro Precision: 0.9573, Macro Recall: 0.9656\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.90      0.93      0.92        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 462\n",
      "Training with F1=16, F2=8, D=8, dropout=0.45423438650443604, LR=0.0002508917377750261, BS=32, WD=2.729824800784064e-05\n",
      "Epoch 1/300 - Train Loss: 0.2754, Val Loss: 0.1008\n",
      "Epoch 2/300 - Train Loss: 0.1170, Val Loss: 0.0841\n",
      "Epoch 3/300 - Train Loss: 0.1047, Val Loss: 0.0779\n",
      "Epoch 4/300 - Train Loss: 0.1021, Val Loss: 0.0696\n",
      "Epoch 5/300 - Train Loss: 0.0991, Val Loss: 0.0715\n",
      "Epoch 6/300 - Train Loss: 0.0965, Val Loss: 0.0710\n",
      "Epoch 7/300 - Train Loss: 0.0950, Val Loss: 0.0729\n",
      "Epoch 8/300 - Train Loss: 0.0916, Val Loss: 0.0728\n",
      "Epoch 9/300 - Train Loss: 0.0898, Val Loss: 0.0764\n",
      "Epoch 10/300 - Train Loss: 0.0932, Val Loss: 0.0931\n",
      "Epoch 11/300 - Train Loss: 0.0914, Val Loss: 0.0752\n",
      "Epoch 12/300 - Train Loss: 0.0883, Val Loss: 0.0678\n",
      "Epoch 13/300 - Train Loss: 0.0877, Val Loss: 0.0761\n",
      "Epoch 14/300 - Train Loss: 0.0884, Val Loss: 0.0687\n",
      "Epoch 15/300 - Train Loss: 0.0901, Val Loss: 0.0703\n",
      "Epoch 16/300 - Train Loss: 0.0894, Val Loss: 0.0740\n",
      "Epoch 17/300 - Train Loss: 0.0868, Val Loss: 0.0694\n",
      "Epoch 18/300 - Train Loss: 0.0885, Val Loss: 0.0685\n",
      "Epoch 19/300 - Train Loss: 0.0862, Val Loss: 0.0707\n",
      "Epoch 20/300 - Train Loss: 0.0829, Val Loss: 0.0642\n",
      "Epoch 21/300 - Train Loss: 0.0869, Val Loss: 0.0650\n",
      "Epoch 22/300 - Train Loss: 0.0844, Val Loss: 0.0664\n",
      "Epoch 23/300 - Train Loss: 0.0826, Val Loss: 0.0714\n",
      "Epoch 24/300 - Train Loss: 0.0850, Val Loss: 0.0698\n",
      "Epoch 25/300 - Train Loss: 0.0819, Val Loss: 0.0645\n",
      "Epoch 26/300 - Train Loss: 0.0873, Val Loss: 0.0741\n",
      "Epoch 27/300 - Train Loss: 0.0821, Val Loss: 0.0696\n",
      "Epoch 28/300 - Train Loss: 0.0814, Val Loss: 0.0660\n",
      "Epoch 29/300 - Train Loss: 0.0833, Val Loss: 0.0666\n",
      "Epoch 30/300 - Train Loss: 0.0867, Val Loss: 0.0682\n",
      "Epoch 31/300 - Train Loss: 0.0828, Val Loss: 0.0656\n",
      "Epoch 32/300 - Train Loss: 0.0822, Val Loss: 0.0660\n",
      "Epoch 33/300 - Train Loss: 0.0815, Val Loss: 0.0645\n",
      "Epoch 34/300 - Train Loss: 0.0823, Val Loss: 0.0687\n",
      "Epoch 35/300 - Train Loss: 0.0809, Val Loss: 0.0686\n",
      "Epoch 36/300 - Train Loss: 0.0811, Val Loss: 0.0637\n",
      "Epoch 37/300 - Train Loss: 0.0815, Val Loss: 0.0665\n",
      "Epoch 38/300 - Train Loss: 0.0803, Val Loss: 0.0669\n",
      "Epoch 39/300 - Train Loss: 0.0802, Val Loss: 0.0635\n",
      "Epoch 40/300 - Train Loss: 0.0828, Val Loss: 0.0698\n",
      "Epoch 41/300 - Train Loss: 0.0823, Val Loss: 0.0661\n",
      "Epoch 42/300 - Train Loss: 0.0816, Val Loss: 0.0658\n",
      "Epoch 43/300 - Train Loss: 0.0814, Val Loss: 0.0731\n",
      "Epoch 44/300 - Train Loss: 0.0804, Val Loss: 0.0620\n",
      "Epoch 45/300 - Train Loss: 0.0805, Val Loss: 0.0634\n",
      "Epoch 46/300 - Train Loss: 0.0814, Val Loss: 0.0656\n",
      "Epoch 47/300 - Train Loss: 0.0802, Val Loss: 0.0686\n",
      "Epoch 48/300 - Train Loss: 0.0793, Val Loss: 0.0635\n",
      "Epoch 49/300 - Train Loss: 0.0809, Val Loss: 0.0640\n",
      "Epoch 50/300 - Train Loss: 0.0791, Val Loss: 0.0599\n",
      "Epoch 51/300 - Train Loss: 0.0813, Val Loss: 0.0657\n",
      "Epoch 52/300 - Train Loss: 0.0814, Val Loss: 0.0628\n",
      "Epoch 53/300 - Train Loss: 0.0794, Val Loss: 0.0706\n",
      "Epoch 54/300 - Train Loss: 0.0787, Val Loss: 0.0702\n",
      "Epoch 55/300 - Train Loss: 0.0776, Val Loss: 0.0647\n",
      "Epoch 56/300 - Train Loss: 0.0789, Val Loss: 0.0639\n",
      "Epoch 57/300 - Train Loss: 0.0785, Val Loss: 0.0635\n",
      "Epoch 58/300 - Train Loss: 0.0799, Val Loss: 0.0630\n",
      "Epoch 59/300 - Train Loss: 0.0798, Val Loss: 0.0623\n",
      "Epoch 60/300 - Train Loss: 0.0769, Val Loss: 0.0647\n",
      "Epoch 61/300 - Train Loss: 0.0763, Val Loss: 0.0639\n",
      "Epoch 62/300 - Train Loss: 0.0770, Val Loss: 0.0618\n",
      "Epoch 63/300 - Train Loss: 0.0809, Val Loss: 0.0699\n",
      "Epoch 64/300 - Train Loss: 0.0779, Val Loss: 0.0628\n",
      "Epoch 65/300 - Train Loss: 0.0777, Val Loss: 0.0645\n",
      "Epoch 66/300 - Train Loss: 0.0774, Val Loss: 0.0671\n",
      "Epoch 67/300 - Train Loss: 0.0767, Val Loss: 0.0687\n",
      "Epoch 68/300 - Train Loss: 0.0761, Val Loss: 0.0653\n",
      "Epoch 69/300 - Train Loss: 0.0762, Val Loss: 0.0644\n",
      "Epoch 70/300 - Train Loss: 0.0774, Val Loss: 0.0645\n",
      "Epoch 71/300 - Train Loss: 0.0784, Val Loss: 0.0693\n",
      "Epoch 72/300 - Train Loss: 0.0755, Val Loss: 0.0681\n",
      "Epoch 73/300 - Train Loss: 0.0768, Val Loss: 0.0644\n",
      "Epoch 74/300 - Train Loss: 0.0770, Val Loss: 0.0638\n",
      "Epoch 75/300 - Train Loss: 0.0775, Val Loss: 0.0644\n",
      "Epoch 76/300 - Train Loss: 0.0773, Val Loss: 0.0647\n",
      "Epoch 77/300 - Train Loss: 0.0767, Val Loss: 0.0628\n",
      "Epoch 78/300 - Train Loss: 0.0782, Val Loss: 0.0641\n",
      "Epoch 79/300 - Train Loss: 0.0762, Val Loss: 0.0684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:17:03,778] Trial 461 finished with value: 0.9740700587607184 and parameters: {'F1': 16, 'F2': 8, 'D': 8, 'dropout': 0.45423438650443604, 'learning_rate': 0.0002508917377750261, 'batch_size': 32, 'weight_decay': 2.729824800784064e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80/300 - Train Loss: 0.0754, Val Loss: 0.0671\n",
      "Early stopping at epoch 80\n",
      "Macro F1 Score: 0.9741, Macro Precision: 0.9703, Macro Recall: 0.9781\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 463\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1546927839927878, LR=0.0007669473920839199, BS=128, WD=0.0009493553524503508\n",
      "Epoch 1/300 - Train Loss: 0.1783, Val Loss: 0.1173\n",
      "Epoch 2/300 - Train Loss: 0.0937, Val Loss: 0.0853\n",
      "Epoch 3/300 - Train Loss: 0.0850, Val Loss: 0.0740\n",
      "Epoch 4/300 - Train Loss: 0.0811, Val Loss: 0.0797\n",
      "Epoch 5/300 - Train Loss: 0.0791, Val Loss: 0.0741\n",
      "Epoch 6/300 - Train Loss: 0.0810, Val Loss: 0.0709\n",
      "Epoch 7/300 - Train Loss: 0.0780, Val Loss: 0.0748\n",
      "Epoch 8/300 - Train Loss: 0.0778, Val Loss: 0.0728\n",
      "Epoch 9/300 - Train Loss: 0.0765, Val Loss: 0.0761\n",
      "Epoch 10/300 - Train Loss: 0.0775, Val Loss: 0.0754\n",
      "Epoch 11/300 - Train Loss: 0.0759, Val Loss: 0.0756\n",
      "Epoch 12/300 - Train Loss: 0.0740, Val Loss: 0.0716\n",
      "Epoch 13/300 - Train Loss: 0.0741, Val Loss: 0.0733\n",
      "Epoch 14/300 - Train Loss: 0.0757, Val Loss: 0.0742\n",
      "Epoch 15/300 - Train Loss: 0.0746, Val Loss: 0.0757\n",
      "Epoch 16/300 - Train Loss: 0.0737, Val Loss: 0.0793\n",
      "Epoch 17/300 - Train Loss: 0.0748, Val Loss: 0.0732\n",
      "Epoch 18/300 - Train Loss: 0.0748, Val Loss: 0.0759\n",
      "Epoch 19/300 - Train Loss: 0.0744, Val Loss: 0.0759\n",
      "Epoch 20/300 - Train Loss: 0.0736, Val Loss: 0.0913\n",
      "Epoch 21/300 - Train Loss: 0.0745, Val Loss: 0.0722\n",
      "Epoch 22/300 - Train Loss: 0.0739, Val Loss: 0.0729\n",
      "Epoch 23/300 - Train Loss: 0.0718, Val Loss: 0.0799\n",
      "Epoch 24/300 - Train Loss: 0.0729, Val Loss: 0.0799\n",
      "Epoch 25/300 - Train Loss: 0.0715, Val Loss: 0.0792\n",
      "Epoch 26/300 - Train Loss: 0.0740, Val Loss: 0.0713\n",
      "Epoch 27/300 - Train Loss: 0.0739, Val Loss: 0.0806\n",
      "Epoch 28/300 - Train Loss: 0.0727, Val Loss: 0.0785\n",
      "Epoch 29/300 - Train Loss: 0.0713, Val Loss: 0.0825\n",
      "Epoch 30/300 - Train Loss: 0.0734, Val Loss: 0.0726\n",
      "Epoch 31/300 - Train Loss: 0.0733, Val Loss: 0.0710\n",
      "Epoch 32/300 - Train Loss: 0.0723, Val Loss: 0.0705\n",
      "Epoch 33/300 - Train Loss: 0.0723, Val Loss: 0.0823\n",
      "Epoch 34/300 - Train Loss: 0.0734, Val Loss: 0.0773\n",
      "Epoch 35/300 - Train Loss: 0.0720, Val Loss: 0.0697\n",
      "Epoch 36/300 - Train Loss: 0.0717, Val Loss: 0.0745\n",
      "Epoch 37/300 - Train Loss: 0.0731, Val Loss: 0.0788\n",
      "Epoch 38/300 - Train Loss: 0.0714, Val Loss: 0.0743\n",
      "Epoch 39/300 - Train Loss: 0.0707, Val Loss: 0.0706\n",
      "Epoch 40/300 - Train Loss: 0.0720, Val Loss: 0.0784\n",
      "Epoch 41/300 - Train Loss: 0.0739, Val Loss: 0.0825\n",
      "Epoch 42/300 - Train Loss: 0.0726, Val Loss: 0.0742\n",
      "Epoch 43/300 - Train Loss: 0.0709, Val Loss: 0.0676\n",
      "Epoch 44/300 - Train Loss: 0.0711, Val Loss: 0.0694\n",
      "Epoch 45/300 - Train Loss: 0.0711, Val Loss: 0.0833\n",
      "Epoch 46/300 - Train Loss: 0.0720, Val Loss: 0.0786\n",
      "Epoch 47/300 - Train Loss: 0.0711, Val Loss: 0.0692\n",
      "Epoch 48/300 - Train Loss: 0.0718, Val Loss: 0.0806\n",
      "Epoch 49/300 - Train Loss: 0.0715, Val Loss: 0.0803\n",
      "Epoch 50/300 - Train Loss: 0.0721, Val Loss: 0.0690\n",
      "Epoch 51/300 - Train Loss: 0.0727, Val Loss: 0.0716\n",
      "Epoch 52/300 - Train Loss: 0.0722, Val Loss: 0.0725\n",
      "Epoch 53/300 - Train Loss: 0.0720, Val Loss: 0.0972\n",
      "Epoch 54/300 - Train Loss: 0.0711, Val Loss: 0.0744\n",
      "Epoch 55/300 - Train Loss: 0.0718, Val Loss: 0.0711\n",
      "Epoch 56/300 - Train Loss: 0.0705, Val Loss: 0.0692\n",
      "Epoch 57/300 - Train Loss: 0.0715, Val Loss: 0.0724\n",
      "Epoch 58/300 - Train Loss: 0.0725, Val Loss: 0.1066\n",
      "Epoch 59/300 - Train Loss: 0.0713, Val Loss: 0.0723\n",
      "Epoch 60/300 - Train Loss: 0.0724, Val Loss: 0.0726\n",
      "Epoch 61/300 - Train Loss: 0.0722, Val Loss: 0.0751\n",
      "Epoch 62/300 - Train Loss: 0.0727, Val Loss: 0.0713\n",
      "Epoch 63/300 - Train Loss: 0.0717, Val Loss: 0.0772\n",
      "Epoch 64/300 - Train Loss: 0.0731, Val Loss: 0.0852\n",
      "Epoch 65/300 - Train Loss: 0.0729, Val Loss: 0.0681\n",
      "Epoch 66/300 - Train Loss: 0.0715, Val Loss: 0.0804\n",
      "Epoch 67/300 - Train Loss: 0.0717, Val Loss: 0.0711\n",
      "Epoch 68/300 - Train Loss: 0.0708, Val Loss: 0.0819\n",
      "Epoch 69/300 - Train Loss: 0.0711, Val Loss: 0.0740\n",
      "Epoch 70/300 - Train Loss: 0.0719, Val Loss: 0.0735\n",
      "Epoch 71/300 - Train Loss: 0.0720, Val Loss: 0.0730\n",
      "Epoch 72/300 - Train Loss: 0.0713, Val Loss: 0.0823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:19:43,570] Trial 462 finished with value: 0.9683372257505205 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1546927839927878, 'learning_rate': 0.0007669473920839199, 'batch_size': 128, 'weight_decay': 0.0009493553524503508}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/300 - Train Loss: 0.0713, Val Loss: 0.0679\n",
      "Early stopping at epoch 73\n",
      "Macro F1 Score: 0.9683, Macro Precision: 0.9631, Macro Recall: 0.9742\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 464\n",
      "Training with F1=32, F2=32, D=4, dropout=0.14127619604255884, LR=0.0009293808508788558, BS=32, WD=3.626344273079794e-05\n",
      "Epoch 1/300 - Train Loss: 0.1383, Val Loss: 0.0856\n",
      "Epoch 2/300 - Train Loss: 0.0981, Val Loss: 0.1044\n",
      "Epoch 3/300 - Train Loss: 0.0929, Val Loss: 0.0755\n",
      "Epoch 4/300 - Train Loss: 0.0916, Val Loss: 0.0726\n",
      "Epoch 5/300 - Train Loss: 0.0844, Val Loss: 0.0753\n",
      "Epoch 6/300 - Train Loss: 0.0802, Val Loss: 0.0762\n",
      "Epoch 7/300 - Train Loss: 0.0805, Val Loss: 0.0816\n",
      "Epoch 8/300 - Train Loss: 0.0817, Val Loss: 0.0771\n",
      "Epoch 9/300 - Train Loss: 0.0773, Val Loss: 0.0713\n",
      "Epoch 10/300 - Train Loss: 0.0769, Val Loss: 0.0765\n",
      "Epoch 11/300 - Train Loss: 0.0754, Val Loss: 0.0758\n",
      "Epoch 12/300 - Train Loss: 0.0715, Val Loss: 0.0804\n",
      "Epoch 13/300 - Train Loss: 0.0729, Val Loss: 0.0774\n",
      "Epoch 14/300 - Train Loss: 0.0705, Val Loss: 0.0813\n",
      "Epoch 15/300 - Train Loss: 0.0676, Val Loss: 0.0784\n",
      "Epoch 16/300 - Train Loss: 0.0695, Val Loss: 0.0761\n",
      "Epoch 17/300 - Train Loss: 0.0657, Val Loss: 0.0817\n",
      "Epoch 18/300 - Train Loss: 0.0646, Val Loss: 0.0807\n",
      "Epoch 19/300 - Train Loss: 0.0646, Val Loss: 0.0760\n",
      "Epoch 20/300 - Train Loss: 0.0627, Val Loss: 0.0868\n",
      "Epoch 21/300 - Train Loss: 0.0617, Val Loss: 0.0877\n",
      "Epoch 22/300 - Train Loss: 0.0627, Val Loss: 0.0755\n",
      "Epoch 23/300 - Train Loss: 0.0590, Val Loss: 0.0829\n",
      "Epoch 24/300 - Train Loss: 0.0590, Val Loss: 0.0842\n",
      "Epoch 25/300 - Train Loss: 0.0581, Val Loss: 0.0804\n",
      "Epoch 26/300 - Train Loss: 0.0565, Val Loss: 0.0838\n",
      "Epoch 27/300 - Train Loss: 0.0591, Val Loss: 0.0787\n",
      "Epoch 28/300 - Train Loss: 0.0586, Val Loss: 0.0843\n",
      "Epoch 29/300 - Train Loss: 0.0553, Val Loss: 0.0872\n",
      "Epoch 30/300 - Train Loss: 0.0526, Val Loss: 0.0812\n",
      "Epoch 31/300 - Train Loss: 0.0517, Val Loss: 0.0824\n",
      "Epoch 32/300 - Train Loss: 0.0516, Val Loss: 0.0856\n",
      "Epoch 33/300 - Train Loss: 0.0489, Val Loss: 0.0807\n",
      "Epoch 34/300 - Train Loss: 0.0528, Val Loss: 0.0852\n",
      "Epoch 35/300 - Train Loss: 0.0484, Val Loss: 0.0866\n",
      "Epoch 36/300 - Train Loss: 0.0487, Val Loss: 0.0876\n",
      "Epoch 37/300 - Train Loss: 0.0492, Val Loss: 0.0909\n",
      "Epoch 38/300 - Train Loss: 0.0511, Val Loss: 0.0892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:22:22,867] Trial 463 finished with value: 0.9676605954317861 and parameters: {'F1': 32, 'F2': 32, 'D': 4, 'dropout': 0.14127619604255884, 'learning_rate': 0.0009293808508788558, 'batch_size': 32, 'weight_decay': 3.626344273079794e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300 - Train Loss: 0.0504, Val Loss: 0.0886\n",
      "Early stopping at epoch 39\n",
      "Macro F1 Score: 0.9677, Macro Precision: 0.9715, Macro Recall: 0.9641\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 465\n",
      "Training with F1=16, F2=8, D=8, dropout=0.18790371626023547, LR=5.482847484606033e-05, BS=32, WD=4.5795873332382396e-05\n",
      "Epoch 1/300 - Train Loss: 0.4759, Val Loss: 0.2100\n",
      "Epoch 2/300 - Train Loss: 0.1751, Val Loss: 0.1239\n",
      "Epoch 3/300 - Train Loss: 0.1307, Val Loss: 0.0992\n",
      "Epoch 4/300 - Train Loss: 0.1158, Val Loss: 0.0806\n",
      "Epoch 5/300 - Train Loss: 0.1068, Val Loss: 0.0794\n",
      "Epoch 6/300 - Train Loss: 0.1000, Val Loss: 0.0776\n",
      "Epoch 7/300 - Train Loss: 0.0981, Val Loss: 0.0808\n",
      "Epoch 8/300 - Train Loss: 0.0964, Val Loss: 0.0751\n",
      "Epoch 9/300 - Train Loss: 0.0934, Val Loss: 0.0708\n",
      "Epoch 10/300 - Train Loss: 0.0927, Val Loss: 0.0768\n",
      "Epoch 11/300 - Train Loss: 0.0903, Val Loss: 0.0693\n",
      "Epoch 12/300 - Train Loss: 0.0868, Val Loss: 0.0756\n",
      "Epoch 13/300 - Train Loss: 0.0889, Val Loss: 0.0776\n",
      "Epoch 14/300 - Train Loss: 0.0874, Val Loss: 0.0698\n",
      "Epoch 15/300 - Train Loss: 0.0869, Val Loss: 0.0666\n",
      "Epoch 16/300 - Train Loss: 0.0858, Val Loss: 0.0760\n",
      "Epoch 17/300 - Train Loss: 0.0872, Val Loss: 0.0690\n",
      "Epoch 18/300 - Train Loss: 0.0843, Val Loss: 0.0712\n",
      "Epoch 19/300 - Train Loss: 0.0832, Val Loss: 0.0700\n",
      "Epoch 20/300 - Train Loss: 0.0841, Val Loss: 0.0796\n",
      "Epoch 21/300 - Train Loss: 0.0829, Val Loss: 0.0698\n",
      "Epoch 22/300 - Train Loss: 0.0835, Val Loss: 0.0747\n",
      "Epoch 23/300 - Train Loss: 0.0821, Val Loss: 0.0714\n",
      "Epoch 24/300 - Train Loss: 0.0806, Val Loss: 0.0721\n",
      "Epoch 25/300 - Train Loss: 0.0816, Val Loss: 0.0719\n",
      "Epoch 26/300 - Train Loss: 0.0800, Val Loss: 0.0657\n",
      "Epoch 27/300 - Train Loss: 0.0789, Val Loss: 0.0665\n",
      "Epoch 28/300 - Train Loss: 0.0813, Val Loss: 0.0726\n",
      "Epoch 29/300 - Train Loss: 0.0785, Val Loss: 0.0713\n",
      "Epoch 30/300 - Train Loss: 0.0796, Val Loss: 0.0677\n",
      "Epoch 31/300 - Train Loss: 0.0776, Val Loss: 0.0707\n",
      "Epoch 32/300 - Train Loss: 0.0798, Val Loss: 0.0697\n",
      "Epoch 33/300 - Train Loss: 0.0785, Val Loss: 0.0696\n",
      "Epoch 34/300 - Train Loss: 0.0773, Val Loss: 0.0673\n",
      "Epoch 35/300 - Train Loss: 0.0792, Val Loss: 0.0660\n",
      "Epoch 36/300 - Train Loss: 0.0785, Val Loss: 0.0759\n",
      "Epoch 37/300 - Train Loss: 0.0777, Val Loss: 0.0678\n",
      "Epoch 38/300 - Train Loss: 0.0766, Val Loss: 0.0665\n",
      "Epoch 39/300 - Train Loss: 0.0778, Val Loss: 0.0675\n",
      "Epoch 40/300 - Train Loss: 0.0769, Val Loss: 0.0663\n",
      "Epoch 41/300 - Train Loss: 0.0774, Val Loss: 0.0703\n",
      "Epoch 42/300 - Train Loss: 0.0797, Val Loss: 0.0744\n",
      "Epoch 43/300 - Train Loss: 0.0761, Val Loss: 0.0698\n",
      "Epoch 44/300 - Train Loss: 0.0777, Val Loss: 0.0668\n",
      "Epoch 45/300 - Train Loss: 0.0749, Val Loss: 0.0695\n",
      "Epoch 46/300 - Train Loss: 0.0769, Val Loss: 0.0656\n",
      "Epoch 47/300 - Train Loss: 0.0751, Val Loss: 0.0726\n",
      "Epoch 48/300 - Train Loss: 0.0763, Val Loss: 0.0658\n",
      "Epoch 49/300 - Train Loss: 0.0762, Val Loss: 0.0691\n",
      "Epoch 50/300 - Train Loss: 0.0756, Val Loss: 0.0682\n",
      "Epoch 51/300 - Train Loss: 0.0745, Val Loss: 0.0679\n",
      "Epoch 52/300 - Train Loss: 0.0738, Val Loss: 0.0684\n",
      "Epoch 53/300 - Train Loss: 0.0747, Val Loss: 0.0680\n",
      "Epoch 54/300 - Train Loss: 0.0735, Val Loss: 0.0650\n",
      "Epoch 55/300 - Train Loss: 0.0766, Val Loss: 0.0680\n",
      "Epoch 56/300 - Train Loss: 0.0753, Val Loss: 0.0693\n",
      "Epoch 57/300 - Train Loss: 0.0733, Val Loss: 0.0646\n",
      "Epoch 58/300 - Train Loss: 0.0762, Val Loss: 0.0735\n",
      "Epoch 59/300 - Train Loss: 0.0740, Val Loss: 0.0686\n",
      "Epoch 60/300 - Train Loss: 0.0727, Val Loss: 0.0669\n",
      "Epoch 61/300 - Train Loss: 0.0751, Val Loss: 0.0680\n",
      "Epoch 62/300 - Train Loss: 0.0734, Val Loss: 0.0644\n",
      "Epoch 63/300 - Train Loss: 0.0727, Val Loss: 0.0659\n",
      "Epoch 64/300 - Train Loss: 0.0727, Val Loss: 0.0660\n",
      "Epoch 65/300 - Train Loss: 0.0730, Val Loss: 0.0645\n",
      "Epoch 66/300 - Train Loss: 0.0727, Val Loss: 0.0633\n",
      "Epoch 67/300 - Train Loss: 0.0734, Val Loss: 0.0643\n",
      "Epoch 68/300 - Train Loss: 0.0743, Val Loss: 0.0666\n",
      "Epoch 69/300 - Train Loss: 0.0719, Val Loss: 0.0642\n",
      "Epoch 70/300 - Train Loss: 0.0727, Val Loss: 0.0620\n",
      "Epoch 71/300 - Train Loss: 0.0724, Val Loss: 0.0645\n",
      "Epoch 72/300 - Train Loss: 0.0740, Val Loss: 0.0664\n",
      "Epoch 73/300 - Train Loss: 0.0707, Val Loss: 0.0686\n",
      "Epoch 74/300 - Train Loss: 0.0734, Val Loss: 0.0714\n",
      "Epoch 75/300 - Train Loss: 0.0724, Val Loss: 0.0682\n",
      "Epoch 76/300 - Train Loss: 0.0711, Val Loss: 0.0637\n",
      "Epoch 77/300 - Train Loss: 0.0722, Val Loss: 0.0764\n",
      "Epoch 78/300 - Train Loss: 0.0692, Val Loss: 0.0615\n",
      "Epoch 79/300 - Train Loss: 0.0717, Val Loss: 0.0647\n",
      "Epoch 80/300 - Train Loss: 0.0699, Val Loss: 0.0652\n",
      "Epoch 81/300 - Train Loss: 0.0701, Val Loss: 0.0765\n",
      "Epoch 82/300 - Train Loss: 0.0706, Val Loss: 0.0699\n",
      "Epoch 83/300 - Train Loss: 0.0704, Val Loss: 0.0648\n",
      "Epoch 84/300 - Train Loss: 0.0707, Val Loss: 0.0659\n",
      "Epoch 85/300 - Train Loss: 0.0730, Val Loss: 0.0651\n",
      "Epoch 86/300 - Train Loss: 0.0716, Val Loss: 0.0751\n",
      "Epoch 87/300 - Train Loss: 0.0694, Val Loss: 0.0651\n",
      "Epoch 88/300 - Train Loss: 0.0707, Val Loss: 0.0676\n",
      "Epoch 89/300 - Train Loss: 0.0713, Val Loss: 0.0713\n",
      "Epoch 90/300 - Train Loss: 0.0692, Val Loss: 0.0649\n",
      "Epoch 91/300 - Train Loss: 0.0699, Val Loss: 0.0644\n",
      "Epoch 92/300 - Train Loss: 0.0689, Val Loss: 0.0660\n",
      "Epoch 93/300 - Train Loss: 0.0698, Val Loss: 0.0651\n",
      "Epoch 94/300 - Train Loss: 0.0690, Val Loss: 0.0640\n",
      "Epoch 95/300 - Train Loss: 0.0688, Val Loss: 0.0626\n",
      "Epoch 96/300 - Train Loss: 0.0697, Val Loss: 0.0637\n",
      "Epoch 97/300 - Train Loss: 0.0690, Val Loss: 0.0694\n",
      "Epoch 98/300 - Train Loss: 0.0677, Val Loss: 0.0640\n",
      "Epoch 99/300 - Train Loss: 0.0670, Val Loss: 0.0655\n",
      "Epoch 100/300 - Train Loss: 0.0697, Val Loss: 0.0640\n",
      "Epoch 101/300 - Train Loss: 0.0677, Val Loss: 0.0651\n",
      "Epoch 102/300 - Train Loss: 0.0689, Val Loss: 0.0657\n",
      "Epoch 103/300 - Train Loss: 0.0673, Val Loss: 0.0615\n",
      "Epoch 104/300 - Train Loss: 0.0672, Val Loss: 0.0663\n",
      "Epoch 105/300 - Train Loss: 0.0669, Val Loss: 0.0649\n",
      "Epoch 106/300 - Train Loss: 0.0679, Val Loss: 0.0648\n",
      "Epoch 107/300 - Train Loss: 0.0691, Val Loss: 0.0687\n",
      "Epoch 108/300 - Train Loss: 0.0677, Val Loss: 0.0674\n",
      "Epoch 109/300 - Train Loss: 0.0684, Val Loss: 0.0631\n",
      "Epoch 110/300 - Train Loss: 0.0688, Val Loss: 0.0636\n",
      "Epoch 111/300 - Train Loss: 0.0678, Val Loss: 0.0646\n",
      "Epoch 112/300 - Train Loss: 0.0669, Val Loss: 0.0644\n",
      "Epoch 113/300 - Train Loss: 0.0660, Val Loss: 0.0655\n",
      "Epoch 114/300 - Train Loss: 0.0674, Val Loss: 0.0661\n",
      "Epoch 115/300 - Train Loss: 0.0651, Val Loss: 0.0623\n",
      "Epoch 116/300 - Train Loss: 0.0670, Val Loss: 0.0676\n",
      "Epoch 117/300 - Train Loss: 0.0669, Val Loss: 0.0768\n",
      "Epoch 118/300 - Train Loss: 0.0675, Val Loss: 0.0661\n",
      "Epoch 119/300 - Train Loss: 0.0657, Val Loss: 0.0657\n",
      "Epoch 120/300 - Train Loss: 0.0681, Val Loss: 0.0696\n",
      "Epoch 121/300 - Train Loss: 0.0665, Val Loss: 0.0642\n",
      "Epoch 122/300 - Train Loss: 0.0641, Val Loss: 0.0687\n",
      "Epoch 123/300 - Train Loss: 0.0684, Val Loss: 0.0656\n",
      "Epoch 124/300 - Train Loss: 0.0674, Val Loss: 0.0631\n",
      "Epoch 125/300 - Train Loss: 0.0653, Val Loss: 0.0645\n",
      "Epoch 126/300 - Train Loss: 0.0660, Val Loss: 0.0641\n",
      "Epoch 127/300 - Train Loss: 0.0654, Val Loss: 0.0633\n",
      "Epoch 128/300 - Train Loss: 0.0656, Val Loss: 0.0626\n",
      "Epoch 129/300 - Train Loss: 0.0663, Val Loss: 0.0672\n",
      "Epoch 130/300 - Train Loss: 0.0663, Val Loss: 0.0676\n",
      "Epoch 131/300 - Train Loss: 0.0660, Val Loss: 0.0630\n",
      "Epoch 132/300 - Train Loss: 0.0648, Val Loss: 0.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:29:58,367] Trial 464 finished with value: 0.9740736520334785 and parameters: {'F1': 16, 'F2': 8, 'D': 8, 'dropout': 0.18790371626023547, 'learning_rate': 5.482847484606033e-05, 'batch_size': 32, 'weight_decay': 4.5795873332382396e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 133/300 - Train Loss: 0.0655, Val Loss: 0.0635\n",
      "Early stopping at epoch 133\n",
      "Macro F1 Score: 0.9741, Macro Precision: 0.9701, Macro Recall: 0.9782\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 466\n",
      "Training with F1=32, F2=32, D=2, dropout=0.17060756505395758, LR=7.325323542900647e-05, BS=32, WD=0.0015045443429068475\n",
      "Epoch 1/300 - Train Loss: 0.3158, Val Loss: 0.1540\n",
      "Epoch 2/300 - Train Loss: 0.1432, Val Loss: 0.1091\n",
      "Epoch 3/300 - Train Loss: 0.1120, Val Loss: 0.0839\n",
      "Epoch 4/300 - Train Loss: 0.1020, Val Loss: 0.0866\n",
      "Epoch 5/300 - Train Loss: 0.0972, Val Loss: 0.0761\n",
      "Epoch 6/300 - Train Loss: 0.0955, Val Loss: 0.0802\n",
      "Epoch 7/300 - Train Loss: 0.0934, Val Loss: 0.0756\n",
      "Epoch 8/300 - Train Loss: 0.0914, Val Loss: 0.0762\n",
      "Epoch 9/300 - Train Loss: 0.0887, Val Loss: 0.0774\n",
      "Epoch 10/300 - Train Loss: 0.0890, Val Loss: 0.0716\n",
      "Epoch 11/300 - Train Loss: 0.0875, Val Loss: 0.0706\n",
      "Epoch 12/300 - Train Loss: 0.0873, Val Loss: 0.0710\n",
      "Epoch 13/300 - Train Loss: 0.0839, Val Loss: 0.0704\n",
      "Epoch 14/300 - Train Loss: 0.0843, Val Loss: 0.0719\n",
      "Epoch 15/300 - Train Loss: 0.0846, Val Loss: 0.0703\n",
      "Epoch 16/300 - Train Loss: 0.0840, Val Loss: 0.0710\n",
      "Epoch 17/300 - Train Loss: 0.0842, Val Loss: 0.0684\n",
      "Epoch 18/300 - Train Loss: 0.0849, Val Loss: 0.0785\n",
      "Epoch 19/300 - Train Loss: 0.0792, Val Loss: 0.0717\n",
      "Epoch 20/300 - Train Loss: 0.0803, Val Loss: 0.0706\n",
      "Epoch 21/300 - Train Loss: 0.0792, Val Loss: 0.0790\n",
      "Epoch 22/300 - Train Loss: 0.0810, Val Loss: 0.0724\n",
      "Epoch 23/300 - Train Loss: 0.0792, Val Loss: 0.0668\n",
      "Epoch 24/300 - Train Loss: 0.0809, Val Loss: 0.0703\n",
      "Epoch 25/300 - Train Loss: 0.0817, Val Loss: 0.0701\n",
      "Epoch 26/300 - Train Loss: 0.0786, Val Loss: 0.0686\n",
      "Epoch 27/300 - Train Loss: 0.0797, Val Loss: 0.0735\n",
      "Epoch 28/300 - Train Loss: 0.0804, Val Loss: 0.0676\n",
      "Epoch 29/300 - Train Loss: 0.0811, Val Loss: 0.0709\n",
      "Epoch 30/300 - Train Loss: 0.0806, Val Loss: 0.0675\n",
      "Epoch 31/300 - Train Loss: 0.0798, Val Loss: 0.0689\n",
      "Epoch 32/300 - Train Loss: 0.0784, Val Loss: 0.0659\n",
      "Epoch 33/300 - Train Loss: 0.0802, Val Loss: 0.0734\n",
      "Epoch 34/300 - Train Loss: 0.0784, Val Loss: 0.0706\n",
      "Epoch 35/300 - Train Loss: 0.0793, Val Loss: 0.0682\n",
      "Epoch 36/300 - Train Loss: 0.0785, Val Loss: 0.0678\n",
      "Epoch 37/300 - Train Loss: 0.0781, Val Loss: 0.0728\n",
      "Epoch 38/300 - Train Loss: 0.0800, Val Loss: 0.0725\n",
      "Epoch 39/300 - Train Loss: 0.0771, Val Loss: 0.0696\n",
      "Epoch 40/300 - Train Loss: 0.0791, Val Loss: 0.0685\n",
      "Epoch 41/300 - Train Loss: 0.0781, Val Loss: 0.0667\n",
      "Epoch 42/300 - Train Loss: 0.0757, Val Loss: 0.0700\n",
      "Epoch 43/300 - Train Loss: 0.0800, Val Loss: 0.0690\n",
      "Epoch 44/300 - Train Loss: 0.0785, Val Loss: 0.0673\n",
      "Epoch 45/300 - Train Loss: 0.0761, Val Loss: 0.0674\n",
      "Epoch 46/300 - Train Loss: 0.0760, Val Loss: 0.0714\n",
      "Epoch 47/300 - Train Loss: 0.0763, Val Loss: 0.0681\n",
      "Epoch 48/300 - Train Loss: 0.0782, Val Loss: 0.0689\n",
      "Epoch 49/300 - Train Loss: 0.0765, Val Loss: 0.0693\n",
      "Epoch 50/300 - Train Loss: 0.0774, Val Loss: 0.0778\n",
      "Epoch 51/300 - Train Loss: 0.0759, Val Loss: 0.0674\n",
      "Epoch 52/300 - Train Loss: 0.0769, Val Loss: 0.0687\n",
      "Epoch 53/300 - Train Loss: 0.0759, Val Loss: 0.0684\n",
      "Epoch 54/300 - Train Loss: 0.0762, Val Loss: 0.0680\n",
      "Epoch 55/300 - Train Loss: 0.0755, Val Loss: 0.0700\n",
      "Epoch 56/300 - Train Loss: 0.0768, Val Loss: 0.0683\n",
      "Epoch 57/300 - Train Loss: 0.0779, Val Loss: 0.0677\n",
      "Epoch 58/300 - Train Loss: 0.0763, Val Loss: 0.0710\n",
      "Epoch 59/300 - Train Loss: 0.0766, Val Loss: 0.0687\n",
      "Epoch 60/300 - Train Loss: 0.0737, Val Loss: 0.0722\n",
      "Epoch 61/300 - Train Loss: 0.0767, Val Loss: 0.0694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:32:48,344] Trial 465 finished with value: 0.9635146124441668 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.17060756505395758, 'learning_rate': 7.325323542900647e-05, 'batch_size': 32, 'weight_decay': 0.0015045443429068475}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/300 - Train Loss: 0.0767, Val Loss: 0.0733\n",
      "Early stopping at epoch 62\n",
      "Macro F1 Score: 0.9635, Macro Precision: 0.9578, Macro Recall: 0.9697\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.95      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 467\n",
      "Training with F1=32, F2=8, D=2, dropout=0.1303356806095111, LR=0.00011966169554294602, BS=32, WD=7.102759158426051e-05\n",
      "Epoch 1/300 - Train Loss: 0.3042, Val Loss: 0.1164\n",
      "Epoch 2/300 - Train Loss: 0.1245, Val Loss: 0.0919\n",
      "Epoch 3/300 - Train Loss: 0.1092, Val Loss: 0.0811\n",
      "Epoch 4/300 - Train Loss: 0.0989, Val Loss: 0.0762\n",
      "Epoch 5/300 - Train Loss: 0.0939, Val Loss: 0.0772\n",
      "Epoch 6/300 - Train Loss: 0.0931, Val Loss: 0.0751\n",
      "Epoch 7/300 - Train Loss: 0.0921, Val Loss: 0.0692\n",
      "Epoch 8/300 - Train Loss: 0.0885, Val Loss: 0.0728\n",
      "Epoch 9/300 - Train Loss: 0.0883, Val Loss: 0.0706\n",
      "Epoch 10/300 - Train Loss: 0.0864, Val Loss: 0.0704\n",
      "Epoch 11/300 - Train Loss: 0.0849, Val Loss: 0.0689\n",
      "Epoch 12/300 - Train Loss: 0.0850, Val Loss: 0.0736\n",
      "Epoch 13/300 - Train Loss: 0.0842, Val Loss: 0.0681\n",
      "Epoch 14/300 - Train Loss: 0.0823, Val Loss: 0.0652\n",
      "Epoch 15/300 - Train Loss: 0.0815, Val Loss: 0.0656\n",
      "Epoch 16/300 - Train Loss: 0.0839, Val Loss: 0.0679\n",
      "Epoch 17/300 - Train Loss: 0.0802, Val Loss: 0.0671\n",
      "Epoch 18/300 - Train Loss: 0.0819, Val Loss: 0.0677\n",
      "Epoch 19/300 - Train Loss: 0.0814, Val Loss: 0.0639\n",
      "Epoch 20/300 - Train Loss: 0.0782, Val Loss: 0.0674\n",
      "Epoch 21/300 - Train Loss: 0.0806, Val Loss: 0.0729\n",
      "Epoch 22/300 - Train Loss: 0.0811, Val Loss: 0.0668\n",
      "Epoch 23/300 - Train Loss: 0.0775, Val Loss: 0.0703\n",
      "Epoch 24/300 - Train Loss: 0.0800, Val Loss: 0.0703\n",
      "Epoch 25/300 - Train Loss: 0.0768, Val Loss: 0.0676\n",
      "Epoch 26/300 - Train Loss: 0.0777, Val Loss: 0.0684\n",
      "Epoch 27/300 - Train Loss: 0.0760, Val Loss: 0.0656\n",
      "Epoch 28/300 - Train Loss: 0.0754, Val Loss: 0.0674\n",
      "Epoch 29/300 - Train Loss: 0.0755, Val Loss: 0.0654\n",
      "Epoch 30/300 - Train Loss: 0.0746, Val Loss: 0.0703\n",
      "Epoch 31/300 - Train Loss: 0.0728, Val Loss: 0.0668\n",
      "Epoch 32/300 - Train Loss: 0.0746, Val Loss: 0.0683\n",
      "Epoch 33/300 - Train Loss: 0.0734, Val Loss: 0.0660\n",
      "Epoch 34/300 - Train Loss: 0.0752, Val Loss: 0.0659\n",
      "Epoch 35/300 - Train Loss: 0.0752, Val Loss: 0.0692\n",
      "Epoch 36/300 - Train Loss: 0.0733, Val Loss: 0.0661\n",
      "Epoch 37/300 - Train Loss: 0.0748, Val Loss: 0.0677\n",
      "Epoch 38/300 - Train Loss: 0.0734, Val Loss: 0.0676\n",
      "Epoch 39/300 - Train Loss: 0.0727, Val Loss: 0.0700\n",
      "Epoch 40/300 - Train Loss: 0.0730, Val Loss: 0.0637\n",
      "Epoch 41/300 - Train Loss: 0.0720, Val Loss: 0.0674\n",
      "Epoch 42/300 - Train Loss: 0.0720, Val Loss: 0.0663\n",
      "Epoch 43/300 - Train Loss: 0.0707, Val Loss: 0.0693\n",
      "Epoch 44/300 - Train Loss: 0.0699, Val Loss: 0.0689\n",
      "Epoch 45/300 - Train Loss: 0.0703, Val Loss: 0.0681\n",
      "Epoch 46/300 - Train Loss: 0.0714, Val Loss: 0.0654\n",
      "Epoch 47/300 - Train Loss: 0.0707, Val Loss: 0.0718\n",
      "Epoch 48/300 - Train Loss: 0.0724, Val Loss: 0.0647\n",
      "Epoch 49/300 - Train Loss: 0.0699, Val Loss: 0.0655\n",
      "Epoch 50/300 - Train Loss: 0.0688, Val Loss: 0.0668\n",
      "Epoch 51/300 - Train Loss: 0.0682, Val Loss: 0.0708\n",
      "Epoch 52/300 - Train Loss: 0.0678, Val Loss: 0.0661\n",
      "Epoch 53/300 - Train Loss: 0.0710, Val Loss: 0.0694\n",
      "Epoch 54/300 - Train Loss: 0.0697, Val Loss: 0.0636\n",
      "Epoch 55/300 - Train Loss: 0.0681, Val Loss: 0.0652\n",
      "Epoch 56/300 - Train Loss: 0.0696, Val Loss: 0.0644\n",
      "Epoch 57/300 - Train Loss: 0.0669, Val Loss: 0.0655\n",
      "Epoch 58/300 - Train Loss: 0.0668, Val Loss: 0.0680\n",
      "Epoch 59/300 - Train Loss: 0.0672, Val Loss: 0.0645\n",
      "Epoch 60/300 - Train Loss: 0.0689, Val Loss: 0.0652\n",
      "Epoch 61/300 - Train Loss: 0.0666, Val Loss: 0.0647\n",
      "Epoch 62/300 - Train Loss: 0.0679, Val Loss: 0.0667\n",
      "Epoch 63/300 - Train Loss: 0.0688, Val Loss: 0.0728\n",
      "Epoch 64/300 - Train Loss: 0.0681, Val Loss: 0.0666\n",
      "Epoch 65/300 - Train Loss: 0.0665, Val Loss: 0.0651\n",
      "Epoch 66/300 - Train Loss: 0.0663, Val Loss: 0.0657\n",
      "Epoch 67/300 - Train Loss: 0.0689, Val Loss: 0.0663\n",
      "Epoch 68/300 - Train Loss: 0.0651, Val Loss: 0.0657\n",
      "Epoch 69/300 - Train Loss: 0.0666, Val Loss: 0.0700\n",
      "Epoch 70/300 - Train Loss: 0.0645, Val Loss: 0.0661\n",
      "Epoch 71/300 - Train Loss: 0.0656, Val Loss: 0.0653\n",
      "Epoch 72/300 - Train Loss: 0.0650, Val Loss: 0.0646\n",
      "Epoch 73/300 - Train Loss: 0.0631, Val Loss: 0.0660\n",
      "Epoch 74/300 - Train Loss: 0.0644, Val Loss: 0.0690\n",
      "Epoch 75/300 - Train Loss: 0.0666, Val Loss: 0.0669\n",
      "Epoch 76/300 - Train Loss: 0.0637, Val Loss: 0.0674\n",
      "Epoch 77/300 - Train Loss: 0.0674, Val Loss: 0.0657\n",
      "Epoch 78/300 - Train Loss: 0.0651, Val Loss: 0.0683\n",
      "Epoch 79/300 - Train Loss: 0.0628, Val Loss: 0.0676\n",
      "Epoch 80/300 - Train Loss: 0.0638, Val Loss: 0.0684\n",
      "Epoch 81/300 - Train Loss: 0.0636, Val Loss: 0.0639\n",
      "Epoch 82/300 - Train Loss: 0.0640, Val Loss: 0.0698\n",
      "Epoch 83/300 - Train Loss: 0.0628, Val Loss: 0.0714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:36:19,960] Trial 466 finished with value: 0.9676278183165795 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1303356806095111, 'learning_rate': 0.00011966169554294602, 'batch_size': 32, 'weight_decay': 7.102759158426051e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/300 - Train Loss: 0.0621, Val Loss: 0.0686\n",
      "Early stopping at epoch 84\n",
      "Macro F1 Score: 0.9676, Macro Precision: 0.9594, Macro Recall: 0.9766\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 468\n",
      "Training with F1=16, F2=32, D=8, dropout=0.14570462085114302, LR=8.404705134861461e-05, BS=32, WD=6.004583123054911e-05\n",
      "Epoch 1/300 - Train Loss: 0.2698, Val Loss: 0.1046\n",
      "Epoch 2/300 - Train Loss: 0.1118, Val Loss: 0.0805\n",
      "Epoch 3/300 - Train Loss: 0.0982, Val Loss: 0.0798\n",
      "Epoch 4/300 - Train Loss: 0.0940, Val Loss: 0.0840\n",
      "Epoch 5/300 - Train Loss: 0.0897, Val Loss: 0.0737\n",
      "Epoch 6/300 - Train Loss: 0.0848, Val Loss: 0.0747\n",
      "Epoch 7/300 - Train Loss: 0.0859, Val Loss: 0.0800\n",
      "Epoch 8/300 - Train Loss: 0.0830, Val Loss: 0.0770\n",
      "Epoch 9/300 - Train Loss: 0.0825, Val Loss: 0.0829\n",
      "Epoch 10/300 - Train Loss: 0.0829, Val Loss: 0.0756\n",
      "Epoch 11/300 - Train Loss: 0.0797, Val Loss: 0.0747\n",
      "Epoch 12/300 - Train Loss: 0.0796, Val Loss: 0.0689\n",
      "Epoch 13/300 - Train Loss: 0.0781, Val Loss: 0.0787\n",
      "Epoch 14/300 - Train Loss: 0.0753, Val Loss: 0.0776\n",
      "Epoch 15/300 - Train Loss: 0.0769, Val Loss: 0.0701\n",
      "Epoch 16/300 - Train Loss: 0.0750, Val Loss: 0.0731\n",
      "Epoch 17/300 - Train Loss: 0.0751, Val Loss: 0.0675\n",
      "Epoch 18/300 - Train Loss: 0.0739, Val Loss: 0.0752\n",
      "Epoch 19/300 - Train Loss: 0.0723, Val Loss: 0.0781\n",
      "Epoch 20/300 - Train Loss: 0.0721, Val Loss: 0.0698\n",
      "Epoch 21/300 - Train Loss: 0.0715, Val Loss: 0.0750\n",
      "Epoch 22/300 - Train Loss: 0.0722, Val Loss: 0.0724\n",
      "Epoch 23/300 - Train Loss: 0.0709, Val Loss: 0.0749\n",
      "Epoch 24/300 - Train Loss: 0.0680, Val Loss: 0.0688\n",
      "Epoch 25/300 - Train Loss: 0.0703, Val Loss: 0.0681\n",
      "Epoch 26/300 - Train Loss: 0.0679, Val Loss: 0.0688\n",
      "Epoch 27/300 - Train Loss: 0.0684, Val Loss: 0.0673\n",
      "Epoch 28/300 - Train Loss: 0.0699, Val Loss: 0.0729\n",
      "Epoch 29/300 - Train Loss: 0.0677, Val Loss: 0.0635\n",
      "Epoch 30/300 - Train Loss: 0.0664, Val Loss: 0.0665\n",
      "Epoch 31/300 - Train Loss: 0.0658, Val Loss: 0.0675\n",
      "Epoch 32/300 - Train Loss: 0.0648, Val Loss: 0.0717\n",
      "Epoch 33/300 - Train Loss: 0.0626, Val Loss: 0.0665\n",
      "Epoch 34/300 - Train Loss: 0.0633, Val Loss: 0.0650\n",
      "Epoch 35/300 - Train Loss: 0.0634, Val Loss: 0.0699\n",
      "Epoch 36/300 - Train Loss: 0.0619, Val Loss: 0.0646\n",
      "Epoch 37/300 - Train Loss: 0.0615, Val Loss: 0.0728\n",
      "Epoch 38/300 - Train Loss: 0.0613, Val Loss: 0.0718\n",
      "Epoch 39/300 - Train Loss: 0.0598, Val Loss: 0.0701\n",
      "Epoch 40/300 - Train Loss: 0.0603, Val Loss: 0.0713\n",
      "Epoch 41/300 - Train Loss: 0.0589, Val Loss: 0.0673\n",
      "Epoch 42/300 - Train Loss: 0.0600, Val Loss: 0.0660\n",
      "Epoch 43/300 - Train Loss: 0.0590, Val Loss: 0.0644\n",
      "Epoch 44/300 - Train Loss: 0.0594, Val Loss: 0.0679\n",
      "Epoch 45/300 - Train Loss: 0.0577, Val Loss: 0.0673\n",
      "Epoch 46/300 - Train Loss: 0.0574, Val Loss: 0.0644\n",
      "Epoch 47/300 - Train Loss: 0.0602, Val Loss: 0.0650\n",
      "Epoch 48/300 - Train Loss: 0.0584, Val Loss: 0.0664\n",
      "Epoch 49/300 - Train Loss: 0.0570, Val Loss: 0.0655\n",
      "Epoch 50/300 - Train Loss: 0.0584, Val Loss: 0.0666\n",
      "Epoch 51/300 - Train Loss: 0.0567, Val Loss: 0.0675\n",
      "Epoch 52/300 - Train Loss: 0.0563, Val Loss: 0.0691\n",
      "Epoch 53/300 - Train Loss: 0.0553, Val Loss: 0.0681\n",
      "Epoch 54/300 - Train Loss: 0.0596, Val Loss: 0.0696\n",
      "Epoch 55/300 - Train Loss: 0.0548, Val Loss: 0.0692\n",
      "Epoch 56/300 - Train Loss: 0.0541, Val Loss: 0.0776\n",
      "Epoch 57/300 - Train Loss: 0.0553, Val Loss: 0.0636\n",
      "Epoch 58/300 - Train Loss: 0.0541, Val Loss: 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:40:07,596] Trial 467 finished with value: 0.972595222170774 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.14570462085114302, 'learning_rate': 8.404705134861461e-05, 'batch_size': 32, 'weight_decay': 6.004583123054911e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300 - Train Loss: 0.0543, Val Loss: 0.0656\n",
      "Early stopping at epoch 59\n",
      "Macro F1 Score: 0.9726, Macro Precision: 0.9686, Macro Recall: 0.9768\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 469\n",
      "Training with F1=32, F2=32, D=2, dropout=0.18329682327847202, LR=0.000823537819881254, BS=32, WD=4.013224743772149e-05\n",
      "Epoch 1/300 - Train Loss: 0.1405, Val Loss: 0.0748\n",
      "Epoch 2/300 - Train Loss: 0.0964, Val Loss: 0.0744\n",
      "Epoch 3/300 - Train Loss: 0.0919, Val Loss: 0.0717\n",
      "Epoch 4/300 - Train Loss: 0.0902, Val Loss: 0.0798\n",
      "Epoch 5/300 - Train Loss: 0.0868, Val Loss: 0.0795\n",
      "Epoch 6/300 - Train Loss: 0.0829, Val Loss: 0.0689\n",
      "Epoch 7/300 - Train Loss: 0.0833, Val Loss: 0.0724\n",
      "Epoch 8/300 - Train Loss: 0.0824, Val Loss: 0.0788\n",
      "Epoch 9/300 - Train Loss: 0.0791, Val Loss: 0.0709\n",
      "Epoch 10/300 - Train Loss: 0.0797, Val Loss: 0.0779\n",
      "Epoch 11/300 - Train Loss: 0.0778, Val Loss: 0.0796\n",
      "Epoch 12/300 - Train Loss: 0.0758, Val Loss: 0.0748\n",
      "Epoch 13/300 - Train Loss: 0.0759, Val Loss: 0.0859\n",
      "Epoch 14/300 - Train Loss: 0.0748, Val Loss: 0.0817\n",
      "Epoch 15/300 - Train Loss: 0.0767, Val Loss: 0.0727\n",
      "Epoch 16/300 - Train Loss: 0.0725, Val Loss: 0.0895\n",
      "Epoch 17/300 - Train Loss: 0.0731, Val Loss: 0.0745\n",
      "Epoch 18/300 - Train Loss: 0.0706, Val Loss: 0.0783\n",
      "Epoch 19/300 - Train Loss: 0.0705, Val Loss: 0.0773\n",
      "Epoch 20/300 - Train Loss: 0.0694, Val Loss: 0.0833\n",
      "Epoch 21/300 - Train Loss: 0.0691, Val Loss: 0.0774\n",
      "Epoch 22/300 - Train Loss: 0.0650, Val Loss: 0.0811\n",
      "Epoch 23/300 - Train Loss: 0.0680, Val Loss: 0.0792\n",
      "Epoch 24/300 - Train Loss: 0.0685, Val Loss: 0.0804\n",
      "Epoch 25/300 - Train Loss: 0.0658, Val Loss: 0.0813\n",
      "Epoch 26/300 - Train Loss: 0.0657, Val Loss: 0.0782\n",
      "Epoch 27/300 - Train Loss: 0.0627, Val Loss: 0.0831\n",
      "Epoch 28/300 - Train Loss: 0.0635, Val Loss: 0.0900\n",
      "Epoch 29/300 - Train Loss: 0.0624, Val Loss: 0.0856\n",
      "Epoch 30/300 - Train Loss: 0.0632, Val Loss: 0.0863\n",
      "Epoch 31/300 - Train Loss: 0.0630, Val Loss: 0.0808\n",
      "Epoch 32/300 - Train Loss: 0.0617, Val Loss: 0.0913\n",
      "Epoch 33/300 - Train Loss: 0.0598, Val Loss: 0.0713\n",
      "Epoch 34/300 - Train Loss: 0.0589, Val Loss: 0.0782\n",
      "Epoch 35/300 - Train Loss: 0.0623, Val Loss: 0.0821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:41:46,402] Trial 468 finished with value: 0.9689940994057915 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.18329682327847202, 'learning_rate': 0.000823537819881254, 'batch_size': 32, 'weight_decay': 4.013224743772149e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/300 - Train Loss: 0.0579, Val Loss: 0.0840\n",
      "Early stopping at epoch 36\n",
      "Macro F1 Score: 0.9690, Macro Precision: 0.9772, Macro Recall: 0.9613\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.92      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 470\n",
      "Training with F1=16, F2=8, D=2, dropout=0.20089808353919988, LR=0.0009985782255729385, BS=32, WD=3.2293276283545145e-05\n",
      "Epoch 1/300 - Train Loss: 0.1634, Val Loss: 0.0803\n",
      "Epoch 2/300 - Train Loss: 0.1038, Val Loss: 0.0780\n",
      "Epoch 3/300 - Train Loss: 0.0971, Val Loss: 0.0764\n",
      "Epoch 4/300 - Train Loss: 0.0963, Val Loss: 0.0751\n",
      "Epoch 5/300 - Train Loss: 0.0931, Val Loss: 0.0702\n",
      "Epoch 6/300 - Train Loss: 0.0933, Val Loss: 0.0803\n",
      "Epoch 7/300 - Train Loss: 0.0896, Val Loss: 0.0730\n",
      "Epoch 8/300 - Train Loss: 0.0889, Val Loss: 0.0684\n",
      "Epoch 9/300 - Train Loss: 0.0877, Val Loss: 0.0740\n",
      "Epoch 10/300 - Train Loss: 0.0868, Val Loss: 0.0690\n",
      "Epoch 11/300 - Train Loss: 0.0875, Val Loss: 0.0709\n",
      "Epoch 12/300 - Train Loss: 0.0828, Val Loss: 0.0758\n",
      "Epoch 13/300 - Train Loss: 0.0843, Val Loss: 0.0796\n",
      "Epoch 14/300 - Train Loss: 0.0846, Val Loss: 0.0793\n",
      "Epoch 15/300 - Train Loss: 0.0841, Val Loss: 0.0752\n",
      "Epoch 16/300 - Train Loss: 0.0836, Val Loss: 0.0722\n",
      "Epoch 17/300 - Train Loss: 0.0831, Val Loss: 0.0745\n",
      "Epoch 18/300 - Train Loss: 0.0839, Val Loss: 0.0721\n",
      "Epoch 19/300 - Train Loss: 0.0821, Val Loss: 0.0732\n",
      "Epoch 20/300 - Train Loss: 0.0785, Val Loss: 0.0729\n",
      "Epoch 21/300 - Train Loss: 0.0819, Val Loss: 0.0786\n",
      "Epoch 22/300 - Train Loss: 0.0807, Val Loss: 0.0737\n",
      "Epoch 23/300 - Train Loss: 0.0797, Val Loss: 0.0740\n",
      "Epoch 24/300 - Train Loss: 0.0820, Val Loss: 0.0733\n",
      "Epoch 25/300 - Train Loss: 0.0785, Val Loss: 0.0845\n",
      "Epoch 26/300 - Train Loss: 0.0797, Val Loss: 0.0694\n",
      "Epoch 27/300 - Train Loss: 0.0788, Val Loss: 0.0807\n",
      "Epoch 28/300 - Train Loss: 0.0765, Val Loss: 0.0719\n",
      "Epoch 29/300 - Train Loss: 0.0779, Val Loss: 0.0735\n",
      "Epoch 30/300 - Train Loss: 0.0758, Val Loss: 0.0736\n",
      "Epoch 31/300 - Train Loss: 0.0785, Val Loss: 0.0698\n",
      "Epoch 32/300 - Train Loss: 0.0761, Val Loss: 0.0727\n",
      "Epoch 33/300 - Train Loss: 0.0772, Val Loss: 0.0741\n",
      "Epoch 34/300 - Train Loss: 0.0783, Val Loss: 0.0739\n",
      "Epoch 35/300 - Train Loss: 0.0762, Val Loss: 0.0722\n",
      "Epoch 36/300 - Train Loss: 0.0735, Val Loss: 0.0719\n",
      "Epoch 37/300 - Train Loss: 0.0772, Val Loss: 0.0741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:43:07,138] Trial 469 finished with value: 0.968653986208845 and parameters: {'F1': 16, 'F2': 8, 'D': 2, 'dropout': 0.20089808353919988, 'learning_rate': 0.0009985782255729385, 'batch_size': 32, 'weight_decay': 3.2293276283545145e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300 - Train Loss: 0.0721, Val Loss: 0.0698\n",
      "Early stopping at epoch 38\n",
      "Macro F1 Score: 0.9687, Macro Precision: 0.9723, Macro Recall: 0.9652\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 471\n",
      "Training with F1=32, F2=32, D=8, dropout=0.16134457349788606, LR=6.241454532216317e-05, BS=32, WD=4.341003956425836e-05\n",
      "Epoch 1/300 - Train Loss: 0.2581, Val Loss: 0.2032\n",
      "Epoch 2/300 - Train Loss: 0.1213, Val Loss: 0.0826\n",
      "Epoch 3/300 - Train Loss: 0.1045, Val Loss: 0.0745\n",
      "Epoch 4/300 - Train Loss: 0.0961, Val Loss: 0.0741\n",
      "Epoch 5/300 - Train Loss: 0.0923, Val Loss: 0.0934\n",
      "Epoch 6/300 - Train Loss: 0.0889, Val Loss: 0.0792\n",
      "Epoch 7/300 - Train Loss: 0.0859, Val Loss: 0.0715\n",
      "Epoch 8/300 - Train Loss: 0.0851, Val Loss: 0.0733\n",
      "Epoch 9/300 - Train Loss: 0.0843, Val Loss: 0.0699\n",
      "Epoch 10/300 - Train Loss: 0.0823, Val Loss: 0.0687\n",
      "Epoch 11/300 - Train Loss: 0.0810, Val Loss: 0.0763\n",
      "Epoch 12/300 - Train Loss: 0.0797, Val Loss: 0.0680\n",
      "Epoch 13/300 - Train Loss: 0.0800, Val Loss: 0.0685\n",
      "Epoch 14/300 - Train Loss: 0.0783, Val Loss: 0.0741\n",
      "Epoch 15/300 - Train Loss: 0.0754, Val Loss: 0.0763\n",
      "Epoch 16/300 - Train Loss: 0.0749, Val Loss: 0.0695\n",
      "Epoch 17/300 - Train Loss: 0.0762, Val Loss: 0.0695\n",
      "Epoch 18/300 - Train Loss: 0.0748, Val Loss: 0.0710\n",
      "Epoch 19/300 - Train Loss: 0.0714, Val Loss: 0.0678\n",
      "Epoch 20/300 - Train Loss: 0.0707, Val Loss: 0.0682\n",
      "Epoch 21/300 - Train Loss: 0.0702, Val Loss: 0.0728\n",
      "Epoch 22/300 - Train Loss: 0.0801, Val Loss: 0.0644\n",
      "Epoch 23/300 - Train Loss: 0.0699, Val Loss: 0.0647\n",
      "Epoch 24/300 - Train Loss: 0.0674, Val Loss: 0.0654\n",
      "Epoch 25/300 - Train Loss: 0.0673, Val Loss: 0.0695\n",
      "Epoch 26/300 - Train Loss: 0.0665, Val Loss: 0.0644\n",
      "Epoch 27/300 - Train Loss: 0.0655, Val Loss: 0.0692\n",
      "Epoch 28/300 - Train Loss: 0.0644, Val Loss: 0.0642\n",
      "Epoch 29/300 - Train Loss: 0.0660, Val Loss: 0.0667\n",
      "Epoch 30/300 - Train Loss: 0.0649, Val Loss: 0.0715\n",
      "Epoch 31/300 - Train Loss: 0.0644, Val Loss: 0.0695\n",
      "Epoch 32/300 - Train Loss: 0.0632, Val Loss: 0.0632\n",
      "Epoch 33/300 - Train Loss: 0.0625, Val Loss: 0.0726\n",
      "Epoch 34/300 - Train Loss: 0.0617, Val Loss: 0.0703\n",
      "Epoch 35/300 - Train Loss: 0.0617, Val Loss: 0.0648\n",
      "Epoch 36/300 - Train Loss: 0.0614, Val Loss: 0.0652\n",
      "Epoch 37/300 - Train Loss: 0.0608, Val Loss: 0.0613\n",
      "Epoch 38/300 - Train Loss: 0.0613, Val Loss: 0.0631\n",
      "Epoch 39/300 - Train Loss: 0.0591, Val Loss: 0.0629\n",
      "Epoch 40/300 - Train Loss: 0.0607, Val Loss: 0.0771\n",
      "Epoch 41/300 - Train Loss: 0.0614, Val Loss: 0.0614\n",
      "Epoch 42/300 - Train Loss: 0.0593, Val Loss: 0.0809\n",
      "Epoch 43/300 - Train Loss: 0.0587, Val Loss: 0.0646\n",
      "Epoch 44/300 - Train Loss: 0.0555, Val Loss: 0.0633\n",
      "Epoch 45/300 - Train Loss: 0.0580, Val Loss: 0.0618\n",
      "Epoch 46/300 - Train Loss: 0.0571, Val Loss: 0.0656\n",
      "Epoch 47/300 - Train Loss: 0.0573, Val Loss: 0.0633\n",
      "Epoch 48/300 - Train Loss: 0.0564, Val Loss: 0.0644\n",
      "Epoch 49/300 - Train Loss: 0.0561, Val Loss: 0.0738\n",
      "Epoch 50/300 - Train Loss: 0.0547, Val Loss: 0.0619\n",
      "Epoch 51/300 - Train Loss: 0.0546, Val Loss: 0.0684\n",
      "Epoch 52/300 - Train Loss: 0.0544, Val Loss: 0.0684\n",
      "Epoch 53/300 - Train Loss: 0.0544, Val Loss: 0.0668\n",
      "Epoch 54/300 - Train Loss: 0.0555, Val Loss: 0.0654\n",
      "Epoch 55/300 - Train Loss: 0.0541, Val Loss: 0.0696\n",
      "Epoch 56/300 - Train Loss: 0.0525, Val Loss: 0.0655\n",
      "Epoch 57/300 - Train Loss: 0.0525, Val Loss: 0.0655\n",
      "Epoch 58/300 - Train Loss: 0.0504, Val Loss: 0.0620\n",
      "Epoch 59/300 - Train Loss: 0.0507, Val Loss: 0.0639\n",
      "Epoch 60/300 - Train Loss: 0.0516, Val Loss: 0.0739\n",
      "Epoch 61/300 - Train Loss: 0.0490, Val Loss: 0.0649\n",
      "Epoch 62/300 - Train Loss: 0.0500, Val Loss: 0.0658\n",
      "Epoch 63/300 - Train Loss: 0.0503, Val Loss: 0.0649\n",
      "Epoch 64/300 - Train Loss: 0.0492, Val Loss: 0.0662\n",
      "Epoch 65/300 - Train Loss: 0.0487, Val Loss: 0.0675\n",
      "Epoch 66/300 - Train Loss: 0.0464, Val Loss: 0.0635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:50:23,879] Trial 470 finished with value: 0.968209423630201 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.16134457349788606, 'learning_rate': 6.241454532216317e-05, 'batch_size': 32, 'weight_decay': 4.341003956425836e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/300 - Train Loss: 0.0473, Val Loss: 0.0643\n",
      "Early stopping at epoch 67\n",
      "Macro F1 Score: 0.9682, Macro Precision: 0.9641, Macro Recall: 0.9725\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 472\n",
      "Training with F1=32, F2=16, D=2, dropout=0.17533841644720968, LR=0.00011155922086129778, BS=32, WD=5.292379729646887e-05\n",
      "Epoch 1/300 - Train Loss: 0.2950, Val Loss: 0.1205\n",
      "Epoch 2/300 - Train Loss: 0.1252, Val Loss: 0.0853\n",
      "Epoch 3/300 - Train Loss: 0.1067, Val Loss: 0.0821\n",
      "Epoch 4/300 - Train Loss: 0.0991, Val Loss: 0.0786\n",
      "Epoch 5/300 - Train Loss: 0.0926, Val Loss: 0.0750\n",
      "Epoch 6/300 - Train Loss: 0.0900, Val Loss: 0.0716\n",
      "Epoch 7/300 - Train Loss: 0.0891, Val Loss: 0.0706\n",
      "Epoch 8/300 - Train Loss: 0.0873, Val Loss: 0.0738\n",
      "Epoch 9/300 - Train Loss: 0.0841, Val Loss: 0.0738\n",
      "Epoch 10/300 - Train Loss: 0.0854, Val Loss: 0.0767\n",
      "Epoch 11/300 - Train Loss: 0.0864, Val Loss: 0.0694\n",
      "Epoch 12/300 - Train Loss: 0.0854, Val Loss: 0.0707\n",
      "Epoch 13/300 - Train Loss: 0.0820, Val Loss: 0.0701\n",
      "Epoch 14/300 - Train Loss: 0.0804, Val Loss: 0.0726\n",
      "Epoch 15/300 - Train Loss: 0.0792, Val Loss: 0.0693\n",
      "Epoch 16/300 - Train Loss: 0.0793, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.0806, Val Loss: 0.0740\n",
      "Epoch 18/300 - Train Loss: 0.0808, Val Loss: 0.0789\n",
      "Epoch 19/300 - Train Loss: 0.0797, Val Loss: 0.0692\n",
      "Epoch 20/300 - Train Loss: 0.0774, Val Loss: 0.0708\n",
      "Epoch 21/300 - Train Loss: 0.0785, Val Loss: 0.0656\n",
      "Epoch 22/300 - Train Loss: 0.0782, Val Loss: 0.0684\n",
      "Epoch 23/300 - Train Loss: 0.0769, Val Loss: 0.0672\n",
      "Epoch 24/300 - Train Loss: 0.0780, Val Loss: 0.0701\n",
      "Epoch 25/300 - Train Loss: 0.0776, Val Loss: 0.0724\n",
      "Epoch 26/300 - Train Loss: 0.0783, Val Loss: 0.0717\n",
      "Epoch 27/300 - Train Loss: 0.0780, Val Loss: 0.0704\n",
      "Epoch 28/300 - Train Loss: 0.0736, Val Loss: 0.0687\n",
      "Epoch 29/300 - Train Loss: 0.0718, Val Loss: 0.0706\n",
      "Epoch 30/300 - Train Loss: 0.0708, Val Loss: 0.0712\n",
      "Epoch 31/300 - Train Loss: 0.0746, Val Loss: 0.0721\n",
      "Epoch 32/300 - Train Loss: 0.0746, Val Loss: 0.0723\n",
      "Epoch 33/300 - Train Loss: 0.0739, Val Loss: 0.0709\n",
      "Epoch 34/300 - Train Loss: 0.0736, Val Loss: 0.0689\n",
      "Epoch 35/300 - Train Loss: 0.0722, Val Loss: 0.0690\n",
      "Epoch 36/300 - Train Loss: 0.0755, Val Loss: 0.0671\n",
      "Epoch 37/300 - Train Loss: 0.0738, Val Loss: 0.0659\n",
      "Epoch 38/300 - Train Loss: 0.0749, Val Loss: 0.0676\n",
      "Epoch 39/300 - Train Loss: 0.0719, Val Loss: 0.0668\n",
      "Epoch 40/300 - Train Loss: 0.0708, Val Loss: 0.0742\n",
      "Epoch 41/300 - Train Loss: 0.0704, Val Loss: 0.0646\n",
      "Epoch 42/300 - Train Loss: 0.0720, Val Loss: 0.0746\n",
      "Epoch 43/300 - Train Loss: 0.0713, Val Loss: 0.0673\n",
      "Epoch 44/300 - Train Loss: 0.0731, Val Loss: 0.0696\n",
      "Epoch 45/300 - Train Loss: 0.0739, Val Loss: 0.0765\n",
      "Epoch 46/300 - Train Loss: 0.0730, Val Loss: 0.0641\n",
      "Epoch 47/300 - Train Loss: 0.0725, Val Loss: 0.0679\n",
      "Epoch 48/300 - Train Loss: 0.0701, Val Loss: 0.0691\n",
      "Epoch 49/300 - Train Loss: 0.0685, Val Loss: 0.0659\n",
      "Epoch 50/300 - Train Loss: 0.0659, Val Loss: 0.0662\n",
      "Epoch 51/300 - Train Loss: 0.0692, Val Loss: 0.0685\n",
      "Epoch 52/300 - Train Loss: 0.0656, Val Loss: 0.0684\n",
      "Epoch 53/300 - Train Loss: 0.0691, Val Loss: 0.0682\n",
      "Epoch 54/300 - Train Loss: 0.0666, Val Loss: 0.0698\n",
      "Epoch 55/300 - Train Loss: 0.0677, Val Loss: 0.0700\n",
      "Epoch 56/300 - Train Loss: 0.0674, Val Loss: 0.0642\n",
      "Epoch 57/300 - Train Loss: 0.0673, Val Loss: 0.0680\n",
      "Epoch 58/300 - Train Loss: 0.0667, Val Loss: 0.0707\n",
      "Epoch 59/300 - Train Loss: 0.0650, Val Loss: 0.0703\n",
      "Epoch 60/300 - Train Loss: 0.0660, Val Loss: 0.0676\n",
      "Epoch 61/300 - Train Loss: 0.0644, Val Loss: 0.0735\n",
      "Epoch 62/300 - Train Loss: 0.0667, Val Loss: 0.0696\n",
      "Epoch 63/300 - Train Loss: 0.0637, Val Loss: 0.0692\n",
      "Epoch 64/300 - Train Loss: 0.0642, Val Loss: 0.0668\n",
      "Epoch 65/300 - Train Loss: 0.0657, Val Loss: 0.0675\n",
      "Epoch 66/300 - Train Loss: 0.0640, Val Loss: 0.0709\n",
      "Epoch 67/300 - Train Loss: 0.0628, Val Loss: 0.0658\n",
      "Epoch 68/300 - Train Loss: 0.0629, Val Loss: 0.0670\n",
      "Epoch 69/300 - Train Loss: 0.0631, Val Loss: 0.0678\n",
      "Epoch 70/300 - Train Loss: 0.0643, Val Loss: 0.0735\n",
      "Epoch 71/300 - Train Loss: 0.0630, Val Loss: 0.0655\n",
      "Epoch 72/300 - Train Loss: 0.0619, Val Loss: 0.0728\n",
      "Epoch 73/300 - Train Loss: 0.0614, Val Loss: 0.0686\n",
      "Epoch 74/300 - Train Loss: 0.0614, Val Loss: 0.0698\n",
      "Epoch 75/300 - Train Loss: 0.0619, Val Loss: 0.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:53:46,745] Trial 471 finished with value: 0.9672826307185359 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.17533841644720968, 'learning_rate': 0.00011155922086129778, 'batch_size': 32, 'weight_decay': 5.292379729646887e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76/300 - Train Loss: 0.0617, Val Loss: 0.0678\n",
      "Early stopping at epoch 76\n",
      "Macro F1 Score: 0.9673, Macro Precision: 0.9637, Macro Recall: 0.9711\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 473\n",
      "Training with F1=16, F2=32, D=2, dropout=0.21245704147458466, LR=0.0008911531403039732, BS=32, WD=9.075120261006693e-05\n",
      "Epoch 1/300 - Train Loss: 0.1508, Val Loss: 0.0897\n",
      "Epoch 2/300 - Train Loss: 0.0991, Val Loss: 0.0939\n",
      "Epoch 3/300 - Train Loss: 0.0943, Val Loss: 0.0852\n",
      "Epoch 4/300 - Train Loss: 0.0911, Val Loss: 0.0742\n",
      "Epoch 5/300 - Train Loss: 0.0917, Val Loss: 0.0696\n",
      "Epoch 6/300 - Train Loss: 0.0894, Val Loss: 0.0923\n",
      "Epoch 7/300 - Train Loss: 0.0896, Val Loss: 0.0720\n",
      "Epoch 8/300 - Train Loss: 0.0869, Val Loss: 0.0856\n",
      "Epoch 9/300 - Train Loss: 0.0825, Val Loss: 0.0813\n",
      "Epoch 10/300 - Train Loss: 0.0815, Val Loss: 0.0727\n",
      "Epoch 11/300 - Train Loss: 0.0823, Val Loss: 0.0757\n",
      "Epoch 12/300 - Train Loss: 0.0812, Val Loss: 0.0792\n",
      "Epoch 13/300 - Train Loss: 0.0780, Val Loss: 0.0886\n",
      "Epoch 14/300 - Train Loss: 0.0769, Val Loss: 0.0759\n",
      "Epoch 15/300 - Train Loss: 0.0805, Val Loss: 0.0719\n",
      "Epoch 16/300 - Train Loss: 0.0773, Val Loss: 0.0695\n",
      "Epoch 17/300 - Train Loss: 0.0758, Val Loss: 0.0693\n",
      "Epoch 18/300 - Train Loss: 0.0744, Val Loss: 0.0733\n",
      "Epoch 19/300 - Train Loss: 0.0745, Val Loss: 0.0728\n",
      "Epoch 20/300 - Train Loss: 0.0740, Val Loss: 0.0724\n",
      "Epoch 21/300 - Train Loss: 0.0735, Val Loss: 0.0717\n",
      "Epoch 22/300 - Train Loss: 0.0723, Val Loss: 0.0777\n",
      "Epoch 23/300 - Train Loss: 0.0720, Val Loss: 0.0752\n",
      "Epoch 24/300 - Train Loss: 0.0718, Val Loss: 0.0849\n",
      "Epoch 25/300 - Train Loss: 0.0752, Val Loss: 0.0753\n",
      "Epoch 26/300 - Train Loss: 0.0705, Val Loss: 0.0707\n",
      "Epoch 27/300 - Train Loss: 0.0694, Val Loss: 0.0771\n",
      "Epoch 28/300 - Train Loss: 0.0714, Val Loss: 0.0810\n",
      "Epoch 29/300 - Train Loss: 0.0715, Val Loss: 0.0729\n",
      "Epoch 30/300 - Train Loss: 0.0685, Val Loss: 0.0754\n",
      "Epoch 31/300 - Train Loss: 0.0692, Val Loss: 0.0805\n",
      "Epoch 32/300 - Train Loss: 0.0678, Val Loss: 0.0728\n",
      "Epoch 33/300 - Train Loss: 0.0683, Val Loss: 0.0764\n",
      "Epoch 34/300 - Train Loss: 0.0695, Val Loss: 0.0721\n",
      "Epoch 35/300 - Train Loss: 0.0677, Val Loss: 0.0769\n",
      "Epoch 36/300 - Train Loss: 0.0655, Val Loss: 0.0791\n",
      "Epoch 37/300 - Train Loss: 0.0678, Val Loss: 0.0791\n",
      "Epoch 38/300 - Train Loss: 0.0676, Val Loss: 0.0754\n",
      "Epoch 39/300 - Train Loss: 0.0640, Val Loss: 0.0749\n",
      "Epoch 40/300 - Train Loss: 0.0651, Val Loss: 0.0748\n",
      "Epoch 41/300 - Train Loss: 0.0643, Val Loss: 0.0822\n",
      "Epoch 42/300 - Train Loss: 0.0681, Val Loss: 0.0744\n",
      "Epoch 43/300 - Train Loss: 0.0679, Val Loss: 0.0745\n",
      "Epoch 44/300 - Train Loss: 0.0637, Val Loss: 0.0821\n",
      "Epoch 45/300 - Train Loss: 0.0658, Val Loss: 0.0731\n",
      "Epoch 46/300 - Train Loss: 0.0639, Val Loss: 0.0839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 03:55:28,399] Trial 472 finished with value: 0.9618489233193932 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.21245704147458466, 'learning_rate': 0.0008911531403039732, 'batch_size': 32, 'weight_decay': 9.075120261006693e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/300 - Train Loss: 0.0648, Val Loss: 0.0743\n",
      "Early stopping at epoch 47\n",
      "Macro F1 Score: 0.9618, Macro Precision: 0.9696, Macro Recall: 0.9546\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.90      0.92        61\n",
      "           2       0.98      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.95      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 474\n",
      "Training with F1=32, F2=8, D=8, dropout=0.11981121253914978, LR=8.699822668454781e-05, BS=256, WD=4.938674122138832e-05\n",
      "Epoch 1/300 - Train Loss: 0.6825, Val Loss: 0.4148\n",
      "Epoch 2/300 - Train Loss: 0.3166, Val Loss: 0.2415\n",
      "Epoch 3/300 - Train Loss: 0.2229, Val Loss: 0.1922\n",
      "Epoch 4/300 - Train Loss: 0.1711, Val Loss: 0.1529\n",
      "Epoch 5/300 - Train Loss: 0.1388, Val Loss: 0.1133\n",
      "Epoch 6/300 - Train Loss: 0.1218, Val Loss: 0.1053\n",
      "Epoch 7/300 - Train Loss: 0.1140, Val Loss: 0.0986\n",
      "Epoch 8/300 - Train Loss: 0.1065, Val Loss: 0.0875\n",
      "Epoch 9/300 - Train Loss: 0.1019, Val Loss: 0.0844\n",
      "Epoch 10/300 - Train Loss: 0.0966, Val Loss: 0.0983\n",
      "Epoch 11/300 - Train Loss: 0.0936, Val Loss: 0.0948\n",
      "Epoch 12/300 - Train Loss: 0.0913, Val Loss: 0.0790\n",
      "Epoch 13/300 - Train Loss: 0.0874, Val Loss: 0.0721\n",
      "Epoch 14/300 - Train Loss: 0.0852, Val Loss: 0.0782\n",
      "Epoch 15/300 - Train Loss: 0.0855, Val Loss: 0.0768\n",
      "Epoch 16/300 - Train Loss: 0.0838, Val Loss: 0.0757\n",
      "Epoch 17/300 - Train Loss: 0.0817, Val Loss: 0.0703\n",
      "Epoch 18/300 - Train Loss: 0.0806, Val Loss: 0.0725\n",
      "Epoch 19/300 - Train Loss: 0.0785, Val Loss: 0.0713\n",
      "Epoch 20/300 - Train Loss: 0.0787, Val Loss: 0.0695\n",
      "Epoch 21/300 - Train Loss: 0.0769, Val Loss: 0.0659\n",
      "Epoch 22/300 - Train Loss: 0.0781, Val Loss: 0.0798\n",
      "Epoch 23/300 - Train Loss: 0.0759, Val Loss: 0.0687\n",
      "Epoch 24/300 - Train Loss: 0.0757, Val Loss: 0.0697\n",
      "Epoch 25/300 - Train Loss: 0.0773, Val Loss: 0.0717\n",
      "Epoch 26/300 - Train Loss: 0.0755, Val Loss: 0.0706\n",
      "Epoch 27/300 - Train Loss: 0.0739, Val Loss: 0.0675\n",
      "Epoch 28/300 - Train Loss: 0.0730, Val Loss: 0.0672\n",
      "Epoch 29/300 - Train Loss: 0.0745, Val Loss: 0.0653\n",
      "Epoch 30/300 - Train Loss: 0.0717, Val Loss: 0.0703\n",
      "Epoch 31/300 - Train Loss: 0.0711, Val Loss: 0.0625\n",
      "Epoch 32/300 - Train Loss: 0.0712, Val Loss: 0.0680\n",
      "Epoch 33/300 - Train Loss: 0.0709, Val Loss: 0.0632\n",
      "Epoch 34/300 - Train Loss: 0.0701, Val Loss: 0.0691\n",
      "Epoch 35/300 - Train Loss: 0.0706, Val Loss: 0.0660\n",
      "Epoch 36/300 - Train Loss: 0.0702, Val Loss: 0.0720\n",
      "Epoch 37/300 - Train Loss: 0.0686, Val Loss: 0.0635\n",
      "Epoch 38/300 - Train Loss: 0.0715, Val Loss: 0.0664\n",
      "Epoch 39/300 - Train Loss: 0.0708, Val Loss: 0.0645\n",
      "Epoch 40/300 - Train Loss: 0.0677, Val Loss: 0.0685\n",
      "Epoch 41/300 - Train Loss: 0.0684, Val Loss: 0.0654\n",
      "Epoch 42/300 - Train Loss: 0.0687, Val Loss: 0.0657\n",
      "Epoch 43/300 - Train Loss: 0.0670, Val Loss: 0.0636\n",
      "Epoch 44/300 - Train Loss: 0.0687, Val Loss: 0.0683\n",
      "Epoch 45/300 - Train Loss: 0.0675, Val Loss: 0.0632\n",
      "Epoch 46/300 - Train Loss: 0.0664, Val Loss: 0.0659\n",
      "Epoch 47/300 - Train Loss: 0.0684, Val Loss: 0.0626\n",
      "Epoch 48/300 - Train Loss: 0.0687, Val Loss: 0.0645\n",
      "Epoch 49/300 - Train Loss: 0.0666, Val Loss: 0.0643\n",
      "Epoch 50/300 - Train Loss: 0.0659, Val Loss: 0.0663\n",
      "Epoch 51/300 - Train Loss: 0.0663, Val Loss: 0.0633\n",
      "Epoch 52/300 - Train Loss: 0.0648, Val Loss: 0.0609\n",
      "Epoch 53/300 - Train Loss: 0.0677, Val Loss: 0.0674\n",
      "Epoch 54/300 - Train Loss: 0.0654, Val Loss: 0.0613\n",
      "Epoch 55/300 - Train Loss: 0.0644, Val Loss: 0.0642\n",
      "Epoch 56/300 - Train Loss: 0.0649, Val Loss: 0.0618\n",
      "Epoch 57/300 - Train Loss: 0.0654, Val Loss: 0.0705\n",
      "Epoch 58/300 - Train Loss: 0.0639, Val Loss: 0.0665\n",
      "Epoch 59/300 - Train Loss: 0.0636, Val Loss: 0.0635\n",
      "Epoch 60/300 - Train Loss: 0.0633, Val Loss: 0.0681\n",
      "Epoch 61/300 - Train Loss: 0.0647, Val Loss: 0.0611\n",
      "Epoch 62/300 - Train Loss: 0.0635, Val Loss: 0.0641\n",
      "Epoch 63/300 - Train Loss: 0.0641, Val Loss: 0.0681\n",
      "Epoch 64/300 - Train Loss: 0.0619, Val Loss: 0.0714\n",
      "Epoch 65/300 - Train Loss: 0.0633, Val Loss: 0.0628\n",
      "Epoch 66/300 - Train Loss: 0.0644, Val Loss: 0.0604\n",
      "Epoch 67/300 - Train Loss: 0.0621, Val Loss: 0.0675\n",
      "Epoch 68/300 - Train Loss: 0.0641, Val Loss: 0.0679\n",
      "Epoch 69/300 - Train Loss: 0.0614, Val Loss: 0.0615\n",
      "Epoch 70/300 - Train Loss: 0.0620, Val Loss: 0.0661\n",
      "Epoch 71/300 - Train Loss: 0.0625, Val Loss: 0.0627\n",
      "Epoch 72/300 - Train Loss: 0.0613, Val Loss: 0.0615\n",
      "Epoch 73/300 - Train Loss: 0.0613, Val Loss: 0.0635\n",
      "Epoch 74/300 - Train Loss: 0.0609, Val Loss: 0.0633\n",
      "Epoch 75/300 - Train Loss: 0.0622, Val Loss: 0.0662\n",
      "Epoch 76/300 - Train Loss: 0.0614, Val Loss: 0.0645\n",
      "Epoch 77/300 - Train Loss: 0.0615, Val Loss: 0.0605\n",
      "Epoch 78/300 - Train Loss: 0.0601, Val Loss: 0.0662\n",
      "Epoch 79/300 - Train Loss: 0.0608, Val Loss: 0.0606\n",
      "Epoch 80/300 - Train Loss: 0.0623, Val Loss: 0.0723\n",
      "Epoch 81/300 - Train Loss: 0.0619, Val Loss: 0.0627\n",
      "Epoch 82/300 - Train Loss: 0.0620, Val Loss: 0.0652\n",
      "Epoch 83/300 - Train Loss: 0.0606, Val Loss: 0.0718\n",
      "Epoch 84/300 - Train Loss: 0.0613, Val Loss: 0.0617\n",
      "Epoch 85/300 - Train Loss: 0.0609, Val Loss: 0.0623\n",
      "Epoch 86/300 - Train Loss: 0.0590, Val Loss: 0.0615\n",
      "Epoch 87/300 - Train Loss: 0.0594, Val Loss: 0.0626\n",
      "Epoch 88/300 - Train Loss: 0.0598, Val Loss: 0.0676\n",
      "Epoch 89/300 - Train Loss: 0.0600, Val Loss: 0.0667\n",
      "Epoch 90/300 - Train Loss: 0.0611, Val Loss: 0.0728\n",
      "Epoch 91/300 - Train Loss: 0.0583, Val Loss: 0.0653\n",
      "Epoch 92/300 - Train Loss: 0.0582, Val Loss: 0.0601\n",
      "Epoch 93/300 - Train Loss: 0.0595, Val Loss: 0.0625\n",
      "Epoch 94/300 - Train Loss: 0.0612, Val Loss: 0.0737\n",
      "Epoch 95/300 - Train Loss: 0.0592, Val Loss: 0.0667\n",
      "Epoch 96/300 - Train Loss: 0.0597, Val Loss: 0.0695\n",
      "Epoch 97/300 - Train Loss: 0.0595, Val Loss: 0.0639\n",
      "Epoch 98/300 - Train Loss: 0.0588, Val Loss: 0.0630\n",
      "Epoch 99/300 - Train Loss: 0.0575, Val Loss: 0.0716\n",
      "Epoch 100/300 - Train Loss: 0.0589, Val Loss: 0.0667\n",
      "Epoch 101/300 - Train Loss: 0.0575, Val Loss: 0.0645\n",
      "Epoch 102/300 - Train Loss: 0.0599, Val Loss: 0.0645\n",
      "Epoch 103/300 - Train Loss: 0.0572, Val Loss: 0.0672\n",
      "Epoch 104/300 - Train Loss: 0.0577, Val Loss: 0.0616\n",
      "Epoch 105/300 - Train Loss: 0.0606, Val Loss: 0.0649\n",
      "Epoch 106/300 - Train Loss: 0.0559, Val Loss: 0.0663\n",
      "Epoch 107/300 - Train Loss: 0.0573, Val Loss: 0.0628\n",
      "Epoch 108/300 - Train Loss: 0.0581, Val Loss: 0.0642\n",
      "Epoch 109/300 - Train Loss: 0.0574, Val Loss: 0.0638\n",
      "Epoch 110/300 - Train Loss: 0.0561, Val Loss: 0.0645\n",
      "Epoch 111/300 - Train Loss: 0.0562, Val Loss: 0.0617\n",
      "Epoch 112/300 - Train Loss: 0.0571, Val Loss: 0.0713\n",
      "Epoch 113/300 - Train Loss: 0.0557, Val Loss: 0.0708\n",
      "Epoch 114/300 - Train Loss: 0.0551, Val Loss: 0.0684\n",
      "Epoch 115/300 - Train Loss: 0.0543, Val Loss: 0.0695\n",
      "Epoch 116/300 - Train Loss: 0.0558, Val Loss: 0.0694\n",
      "Epoch 117/300 - Train Loss: 0.0569, Val Loss: 0.0663\n",
      "Epoch 118/300 - Train Loss: 0.0571, Val Loss: 0.0664\n",
      "Epoch 119/300 - Train Loss: 0.0563, Val Loss: 0.0653\n",
      "Epoch 120/300 - Train Loss: 0.0558, Val Loss: 0.0740\n",
      "Epoch 121/300 - Train Loss: 0.0569, Val Loss: 0.0646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:06:26,631] Trial 473 finished with value: 0.9739760659054837 and parameters: {'F1': 32, 'F2': 8, 'D': 8, 'dropout': 0.11981121253914978, 'learning_rate': 8.699822668454781e-05, 'batch_size': 256, 'weight_decay': 4.938674122138832e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 122/300 - Train Loss: 0.0558, Val Loss: 0.0757\n",
      "Early stopping at epoch 122\n",
      "Macro F1 Score: 0.9740, Macro Precision: 0.9723, Macro Recall: 0.9757\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.97      0.96        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 475\n",
      "Training with F1=32, F2=32, D=2, dropout=0.1537931877822222, LR=0.0007415551248226769, BS=32, WD=3.684691457498032e-05\n",
      "Epoch 1/300 - Train Loss: 0.1420, Val Loss: 0.0727\n",
      "Epoch 2/300 - Train Loss: 0.1003, Val Loss: 0.0825\n",
      "Epoch 3/300 - Train Loss: 0.0953, Val Loss: 0.0709\n",
      "Epoch 4/300 - Train Loss: 0.0931, Val Loss: 0.0875\n",
      "Epoch 5/300 - Train Loss: 0.0870, Val Loss: 0.0810\n",
      "Epoch 6/300 - Train Loss: 0.0864, Val Loss: 0.0759\n",
      "Epoch 7/300 - Train Loss: 0.0843, Val Loss: 0.0915\n",
      "Epoch 8/300 - Train Loss: 0.0816, Val Loss: 0.0914\n",
      "Epoch 9/300 - Train Loss: 0.0781, Val Loss: 0.0732\n",
      "Epoch 10/300 - Train Loss: 0.0757, Val Loss: 0.0725\n",
      "Epoch 11/300 - Train Loss: 0.0753, Val Loss: 0.0729\n",
      "Epoch 12/300 - Train Loss: 0.0742, Val Loss: 0.0704\n",
      "Epoch 13/300 - Train Loss: 0.0724, Val Loss: 0.0751\n",
      "Epoch 14/300 - Train Loss: 0.0721, Val Loss: 0.0770\n",
      "Epoch 15/300 - Train Loss: 0.0700, Val Loss: 0.0709\n",
      "Epoch 16/300 - Train Loss: 0.0659, Val Loss: 0.0700\n",
      "Epoch 17/300 - Train Loss: 0.0681, Val Loss: 0.0719\n",
      "Epoch 18/300 - Train Loss: 0.0642, Val Loss: 0.0724\n",
      "Epoch 19/300 - Train Loss: 0.0637, Val Loss: 0.0837\n",
      "Epoch 20/300 - Train Loss: 0.0645, Val Loss: 0.0740\n",
      "Epoch 21/300 - Train Loss: 0.0592, Val Loss: 0.0835\n",
      "Epoch 22/300 - Train Loss: 0.0666, Val Loss: 0.0793\n",
      "Epoch 23/300 - Train Loss: 0.0615, Val Loss: 0.0801\n",
      "Epoch 24/300 - Train Loss: 0.0609, Val Loss: 0.0868\n",
      "Epoch 25/300 - Train Loss: 0.0594, Val Loss: 0.0817\n",
      "Epoch 26/300 - Train Loss: 0.0568, Val Loss: 0.0814\n",
      "Epoch 27/300 - Train Loss: 0.0562, Val Loss: 0.0743\n",
      "Epoch 28/300 - Train Loss: 0.0600, Val Loss: 0.0818\n",
      "Epoch 29/300 - Train Loss: 0.0569, Val Loss: 0.0888\n",
      "Epoch 30/300 - Train Loss: 0.0559, Val Loss: 0.0836\n",
      "Epoch 31/300 - Train Loss: 0.0537, Val Loss: 0.0813\n",
      "Epoch 32/300 - Train Loss: 0.0517, Val Loss: 0.0846\n",
      "Epoch 33/300 - Train Loss: 0.0544, Val Loss: 0.0859\n",
      "Epoch 34/300 - Train Loss: 0.0521, Val Loss: 0.0836\n",
      "Epoch 35/300 - Train Loss: 0.0544, Val Loss: 0.0845\n",
      "Epoch 36/300 - Train Loss: 0.0505, Val Loss: 0.0811\n",
      "Epoch 37/300 - Train Loss: 0.0501, Val Loss: 0.0790\n",
      "Epoch 38/300 - Train Loss: 0.0518, Val Loss: 0.0909\n",
      "Epoch 39/300 - Train Loss: 0.0554, Val Loss: 0.0905\n",
      "Epoch 40/300 - Train Loss: 0.0503, Val Loss: 0.0965\n",
      "Epoch 41/300 - Train Loss: 0.0487, Val Loss: 0.0852\n",
      "Epoch 42/300 - Train Loss: 0.0520, Val Loss: 0.0954\n",
      "Epoch 43/300 - Train Loss: 0.0494, Val Loss: 0.0964\n",
      "Epoch 44/300 - Train Loss: 0.0473, Val Loss: 0.0925\n",
      "Epoch 45/300 - Train Loss: 0.0498, Val Loss: 0.0968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:08:32,678] Trial 474 finished with value: 0.9501121698210326 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.1537931877822222, 'learning_rate': 0.0007415551248226769, 'batch_size': 32, 'weight_decay': 3.684691457498032e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/300 - Train Loss: 0.0489, Val Loss: 0.0936\n",
      "Early stopping at epoch 46\n",
      "Macro F1 Score: 0.9501, Macro Precision: 0.9461, Macro Recall: 0.9543\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.87      0.90      0.89        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.95      0.95      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 476\n",
      "Training with F1=16, F2=32, D=2, dropout=0.1949223202894972, LR=1.3711051204531722e-05, BS=32, WD=6.033744446460347e-05\n",
      "Epoch 1/300 - Train Loss: 0.7710, Val Loss: 0.5217\n",
      "Epoch 2/300 - Train Loss: 0.3884, Val Loss: 0.2872\n",
      "Epoch 3/300 - Train Loss: 0.2768, Val Loss: 0.2263\n",
      "Epoch 4/300 - Train Loss: 0.2338, Val Loss: 0.1953\n",
      "Epoch 5/300 - Train Loss: 0.2087, Val Loss: 0.1779\n",
      "Epoch 6/300 - Train Loss: 0.1944, Val Loss: 0.1581\n",
      "Epoch 7/300 - Train Loss: 0.1829, Val Loss: 0.1558\n",
      "Epoch 8/300 - Train Loss: 0.1715, Val Loss: 0.1457\n",
      "Epoch 9/300 - Train Loss: 0.1619, Val Loss: 0.1324\n",
      "Epoch 10/300 - Train Loss: 0.1521, Val Loss: 0.1287\n",
      "Epoch 11/300 - Train Loss: 0.1424, Val Loss: 0.1255\n",
      "Epoch 12/300 - Train Loss: 0.1343, Val Loss: 0.1161\n",
      "Epoch 13/300 - Train Loss: 0.1285, Val Loss: 0.1089\n",
      "Epoch 14/300 - Train Loss: 0.1241, Val Loss: 0.1046\n",
      "Epoch 15/300 - Train Loss: 0.1195, Val Loss: 0.1015\n",
      "Epoch 16/300 - Train Loss: 0.1164, Val Loss: 0.0958\n",
      "Epoch 17/300 - Train Loss: 0.1133, Val Loss: 0.1039\n",
      "Epoch 18/300 - Train Loss: 0.1101, Val Loss: 0.0993\n",
      "Epoch 19/300 - Train Loss: 0.1074, Val Loss: 0.0899\n",
      "Epoch 20/300 - Train Loss: 0.1074, Val Loss: 0.0936\n",
      "Epoch 21/300 - Train Loss: 0.1048, Val Loss: 0.0893\n",
      "Epoch 22/300 - Train Loss: 0.1049, Val Loss: 0.0922\n",
      "Epoch 23/300 - Train Loss: 0.1046, Val Loss: 0.0910\n",
      "Epoch 24/300 - Train Loss: 0.0997, Val Loss: 0.0881\n",
      "Epoch 25/300 - Train Loss: 0.1010, Val Loss: 0.0899\n",
      "Epoch 26/300 - Train Loss: 0.0986, Val Loss: 0.0875\n",
      "Epoch 27/300 - Train Loss: 0.1002, Val Loss: 0.0875\n",
      "Epoch 28/300 - Train Loss: 0.1001, Val Loss: 0.0850\n",
      "Epoch 29/300 - Train Loss: 0.0973, Val Loss: 0.0886\n",
      "Epoch 30/300 - Train Loss: 0.0985, Val Loss: 0.0863\n",
      "Epoch 31/300 - Train Loss: 0.0979, Val Loss: 0.0845\n",
      "Epoch 32/300 - Train Loss: 0.0956, Val Loss: 0.0869\n",
      "Epoch 33/300 - Train Loss: 0.0972, Val Loss: 0.0850\n",
      "Epoch 34/300 - Train Loss: 0.0964, Val Loss: 0.0807\n",
      "Epoch 35/300 - Train Loss: 0.0930, Val Loss: 0.0840\n",
      "Epoch 36/300 - Train Loss: 0.0985, Val Loss: 0.0818\n",
      "Epoch 37/300 - Train Loss: 0.0940, Val Loss: 0.0832\n",
      "Epoch 38/300 - Train Loss: 0.0921, Val Loss: 0.0821\n",
      "Epoch 39/300 - Train Loss: 0.0938, Val Loss: 0.0855\n",
      "Epoch 40/300 - Train Loss: 0.0929, Val Loss: 0.0798\n",
      "Epoch 41/300 - Train Loss: 0.0930, Val Loss: 0.0784\n",
      "Epoch 42/300 - Train Loss: 0.0916, Val Loss: 0.0809\n",
      "Epoch 43/300 - Train Loss: 0.0918, Val Loss: 0.0809\n",
      "Epoch 44/300 - Train Loss: 0.0920, Val Loss: 0.0808\n",
      "Epoch 45/300 - Train Loss: 0.0907, Val Loss: 0.0826\n",
      "Epoch 46/300 - Train Loss: 0.0904, Val Loss: 0.0805\n",
      "Epoch 47/300 - Train Loss: 0.0914, Val Loss: 0.0780\n",
      "Epoch 48/300 - Train Loss: 0.0908, Val Loss: 0.0862\n",
      "Epoch 49/300 - Train Loss: 0.0895, Val Loss: 0.0801\n",
      "Epoch 50/300 - Train Loss: 0.0882, Val Loss: 0.0820\n",
      "Epoch 51/300 - Train Loss: 0.0892, Val Loss: 0.0796\n",
      "Epoch 52/300 - Train Loss: 0.0903, Val Loss: 0.0819\n",
      "Epoch 53/300 - Train Loss: 0.0867, Val Loss: 0.0794\n",
      "Epoch 54/300 - Train Loss: 0.0870, Val Loss: 0.0818\n",
      "Epoch 55/300 - Train Loss: 0.0895, Val Loss: 0.0822\n",
      "Epoch 56/300 - Train Loss: 0.0886, Val Loss: 0.0768\n",
      "Epoch 57/300 - Train Loss: 0.0876, Val Loss: 0.0770\n",
      "Epoch 58/300 - Train Loss: 0.0874, Val Loss: 0.0844\n",
      "Epoch 59/300 - Train Loss: 0.0874, Val Loss: 0.0796\n",
      "Epoch 60/300 - Train Loss: 0.0868, Val Loss: 0.0796\n",
      "Epoch 61/300 - Train Loss: 0.0878, Val Loss: 0.0786\n",
      "Epoch 62/300 - Train Loss: 0.0876, Val Loss: 0.0830\n",
      "Epoch 63/300 - Train Loss: 0.0883, Val Loss: 0.0794\n",
      "Epoch 64/300 - Train Loss: 0.0851, Val Loss: 0.0795\n",
      "Epoch 65/300 - Train Loss: 0.0845, Val Loss: 0.0785\n",
      "Epoch 66/300 - Train Loss: 0.0876, Val Loss: 0.0774\n",
      "Epoch 67/300 - Train Loss: 0.0842, Val Loss: 0.0800\n",
      "Epoch 68/300 - Train Loss: 0.0867, Val Loss: 0.0844\n",
      "Epoch 69/300 - Train Loss: 0.0851, Val Loss: 0.0770\n",
      "Epoch 70/300 - Train Loss: 0.0843, Val Loss: 0.0811\n",
      "Epoch 71/300 - Train Loss: 0.0844, Val Loss: 0.0788\n",
      "Epoch 72/300 - Train Loss: 0.0875, Val Loss: 0.0766\n",
      "Epoch 73/300 - Train Loss: 0.0871, Val Loss: 0.0840\n",
      "Epoch 74/300 - Train Loss: 0.0834, Val Loss: 0.0763\n",
      "Epoch 75/300 - Train Loss: 0.0860, Val Loss: 0.0815\n",
      "Epoch 76/300 - Train Loss: 0.0879, Val Loss: 0.0760\n",
      "Epoch 77/300 - Train Loss: 0.0832, Val Loss: 0.0761\n",
      "Epoch 78/300 - Train Loss: 0.0847, Val Loss: 0.0831\n",
      "Epoch 79/300 - Train Loss: 0.0848, Val Loss: 0.0769\n",
      "Epoch 80/300 - Train Loss: 0.0872, Val Loss: 0.0783\n",
      "Epoch 81/300 - Train Loss: 0.0836, Val Loss: 0.0794\n",
      "Epoch 82/300 - Train Loss: 0.0849, Val Loss: 0.0758\n",
      "Epoch 83/300 - Train Loss: 0.0832, Val Loss: 0.0781\n",
      "Epoch 84/300 - Train Loss: 0.0836, Val Loss: 0.0785\n",
      "Epoch 85/300 - Train Loss: 0.0833, Val Loss: 0.0836\n",
      "Epoch 86/300 - Train Loss: 0.0818, Val Loss: 0.0799\n",
      "Epoch 87/300 - Train Loss: 0.0832, Val Loss: 0.0771\n",
      "Epoch 88/300 - Train Loss: 0.0819, Val Loss: 0.0816\n",
      "Epoch 89/300 - Train Loss: 0.0825, Val Loss: 0.0756\n",
      "Epoch 90/300 - Train Loss: 0.0823, Val Loss: 0.0786\n",
      "Epoch 91/300 - Train Loss: 0.0807, Val Loss: 0.0809\n",
      "Epoch 92/300 - Train Loss: 0.0814, Val Loss: 0.0823\n",
      "Epoch 93/300 - Train Loss: 0.0815, Val Loss: 0.0766\n",
      "Epoch 94/300 - Train Loss: 0.0826, Val Loss: 0.0792\n",
      "Epoch 95/300 - Train Loss: 0.0817, Val Loss: 0.0758\n",
      "Epoch 96/300 - Train Loss: 0.0806, Val Loss: 0.0749\n",
      "Epoch 97/300 - Train Loss: 0.0834, Val Loss: 0.0825\n",
      "Epoch 98/300 - Train Loss: 0.0816, Val Loss: 0.0772\n",
      "Epoch 99/300 - Train Loss: 0.0802, Val Loss: 0.0768\n",
      "Epoch 100/300 - Train Loss: 0.0805, Val Loss: 0.0742\n",
      "Epoch 101/300 - Train Loss: 0.0832, Val Loss: 0.0727\n",
      "Epoch 102/300 - Train Loss: 0.0803, Val Loss: 0.0774\n",
      "Epoch 103/300 - Train Loss: 0.0787, Val Loss: 0.0761\n",
      "Epoch 104/300 - Train Loss: 0.0825, Val Loss: 0.0769\n",
      "Epoch 105/300 - Train Loss: 0.0800, Val Loss: 0.0765\n",
      "Epoch 106/300 - Train Loss: 0.0819, Val Loss: 0.0750\n",
      "Epoch 107/300 - Train Loss: 0.0793, Val Loss: 0.0736\n",
      "Epoch 108/300 - Train Loss: 0.0807, Val Loss: 0.0785\n",
      "Epoch 109/300 - Train Loss: 0.0805, Val Loss: 0.0794\n",
      "Epoch 110/300 - Train Loss: 0.0792, Val Loss: 0.0788\n",
      "Epoch 111/300 - Train Loss: 0.0812, Val Loss: 0.0774\n",
      "Epoch 112/300 - Train Loss: 0.0783, Val Loss: 0.0741\n",
      "Epoch 113/300 - Train Loss: 0.0774, Val Loss: 0.0737\n",
      "Epoch 114/300 - Train Loss: 0.0788, Val Loss: 0.0778\n",
      "Epoch 115/300 - Train Loss: 0.0799, Val Loss: 0.0769\n",
      "Epoch 116/300 - Train Loss: 0.0818, Val Loss: 0.0869\n",
      "Epoch 117/300 - Train Loss: 0.0779, Val Loss: 0.0732\n",
      "Epoch 118/300 - Train Loss: 0.0780, Val Loss: 0.0727\n",
      "Epoch 119/300 - Train Loss: 0.0802, Val Loss: 0.0772\n",
      "Epoch 120/300 - Train Loss: 0.0802, Val Loss: 0.0807\n",
      "Epoch 121/300 - Train Loss: 0.0800, Val Loss: 0.0753\n",
      "Epoch 122/300 - Train Loss: 0.0795, Val Loss: 0.0736\n",
      "Epoch 123/300 - Train Loss: 0.0795, Val Loss: 0.0763\n",
      "Epoch 124/300 - Train Loss: 0.0796, Val Loss: 0.0767\n",
      "Epoch 125/300 - Train Loss: 0.0774, Val Loss: 0.0764\n",
      "Epoch 126/300 - Train Loss: 0.0789, Val Loss: 0.0757\n",
      "Epoch 127/300 - Train Loss: 0.0784, Val Loss: 0.0738\n",
      "Epoch 128/300 - Train Loss: 0.0772, Val Loss: 0.0748\n",
      "Epoch 129/300 - Train Loss: 0.0776, Val Loss: 0.0738\n",
      "Epoch 130/300 - Train Loss: 0.0791, Val Loss: 0.0775\n",
      "Epoch 131/300 - Train Loss: 0.0795, Val Loss: 0.0767\n",
      "Epoch 132/300 - Train Loss: 0.0759, Val Loss: 0.0740\n",
      "Epoch 133/300 - Train Loss: 0.0786, Val Loss: 0.0793\n",
      "Epoch 134/300 - Train Loss: 0.0781, Val Loss: 0.0908\n",
      "Epoch 135/300 - Train Loss: 0.0780, Val Loss: 0.0775\n",
      "Epoch 136/300 - Train Loss: 0.0768, Val Loss: 0.0745\n",
      "Epoch 137/300 - Train Loss: 0.0776, Val Loss: 0.0769\n",
      "Epoch 138/300 - Train Loss: 0.0771, Val Loss: 0.0771\n",
      "Epoch 139/300 - Train Loss: 0.0779, Val Loss: 0.0778\n",
      "Epoch 140/300 - Train Loss: 0.0761, Val Loss: 0.0765\n",
      "Epoch 141/300 - Train Loss: 0.0767, Val Loss: 0.0744\n",
      "Epoch 142/300 - Train Loss: 0.0770, Val Loss: 0.0816\n",
      "Epoch 143/300 - Train Loss: 0.0772, Val Loss: 0.0758\n",
      "Epoch 144/300 - Train Loss: 0.0761, Val Loss: 0.0759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0781, Val Loss: 0.0768\n",
      "Epoch 146/300 - Train Loss: 0.0777, Val Loss: 0.0778\n",
      "Epoch 147/300 - Train Loss: 0.0766, Val Loss: 0.0770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:13:50,518] Trial 475 finished with value: 0.9608330119103162 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.1949223202894972, 'learning_rate': 1.3711051204531722e-05, 'batch_size': 32, 'weight_decay': 6.033744446460347e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 148/300 - Train Loss: 0.0770, Val Loss: 0.0761\n",
      "Early stopping at epoch 148\n",
      "Macro F1 Score: 0.9608, Macro Precision: 0.9536, Macro Recall: 0.9688\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.89      0.95      0.92        61\n",
      "           2       0.99      0.96      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.97      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 477\n",
      "Training with F1=32, F2=8, D=8, dropout=0.1685813833749746, LR=9.994445897475202e-05, BS=32, WD=3.0292035295010103e-05\n",
      "Epoch 1/300 - Train Loss: 0.3212, Val Loss: 0.1072\n",
      "Epoch 2/300 - Train Loss: 0.1259, Val Loss: 0.0970\n",
      "Epoch 3/300 - Train Loss: 0.1097, Val Loss: 0.0842\n",
      "Epoch 4/300 - Train Loss: 0.1025, Val Loss: 0.0901\n",
      "Epoch 5/300 - Train Loss: 0.0953, Val Loss: 0.0755\n",
      "Epoch 6/300 - Train Loss: 0.0927, Val Loss: 0.0797\n",
      "Epoch 7/300 - Train Loss: 0.0890, Val Loss: 0.0723\n",
      "Epoch 8/300 - Train Loss: 0.0886, Val Loss: 0.0708\n",
      "Epoch 9/300 - Train Loss: 0.0857, Val Loss: 0.0735\n",
      "Epoch 10/300 - Train Loss: 0.0856, Val Loss: 0.0673\n",
      "Epoch 11/300 - Train Loss: 0.0820, Val Loss: 0.0836\n",
      "Epoch 12/300 - Train Loss: 0.0813, Val Loss: 0.0774\n",
      "Epoch 13/300 - Train Loss: 0.0825, Val Loss: 0.0688\n",
      "Epoch 14/300 - Train Loss: 0.0801, Val Loss: 0.0715\n",
      "Epoch 15/300 - Train Loss: 0.0812, Val Loss: 0.0676\n",
      "Epoch 16/300 - Train Loss: 0.0800, Val Loss: 0.0722\n",
      "Epoch 17/300 - Train Loss: 0.0807, Val Loss: 0.0683\n",
      "Epoch 18/300 - Train Loss: 0.0771, Val Loss: 0.0677\n",
      "Epoch 19/300 - Train Loss: 0.0781, Val Loss: 0.0700\n",
      "Epoch 20/300 - Train Loss: 0.0760, Val Loss: 0.0687\n",
      "Epoch 21/300 - Train Loss: 0.0789, Val Loss: 0.0642\n",
      "Epoch 22/300 - Train Loss: 0.0761, Val Loss: 0.0627\n",
      "Epoch 23/300 - Train Loss: 0.0751, Val Loss: 0.0620\n",
      "Epoch 24/300 - Train Loss: 0.0752, Val Loss: 0.0648\n",
      "Epoch 25/300 - Train Loss: 0.0737, Val Loss: 0.0655\n",
      "Epoch 26/300 - Train Loss: 0.0760, Val Loss: 0.0745\n",
      "Epoch 27/300 - Train Loss: 0.0745, Val Loss: 0.0631\n",
      "Epoch 28/300 - Train Loss: 0.0729, Val Loss: 0.0761\n",
      "Epoch 29/300 - Train Loss: 0.0748, Val Loss: 0.0661\n",
      "Epoch 30/300 - Train Loss: 0.0731, Val Loss: 0.0730\n",
      "Epoch 31/300 - Train Loss: 0.0744, Val Loss: 0.0732\n",
      "Epoch 32/300 - Train Loss: 0.0717, Val Loss: 0.0692\n",
      "Epoch 33/300 - Train Loss: 0.0704, Val Loss: 0.0705\n",
      "Epoch 34/300 - Train Loss: 0.0723, Val Loss: 0.0699\n",
      "Epoch 35/300 - Train Loss: 0.0713, Val Loss: 0.0648\n",
      "Epoch 36/300 - Train Loss: 0.0720, Val Loss: 0.0633\n",
      "Epoch 37/300 - Train Loss: 0.0701, Val Loss: 0.0629\n",
      "Epoch 38/300 - Train Loss: 0.0701, Val Loss: 0.0692\n",
      "Epoch 39/300 - Train Loss: 0.0692, Val Loss: 0.0655\n",
      "Epoch 40/300 - Train Loss: 0.0682, Val Loss: 0.0655\n",
      "Epoch 41/300 - Train Loss: 0.0683, Val Loss: 0.0633\n",
      "Epoch 42/300 - Train Loss: 0.0700, Val Loss: 0.0719\n",
      "Epoch 43/300 - Train Loss: 0.0696, Val Loss: 0.0676\n",
      "Epoch 44/300 - Train Loss: 0.0682, Val Loss: 0.0616\n",
      "Epoch 45/300 - Train Loss: 0.0691, Val Loss: 0.0593\n",
      "Epoch 46/300 - Train Loss: 0.0678, Val Loss: 0.0688\n",
      "Epoch 47/300 - Train Loss: 0.0695, Val Loss: 0.0689\n",
      "Epoch 48/300 - Train Loss: 0.0677, Val Loss: 0.0641\n",
      "Epoch 49/300 - Train Loss: 0.0668, Val Loss: 0.0625\n",
      "Epoch 50/300 - Train Loss: 0.0665, Val Loss: 0.0729\n",
      "Epoch 51/300 - Train Loss: 0.0674, Val Loss: 0.0649\n",
      "Epoch 52/300 - Train Loss: 0.0651, Val Loss: 0.0631\n",
      "Epoch 53/300 - Train Loss: 0.0675, Val Loss: 0.0697\n",
      "Epoch 54/300 - Train Loss: 0.0666, Val Loss: 0.0664\n",
      "Epoch 55/300 - Train Loss: 0.0638, Val Loss: 0.0675\n",
      "Epoch 56/300 - Train Loss: 0.0648, Val Loss: 0.0699\n",
      "Epoch 57/300 - Train Loss: 0.0662, Val Loss: 0.0630\n",
      "Epoch 58/300 - Train Loss: 0.0654, Val Loss: 0.0642\n",
      "Epoch 59/300 - Train Loss: 0.0639, Val Loss: 0.0649\n",
      "Epoch 60/300 - Train Loss: 0.0645, Val Loss: 0.0640\n",
      "Epoch 61/300 - Train Loss: 0.0633, Val Loss: 0.0609\n",
      "Epoch 62/300 - Train Loss: 0.0631, Val Loss: 0.0620\n",
      "Epoch 63/300 - Train Loss: 0.0635, Val Loss: 0.0648\n",
      "Epoch 64/300 - Train Loss: 0.0648, Val Loss: 0.0625\n",
      "Epoch 65/300 - Train Loss: 0.0634, Val Loss: 0.0632\n",
      "Epoch 66/300 - Train Loss: 0.0614, Val Loss: 0.0634\n",
      "Epoch 67/300 - Train Loss: 0.0620, Val Loss: 0.0630\n",
      "Epoch 68/300 - Train Loss: 0.0653, Val Loss: 0.0766\n",
      "Epoch 69/300 - Train Loss: 0.0627, Val Loss: 0.0649\n",
      "Epoch 70/300 - Train Loss: 0.0637, Val Loss: 0.0650\n",
      "Epoch 71/300 - Train Loss: 0.0623, Val Loss: 0.0638\n",
      "Epoch 72/300 - Train Loss: 0.0605, Val Loss: 0.0686\n",
      "Epoch 73/300 - Train Loss: 0.0597, Val Loss: 0.0624\n",
      "Epoch 74/300 - Train Loss: 0.0630, Val Loss: 0.0642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:21:00,134] Trial 476 finished with value: 0.9692400152333086 and parameters: {'F1': 32, 'F2': 8, 'D': 8, 'dropout': 0.1685813833749746, 'learning_rate': 9.994445897475202e-05, 'batch_size': 32, 'weight_decay': 3.0292035295010103e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300 - Train Loss: 0.0600, Val Loss: 0.0619\n",
      "Early stopping at epoch 75\n",
      "Macro F1 Score: 0.9692, Macro Precision: 0.9722, Macro Recall: 0.9663\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 478\n",
      "Training with F1=16, F2=32, D=2, dropout=0.13918116166264655, LR=7.391264653599355e-05, BS=32, WD=2.0170350411391843e-05\n",
      "Epoch 1/300 - Train Loss: 0.3202, Val Loss: 0.1347\n",
      "Epoch 2/300 - Train Loss: 0.1391, Val Loss: 0.1055\n",
      "Epoch 3/300 - Train Loss: 0.1124, Val Loss: 0.0930\n",
      "Epoch 4/300 - Train Loss: 0.1035, Val Loss: 0.0895\n",
      "Epoch 5/300 - Train Loss: 0.1006, Val Loss: 0.0850\n",
      "Epoch 6/300 - Train Loss: 0.0974, Val Loss: 0.0836\n",
      "Epoch 7/300 - Train Loss: 0.0924, Val Loss: 0.0840\n",
      "Epoch 8/300 - Train Loss: 0.0943, Val Loss: 0.0835\n",
      "Epoch 9/300 - Train Loss: 0.0896, Val Loss: 0.0808\n",
      "Epoch 10/300 - Train Loss: 0.0884, Val Loss: 0.0780\n",
      "Epoch 11/300 - Train Loss: 0.0848, Val Loss: 0.0759\n",
      "Epoch 12/300 - Train Loss: 0.0861, Val Loss: 0.0773\n",
      "Epoch 13/300 - Train Loss: 0.0856, Val Loss: 0.0747\n",
      "Epoch 14/300 - Train Loss: 0.0844, Val Loss: 0.0781\n",
      "Epoch 15/300 - Train Loss: 0.0843, Val Loss: 0.0764\n",
      "Epoch 16/300 - Train Loss: 0.0836, Val Loss: 0.0787\n",
      "Epoch 17/300 - Train Loss: 0.0822, Val Loss: 0.0752\n",
      "Epoch 18/300 - Train Loss: 0.0808, Val Loss: 0.0741\n",
      "Epoch 19/300 - Train Loss: 0.0803, Val Loss: 0.0739\n",
      "Epoch 20/300 - Train Loss: 0.0820, Val Loss: 0.0746\n",
      "Epoch 21/300 - Train Loss: 0.0815, Val Loss: 0.0805\n",
      "Epoch 22/300 - Train Loss: 0.0778, Val Loss: 0.0735\n",
      "Epoch 23/300 - Train Loss: 0.0779, Val Loss: 0.0745\n",
      "Epoch 24/300 - Train Loss: 0.0763, Val Loss: 0.0774\n",
      "Epoch 25/300 - Train Loss: 0.0788, Val Loss: 0.0744\n",
      "Epoch 26/300 - Train Loss: 0.0751, Val Loss: 0.0793\n",
      "Epoch 27/300 - Train Loss: 0.0768, Val Loss: 0.0773\n",
      "Epoch 28/300 - Train Loss: 0.0765, Val Loss: 0.0761\n",
      "Epoch 29/300 - Train Loss: 0.0754, Val Loss: 0.0731\n",
      "Epoch 30/300 - Train Loss: 0.0744, Val Loss: 0.0702\n",
      "Epoch 31/300 - Train Loss: 0.0755, Val Loss: 0.0717\n",
      "Epoch 32/300 - Train Loss: 0.0732, Val Loss: 0.0729\n",
      "Epoch 33/300 - Train Loss: 0.0730, Val Loss: 0.0768\n",
      "Epoch 34/300 - Train Loss: 0.0733, Val Loss: 0.0756\n",
      "Epoch 35/300 - Train Loss: 0.0716, Val Loss: 0.0734\n",
      "Epoch 36/300 - Train Loss: 0.0751, Val Loss: 0.0710\n",
      "Epoch 37/300 - Train Loss: 0.0712, Val Loss: 0.0727\n",
      "Epoch 38/300 - Train Loss: 0.0693, Val Loss: 0.0712\n",
      "Epoch 39/300 - Train Loss: 0.0751, Val Loss: 0.0710\n",
      "Epoch 40/300 - Train Loss: 0.0704, Val Loss: 0.0706\n",
      "Epoch 41/300 - Train Loss: 0.0699, Val Loss: 0.0731\n",
      "Epoch 42/300 - Train Loss: 0.0702, Val Loss: 0.0756\n",
      "Epoch 43/300 - Train Loss: 0.0694, Val Loss: 0.0713\n",
      "Epoch 44/300 - Train Loss: 0.0724, Val Loss: 0.0708\n",
      "Epoch 45/300 - Train Loss: 0.0700, Val Loss: 0.0761\n",
      "Epoch 46/300 - Train Loss: 0.0701, Val Loss: 0.0736\n",
      "Epoch 47/300 - Train Loss: 0.0675, Val Loss: 0.0729\n",
      "Epoch 48/300 - Train Loss: 0.0707, Val Loss: 0.0861\n",
      "Epoch 49/300 - Train Loss: 0.0673, Val Loss: 0.0745\n",
      "Epoch 50/300 - Train Loss: 0.0654, Val Loss: 0.0710\n",
      "Epoch 51/300 - Train Loss: 0.0686, Val Loss: 0.0697\n",
      "Epoch 52/300 - Train Loss: 0.0677, Val Loss: 0.0717\n",
      "Epoch 53/300 - Train Loss: 0.0665, Val Loss: 0.0723\n",
      "Epoch 54/300 - Train Loss: 0.0644, Val Loss: 0.0762\n",
      "Epoch 55/300 - Train Loss: 0.0663, Val Loss: 0.0726\n",
      "Epoch 56/300 - Train Loss: 0.0655, Val Loss: 0.0715\n",
      "Epoch 57/300 - Train Loss: 0.0666, Val Loss: 0.0716\n",
      "Epoch 58/300 - Train Loss: 0.0656, Val Loss: 0.0743\n",
      "Epoch 59/300 - Train Loss: 0.0657, Val Loss: 0.0761\n",
      "Epoch 60/300 - Train Loss: 0.0638, Val Loss: 0.0733\n",
      "Epoch 61/300 - Train Loss: 0.0632, Val Loss: 0.0750\n",
      "Epoch 62/300 - Train Loss: 0.0642, Val Loss: 0.0735\n",
      "Epoch 63/300 - Train Loss: 0.0615, Val Loss: 0.0731\n",
      "Epoch 64/300 - Train Loss: 0.0625, Val Loss: 0.0677\n",
      "Epoch 65/300 - Train Loss: 0.0633, Val Loss: 0.0692\n",
      "Epoch 66/300 - Train Loss: 0.0630, Val Loss: 0.0686\n",
      "Epoch 67/300 - Train Loss: 0.0617, Val Loss: 0.0721\n",
      "Epoch 68/300 - Train Loss: 0.0629, Val Loss: 0.0697\n",
      "Epoch 69/300 - Train Loss: 0.0617, Val Loss: 0.0733\n",
      "Epoch 70/300 - Train Loss: 0.0591, Val Loss: 0.0762\n",
      "Epoch 71/300 - Train Loss: 0.0621, Val Loss: 0.0736\n",
      "Epoch 72/300 - Train Loss: 0.0607, Val Loss: 0.0711\n",
      "Epoch 73/300 - Train Loss: 0.0593, Val Loss: 0.0691\n",
      "Epoch 74/300 - Train Loss: 0.0600, Val Loss: 0.0747\n",
      "Epoch 75/300 - Train Loss: 0.0596, Val Loss: 0.0723\n",
      "Epoch 76/300 - Train Loss: 0.0599, Val Loss: 0.0695\n",
      "Epoch 77/300 - Train Loss: 0.0586, Val Loss: 0.0724\n",
      "Epoch 78/300 - Train Loss: 0.0599, Val Loss: 0.0698\n",
      "Epoch 79/300 - Train Loss: 0.0601, Val Loss: 0.0744\n",
      "Epoch 80/300 - Train Loss: 0.0603, Val Loss: 0.0698\n",
      "Epoch 81/300 - Train Loss: 0.0588, Val Loss: 0.0763\n",
      "Epoch 82/300 - Train Loss: 0.0590, Val Loss: 0.0679\n",
      "Epoch 83/300 - Train Loss: 0.0578, Val Loss: 0.0722\n",
      "Epoch 84/300 - Train Loss: 0.0594, Val Loss: 0.0718\n",
      "Epoch 85/300 - Train Loss: 0.0601, Val Loss: 0.0715\n",
      "Epoch 86/300 - Train Loss: 0.0568, Val Loss: 0.0703\n",
      "Epoch 87/300 - Train Loss: 0.0577, Val Loss: 0.0722\n",
      "Epoch 88/300 - Train Loss: 0.0593, Val Loss: 0.0707\n",
      "Epoch 89/300 - Train Loss: 0.0546, Val Loss: 0.0735\n",
      "Epoch 90/300 - Train Loss: 0.0593, Val Loss: 0.0683\n",
      "Epoch 91/300 - Train Loss: 0.0566, Val Loss: 0.0704\n",
      "Epoch 92/300 - Train Loss: 0.0579, Val Loss: 0.0762\n",
      "Epoch 93/300 - Train Loss: 0.0594, Val Loss: 0.0767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:24:21,060] Trial 477 finished with value: 0.9716042439251522 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.13918116166264655, 'learning_rate': 7.391264653599355e-05, 'batch_size': 32, 'weight_decay': 2.0170350411391843e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300 - Train Loss: 0.0571, Val Loss: 0.0715\n",
      "Early stopping at epoch 94\n",
      "Macro F1 Score: 0.9716, Macro Precision: 0.9617, Macro Recall: 0.9826\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.98      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 479\n",
      "Training with F1=32, F2=32, D=4, dropout=0.18373429026992535, LR=0.0006107623579427075, BS=32, WD=0.0005151967025173103\n",
      "Epoch 1/300 - Train Loss: 0.1402, Val Loss: 0.0844\n",
      "Epoch 2/300 - Train Loss: 0.0996, Val Loss: 0.0770\n",
      "Epoch 3/300 - Train Loss: 0.0920, Val Loss: 0.0810\n",
      "Epoch 4/300 - Train Loss: 0.0896, Val Loss: 0.0810\n",
      "Epoch 5/300 - Train Loss: 0.0913, Val Loss: 0.0728\n",
      "Epoch 6/300 - Train Loss: 0.0878, Val Loss: 0.0725\n",
      "Epoch 7/300 - Train Loss: 0.0839, Val Loss: 0.0784\n",
      "Epoch 8/300 - Train Loss: 0.0843, Val Loss: 0.0715\n",
      "Epoch 9/300 - Train Loss: 0.0825, Val Loss: 0.0755\n",
      "Epoch 10/300 - Train Loss: 0.0835, Val Loss: 0.0790\n",
      "Epoch 11/300 - Train Loss: 0.0828, Val Loss: 0.0699\n",
      "Epoch 12/300 - Train Loss: 0.0824, Val Loss: 0.0800\n",
      "Epoch 13/300 - Train Loss: 0.0803, Val Loss: 0.0886\n",
      "Epoch 14/300 - Train Loss: 0.0830, Val Loss: 0.0892\n",
      "Epoch 15/300 - Train Loss: 0.0812, Val Loss: 0.0764\n",
      "Epoch 16/300 - Train Loss: 0.0811, Val Loss: 0.0731\n",
      "Epoch 17/300 - Train Loss: 0.0787, Val Loss: 0.0803\n",
      "Epoch 18/300 - Train Loss: 0.0804, Val Loss: 0.0785\n",
      "Epoch 19/300 - Train Loss: 0.0812, Val Loss: 0.0752\n",
      "Epoch 20/300 - Train Loss: 0.0812, Val Loss: 0.0790\n",
      "Epoch 21/300 - Train Loss: 0.0811, Val Loss: 0.0705\n",
      "Epoch 22/300 - Train Loss: 0.0807, Val Loss: 0.0712\n",
      "Epoch 23/300 - Train Loss: 0.0785, Val Loss: 0.0823\n",
      "Epoch 24/300 - Train Loss: 0.0809, Val Loss: 0.0722\n",
      "Epoch 25/300 - Train Loss: 0.0797, Val Loss: 0.0791\n",
      "Epoch 26/300 - Train Loss: 0.0801, Val Loss: 0.0767\n",
      "Epoch 27/300 - Train Loss: 0.0783, Val Loss: 0.0802\n",
      "Epoch 28/300 - Train Loss: 0.0805, Val Loss: 0.0674\n",
      "Epoch 29/300 - Train Loss: 0.0788, Val Loss: 0.0751\n",
      "Epoch 30/300 - Train Loss: 0.0771, Val Loss: 0.0732\n",
      "Epoch 31/300 - Train Loss: 0.0786, Val Loss: 0.0845\n",
      "Epoch 32/300 - Train Loss: 0.0801, Val Loss: 0.0702\n",
      "Epoch 33/300 - Train Loss: 0.0786, Val Loss: 0.0728\n",
      "Epoch 34/300 - Train Loss: 0.0771, Val Loss: 0.0777\n",
      "Epoch 35/300 - Train Loss: 0.0789, Val Loss: 0.0721\n",
      "Epoch 36/300 - Train Loss: 0.0767, Val Loss: 0.0709\n",
      "Epoch 37/300 - Train Loss: 0.0795, Val Loss: 0.0709\n",
      "Epoch 38/300 - Train Loss: 0.0776, Val Loss: 0.0697\n",
      "Epoch 39/300 - Train Loss: 0.0786, Val Loss: 0.0727\n",
      "Epoch 40/300 - Train Loss: 0.0791, Val Loss: 0.0773\n",
      "Epoch 41/300 - Train Loss: 0.0797, Val Loss: 0.0791\n",
      "Epoch 42/300 - Train Loss: 0.0785, Val Loss: 0.0822\n",
      "Epoch 43/300 - Train Loss: 0.0791, Val Loss: 0.0736\n",
      "Epoch 44/300 - Train Loss: 0.0779, Val Loss: 0.0734\n",
      "Epoch 45/300 - Train Loss: 0.0794, Val Loss: 0.0717\n",
      "Epoch 46/300 - Train Loss: 0.0766, Val Loss: 0.0823\n",
      "Epoch 47/300 - Train Loss: 0.0799, Val Loss: 0.0689\n",
      "Epoch 48/300 - Train Loss: 0.0801, Val Loss: 0.0900\n",
      "Epoch 49/300 - Train Loss: 0.0782, Val Loss: 0.0673\n",
      "Epoch 50/300 - Train Loss: 0.0768, Val Loss: 0.0715\n",
      "Epoch 51/300 - Train Loss: 0.0771, Val Loss: 0.0755\n",
      "Epoch 52/300 - Train Loss: 0.0778, Val Loss: 0.0785\n",
      "Epoch 53/300 - Train Loss: 0.0799, Val Loss: 0.0732\n",
      "Epoch 54/300 - Train Loss: 0.0804, Val Loss: 0.0749\n",
      "Epoch 55/300 - Train Loss: 0.0777, Val Loss: 0.0782\n",
      "Epoch 56/300 - Train Loss: 0.0775, Val Loss: 0.0717\n",
      "Epoch 57/300 - Train Loss: 0.0781, Val Loss: 0.0794\n",
      "Epoch 58/300 - Train Loss: 0.0785, Val Loss: 0.0691\n",
      "Epoch 59/300 - Train Loss: 0.0771, Val Loss: 0.0980\n",
      "Epoch 60/300 - Train Loss: 0.0781, Val Loss: 0.0755\n",
      "Epoch 61/300 - Train Loss: 0.0767, Val Loss: 0.0732\n",
      "Epoch 62/300 - Train Loss: 0.0767, Val Loss: 0.0738\n",
      "Epoch 63/300 - Train Loss: 0.0801, Val Loss: 0.0726\n",
      "Epoch 64/300 - Train Loss: 0.0775, Val Loss: 0.0732\n",
      "Epoch 65/300 - Train Loss: 0.0757, Val Loss: 0.0771\n",
      "Epoch 66/300 - Train Loss: 0.0776, Val Loss: 0.0770\n",
      "Epoch 67/300 - Train Loss: 0.0754, Val Loss: 0.0754\n",
      "Epoch 68/300 - Train Loss: 0.0763, Val Loss: 0.0722\n",
      "Epoch 69/300 - Train Loss: 0.0767, Val Loss: 0.0784\n",
      "Epoch 70/300 - Train Loss: 0.0786, Val Loss: 0.0722\n",
      "Epoch 71/300 - Train Loss: 0.0800, Val Loss: 0.0679\n",
      "Epoch 72/300 - Train Loss: 0.0782, Val Loss: 0.0836\n",
      "Epoch 73/300 - Train Loss: 0.0761, Val Loss: 0.0696\n",
      "Epoch 74/300 - Train Loss: 0.0761, Val Loss: 0.0770\n",
      "Epoch 75/300 - Train Loss: 0.0792, Val Loss: 0.0720\n",
      "Epoch 76/300 - Train Loss: 0.0760, Val Loss: 0.0720\n",
      "Epoch 77/300 - Train Loss: 0.0771, Val Loss: 0.0842\n",
      "Epoch 78/300 - Train Loss: 0.0751, Val Loss: 0.0679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:29:43,711] Trial 478 finished with value: 0.9714177950973694 and parameters: {'F1': 32, 'F2': 32, 'D': 4, 'dropout': 0.18373429026992535, 'learning_rate': 0.0006107623579427075, 'batch_size': 32, 'weight_decay': 0.0005151967025173103}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79/300 - Train Loss: 0.0782, Val Loss: 0.0715\n",
      "Early stopping at epoch 79\n",
      "Macro F1 Score: 0.9714, Macro Precision: 0.9723, Macro Recall: 0.9707\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.95      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 480\n",
      "Training with F1=32, F2=8, D=2, dropout=0.15301071119694457, LR=0.0006932040539960214, BS=32, WD=2.5029571227013184e-05\n",
      "Epoch 1/300 - Train Loss: 0.1766, Val Loss: 0.1038\n",
      "Epoch 2/300 - Train Loss: 0.0972, Val Loss: 0.0816\n",
      "Epoch 3/300 - Train Loss: 0.0925, Val Loss: 0.0708\n",
      "Epoch 4/300 - Train Loss: 0.0885, Val Loss: 0.0909\n",
      "Epoch 5/300 - Train Loss: 0.0845, Val Loss: 0.0827\n",
      "Epoch 6/300 - Train Loss: 0.0847, Val Loss: 0.0798\n",
      "Epoch 7/300 - Train Loss: 0.0835, Val Loss: 0.0752\n",
      "Epoch 8/300 - Train Loss: 0.0814, Val Loss: 0.0741\n",
      "Epoch 9/300 - Train Loss: 0.0799, Val Loss: 0.0798\n",
      "Epoch 10/300 - Train Loss: 0.0802, Val Loss: 0.0716\n",
      "Epoch 11/300 - Train Loss: 0.0801, Val Loss: 0.0690\n",
      "Epoch 12/300 - Train Loss: 0.0779, Val Loss: 0.0670\n",
      "Epoch 13/300 - Train Loss: 0.0765, Val Loss: 0.0713\n",
      "Epoch 14/300 - Train Loss: 0.0791, Val Loss: 0.0674\n",
      "Epoch 15/300 - Train Loss: 0.0764, Val Loss: 0.0657\n",
      "Epoch 16/300 - Train Loss: 0.0762, Val Loss: 0.0676\n",
      "Epoch 17/300 - Train Loss: 0.0764, Val Loss: 0.0715\n",
      "Epoch 18/300 - Train Loss: 0.0771, Val Loss: 0.0755\n",
      "Epoch 19/300 - Train Loss: 0.0732, Val Loss: 0.0655\n",
      "Epoch 20/300 - Train Loss: 0.0739, Val Loss: 0.0643\n",
      "Epoch 21/300 - Train Loss: 0.0754, Val Loss: 0.0786\n",
      "Epoch 22/300 - Train Loss: 0.0729, Val Loss: 0.0654\n",
      "Epoch 23/300 - Train Loss: 0.0723, Val Loss: 0.0778\n",
      "Epoch 24/300 - Train Loss: 0.0712, Val Loss: 0.0689\n",
      "Epoch 25/300 - Train Loss: 0.0743, Val Loss: 0.0873\n",
      "Epoch 26/300 - Train Loss: 0.0716, Val Loss: 0.0709\n",
      "Epoch 27/300 - Train Loss: 0.0700, Val Loss: 0.0722\n",
      "Epoch 28/300 - Train Loss: 0.0703, Val Loss: 0.0700\n",
      "Epoch 29/300 - Train Loss: 0.0693, Val Loss: 0.0690\n",
      "Epoch 30/300 - Train Loss: 0.0720, Val Loss: 0.0737\n",
      "Epoch 31/300 - Train Loss: 0.0704, Val Loss: 0.0737\n",
      "Epoch 32/300 - Train Loss: 0.0677, Val Loss: 0.0700\n",
      "Epoch 33/300 - Train Loss: 0.0694, Val Loss: 0.0776\n",
      "Epoch 34/300 - Train Loss: 0.0665, Val Loss: 0.0694\n",
      "Epoch 35/300 - Train Loss: 0.0702, Val Loss: 0.0703\n",
      "Epoch 36/300 - Train Loss: 0.0678, Val Loss: 0.0727\n",
      "Epoch 37/300 - Train Loss: 0.0683, Val Loss: 0.0746\n",
      "Epoch 38/300 - Train Loss: 0.0660, Val Loss: 0.0741\n",
      "Epoch 39/300 - Train Loss: 0.0687, Val Loss: 0.0766\n",
      "Epoch 40/300 - Train Loss: 0.0671, Val Loss: 0.0707\n",
      "Epoch 41/300 - Train Loss: 0.0683, Val Loss: 0.0713\n",
      "Epoch 42/300 - Train Loss: 0.0649, Val Loss: 0.0716\n",
      "Epoch 43/300 - Train Loss: 0.0647, Val Loss: 0.0845\n",
      "Epoch 44/300 - Train Loss: 0.0640, Val Loss: 0.0713\n",
      "Epoch 45/300 - Train Loss: 0.0661, Val Loss: 0.0765\n",
      "Epoch 46/300 - Train Loss: 0.0661, Val Loss: 0.0702\n",
      "Epoch 47/300 - Train Loss: 0.0651, Val Loss: 0.0734\n",
      "Epoch 48/300 - Train Loss: 0.0628, Val Loss: 0.0695\n",
      "Epoch 49/300 - Train Loss: 0.0638, Val Loss: 0.0719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:31:49,530] Trial 479 finished with value: 0.975057514462408 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.15301071119694457, 'learning_rate': 0.0006932040539960214, 'batch_size': 32, 'weight_decay': 2.5029571227013184e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/300 - Train Loss: 0.0641, Val Loss: 0.0689\n",
      "Early stopping at epoch 50\n",
      "Macro F1 Score: 0.9751, Macro Precision: 0.9713, Macro Recall: 0.9791\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.98      0.99       593\n",
      "\n",
      "    accuracy                           0.99      1443\n",
      "   macro avg       0.97      0.98      0.98      1443\n",
      "weighted avg       0.99      0.99      0.99      1443\n",
      "\n",
      "\n",
      "Trial 481\n",
      "Training with F1=16, F2=32, D=8, dropout=0.1309941265875619, LR=0.0006735816170900502, BS=32, WD=2.486852697841485e-05\n",
      "Epoch 1/300 - Train Loss: 0.1385, Val Loss: 0.1330\n",
      "Epoch 2/300 - Train Loss: 0.0940, Val Loss: 0.0897\n",
      "Epoch 3/300 - Train Loss: 0.0907, Val Loss: 0.0923\n",
      "Epoch 4/300 - Train Loss: 0.0887, Val Loss: 0.0875\n",
      "Epoch 5/300 - Train Loss: 0.0841, Val Loss: 0.0824\n",
      "Epoch 6/300 - Train Loss: 0.0802, Val Loss: 0.0753\n",
      "Epoch 7/300 - Train Loss: 0.0807, Val Loss: 0.0798\n",
      "Epoch 8/300 - Train Loss: 0.0800, Val Loss: 0.0691\n",
      "Epoch 9/300 - Train Loss: 0.0751, Val Loss: 0.0793\n",
      "Epoch 10/300 - Train Loss: 0.0749, Val Loss: 0.0731\n",
      "Epoch 11/300 - Train Loss: 0.0751, Val Loss: 0.0768\n",
      "Epoch 12/300 - Train Loss: 0.0740, Val Loss: 0.0761\n",
      "Epoch 13/300 - Train Loss: 0.0697, Val Loss: 0.0899\n",
      "Epoch 14/300 - Train Loss: 0.0734, Val Loss: 0.0761\n",
      "Epoch 15/300 - Train Loss: 0.0696, Val Loss: 0.0810\n",
      "Epoch 16/300 - Train Loss: 0.0663, Val Loss: 0.0763\n",
      "Epoch 17/300 - Train Loss: 0.0648, Val Loss: 0.0800\n",
      "Epoch 18/300 - Train Loss: 0.0638, Val Loss: 0.0866\n",
      "Epoch 19/300 - Train Loss: 0.0622, Val Loss: 0.0792\n",
      "Epoch 20/300 - Train Loss: 0.0630, Val Loss: 0.0782\n",
      "Epoch 21/300 - Train Loss: 0.0638, Val Loss: 0.0809\n",
      "Epoch 22/300 - Train Loss: 0.0604, Val Loss: 0.0795\n",
      "Epoch 23/300 - Train Loss: 0.0583, Val Loss: 0.0757\n",
      "Epoch 24/300 - Train Loss: 0.0589, Val Loss: 0.0775\n",
      "Epoch 25/300 - Train Loss: 0.0567, Val Loss: 0.0827\n",
      "Epoch 26/300 - Train Loss: 0.0573, Val Loss: 0.0765\n",
      "Epoch 27/300 - Train Loss: 0.0562, Val Loss: 0.0834\n",
      "Epoch 28/300 - Train Loss: 0.0555, Val Loss: 0.0785\n",
      "Epoch 29/300 - Train Loss: 0.0539, Val Loss: 0.0839\n",
      "Epoch 30/300 - Train Loss: 0.0509, Val Loss: 0.0790\n",
      "Epoch 31/300 - Train Loss: 0.0539, Val Loss: 0.0799\n",
      "Epoch 32/300 - Train Loss: 0.0512, Val Loss: 0.0863\n",
      "Epoch 33/300 - Train Loss: 0.0515, Val Loss: 0.0834\n",
      "Epoch 34/300 - Train Loss: 0.0509, Val Loss: 0.0763\n",
      "Epoch 35/300 - Train Loss: 0.0526, Val Loss: 0.0826\n",
      "Epoch 36/300 - Train Loss: 0.0502, Val Loss: 0.0806\n",
      "Epoch 37/300 - Train Loss: 0.0486, Val Loss: 0.0758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:34:16,159] Trial 480 finished with value: 0.9677848090411757 and parameters: {'F1': 16, 'F2': 32, 'D': 8, 'dropout': 0.1309941265875619, 'learning_rate': 0.0006735816170900502, 'batch_size': 32, 'weight_decay': 2.486852697841485e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/300 - Train Loss: 0.0474, Val Loss: 0.0765\n",
      "Early stopping at epoch 38\n",
      "Macro F1 Score: 0.9678, Macro Precision: 0.9638, Macro Recall: 0.9719\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 482\n",
      "Training with F1=32, F2=32, D=2, dropout=0.14924378774738273, LR=5.926844346803436e-05, BS=64, WD=2.2788205443528047e-05\n",
      "Epoch 1/300 - Train Loss: 0.3836, Val Loss: 0.2032\n",
      "Epoch 2/300 - Train Loss: 0.1749, Val Loss: 0.1278\n",
      "Epoch 3/300 - Train Loss: 0.1262, Val Loss: 0.0975\n",
      "Epoch 4/300 - Train Loss: 0.1053, Val Loss: 0.0904\n",
      "Epoch 5/300 - Train Loss: 0.0982, Val Loss: 0.0922\n",
      "Epoch 6/300 - Train Loss: 0.0922, Val Loss: 0.0903\n",
      "Epoch 7/300 - Train Loss: 0.0885, Val Loss: 0.0818\n",
      "Epoch 8/300 - Train Loss: 0.0865, Val Loss: 0.0803\n",
      "Epoch 9/300 - Train Loss: 0.0854, Val Loss: 0.0815\n",
      "Epoch 10/300 - Train Loss: 0.0825, Val Loss: 0.0757\n",
      "Epoch 11/300 - Train Loss: 0.0808, Val Loss: 0.0764\n",
      "Epoch 12/300 - Train Loss: 0.0805, Val Loss: 0.0779\n",
      "Epoch 13/300 - Train Loss: 0.0809, Val Loss: 0.0802\n",
      "Epoch 14/300 - Train Loss: 0.0794, Val Loss: 0.0771\n",
      "Epoch 15/300 - Train Loss: 0.0807, Val Loss: 0.0741\n",
      "Epoch 16/300 - Train Loss: 0.0776, Val Loss: 0.0778\n",
      "Epoch 17/300 - Train Loss: 0.0763, Val Loss: 0.0758\n",
      "Epoch 18/300 - Train Loss: 0.0747, Val Loss: 0.0809\n",
      "Epoch 19/300 - Train Loss: 0.0751, Val Loss: 0.0709\n",
      "Epoch 20/300 - Train Loss: 0.0741, Val Loss: 0.0741\n",
      "Epoch 21/300 - Train Loss: 0.0749, Val Loss: 0.0791\n",
      "Epoch 22/300 - Train Loss: 0.0737, Val Loss: 0.0757\n",
      "Epoch 23/300 - Train Loss: 0.0725, Val Loss: 0.0783\n",
      "Epoch 24/300 - Train Loss: 0.0725, Val Loss: 0.0747\n",
      "Epoch 25/300 - Train Loss: 0.0708, Val Loss: 0.0723\n",
      "Epoch 26/300 - Train Loss: 0.0704, Val Loss: 0.0725\n",
      "Epoch 27/300 - Train Loss: 0.0715, Val Loss: 0.0764\n",
      "Epoch 28/300 - Train Loss: 0.0692, Val Loss: 0.0718\n",
      "Epoch 29/300 - Train Loss: 0.0689, Val Loss: 0.0760\n",
      "Epoch 30/300 - Train Loss: 0.0704, Val Loss: 0.0738\n",
      "Epoch 31/300 - Train Loss: 0.0692, Val Loss: 0.0720\n",
      "Epoch 32/300 - Train Loss: 0.0692, Val Loss: 0.0742\n",
      "Epoch 33/300 - Train Loss: 0.0686, Val Loss: 0.0740\n",
      "Epoch 34/300 - Train Loss: 0.0674, Val Loss: 0.0733\n",
      "Epoch 35/300 - Train Loss: 0.0662, Val Loss: 0.0753\n",
      "Epoch 36/300 - Train Loss: 0.0680, Val Loss: 0.0688\n",
      "Epoch 37/300 - Train Loss: 0.0667, Val Loss: 0.0744\n",
      "Epoch 38/300 - Train Loss: 0.0656, Val Loss: 0.0734\n",
      "Epoch 39/300 - Train Loss: 0.0647, Val Loss: 0.0689\n",
      "Epoch 40/300 - Train Loss: 0.0663, Val Loss: 0.0694\n",
      "Epoch 41/300 - Train Loss: 0.0671, Val Loss: 0.0775\n",
      "Epoch 42/300 - Train Loss: 0.0657, Val Loss: 0.0706\n",
      "Epoch 43/300 - Train Loss: 0.0652, Val Loss: 0.0724\n",
      "Epoch 44/300 - Train Loss: 0.0645, Val Loss: 0.0784\n",
      "Epoch 45/300 - Train Loss: 0.0655, Val Loss: 0.0677\n",
      "Epoch 46/300 - Train Loss: 0.0634, Val Loss: 0.0683\n",
      "Epoch 47/300 - Train Loss: 0.0639, Val Loss: 0.0725\n",
      "Epoch 48/300 - Train Loss: 0.0642, Val Loss: 0.0696\n",
      "Epoch 49/300 - Train Loss: 0.0624, Val Loss: 0.0746\n",
      "Epoch 50/300 - Train Loss: 0.0633, Val Loss: 0.0716\n",
      "Epoch 51/300 - Train Loss: 0.0627, Val Loss: 0.0712\n",
      "Epoch 52/300 - Train Loss: 0.0598, Val Loss: 0.0736\n",
      "Epoch 53/300 - Train Loss: 0.0611, Val Loss: 0.0711\n",
      "Epoch 54/300 - Train Loss: 0.0608, Val Loss: 0.0703\n",
      "Epoch 55/300 - Train Loss: 0.0613, Val Loss: 0.0710\n",
      "Epoch 56/300 - Train Loss: 0.0612, Val Loss: 0.0730\n",
      "Epoch 57/300 - Train Loss: 0.0580, Val Loss: 0.0711\n",
      "Epoch 58/300 - Train Loss: 0.0590, Val Loss: 0.0701\n",
      "Epoch 59/300 - Train Loss: 0.0595, Val Loss: 0.0708\n",
      "Epoch 60/300 - Train Loss: 0.0599, Val Loss: 0.0697\n",
      "Epoch 61/300 - Train Loss: 0.0597, Val Loss: 0.0715\n",
      "Epoch 62/300 - Train Loss: 0.0591, Val Loss: 0.0703\n",
      "Epoch 63/300 - Train Loss: 0.0588, Val Loss: 0.0706\n",
      "Epoch 64/300 - Train Loss: 0.0595, Val Loss: 0.0711\n",
      "Epoch 65/300 - Train Loss: 0.0586, Val Loss: 0.0710\n",
      "Epoch 66/300 - Train Loss: 0.0577, Val Loss: 0.0699\n",
      "Epoch 67/300 - Train Loss: 0.0575, Val Loss: 0.0730\n",
      "Epoch 68/300 - Train Loss: 0.0572, Val Loss: 0.0742\n",
      "Epoch 69/300 - Train Loss: 0.0578, Val Loss: 0.0740\n",
      "Epoch 70/300 - Train Loss: 0.0550, Val Loss: 0.0734\n",
      "Epoch 71/300 - Train Loss: 0.0571, Val Loss: 0.0730\n",
      "Epoch 72/300 - Train Loss: 0.0562, Val Loss: 0.0703\n",
      "Epoch 73/300 - Train Loss: 0.0570, Val Loss: 0.0697\n",
      "Epoch 74/300 - Train Loss: 0.0562, Val Loss: 0.0682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:37:13,085] Trial 481 finished with value: 0.9622367421063428 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.14924378774738273, 'learning_rate': 5.926844346803436e-05, 'batch_size': 64, 'weight_decay': 2.2788205443528047e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75/300 - Train Loss: 0.0551, Val Loss: 0.0710\n",
      "Early stopping at epoch 75\n",
      "Macro F1 Score: 0.9622, Macro Precision: 0.9508, Macro Recall: 0.9752\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 483\n",
      "Training with F1=32, F2=8, D=8, dropout=0.10047904930280557, LR=0.0006945453519562264, BS=32, WD=2.456132646903478e-05\n",
      "Epoch 1/300 - Train Loss: 0.1545, Val Loss: 0.0939\n",
      "Epoch 2/300 - Train Loss: 0.0955, Val Loss: 0.0859\n",
      "Epoch 3/300 - Train Loss: 0.0924, Val Loss: 0.0764\n",
      "Epoch 4/300 - Train Loss: 0.0886, Val Loss: 0.0766\n",
      "Epoch 5/300 - Train Loss: 0.0833, Val Loss: 0.0687\n",
      "Epoch 6/300 - Train Loss: 0.0814, Val Loss: 0.0707\n",
      "Epoch 7/300 - Train Loss: 0.0805, Val Loss: 0.0705\n",
      "Epoch 8/300 - Train Loss: 0.0796, Val Loss: 0.0814\n",
      "Epoch 9/300 - Train Loss: 0.0773, Val Loss: 0.0656\n",
      "Epoch 10/300 - Train Loss: 0.0765, Val Loss: 0.0780\n",
      "Epoch 11/300 - Train Loss: 0.0746, Val Loss: 0.0737\n",
      "Epoch 12/300 - Train Loss: 0.0773, Val Loss: 0.0761\n",
      "Epoch 13/300 - Train Loss: 0.0748, Val Loss: 0.0682\n",
      "Epoch 14/300 - Train Loss: 0.0735, Val Loss: 0.0681\n",
      "Epoch 15/300 - Train Loss: 0.0740, Val Loss: 0.0756\n",
      "Epoch 16/300 - Train Loss: 0.0701, Val Loss: 0.0759\n",
      "Epoch 17/300 - Train Loss: 0.0728, Val Loss: 0.0739\n",
      "Epoch 18/300 - Train Loss: 0.0690, Val Loss: 0.0787\n",
      "Epoch 19/300 - Train Loss: 0.0705, Val Loss: 0.0877\n",
      "Epoch 20/300 - Train Loss: 0.0678, Val Loss: 0.0659\n",
      "Epoch 21/300 - Train Loss: 0.0671, Val Loss: 0.0690\n",
      "Epoch 22/300 - Train Loss: 0.0676, Val Loss: 0.0768\n",
      "Epoch 23/300 - Train Loss: 0.0659, Val Loss: 0.0710\n",
      "Epoch 24/300 - Train Loss: 0.0652, Val Loss: 0.0715\n",
      "Epoch 25/300 - Train Loss: 0.0654, Val Loss: 0.0698\n",
      "Epoch 26/300 - Train Loss: 0.0659, Val Loss: 0.0712\n",
      "Epoch 27/300 - Train Loss: 0.0646, Val Loss: 0.0700\n",
      "Epoch 28/300 - Train Loss: 0.0635, Val Loss: 0.0702\n",
      "Epoch 29/300 - Train Loss: 0.0615, Val Loss: 0.0735\n",
      "Epoch 30/300 - Train Loss: 0.0628, Val Loss: 0.0669\n",
      "Epoch 31/300 - Train Loss: 0.0620, Val Loss: 0.0706\n",
      "Epoch 32/300 - Train Loss: 0.0615, Val Loss: 0.0713\n",
      "Epoch 33/300 - Train Loss: 0.0610, Val Loss: 0.0683\n",
      "Epoch 34/300 - Train Loss: 0.0591, Val Loss: 0.0752\n",
      "Epoch 35/300 - Train Loss: 0.0591, Val Loss: 0.0709\n",
      "Epoch 36/300 - Train Loss: 0.0607, Val Loss: 0.0775\n",
      "Epoch 37/300 - Train Loss: 0.0565, Val Loss: 0.0712\n",
      "Epoch 38/300 - Train Loss: 0.0587, Val Loss: 0.0755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:40:56,497] Trial 482 finished with value: 0.9708343642906406 and parameters: {'F1': 32, 'F2': 8, 'D': 8, 'dropout': 0.10047904930280557, 'learning_rate': 0.0006945453519562264, 'batch_size': 32, 'weight_decay': 2.456132646903478e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/300 - Train Loss: 0.0570, Val Loss: 0.0774\n",
      "Early stopping at epoch 39\n",
      "Macro F1 Score: 0.9708, Macro Precision: 0.9645, Macro Recall: 0.9775\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 484\n",
      "Training with F1=16, F2=32, D=2, dropout=0.1560033006583496, LR=7.972986756741375e-05, BS=32, WD=2.9414702635252603e-05\n",
      "Epoch 1/300 - Train Loss: 0.3345, Val Loss: 0.1401\n",
      "Epoch 2/300 - Train Loss: 0.1398, Val Loss: 0.1031\n",
      "Epoch 3/300 - Train Loss: 0.1110, Val Loss: 0.0881\n",
      "Epoch 4/300 - Train Loss: 0.1026, Val Loss: 0.0770\n",
      "Epoch 5/300 - Train Loss: 0.0982, Val Loss: 0.0798\n",
      "Epoch 6/300 - Train Loss: 0.0957, Val Loss: 0.0784\n",
      "Epoch 7/300 - Train Loss: 0.0930, Val Loss: 0.0820\n",
      "Epoch 8/300 - Train Loss: 0.0889, Val Loss: 0.0752\n",
      "Epoch 9/300 - Train Loss: 0.0895, Val Loss: 0.0758\n",
      "Epoch 10/300 - Train Loss: 0.0893, Val Loss: 0.0758\n",
      "Epoch 11/300 - Train Loss: 0.0860, Val Loss: 0.0784\n",
      "Epoch 12/300 - Train Loss: 0.0868, Val Loss: 0.0735\n",
      "Epoch 13/300 - Train Loss: 0.0838, Val Loss: 0.0703\n",
      "Epoch 14/300 - Train Loss: 0.0832, Val Loss: 0.0731\n",
      "Epoch 15/300 - Train Loss: 0.0815, Val Loss: 0.0708\n",
      "Epoch 16/300 - Train Loss: 0.0800, Val Loss: 0.0695\n",
      "Epoch 17/300 - Train Loss: 0.0800, Val Loss: 0.0765\n",
      "Epoch 18/300 - Train Loss: 0.0796, Val Loss: 0.0757\n",
      "Epoch 19/300 - Train Loss: 0.0783, Val Loss: 0.0700\n",
      "Epoch 20/300 - Train Loss: 0.0787, Val Loss: 0.0675\n",
      "Epoch 21/300 - Train Loss: 0.0792, Val Loss: 0.0705\n",
      "Epoch 22/300 - Train Loss: 0.0794, Val Loss: 0.0784\n",
      "Epoch 23/300 - Train Loss: 0.0767, Val Loss: 0.0697\n",
      "Epoch 24/300 - Train Loss: 0.0777, Val Loss: 0.0671\n",
      "Epoch 25/300 - Train Loss: 0.0758, Val Loss: 0.0725\n",
      "Epoch 26/300 - Train Loss: 0.0762, Val Loss: 0.0734\n",
      "Epoch 27/300 - Train Loss: 0.0758, Val Loss: 0.0752\n",
      "Epoch 28/300 - Train Loss: 0.0758, Val Loss: 0.0694\n",
      "Epoch 29/300 - Train Loss: 0.0749, Val Loss: 0.0748\n",
      "Epoch 30/300 - Train Loss: 0.0747, Val Loss: 0.0681\n",
      "Epoch 31/300 - Train Loss: 0.0742, Val Loss: 0.0701\n",
      "Epoch 32/300 - Train Loss: 0.0749, Val Loss: 0.0700\n",
      "Epoch 33/300 - Train Loss: 0.0739, Val Loss: 0.0690\n",
      "Epoch 34/300 - Train Loss: 0.0745, Val Loss: 0.0709\n",
      "Epoch 35/300 - Train Loss: 0.0724, Val Loss: 0.0712\n",
      "Epoch 36/300 - Train Loss: 0.0731, Val Loss: 0.0672\n",
      "Epoch 37/300 - Train Loss: 0.0719, Val Loss: 0.0685\n",
      "Epoch 38/300 - Train Loss: 0.0759, Val Loss: 0.0773\n",
      "Epoch 39/300 - Train Loss: 0.0733, Val Loss: 0.0736\n",
      "Epoch 40/300 - Train Loss: 0.0704, Val Loss: 0.0723\n",
      "Epoch 41/300 - Train Loss: 0.0711, Val Loss: 0.0672\n",
      "Epoch 42/300 - Train Loss: 0.0698, Val Loss: 0.0655\n",
      "Epoch 43/300 - Train Loss: 0.0708, Val Loss: 0.0749\n",
      "Epoch 44/300 - Train Loss: 0.0684, Val Loss: 0.0682\n",
      "Epoch 45/300 - Train Loss: 0.0691, Val Loss: 0.0729\n",
      "Epoch 46/300 - Train Loss: 0.0696, Val Loss: 0.0679\n",
      "Epoch 47/300 - Train Loss: 0.0679, Val Loss: 0.0698\n",
      "Epoch 48/300 - Train Loss: 0.0682, Val Loss: 0.0744\n",
      "Epoch 49/300 - Train Loss: 0.0687, Val Loss: 0.0688\n",
      "Epoch 50/300 - Train Loss: 0.0675, Val Loss: 0.0680\n",
      "Epoch 51/300 - Train Loss: 0.0694, Val Loss: 0.0695\n",
      "Epoch 52/300 - Train Loss: 0.0678, Val Loss: 0.0667\n",
      "Epoch 53/300 - Train Loss: 0.0685, Val Loss: 0.0699\n",
      "Epoch 54/300 - Train Loss: 0.0682, Val Loss: 0.0720\n",
      "Epoch 55/300 - Train Loss: 0.0670, Val Loss: 0.0742\n",
      "Epoch 56/300 - Train Loss: 0.0673, Val Loss: 0.0672\n",
      "Epoch 57/300 - Train Loss: 0.0662, Val Loss: 0.0720\n",
      "Epoch 58/300 - Train Loss: 0.0648, Val Loss: 0.0680\n",
      "Epoch 59/300 - Train Loss: 0.0648, Val Loss: 0.0718\n",
      "Epoch 60/300 - Train Loss: 0.0655, Val Loss: 0.0678\n",
      "Epoch 61/300 - Train Loss: 0.0643, Val Loss: 0.0661\n",
      "Epoch 62/300 - Train Loss: 0.0641, Val Loss: 0.0635\n",
      "Epoch 63/300 - Train Loss: 0.0644, Val Loss: 0.0635\n",
      "Epoch 64/300 - Train Loss: 0.0656, Val Loss: 0.0693\n",
      "Epoch 65/300 - Train Loss: 0.0626, Val Loss: 0.0663\n",
      "Epoch 66/300 - Train Loss: 0.0626, Val Loss: 0.0764\n",
      "Epoch 67/300 - Train Loss: 0.0659, Val Loss: 0.0696\n",
      "Epoch 68/300 - Train Loss: 0.0640, Val Loss: 0.0707\n",
      "Epoch 69/300 - Train Loss: 0.0637, Val Loss: 0.0644\n",
      "Epoch 70/300 - Train Loss: 0.0612, Val Loss: 0.0711\n",
      "Epoch 71/300 - Train Loss: 0.0599, Val Loss: 0.0658\n",
      "Epoch 72/300 - Train Loss: 0.0622, Val Loss: 0.0667\n",
      "Epoch 73/300 - Train Loss: 0.0618, Val Loss: 0.0748\n",
      "Epoch 74/300 - Train Loss: 0.0633, Val Loss: 0.0640\n",
      "Epoch 75/300 - Train Loss: 0.0606, Val Loss: 0.0667\n",
      "Epoch 76/300 - Train Loss: 0.0628, Val Loss: 0.0698\n",
      "Epoch 77/300 - Train Loss: 0.0628, Val Loss: 0.0784\n",
      "Epoch 78/300 - Train Loss: 0.0601, Val Loss: 0.0733\n",
      "Epoch 79/300 - Train Loss: 0.0597, Val Loss: 0.0679\n",
      "Epoch 80/300 - Train Loss: 0.0598, Val Loss: 0.0643\n",
      "Epoch 81/300 - Train Loss: 0.0618, Val Loss: 0.0701\n",
      "Epoch 82/300 - Train Loss: 0.0598, Val Loss: 0.0721\n",
      "Epoch 83/300 - Train Loss: 0.0594, Val Loss: 0.0669\n",
      "Epoch 84/300 - Train Loss: 0.0608, Val Loss: 0.0659\n",
      "Epoch 85/300 - Train Loss: 0.0590, Val Loss: 0.0667\n",
      "Epoch 86/300 - Train Loss: 0.0607, Val Loss: 0.0635\n",
      "Epoch 87/300 - Train Loss: 0.0608, Val Loss: 0.0700\n",
      "Epoch 88/300 - Train Loss: 0.0578, Val Loss: 0.0683\n",
      "Epoch 89/300 - Train Loss: 0.0580, Val Loss: 0.0651\n",
      "Epoch 90/300 - Train Loss: 0.0608, Val Loss: 0.0689\n",
      "Epoch 91/300 - Train Loss: 0.0593, Val Loss: 0.0691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:44:13,195] Trial 483 finished with value: 0.9738284202569917 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.1560033006583496, 'learning_rate': 7.972986756741375e-05, 'batch_size': 32, 'weight_decay': 2.9414702635252603e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92/300 - Train Loss: 0.0572, Val Loss: 0.0688\n",
      "Early stopping at epoch 92\n",
      "Macro F1 Score: 0.9738, Macro Precision: 0.9656, Macro Recall: 0.9827\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.98      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 485\n",
      "Training with F1=32, F2=16, D=2, dropout=0.14116114472905134, LR=0.000546586646533321, BS=32, WD=1.779035472472543e-05\n",
      "Epoch 1/300 - Train Loss: 0.1564, Val Loss: 0.0722\n",
      "Epoch 2/300 - Train Loss: 0.0987, Val Loss: 0.0758\n",
      "Epoch 3/300 - Train Loss: 0.0913, Val Loss: 0.0798\n",
      "Epoch 4/300 - Train Loss: 0.0879, Val Loss: 0.0729\n",
      "Epoch 5/300 - Train Loss: 0.0863, Val Loss: 0.0802\n",
      "Epoch 6/300 - Train Loss: 0.0851, Val Loss: 0.0792\n",
      "Epoch 7/300 - Train Loss: 0.0837, Val Loss: 0.0773\n",
      "Epoch 8/300 - Train Loss: 0.0809, Val Loss: 0.0795\n",
      "Epoch 9/300 - Train Loss: 0.0769, Val Loss: 0.0727\n",
      "Epoch 10/300 - Train Loss: 0.0812, Val Loss: 0.0687\n",
      "Epoch 11/300 - Train Loss: 0.0772, Val Loss: 0.0712\n",
      "Epoch 12/300 - Train Loss: 0.0801, Val Loss: 0.0722\n",
      "Epoch 13/300 - Train Loss: 0.0798, Val Loss: 0.0753\n",
      "Epoch 14/300 - Train Loss: 0.0754, Val Loss: 0.0782\n",
      "Epoch 15/300 - Train Loss: 0.0730, Val Loss: 0.0757\n",
      "Epoch 16/300 - Train Loss: 0.0741, Val Loss: 0.0730\n",
      "Epoch 17/300 - Train Loss: 0.0719, Val Loss: 0.0777\n",
      "Epoch 18/300 - Train Loss: 0.0739, Val Loss: 0.0760\n",
      "Epoch 19/300 - Train Loss: 0.0727, Val Loss: 0.0722\n",
      "Epoch 20/300 - Train Loss: 0.0717, Val Loss: 0.0806\n",
      "Epoch 21/300 - Train Loss: 0.0719, Val Loss: 0.0712\n",
      "Epoch 22/300 - Train Loss: 0.0715, Val Loss: 0.0692\n",
      "Epoch 23/300 - Train Loss: 0.0683, Val Loss: 0.0703\n",
      "Epoch 24/300 - Train Loss: 0.0681, Val Loss: 0.0798\n",
      "Epoch 25/300 - Train Loss: 0.0667, Val Loss: 0.0745\n",
      "Epoch 26/300 - Train Loss: 0.0684, Val Loss: 0.0701\n",
      "Epoch 27/300 - Train Loss: 0.0675, Val Loss: 0.0711\n",
      "Epoch 28/300 - Train Loss: 0.0659, Val Loss: 0.0706\n",
      "Epoch 29/300 - Train Loss: 0.0660, Val Loss: 0.0753\n",
      "Epoch 30/300 - Train Loss: 0.0639, Val Loss: 0.0858\n",
      "Epoch 31/300 - Train Loss: 0.0643, Val Loss: 0.0740\n",
      "Epoch 32/300 - Train Loss: 0.0642, Val Loss: 0.0673\n",
      "Epoch 33/300 - Train Loss: 0.0643, Val Loss: 0.0721\n",
      "Epoch 34/300 - Train Loss: 0.0618, Val Loss: 0.0731\n",
      "Epoch 35/300 - Train Loss: 0.0646, Val Loss: 0.0823\n",
      "Epoch 36/300 - Train Loss: 0.0609, Val Loss: 0.0741\n",
      "Epoch 37/300 - Train Loss: 0.0610, Val Loss: 0.0821\n",
      "Epoch 38/300 - Train Loss: 0.0603, Val Loss: 0.0760\n",
      "Epoch 39/300 - Train Loss: 0.0583, Val Loss: 0.0739\n",
      "Epoch 40/300 - Train Loss: 0.0600, Val Loss: 0.0787\n",
      "Epoch 41/300 - Train Loss: 0.0589, Val Loss: 0.0731\n",
      "Epoch 42/300 - Train Loss: 0.0578, Val Loss: 0.0772\n",
      "Epoch 43/300 - Train Loss: 0.0576, Val Loss: 0.0827\n",
      "Epoch 44/300 - Train Loss: 0.0590, Val Loss: 0.0759\n",
      "Epoch 45/300 - Train Loss: 0.0555, Val Loss: 0.0722\n",
      "Epoch 46/300 - Train Loss: 0.0569, Val Loss: 0.0724\n",
      "Epoch 47/300 - Train Loss: 0.0565, Val Loss: 0.0733\n",
      "Epoch 48/300 - Train Loss: 0.0559, Val Loss: 0.0722\n",
      "Epoch 49/300 - Train Loss: 0.0560, Val Loss: 0.0702\n",
      "Epoch 50/300 - Train Loss: 0.0529, Val Loss: 0.0737\n",
      "Epoch 51/300 - Train Loss: 0.0541, Val Loss: 0.0758\n",
      "Epoch 52/300 - Train Loss: 0.0530, Val Loss: 0.0840\n",
      "Epoch 53/300 - Train Loss: 0.0545, Val Loss: 0.0829\n",
      "Epoch 54/300 - Train Loss: 0.0551, Val Loss: 0.0740\n",
      "Epoch 55/300 - Train Loss: 0.0530, Val Loss: 0.0786\n",
      "Epoch 56/300 - Train Loss: 0.0519, Val Loss: 0.0773\n",
      "Epoch 57/300 - Train Loss: 0.0502, Val Loss: 0.0764\n",
      "Epoch 58/300 - Train Loss: 0.0548, Val Loss: 0.0771\n",
      "Epoch 59/300 - Train Loss: 0.0515, Val Loss: 0.0842\n",
      "Epoch 60/300 - Train Loss: 0.0497, Val Loss: 0.0711\n",
      "Epoch 61/300 - Train Loss: 0.0497, Val Loss: 0.0781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:46:58,998] Trial 484 finished with value: 0.971931698614492 and parameters: {'F1': 32, 'F2': 16, 'D': 2, 'dropout': 0.14116114472905134, 'learning_rate': 0.000546586646533321, 'batch_size': 32, 'weight_decay': 1.779035472472543e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/300 - Train Loss: 0.0504, Val Loss: 0.0768\n",
      "Early stopping at epoch 62\n",
      "Macro F1 Score: 0.9719, Macro Precision: 0.9635, Macro Recall: 0.9810\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.92      0.98      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 486\n",
      "Training with F1=16, F2=8, D=8, dropout=0.12112353926224058, LR=8.974149444280498e-05, BS=32, WD=2.8200762414329265e-05\n",
      "Epoch 1/300 - Train Loss: 0.3273, Val Loss: 0.1397\n",
      "Epoch 2/300 - Train Loss: 0.1322, Val Loss: 0.0860\n",
      "Epoch 3/300 - Train Loss: 0.1084, Val Loss: 0.0850\n",
      "Epoch 4/300 - Train Loss: 0.0998, Val Loss: 0.0765\n",
      "Epoch 5/300 - Train Loss: 0.0937, Val Loss: 0.0751\n",
      "Epoch 6/300 - Train Loss: 0.0919, Val Loss: 0.0701\n",
      "Epoch 7/300 - Train Loss: 0.0895, Val Loss: 0.0714\n",
      "Epoch 8/300 - Train Loss: 0.0896, Val Loss: 0.0732\n",
      "Epoch 9/300 - Train Loss: 0.0861, Val Loss: 0.0720\n",
      "Epoch 10/300 - Train Loss: 0.0846, Val Loss: 0.0702\n",
      "Epoch 11/300 - Train Loss: 0.0848, Val Loss: 0.0618\n",
      "Epoch 12/300 - Train Loss: 0.0818, Val Loss: 0.0659\n",
      "Epoch 13/300 - Train Loss: 0.0836, Val Loss: 0.0718\n",
      "Epoch 14/300 - Train Loss: 0.0809, Val Loss: 0.0672\n",
      "Epoch 15/300 - Train Loss: 0.0820, Val Loss: 0.0679\n",
      "Epoch 16/300 - Train Loss: 0.0815, Val Loss: 0.0699\n",
      "Epoch 17/300 - Train Loss: 0.0811, Val Loss: 0.0686\n",
      "Epoch 18/300 - Train Loss: 0.0797, Val Loss: 0.0676\n",
      "Epoch 19/300 - Train Loss: 0.0769, Val Loss: 0.0750\n",
      "Epoch 20/300 - Train Loss: 0.0784, Val Loss: 0.0660\n",
      "Epoch 21/300 - Train Loss: 0.0781, Val Loss: 0.0760\n",
      "Epoch 22/300 - Train Loss: 0.0770, Val Loss: 0.0637\n",
      "Epoch 23/300 - Train Loss: 0.0757, Val Loss: 0.0650\n",
      "Epoch 24/300 - Train Loss: 0.0755, Val Loss: 0.0629\n",
      "Epoch 25/300 - Train Loss: 0.0754, Val Loss: 0.0646\n",
      "Epoch 26/300 - Train Loss: 0.0746, Val Loss: 0.0661\n",
      "Epoch 27/300 - Train Loss: 0.0737, Val Loss: 0.0679\n",
      "Epoch 28/300 - Train Loss: 0.0751, Val Loss: 0.0626\n",
      "Epoch 29/300 - Train Loss: 0.0752, Val Loss: 0.0652\n",
      "Epoch 30/300 - Train Loss: 0.0751, Val Loss: 0.0725\n",
      "Epoch 31/300 - Train Loss: 0.0730, Val Loss: 0.0642\n",
      "Epoch 32/300 - Train Loss: 0.0737, Val Loss: 0.0700\n",
      "Epoch 33/300 - Train Loss: 0.0725, Val Loss: 0.0601\n",
      "Epoch 34/300 - Train Loss: 0.0733, Val Loss: 0.0624\n",
      "Epoch 35/300 - Train Loss: 0.0716, Val Loss: 0.0604\n",
      "Epoch 36/300 - Train Loss: 0.0702, Val Loss: 0.0660\n",
      "Epoch 37/300 - Train Loss: 0.0702, Val Loss: 0.0612\n",
      "Epoch 38/300 - Train Loss: 0.0715, Val Loss: 0.0683\n",
      "Epoch 39/300 - Train Loss: 0.0733, Val Loss: 0.0599\n",
      "Epoch 40/300 - Train Loss: 0.0723, Val Loss: 0.0727\n",
      "Epoch 41/300 - Train Loss: 0.0723, Val Loss: 0.0632\n",
      "Epoch 42/300 - Train Loss: 0.0718, Val Loss: 0.0640\n",
      "Epoch 43/300 - Train Loss: 0.0704, Val Loss: 0.0624\n",
      "Epoch 44/300 - Train Loss: 0.0693, Val Loss: 0.0752\n",
      "Epoch 45/300 - Train Loss: 0.0683, Val Loss: 0.0624\n",
      "Epoch 46/300 - Train Loss: 0.0709, Val Loss: 0.0601\n",
      "Epoch 47/300 - Train Loss: 0.0678, Val Loss: 0.0622\n",
      "Epoch 48/300 - Train Loss: 0.0693, Val Loss: 0.0591\n",
      "Epoch 49/300 - Train Loss: 0.0669, Val Loss: 0.0636\n",
      "Epoch 50/300 - Train Loss: 0.0694, Val Loss: 0.0639\n",
      "Epoch 51/300 - Train Loss: 0.0663, Val Loss: 0.0664\n",
      "Epoch 52/300 - Train Loss: 0.0670, Val Loss: 0.0787\n",
      "Epoch 53/300 - Train Loss: 0.0677, Val Loss: 0.0665\n",
      "Epoch 54/300 - Train Loss: 0.0684, Val Loss: 0.0623\n",
      "Epoch 55/300 - Train Loss: 0.0652, Val Loss: 0.0617\n",
      "Epoch 56/300 - Train Loss: 0.0677, Val Loss: 0.0639\n",
      "Epoch 57/300 - Train Loss: 0.0664, Val Loss: 0.0646\n",
      "Epoch 58/300 - Train Loss: 0.0643, Val Loss: 0.0614\n",
      "Epoch 59/300 - Train Loss: 0.0651, Val Loss: 0.0620\n",
      "Epoch 60/300 - Train Loss: 0.0658, Val Loss: 0.0595\n",
      "Epoch 61/300 - Train Loss: 0.0650, Val Loss: 0.0623\n",
      "Epoch 62/300 - Train Loss: 0.0647, Val Loss: 0.0691\n",
      "Epoch 63/300 - Train Loss: 0.0672, Val Loss: 0.0649\n",
      "Epoch 64/300 - Train Loss: 0.0642, Val Loss: 0.0661\n",
      "Epoch 65/300 - Train Loss: 0.0657, Val Loss: 0.0647\n",
      "Epoch 66/300 - Train Loss: 0.0663, Val Loss: 0.0717\n",
      "Epoch 67/300 - Train Loss: 0.0646, Val Loss: 0.0656\n",
      "Epoch 68/300 - Train Loss: 0.0646, Val Loss: 0.0609\n",
      "Epoch 69/300 - Train Loss: 0.0655, Val Loss: 0.0656\n",
      "Epoch 70/300 - Train Loss: 0.0646, Val Loss: 0.0593\n",
      "Epoch 71/300 - Train Loss: 0.0623, Val Loss: 0.0663\n",
      "Epoch 72/300 - Train Loss: 0.0639, Val Loss: 0.0622\n",
      "Epoch 73/300 - Train Loss: 0.0628, Val Loss: 0.0657\n",
      "Epoch 74/300 - Train Loss: 0.0655, Val Loss: 0.0616\n",
      "Epoch 75/300 - Train Loss: 0.0638, Val Loss: 0.0602\n",
      "Epoch 76/300 - Train Loss: 0.0630, Val Loss: 0.0627\n",
      "Epoch 77/300 - Train Loss: 0.0647, Val Loss: 0.0641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:51:26,540] Trial 485 finished with value: 0.9697311463638782 and parameters: {'F1': 16, 'F2': 8, 'D': 8, 'dropout': 0.12112353926224058, 'learning_rate': 8.974149444280498e-05, 'batch_size': 32, 'weight_decay': 2.8200762414329265e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300 - Train Loss: 0.0644, Val Loss: 0.0608\n",
      "Early stopping at epoch 78\n",
      "Macro F1 Score: 0.9697, Macro Precision: 0.9728, Macro Recall: 0.9668\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 487\n",
      "Training with F1=32, F2=32, D=2, dropout=0.5677432300231845, LR=6.622599151449861e-05, BS=128, WD=1.9779611737524573e-05\n",
      "Epoch 1/300 - Train Loss: 0.6064, Val Loss: 0.3667\n",
      "Epoch 2/300 - Train Loss: 0.2763, Val Loss: 0.2551\n",
      "Epoch 3/300 - Train Loss: 0.2145, Val Loss: 0.1976\n",
      "Epoch 4/300 - Train Loss: 0.1771, Val Loss: 0.1676\n",
      "Epoch 5/300 - Train Loss: 0.1559, Val Loss: 0.1414\n",
      "Epoch 6/300 - Train Loss: 0.1385, Val Loss: 0.1382\n",
      "Epoch 7/300 - Train Loss: 0.1274, Val Loss: 0.1241\n",
      "Epoch 8/300 - Train Loss: 0.1203, Val Loss: 0.1189\n",
      "Epoch 9/300 - Train Loss: 0.1127, Val Loss: 0.1105\n",
      "Epoch 10/300 - Train Loss: 0.1088, Val Loss: 0.1085\n",
      "Epoch 11/300 - Train Loss: 0.1087, Val Loss: 0.1088\n",
      "Epoch 12/300 - Train Loss: 0.1045, Val Loss: 0.1038\n",
      "Epoch 13/300 - Train Loss: 0.1009, Val Loss: 0.1028\n",
      "Epoch 14/300 - Train Loss: 0.1001, Val Loss: 0.1009\n",
      "Epoch 15/300 - Train Loss: 0.1017, Val Loss: 0.0964\n",
      "Epoch 16/300 - Train Loss: 0.0984, Val Loss: 0.0923\n",
      "Epoch 17/300 - Train Loss: 0.0975, Val Loss: 0.0938\n",
      "Epoch 18/300 - Train Loss: 0.0961, Val Loss: 0.0920\n",
      "Epoch 19/300 - Train Loss: 0.0959, Val Loss: 0.0897\n",
      "Epoch 20/300 - Train Loss: 0.0947, Val Loss: 0.0931\n",
      "Epoch 21/300 - Train Loss: 0.0950, Val Loss: 0.0912\n",
      "Epoch 22/300 - Train Loss: 0.0937, Val Loss: 0.0887\n",
      "Epoch 23/300 - Train Loss: 0.0916, Val Loss: 0.0894\n",
      "Epoch 24/300 - Train Loss: 0.0908, Val Loss: 0.0890\n",
      "Epoch 25/300 - Train Loss: 0.0928, Val Loss: 0.0859\n",
      "Epoch 26/300 - Train Loss: 0.0895, Val Loss: 0.0924\n",
      "Epoch 27/300 - Train Loss: 0.0904, Val Loss: 0.0855\n",
      "Epoch 28/300 - Train Loss: 0.0889, Val Loss: 0.0847\n",
      "Epoch 29/300 - Train Loss: 0.0882, Val Loss: 0.0867\n",
      "Epoch 30/300 - Train Loss: 0.0893, Val Loss: 0.0852\n",
      "Epoch 31/300 - Train Loss: 0.0879, Val Loss: 0.0877\n",
      "Epoch 32/300 - Train Loss: 0.0875, Val Loss: 0.0911\n",
      "Epoch 33/300 - Train Loss: 0.0870, Val Loss: 0.0848\n",
      "Epoch 34/300 - Train Loss: 0.0874, Val Loss: 0.0865\n",
      "Epoch 35/300 - Train Loss: 0.0876, Val Loss: 0.0843\n",
      "Epoch 36/300 - Train Loss: 0.0868, Val Loss: 0.0845\n",
      "Epoch 37/300 - Train Loss: 0.0861, Val Loss: 0.0813\n",
      "Epoch 38/300 - Train Loss: 0.0858, Val Loss: 0.0846\n",
      "Epoch 39/300 - Train Loss: 0.0855, Val Loss: 0.0839\n",
      "Epoch 40/300 - Train Loss: 0.0855, Val Loss: 0.0852\n",
      "Epoch 41/300 - Train Loss: 0.0859, Val Loss: 0.0834\n",
      "Epoch 42/300 - Train Loss: 0.0857, Val Loss: 0.0810\n",
      "Epoch 43/300 - Train Loss: 0.0843, Val Loss: 0.0828\n",
      "Epoch 44/300 - Train Loss: 0.0854, Val Loss: 0.0816\n",
      "Epoch 45/300 - Train Loss: 0.0854, Val Loss: 0.0815\n",
      "Epoch 46/300 - Train Loss: 0.0837, Val Loss: 0.0798\n",
      "Epoch 47/300 - Train Loss: 0.0839, Val Loss: 0.0820\n",
      "Epoch 48/300 - Train Loss: 0.0854, Val Loss: 0.0823\n",
      "Epoch 49/300 - Train Loss: 0.0818, Val Loss: 0.0827\n",
      "Epoch 50/300 - Train Loss: 0.0844, Val Loss: 0.0801\n",
      "Epoch 51/300 - Train Loss: 0.0818, Val Loss: 0.0832\n",
      "Epoch 52/300 - Train Loss: 0.0833, Val Loss: 0.0832\n",
      "Epoch 53/300 - Train Loss: 0.0821, Val Loss: 0.0808\n",
      "Epoch 54/300 - Train Loss: 0.0844, Val Loss: 0.0817\n",
      "Epoch 55/300 - Train Loss: 0.0833, Val Loss: 0.0824\n",
      "Epoch 56/300 - Train Loss: 0.0820, Val Loss: 0.0832\n",
      "Epoch 57/300 - Train Loss: 0.0823, Val Loss: 0.0820\n",
      "Epoch 58/300 - Train Loss: 0.0814, Val Loss: 0.0789\n",
      "Epoch 59/300 - Train Loss: 0.0823, Val Loss: 0.0812\n",
      "Epoch 60/300 - Train Loss: 0.0820, Val Loss: 0.0835\n",
      "Epoch 61/300 - Train Loss: 0.0811, Val Loss: 0.0814\n",
      "Epoch 62/300 - Train Loss: 0.0810, Val Loss: 0.0824\n",
      "Epoch 63/300 - Train Loss: 0.0807, Val Loss: 0.0790\n",
      "Epoch 64/300 - Train Loss: 0.0814, Val Loss: 0.0804\n",
      "Epoch 65/300 - Train Loss: 0.0806, Val Loss: 0.0813\n",
      "Epoch 66/300 - Train Loss: 0.0818, Val Loss: 0.0816\n",
      "Epoch 67/300 - Train Loss: 0.0820, Val Loss: 0.0788\n",
      "Epoch 68/300 - Train Loss: 0.0808, Val Loss: 0.0802\n",
      "Epoch 69/300 - Train Loss: 0.0794, Val Loss: 0.0805\n",
      "Epoch 70/300 - Train Loss: 0.0796, Val Loss: 0.0805\n",
      "Epoch 71/300 - Train Loss: 0.0796, Val Loss: 0.0794\n",
      "Epoch 72/300 - Train Loss: 0.0789, Val Loss: 0.0811\n",
      "Epoch 73/300 - Train Loss: 0.0802, Val Loss: 0.0779\n",
      "Epoch 74/300 - Train Loss: 0.0776, Val Loss: 0.0813\n",
      "Epoch 75/300 - Train Loss: 0.0790, Val Loss: 0.0800\n",
      "Epoch 76/300 - Train Loss: 0.0796, Val Loss: 0.0807\n",
      "Epoch 77/300 - Train Loss: 0.0784, Val Loss: 0.0777\n",
      "Epoch 78/300 - Train Loss: 0.0792, Val Loss: 0.0771\n",
      "Epoch 79/300 - Train Loss: 0.0795, Val Loss: 0.0828\n",
      "Epoch 80/300 - Train Loss: 0.0797, Val Loss: 0.0782\n",
      "Epoch 81/300 - Train Loss: 0.0782, Val Loss: 0.0769\n",
      "Epoch 82/300 - Train Loss: 0.0791, Val Loss: 0.0815\n",
      "Epoch 83/300 - Train Loss: 0.0778, Val Loss: 0.0823\n",
      "Epoch 84/300 - Train Loss: 0.0788, Val Loss: 0.0798\n",
      "Epoch 85/300 - Train Loss: 0.0787, Val Loss: 0.0780\n",
      "Epoch 86/300 - Train Loss: 0.0779, Val Loss: 0.0806\n",
      "Epoch 87/300 - Train Loss: 0.0785, Val Loss: 0.0784\n",
      "Epoch 88/300 - Train Loss: 0.0770, Val Loss: 0.0807\n",
      "Epoch 89/300 - Train Loss: 0.0765, Val Loss: 0.0811\n",
      "Epoch 90/300 - Train Loss: 0.0777, Val Loss: 0.0784\n",
      "Epoch 91/300 - Train Loss: 0.0773, Val Loss: 0.0804\n",
      "Epoch 92/300 - Train Loss: 0.0776, Val Loss: 0.0772\n",
      "Epoch 93/300 - Train Loss: 0.0779, Val Loss: 0.0789\n",
      "Epoch 94/300 - Train Loss: 0.0783, Val Loss: 0.0786\n",
      "Epoch 95/300 - Train Loss: 0.0774, Val Loss: 0.0795\n",
      "Epoch 96/300 - Train Loss: 0.0773, Val Loss: 0.0780\n",
      "Epoch 97/300 - Train Loss: 0.0766, Val Loss: 0.0787\n",
      "Epoch 98/300 - Train Loss: 0.0766, Val Loss: 0.0763\n",
      "Epoch 99/300 - Train Loss: 0.0775, Val Loss: 0.0780\n",
      "Epoch 100/300 - Train Loss: 0.0775, Val Loss: 0.0748\n",
      "Epoch 101/300 - Train Loss: 0.0754, Val Loss: 0.0793\n",
      "Epoch 102/300 - Train Loss: 0.0772, Val Loss: 0.0777\n",
      "Epoch 103/300 - Train Loss: 0.0759, Val Loss: 0.0777\n",
      "Epoch 104/300 - Train Loss: 0.0761, Val Loss: 0.0794\n",
      "Epoch 105/300 - Train Loss: 0.0754, Val Loss: 0.0771\n",
      "Epoch 106/300 - Train Loss: 0.0735, Val Loss: 0.0805\n",
      "Epoch 107/300 - Train Loss: 0.0754, Val Loss: 0.0789\n",
      "Epoch 108/300 - Train Loss: 0.0757, Val Loss: 0.0783\n",
      "Epoch 109/300 - Train Loss: 0.0747, Val Loss: 0.0764\n",
      "Epoch 110/300 - Train Loss: 0.0774, Val Loss: 0.0769\n",
      "Epoch 111/300 - Train Loss: 0.0746, Val Loss: 0.0758\n",
      "Epoch 112/300 - Train Loss: 0.0763, Val Loss: 0.0803\n",
      "Epoch 113/300 - Train Loss: 0.0758, Val Loss: 0.0793\n",
      "Epoch 114/300 - Train Loss: 0.0758, Val Loss: 0.0764\n",
      "Epoch 115/300 - Train Loss: 0.0758, Val Loss: 0.0771\n",
      "Epoch 116/300 - Train Loss: 0.0755, Val Loss: 0.0768\n",
      "Epoch 117/300 - Train Loss: 0.0733, Val Loss: 0.0757\n",
      "Epoch 118/300 - Train Loss: 0.0742, Val Loss: 0.0755\n",
      "Epoch 119/300 - Train Loss: 0.0739, Val Loss: 0.0762\n",
      "Epoch 120/300 - Train Loss: 0.0753, Val Loss: 0.0737\n",
      "Epoch 121/300 - Train Loss: 0.0747, Val Loss: 0.0752\n",
      "Epoch 122/300 - Train Loss: 0.0755, Val Loss: 0.0773\n",
      "Epoch 123/300 - Train Loss: 0.0767, Val Loss: 0.0761\n",
      "Epoch 124/300 - Train Loss: 0.0746, Val Loss: 0.0780\n",
      "Epoch 125/300 - Train Loss: 0.0748, Val Loss: 0.0788\n",
      "Epoch 126/300 - Train Loss: 0.0747, Val Loss: 0.0781\n",
      "Epoch 127/300 - Train Loss: 0.0749, Val Loss: 0.0798\n",
      "Epoch 128/300 - Train Loss: 0.0752, Val Loss: 0.0765\n",
      "Epoch 129/300 - Train Loss: 0.0732, Val Loss: 0.0769\n",
      "Epoch 130/300 - Train Loss: 0.0755, Val Loss: 0.0758\n",
      "Epoch 131/300 - Train Loss: 0.0730, Val Loss: 0.0747\n",
      "Epoch 132/300 - Train Loss: 0.0734, Val Loss: 0.0757\n",
      "Epoch 133/300 - Train Loss: 0.0720, Val Loss: 0.0732\n",
      "Epoch 134/300 - Train Loss: 0.0752, Val Loss: 0.0758\n",
      "Epoch 135/300 - Train Loss: 0.0736, Val Loss: 0.0787\n",
      "Epoch 136/300 - Train Loss: 0.0744, Val Loss: 0.0758\n",
      "Epoch 137/300 - Train Loss: 0.0718, Val Loss: 0.0775\n",
      "Epoch 138/300 - Train Loss: 0.0751, Val Loss: 0.0734\n",
      "Epoch 139/300 - Train Loss: 0.0716, Val Loss: 0.0751\n",
      "Epoch 140/300 - Train Loss: 0.0746, Val Loss: 0.0755\n",
      "Epoch 141/300 - Train Loss: 0.0739, Val Loss: 0.0743\n",
      "Epoch 142/300 - Train Loss: 0.0738, Val Loss: 0.0782\n",
      "Epoch 143/300 - Train Loss: 0.0718, Val Loss: 0.0746\n",
      "Epoch 144/300 - Train Loss: 0.0726, Val Loss: 0.0754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/300 - Train Loss: 0.0721, Val Loss: 0.0766\n",
      "Epoch 146/300 - Train Loss: 0.0732, Val Loss: 0.0753\n",
      "Epoch 147/300 - Train Loss: 0.0719, Val Loss: 0.0764\n",
      "Epoch 148/300 - Train Loss: 0.0721, Val Loss: 0.0745\n",
      "Epoch 149/300 - Train Loss: 0.0729, Val Loss: 0.0775\n",
      "Epoch 150/300 - Train Loss: 0.0698, Val Loss: 0.0758\n",
      "Epoch 151/300 - Train Loss: 0.0723, Val Loss: 0.0788\n",
      "Epoch 152/300 - Train Loss: 0.0718, Val Loss: 0.0744\n",
      "Epoch 153/300 - Train Loss: 0.0726, Val Loss: 0.0759\n",
      "Epoch 154/300 - Train Loss: 0.0714, Val Loss: 0.0756\n",
      "Epoch 155/300 - Train Loss: 0.0715, Val Loss: 0.0731\n",
      "Epoch 156/300 - Train Loss: 0.0729, Val Loss: 0.0771\n",
      "Epoch 157/300 - Train Loss: 0.0712, Val Loss: 0.0754\n",
      "Epoch 158/300 - Train Loss: 0.0705, Val Loss: 0.0746\n",
      "Epoch 159/300 - Train Loss: 0.0702, Val Loss: 0.0732\n",
      "Epoch 160/300 - Train Loss: 0.0726, Val Loss: 0.0756\n",
      "Epoch 161/300 - Train Loss: 0.0719, Val Loss: 0.0738\n",
      "Epoch 162/300 - Train Loss: 0.0729, Val Loss: 0.0763\n",
      "Epoch 163/300 - Train Loss: 0.0716, Val Loss: 0.0767\n",
      "Epoch 164/300 - Train Loss: 0.0718, Val Loss: 0.0744\n",
      "Epoch 165/300 - Train Loss: 0.0729, Val Loss: 0.0748\n",
      "Epoch 166/300 - Train Loss: 0.0726, Val Loss: 0.0748\n",
      "Epoch 167/300 - Train Loss: 0.0697, Val Loss: 0.0747\n",
      "Epoch 168/300 - Train Loss: 0.0700, Val Loss: 0.0732\n",
      "Epoch 169/300 - Train Loss: 0.0712, Val Loss: 0.0730\n",
      "Epoch 170/300 - Train Loss: 0.0709, Val Loss: 0.0750\n",
      "Epoch 171/300 - Train Loss: 0.0728, Val Loss: 0.0754\n",
      "Epoch 172/300 - Train Loss: 0.0704, Val Loss: 0.0741\n",
      "Epoch 173/300 - Train Loss: 0.0704, Val Loss: 0.0742\n",
      "Epoch 174/300 - Train Loss: 0.0696, Val Loss: 0.0716\n",
      "Epoch 175/300 - Train Loss: 0.0695, Val Loss: 0.0768\n",
      "Epoch 176/300 - Train Loss: 0.0701, Val Loss: 0.0722\n",
      "Epoch 177/300 - Train Loss: 0.0725, Val Loss: 0.0730\n",
      "Epoch 178/300 - Train Loss: 0.0711, Val Loss: 0.0757\n",
      "Epoch 179/300 - Train Loss: 0.0700, Val Loss: 0.0747\n",
      "Epoch 180/300 - Train Loss: 0.0701, Val Loss: 0.0729\n",
      "Epoch 181/300 - Train Loss: 0.0707, Val Loss: 0.0744\n",
      "Epoch 182/300 - Train Loss: 0.0712, Val Loss: 0.0750\n",
      "Epoch 183/300 - Train Loss: 0.0706, Val Loss: 0.0750\n",
      "Epoch 184/300 - Train Loss: 0.0703, Val Loss: 0.0778\n",
      "Epoch 185/300 - Train Loss: 0.0699, Val Loss: 0.0742\n",
      "Epoch 186/300 - Train Loss: 0.0717, Val Loss: 0.0759\n",
      "Epoch 187/300 - Train Loss: 0.0721, Val Loss: 0.0766\n",
      "Epoch 188/300 - Train Loss: 0.0728, Val Loss: 0.0753\n",
      "Epoch 189/300 - Train Loss: 0.0681, Val Loss: 0.0748\n",
      "Epoch 190/300 - Train Loss: 0.0720, Val Loss: 0.0739\n",
      "Epoch 191/300 - Train Loss: 0.0720, Val Loss: 0.0756\n",
      "Epoch 192/300 - Train Loss: 0.0714, Val Loss: 0.0738\n",
      "Epoch 193/300 - Train Loss: 0.0689, Val Loss: 0.0732\n",
      "Epoch 194/300 - Train Loss: 0.0694, Val Loss: 0.0752\n",
      "Epoch 195/300 - Train Loss: 0.0694, Val Loss: 0.0730\n",
      "Epoch 196/300 - Train Loss: 0.0677, Val Loss: 0.0769\n",
      "Epoch 197/300 - Train Loss: 0.0707, Val Loss: 0.0733\n",
      "Epoch 198/300 - Train Loss: 0.0710, Val Loss: 0.0749\n",
      "Epoch 199/300 - Train Loss: 0.0696, Val Loss: 0.0738\n",
      "Epoch 200/300 - Train Loss: 0.0678, Val Loss: 0.0743\n",
      "Epoch 201/300 - Train Loss: 0.0681, Val Loss: 0.0737\n",
      "Epoch 202/300 - Train Loss: 0.0696, Val Loss: 0.0732\n",
      "Epoch 203/300 - Train Loss: 0.0678, Val Loss: 0.0755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 04:58:53,203] Trial 486 finished with value: 0.9627331053461022 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.5677432300231845, 'learning_rate': 6.622599151449861e-05, 'batch_size': 128, 'weight_decay': 1.9779611737524573e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 204/300 - Train Loss: 0.0715, Val Loss: 0.0735\n",
      "Early stopping at epoch 204\n",
      "Macro F1 Score: 0.9627, Macro Precision: 0.9512, Macro Recall: 0.9757\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.88      0.97      0.92        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 488\n",
      "Training with F1=32, F2=32, D=2, dropout=0.16238393634342083, LR=0.00010376249154485847, BS=32, WD=3.3666150711040286e-05\n",
      "Epoch 1/300 - Train Loss: 0.2628, Val Loss: 0.1201\n",
      "Epoch 2/300 - Train Loss: 0.1132, Val Loss: 0.0771\n",
      "Epoch 3/300 - Train Loss: 0.1037, Val Loss: 0.0778\n",
      "Epoch 4/300 - Train Loss: 0.0943, Val Loss: 0.0771\n",
      "Epoch 5/300 - Train Loss: 0.0917, Val Loss: 0.0777\n",
      "Epoch 6/300 - Train Loss: 0.0872, Val Loss: 0.0783\n",
      "Epoch 7/300 - Train Loss: 0.0880, Val Loss: 0.0758\n",
      "Epoch 8/300 - Train Loss: 0.0869, Val Loss: 0.0782\n",
      "Epoch 9/300 - Train Loss: 0.0822, Val Loss: 0.0711\n",
      "Epoch 10/300 - Train Loss: 0.0816, Val Loss: 0.0694\n",
      "Epoch 11/300 - Train Loss: 0.0827, Val Loss: 0.0760\n",
      "Epoch 12/300 - Train Loss: 0.0831, Val Loss: 0.0671\n",
      "Epoch 13/300 - Train Loss: 0.0814, Val Loss: 0.0743\n",
      "Epoch 14/300 - Train Loss: 0.0777, Val Loss: 0.0713\n",
      "Epoch 15/300 - Train Loss: 0.0771, Val Loss: 0.0805\n",
      "Epoch 16/300 - Train Loss: 0.0787, Val Loss: 0.0710\n",
      "Epoch 17/300 - Train Loss: 0.0784, Val Loss: 0.0708\n",
      "Epoch 18/300 - Train Loss: 0.0774, Val Loss: 0.0757\n",
      "Epoch 19/300 - Train Loss: 0.0774, Val Loss: 0.0680\n",
      "Epoch 20/300 - Train Loss: 0.0750, Val Loss: 0.0701\n",
      "Epoch 21/300 - Train Loss: 0.0745, Val Loss: 0.0739\n",
      "Epoch 22/300 - Train Loss: 0.0742, Val Loss: 0.0702\n",
      "Epoch 23/300 - Train Loss: 0.0750, Val Loss: 0.0720\n",
      "Epoch 24/300 - Train Loss: 0.0741, Val Loss: 0.0706\n",
      "Epoch 25/300 - Train Loss: 0.0721, Val Loss: 0.0705\n",
      "Epoch 26/300 - Train Loss: 0.0713, Val Loss: 0.0684\n",
      "Epoch 27/300 - Train Loss: 0.0710, Val Loss: 0.0706\n",
      "Epoch 28/300 - Train Loss: 0.0709, Val Loss: 0.0715\n",
      "Epoch 29/300 - Train Loss: 0.0681, Val Loss: 0.0739\n",
      "Epoch 30/300 - Train Loss: 0.0701, Val Loss: 0.0727\n",
      "Epoch 31/300 - Train Loss: 0.0710, Val Loss: 0.0699\n",
      "Epoch 32/300 - Train Loss: 0.0671, Val Loss: 0.0689\n",
      "Epoch 33/300 - Train Loss: 0.0709, Val Loss: 0.0719\n",
      "Epoch 34/300 - Train Loss: 0.0680, Val Loss: 0.0738\n",
      "Epoch 35/300 - Train Loss: 0.0676, Val Loss: 0.0679\n",
      "Epoch 36/300 - Train Loss: 0.0659, Val Loss: 0.0740\n",
      "Epoch 37/300 - Train Loss: 0.0671, Val Loss: 0.0722\n",
      "Epoch 38/300 - Train Loss: 0.0646, Val Loss: 0.0697\n",
      "Epoch 39/300 - Train Loss: 0.0647, Val Loss: 0.0727\n",
      "Epoch 40/300 - Train Loss: 0.0653, Val Loss: 0.0758\n",
      "Epoch 41/300 - Train Loss: 0.0649, Val Loss: 0.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:00:48,229] Trial 487 finished with value: 0.9720574024034955 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.16238393634342083, 'learning_rate': 0.00010376249154485847, 'batch_size': 32, 'weight_decay': 3.3666150711040286e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300 - Train Loss: 0.0636, Val Loss: 0.0699\n",
      "Early stopping at epoch 42\n",
      "Macro F1 Score: 0.9721, Macro Precision: 0.9781, Macro Recall: 0.9663\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.97      0.93      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.98      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 489\n",
      "Training with F1=16, F2=8, D=8, dropout=0.1316063585172962, LR=0.0006294178003131345, BS=32, WD=7.494996257689317e-05\n",
      "Epoch 1/300 - Train Loss: 0.1593, Val Loss: 0.0863\n",
      "Epoch 2/300 - Train Loss: 0.0971, Val Loss: 0.0891\n",
      "Epoch 3/300 - Train Loss: 0.0907, Val Loss: 0.0677\n",
      "Epoch 4/300 - Train Loss: 0.0902, Val Loss: 0.0724\n",
      "Epoch 5/300 - Train Loss: 0.0875, Val Loss: 0.0718\n",
      "Epoch 6/300 - Train Loss: 0.0855, Val Loss: 0.0757\n",
      "Epoch 7/300 - Train Loss: 0.0831, Val Loss: 0.0840\n",
      "Epoch 8/300 - Train Loss: 0.0831, Val Loss: 0.0758\n",
      "Epoch 9/300 - Train Loss: 0.0802, Val Loss: 0.0785\n",
      "Epoch 10/300 - Train Loss: 0.0805, Val Loss: 0.0764\n",
      "Epoch 11/300 - Train Loss: 0.0782, Val Loss: 0.0812\n",
      "Epoch 12/300 - Train Loss: 0.0779, Val Loss: 0.0704\n",
      "Epoch 13/300 - Train Loss: 0.0778, Val Loss: 0.0666\n",
      "Epoch 14/300 - Train Loss: 0.0751, Val Loss: 0.0721\n",
      "Epoch 15/300 - Train Loss: 0.0756, Val Loss: 0.0743\n",
      "Epoch 16/300 - Train Loss: 0.0743, Val Loss: 0.0716\n",
      "Epoch 17/300 - Train Loss: 0.0730, Val Loss: 0.0781\n",
      "Epoch 18/300 - Train Loss: 0.0741, Val Loss: 0.0711\n",
      "Epoch 19/300 - Train Loss: 0.0743, Val Loss: 0.0753\n",
      "Epoch 20/300 - Train Loss: 0.0754, Val Loss: 0.0701\n",
      "Epoch 21/300 - Train Loss: 0.0716, Val Loss: 0.0692\n",
      "Epoch 22/300 - Train Loss: 0.0710, Val Loss: 0.0726\n",
      "Epoch 23/300 - Train Loss: 0.0694, Val Loss: 0.0697\n",
      "Epoch 24/300 - Train Loss: 0.0695, Val Loss: 0.0667\n",
      "Epoch 25/300 - Train Loss: 0.0694, Val Loss: 0.0660\n",
      "Epoch 26/300 - Train Loss: 0.0692, Val Loss: 0.0710\n",
      "Epoch 27/300 - Train Loss: 0.0682, Val Loss: 0.0811\n",
      "Epoch 28/300 - Train Loss: 0.0698, Val Loss: 0.0693\n",
      "Epoch 29/300 - Train Loss: 0.0685, Val Loss: 0.0660\n",
      "Epoch 30/300 - Train Loss: 0.0672, Val Loss: 0.0693\n",
      "Epoch 31/300 - Train Loss: 0.0706, Val Loss: 0.0784\n",
      "Epoch 32/300 - Train Loss: 0.0723, Val Loss: 0.0680\n",
      "Epoch 33/300 - Train Loss: 0.0662, Val Loss: 0.0690\n",
      "Epoch 34/300 - Train Loss: 0.0642, Val Loss: 0.0670\n",
      "Epoch 35/300 - Train Loss: 0.0644, Val Loss: 0.0668\n",
      "Epoch 36/300 - Train Loss: 0.0640, Val Loss: 0.0798\n",
      "Epoch 37/300 - Train Loss: 0.0683, Val Loss: 0.0695\n",
      "Epoch 38/300 - Train Loss: 0.0659, Val Loss: 0.0724\n",
      "Epoch 39/300 - Train Loss: 0.0634, Val Loss: 0.0688\n",
      "Epoch 40/300 - Train Loss: 0.0652, Val Loss: 0.0680\n",
      "Epoch 41/300 - Train Loss: 0.0662, Val Loss: 0.0727\n",
      "Epoch 42/300 - Train Loss: 0.0665, Val Loss: 0.0780\n",
      "Epoch 43/300 - Train Loss: 0.0680, Val Loss: 0.0666\n",
      "Epoch 44/300 - Train Loss: 0.0648, Val Loss: 0.0674\n",
      "Epoch 45/300 - Train Loss: 0.0615, Val Loss: 0.0702\n",
      "Epoch 46/300 - Train Loss: 0.0625, Val Loss: 0.0720\n",
      "Epoch 47/300 - Train Loss: 0.0652, Val Loss: 0.0772\n",
      "Epoch 48/300 - Train Loss: 0.0644, Val Loss: 0.0697\n",
      "Epoch 49/300 - Train Loss: 0.0614, Val Loss: 0.0752\n",
      "Epoch 50/300 - Train Loss: 0.0623, Val Loss: 0.0725\n",
      "Epoch 51/300 - Train Loss: 0.0636, Val Loss: 0.0679\n",
      "Epoch 52/300 - Train Loss: 0.0599, Val Loss: 0.0741\n",
      "Epoch 53/300 - Train Loss: 0.0612, Val Loss: 0.0730\n",
      "Epoch 54/300 - Train Loss: 0.0618, Val Loss: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:03:56,446] Trial 488 finished with value: 0.9691330369607457 and parameters: {'F1': 16, 'F2': 8, 'D': 8, 'dropout': 0.1316063585172962, 'learning_rate': 0.0006294178003131345, 'batch_size': 32, 'weight_decay': 7.494996257689317e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/300 - Train Loss: 0.0608, Val Loss: 0.0702\n",
      "Early stopping at epoch 55\n",
      "Macro F1 Score: 0.9691, Macro Precision: 0.9671, Macro Recall: 0.9712\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 490\n",
      "Training with F1=32, F2=32, D=4, dropout=0.14663087225859475, LR=0.0007938692832392378, BS=256, WD=0.00012049772656053933\n",
      "Epoch 1/300 - Train Loss: 0.2107, Val Loss: 0.2122\n",
      "Epoch 2/300 - Train Loss: 0.0928, Val Loss: 0.0897\n",
      "Epoch 3/300 - Train Loss: 0.0828, Val Loss: 0.0850\n",
      "Epoch 4/300 - Train Loss: 0.0775, Val Loss: 0.0685\n",
      "Epoch 5/300 - Train Loss: 0.0771, Val Loss: 0.0986\n",
      "Epoch 6/300 - Train Loss: 0.0755, Val Loss: 0.0757\n",
      "Epoch 7/300 - Train Loss: 0.0740, Val Loss: 0.0855\n",
      "Epoch 8/300 - Train Loss: 0.0701, Val Loss: 0.0768\n",
      "Epoch 9/300 - Train Loss: 0.0703, Val Loss: 0.0763\n",
      "Epoch 10/300 - Train Loss: 0.0692, Val Loss: 0.0693\n",
      "Epoch 11/300 - Train Loss: 0.0685, Val Loss: 0.0837\n",
      "Epoch 12/300 - Train Loss: 0.0659, Val Loss: 0.0671\n",
      "Epoch 13/300 - Train Loss: 0.0655, Val Loss: 0.0715\n",
      "Epoch 14/300 - Train Loss: 0.0651, Val Loss: 0.0673\n",
      "Epoch 15/300 - Train Loss: 0.0639, Val Loss: 0.0706\n",
      "Epoch 16/300 - Train Loss: 0.0645, Val Loss: 0.0709\n",
      "Epoch 17/300 - Train Loss: 0.0612, Val Loss: 0.0861\n",
      "Epoch 18/300 - Train Loss: 0.0607, Val Loss: 0.0769\n",
      "Epoch 19/300 - Train Loss: 0.0606, Val Loss: 0.0845\n",
      "Epoch 20/300 - Train Loss: 0.0614, Val Loss: 0.0685\n",
      "Epoch 21/300 - Train Loss: 0.0581, Val Loss: 0.0744\n",
      "Epoch 22/300 - Train Loss: 0.0576, Val Loss: 0.0683\n",
      "Epoch 23/300 - Train Loss: 0.0563, Val Loss: 0.0825\n",
      "Epoch 24/300 - Train Loss: 0.0562, Val Loss: 0.0711\n",
      "Epoch 25/300 - Train Loss: 0.0552, Val Loss: 0.0677\n",
      "Epoch 26/300 - Train Loss: 0.0549, Val Loss: 0.0706\n",
      "Epoch 27/300 - Train Loss: 0.0536, Val Loss: 0.0689\n",
      "Epoch 28/300 - Train Loss: 0.0544, Val Loss: 0.0766\n",
      "Epoch 29/300 - Train Loss: 0.0531, Val Loss: 0.0700\n",
      "Epoch 30/300 - Train Loss: 0.0510, Val Loss: 0.0734\n",
      "Epoch 31/300 - Train Loss: 0.0498, Val Loss: 0.0720\n",
      "Epoch 32/300 - Train Loss: 0.0507, Val Loss: 0.0724\n",
      "Epoch 33/300 - Train Loss: 0.0481, Val Loss: 0.0707\n",
      "Epoch 34/300 - Train Loss: 0.0479, Val Loss: 0.0735\n",
      "Epoch 35/300 - Train Loss: 0.0491, Val Loss: 0.0770\n",
      "Epoch 36/300 - Train Loss: 0.0475, Val Loss: 0.0802\n",
      "Epoch 37/300 - Train Loss: 0.0479, Val Loss: 0.0770\n",
      "Epoch 38/300 - Train Loss: 0.0449, Val Loss: 0.0733\n",
      "Epoch 39/300 - Train Loss: 0.0437, Val Loss: 0.0752\n",
      "Epoch 40/300 - Train Loss: 0.0460, Val Loss: 0.0765\n",
      "Epoch 41/300 - Train Loss: 0.0451, Val Loss: 0.0713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:06:17,464] Trial 489 finished with value: 0.9660655468159328 and parameters: {'F1': 32, 'F2': 32, 'D': 4, 'dropout': 0.14663087225859475, 'learning_rate': 0.0007938692832392378, 'batch_size': 256, 'weight_decay': 0.00012049772656053933}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/300 - Train Loss: 0.0428, Val Loss: 0.0779\n",
      "Early stopping at epoch 42\n",
      "Macro F1 Score: 0.9661, Macro Precision: 0.9581, Macro Recall: 0.9747\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 491\n",
      "Training with F1=32, F2=32, D=2, dropout=0.11252443547550733, LR=5.3455652540619117e-05, BS=32, WD=3.805101043357868e-05\n",
      "Epoch 1/300 - Train Loss: 0.3495, Val Loss: 0.1444\n",
      "Epoch 2/300 - Train Loss: 0.1417, Val Loss: 0.1008\n",
      "Epoch 3/300 - Train Loss: 0.1124, Val Loss: 0.0927\n",
      "Epoch 4/300 - Train Loss: 0.1033, Val Loss: 0.0797\n",
      "Epoch 5/300 - Train Loss: 0.0967, Val Loss: 0.0852\n",
      "Epoch 6/300 - Train Loss: 0.0960, Val Loss: 0.0838\n",
      "Epoch 7/300 - Train Loss: 0.0927, Val Loss: 0.0721\n",
      "Epoch 8/300 - Train Loss: 0.0929, Val Loss: 0.0835\n",
      "Epoch 9/300 - Train Loss: 0.0876, Val Loss: 0.0719\n",
      "Epoch 10/300 - Train Loss: 0.0866, Val Loss: 0.0759\n",
      "Epoch 11/300 - Train Loss: 0.0857, Val Loss: 0.0738\n",
      "Epoch 12/300 - Train Loss: 0.0857, Val Loss: 0.0812\n",
      "Epoch 13/300 - Train Loss: 0.0840, Val Loss: 0.0741\n",
      "Epoch 14/300 - Train Loss: 0.0839, Val Loss: 0.0746\n",
      "Epoch 15/300 - Train Loss: 0.0840, Val Loss: 0.0777\n",
      "Epoch 16/300 - Train Loss: 0.0814, Val Loss: 0.0710\n",
      "Epoch 17/300 - Train Loss: 0.0796, Val Loss: 0.0739\n",
      "Epoch 18/300 - Train Loss: 0.0791, Val Loss: 0.0731\n",
      "Epoch 19/300 - Train Loss: 0.0779, Val Loss: 0.0721\n",
      "Epoch 20/300 - Train Loss: 0.0794, Val Loss: 0.0776\n",
      "Epoch 21/300 - Train Loss: 0.0778, Val Loss: 0.0797\n",
      "Epoch 22/300 - Train Loss: 0.0777, Val Loss: 0.0761\n",
      "Epoch 23/300 - Train Loss: 0.0773, Val Loss: 0.0747\n",
      "Epoch 24/300 - Train Loss: 0.0752, Val Loss: 0.0720\n",
      "Epoch 25/300 - Train Loss: 0.0765, Val Loss: 0.0743\n",
      "Epoch 26/300 - Train Loss: 0.0737, Val Loss: 0.0745\n",
      "Epoch 27/300 - Train Loss: 0.0759, Val Loss: 0.0706\n",
      "Epoch 28/300 - Train Loss: 0.0742, Val Loss: 0.0761\n",
      "Epoch 29/300 - Train Loss: 0.0751, Val Loss: 0.0832\n",
      "Epoch 30/300 - Train Loss: 0.0729, Val Loss: 0.0714\n",
      "Epoch 31/300 - Train Loss: 0.0733, Val Loss: 0.0710\n",
      "Epoch 32/300 - Train Loss: 0.0726, Val Loss: 0.0829\n",
      "Epoch 33/300 - Train Loss: 0.0717, Val Loss: 0.0726\n",
      "Epoch 34/300 - Train Loss: 0.0723, Val Loss: 0.0741\n",
      "Epoch 35/300 - Train Loss: 0.0721, Val Loss: 0.0742\n",
      "Epoch 36/300 - Train Loss: 0.0716, Val Loss: 0.0717\n",
      "Epoch 37/300 - Train Loss: 0.0726, Val Loss: 0.0834\n",
      "Epoch 38/300 - Train Loss: 0.0700, Val Loss: 0.0755\n",
      "Epoch 39/300 - Train Loss: 0.0713, Val Loss: 0.0797\n",
      "Epoch 40/300 - Train Loss: 0.0730, Val Loss: 0.0684\n",
      "Epoch 41/300 - Train Loss: 0.0710, Val Loss: 0.0722\n",
      "Epoch 42/300 - Train Loss: 0.0682, Val Loss: 0.0733\n",
      "Epoch 43/300 - Train Loss: 0.0690, Val Loss: 0.0770\n",
      "Epoch 44/300 - Train Loss: 0.0675, Val Loss: 0.0697\n",
      "Epoch 45/300 - Train Loss: 0.0669, Val Loss: 0.0692\n",
      "Epoch 46/300 - Train Loss: 0.0676, Val Loss: 0.0798\n",
      "Epoch 47/300 - Train Loss: 0.0666, Val Loss: 0.0688\n",
      "Epoch 48/300 - Train Loss: 0.0667, Val Loss: 0.0728\n",
      "Epoch 49/300 - Train Loss: 0.0698, Val Loss: 0.0724\n",
      "Epoch 50/300 - Train Loss: 0.0659, Val Loss: 0.0743\n",
      "Epoch 51/300 - Train Loss: 0.0657, Val Loss: 0.0734\n",
      "Epoch 52/300 - Train Loss: 0.0646, Val Loss: 0.0660\n",
      "Epoch 53/300 - Train Loss: 0.0646, Val Loss: 0.0737\n",
      "Epoch 54/300 - Train Loss: 0.0662, Val Loss: 0.0716\n",
      "Epoch 55/300 - Train Loss: 0.0655, Val Loss: 0.0741\n",
      "Epoch 56/300 - Train Loss: 0.0643, Val Loss: 0.0715\n",
      "Epoch 57/300 - Train Loss: 0.0622, Val Loss: 0.0710\n",
      "Epoch 58/300 - Train Loss: 0.0613, Val Loss: 0.0708\n",
      "Epoch 59/300 - Train Loss: 0.0613, Val Loss: 0.0754\n",
      "Epoch 60/300 - Train Loss: 0.0625, Val Loss: 0.0702\n",
      "Epoch 61/300 - Train Loss: 0.0623, Val Loss: 0.0729\n",
      "Epoch 62/300 - Train Loss: 0.0627, Val Loss: 0.0688\n",
      "Epoch 63/300 - Train Loss: 0.0602, Val Loss: 0.0668\n",
      "Epoch 64/300 - Train Loss: 0.0624, Val Loss: 0.0717\n",
      "Epoch 65/300 - Train Loss: 0.0588, Val Loss: 0.0720\n",
      "Epoch 66/300 - Train Loss: 0.0606, Val Loss: 0.0735\n",
      "Epoch 67/300 - Train Loss: 0.0608, Val Loss: 0.0706\n",
      "Epoch 68/300 - Train Loss: 0.0588, Val Loss: 0.0681\n",
      "Epoch 69/300 - Train Loss: 0.0604, Val Loss: 0.0706\n",
      "Epoch 70/300 - Train Loss: 0.0575, Val Loss: 0.0705\n",
      "Epoch 71/300 - Train Loss: 0.0572, Val Loss: 0.0709\n",
      "Epoch 72/300 - Train Loss: 0.0580, Val Loss: 0.0696\n",
      "Epoch 73/300 - Train Loss: 0.0564, Val Loss: 0.0754\n",
      "Epoch 74/300 - Train Loss: 0.0591, Val Loss: 0.0681\n",
      "Epoch 75/300 - Train Loss: 0.0571, Val Loss: 0.0680\n",
      "Epoch 76/300 - Train Loss: 0.0603, Val Loss: 0.0661\n",
      "Epoch 77/300 - Train Loss: 0.0582, Val Loss: 0.0725\n",
      "Epoch 78/300 - Train Loss: 0.0586, Val Loss: 0.0689\n",
      "Epoch 79/300 - Train Loss: 0.0554, Val Loss: 0.0726\n",
      "Epoch 80/300 - Train Loss: 0.0539, Val Loss: 0.0718\n",
      "Epoch 81/300 - Train Loss: 0.0536, Val Loss: 0.0716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:10:02,326] Trial 490 finished with value: 0.9741422671095664 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.11252443547550733, 'learning_rate': 5.3455652540619117e-05, 'batch_size': 32, 'weight_decay': 3.805101043357868e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/300 - Train Loss: 0.0542, Val Loss: 0.0694\n",
      "Early stopping at epoch 82\n",
      "Macro F1 Score: 0.9741, Macro Precision: 0.9704, Macro Recall: 0.9781\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.97      0.95        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 492\n",
      "Training with F1=16, F2=8, D=2, dropout=0.17129803711243036, LR=0.0007395892882372094, BS=32, WD=6.435692751256417e-05\n",
      "Epoch 1/300 - Train Loss: 0.1804, Val Loss: 0.0956\n",
      "Epoch 2/300 - Train Loss: 0.0990, Val Loss: 0.0719\n",
      "Epoch 3/300 - Train Loss: 0.0952, Val Loss: 0.0774\n",
      "Epoch 4/300 - Train Loss: 0.0942, Val Loss: 0.0654\n",
      "Epoch 5/300 - Train Loss: 0.0900, Val Loss: 0.0689\n",
      "Epoch 6/300 - Train Loss: 0.0880, Val Loss: 0.0701\n",
      "Epoch 7/300 - Train Loss: 0.0866, Val Loss: 0.0686\n",
      "Epoch 8/300 - Train Loss: 0.0883, Val Loss: 0.0714\n",
      "Epoch 9/300 - Train Loss: 0.0871, Val Loss: 0.0753\n",
      "Epoch 10/300 - Train Loss: 0.0833, Val Loss: 0.0788\n",
      "Epoch 11/300 - Train Loss: 0.0859, Val Loss: 0.0793\n",
      "Epoch 12/300 - Train Loss: 0.0838, Val Loss: 0.0663\n",
      "Epoch 13/300 - Train Loss: 0.0829, Val Loss: 0.0902\n",
      "Epoch 14/300 - Train Loss: 0.0831, Val Loss: 0.0961\n",
      "Epoch 15/300 - Train Loss: 0.0820, Val Loss: 0.0724\n",
      "Epoch 16/300 - Train Loss: 0.0832, Val Loss: 0.0733\n",
      "Epoch 17/300 - Train Loss: 0.0837, Val Loss: 0.0822\n",
      "Epoch 18/300 - Train Loss: 0.0803, Val Loss: 0.0715\n",
      "Epoch 19/300 - Train Loss: 0.0812, Val Loss: 0.0710\n",
      "Epoch 20/300 - Train Loss: 0.0803, Val Loss: 0.0775\n",
      "Epoch 21/300 - Train Loss: 0.0816, Val Loss: 0.0766\n",
      "Epoch 22/300 - Train Loss: 0.0812, Val Loss: 0.0927\n",
      "Epoch 23/300 - Train Loss: 0.0773, Val Loss: 0.0719\n",
      "Epoch 24/300 - Train Loss: 0.0779, Val Loss: 0.0707\n",
      "Epoch 25/300 - Train Loss: 0.0794, Val Loss: 0.0726\n",
      "Epoch 26/300 - Train Loss: 0.0773, Val Loss: 0.0676\n",
      "Epoch 27/300 - Train Loss: 0.0766, Val Loss: 0.0780\n",
      "Epoch 28/300 - Train Loss: 0.0779, Val Loss: 0.0708\n",
      "Epoch 29/300 - Train Loss: 0.0751, Val Loss: 0.0784\n",
      "Epoch 30/300 - Train Loss: 0.0788, Val Loss: 0.0711\n",
      "Epoch 31/300 - Train Loss: 0.0749, Val Loss: 0.0719\n",
      "Epoch 32/300 - Train Loss: 0.0740, Val Loss: 0.0684\n",
      "Epoch 33/300 - Train Loss: 0.0763, Val Loss: 0.0718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:11:13,946] Trial 491 finished with value: 0.9676717691489696 and parameters: {'F1': 16, 'F2': 8, 'D': 2, 'dropout': 0.17129803711243036, 'learning_rate': 0.0007395892882372094, 'batch_size': 32, 'weight_decay': 6.435692751256417e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/300 - Train Loss: 0.0747, Val Loss: 0.0731\n",
      "Early stopping at epoch 34\n",
      "Macro F1 Score: 0.9677, Macro Precision: 0.9712, Macro Recall: 0.9644\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       789\n",
      "           1       0.95      0.93      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.96      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 493\n",
      "Training with F1=32, F2=32, D=8, dropout=0.1576417900368438, LR=0.00012548801354653525, BS=32, WD=4.24601210814888e-05\n",
      "Epoch 1/300 - Train Loss: 0.2079, Val Loss: 0.0892\n",
      "Epoch 2/300 - Train Loss: 0.1033, Val Loss: 0.1026\n",
      "Epoch 3/300 - Train Loss: 0.0936, Val Loss: 0.0754\n",
      "Epoch 4/300 - Train Loss: 0.0892, Val Loss: 0.0878\n",
      "Epoch 5/300 - Train Loss: 0.0846, Val Loss: 0.0818\n",
      "Epoch 6/300 - Train Loss: 0.0830, Val Loss: 0.0879\n",
      "Epoch 7/300 - Train Loss: 0.0805, Val Loss: 0.0736\n",
      "Epoch 8/300 - Train Loss: 0.0791, Val Loss: 0.0844\n",
      "Epoch 9/300 - Train Loss: 0.0772, Val Loss: 0.0805\n",
      "Epoch 10/300 - Train Loss: 0.0763, Val Loss: 0.0764\n",
      "Epoch 11/300 - Train Loss: 0.0741, Val Loss: 0.0675\n",
      "Epoch 12/300 - Train Loss: 0.0727, Val Loss: 0.0758\n",
      "Epoch 13/300 - Train Loss: 0.0721, Val Loss: 0.0734\n",
      "Epoch 14/300 - Train Loss: 0.0704, Val Loss: 0.0735\n",
      "Epoch 15/300 - Train Loss: 0.0697, Val Loss: 0.0758\n",
      "Epoch 16/300 - Train Loss: 0.0694, Val Loss: 0.0685\n",
      "Epoch 17/300 - Train Loss: 0.0683, Val Loss: 0.0695\n",
      "Epoch 18/300 - Train Loss: 0.0685, Val Loss: 0.0727\n",
      "Epoch 19/300 - Train Loss: 0.0670, Val Loss: 0.0697\n",
      "Epoch 20/300 - Train Loss: 0.0665, Val Loss: 0.0728\n",
      "Epoch 21/300 - Train Loss: 0.0645, Val Loss: 0.0716\n",
      "Epoch 22/300 - Train Loss: 0.0678, Val Loss: 0.0711\n",
      "Epoch 23/300 - Train Loss: 0.0645, Val Loss: 0.0696\n",
      "Epoch 24/300 - Train Loss: 0.0639, Val Loss: 0.0707\n",
      "Epoch 25/300 - Train Loss: 0.0607, Val Loss: 0.0728\n",
      "Epoch 26/300 - Train Loss: 0.0629, Val Loss: 0.0687\n",
      "Epoch 27/300 - Train Loss: 0.0628, Val Loss: 0.0760\n",
      "Epoch 28/300 - Train Loss: 0.0589, Val Loss: 0.0702\n",
      "Epoch 29/300 - Train Loss: 0.0600, Val Loss: 0.0705\n",
      "Epoch 30/300 - Train Loss: 0.0586, Val Loss: 0.0735\n",
      "Epoch 31/300 - Train Loss: 0.0590, Val Loss: 0.0671\n",
      "Epoch 32/300 - Train Loss: 0.0607, Val Loss: 0.0694\n",
      "Epoch 33/300 - Train Loss: 0.0565, Val Loss: 0.0748\n",
      "Epoch 34/300 - Train Loss: 0.0567, Val Loss: 0.0699\n",
      "Epoch 35/300 - Train Loss: 0.0556, Val Loss: 0.0796\n",
      "Epoch 36/300 - Train Loss: 0.0564, Val Loss: 0.0717\n",
      "Epoch 37/300 - Train Loss: 0.0536, Val Loss: 0.0708\n",
      "Epoch 38/300 - Train Loss: 0.0577, Val Loss: 0.0688\n",
      "Epoch 39/300 - Train Loss: 0.0535, Val Loss: 0.0713\n",
      "Epoch 40/300 - Train Loss: 0.0535, Val Loss: 0.0688\n",
      "Epoch 41/300 - Train Loss: 0.0536, Val Loss: 0.0718\n",
      "Epoch 42/300 - Train Loss: 0.0526, Val Loss: 0.0739\n",
      "Epoch 43/300 - Train Loss: 0.0527, Val Loss: 0.0733\n",
      "Epoch 44/300 - Train Loss: 0.0514, Val Loss: 0.0690\n",
      "Epoch 45/300 - Train Loss: 0.0503, Val Loss: 0.0732\n",
      "Epoch 46/300 - Train Loss: 0.0505, Val Loss: 0.0747\n",
      "Epoch 47/300 - Train Loss: 0.0492, Val Loss: 0.0715\n",
      "Epoch 48/300 - Train Loss: 0.0494, Val Loss: 0.0829\n",
      "Epoch 49/300 - Train Loss: 0.0477, Val Loss: 0.0772\n",
      "Epoch 50/300 - Train Loss: 0.0486, Val Loss: 0.0758\n",
      "Epoch 51/300 - Train Loss: 0.0495, Val Loss: 0.0734\n",
      "Epoch 52/300 - Train Loss: 0.0465, Val Loss: 0.0692\n",
      "Epoch 53/300 - Train Loss: 0.0473, Val Loss: 0.0714\n",
      "Epoch 54/300 - Train Loss: 0.0456, Val Loss: 0.0737\n",
      "Epoch 55/300 - Train Loss: 0.0488, Val Loss: 0.0689\n",
      "Epoch 56/300 - Train Loss: 0.0461, Val Loss: 0.0729\n",
      "Epoch 57/300 - Train Loss: 0.0444, Val Loss: 0.0743\n",
      "Epoch 58/300 - Train Loss: 0.0443, Val Loss: 0.0717\n",
      "Epoch 59/300 - Train Loss: 0.0436, Val Loss: 0.0748\n",
      "Epoch 60/300 - Train Loss: 0.0431, Val Loss: 0.0764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:17:51,433] Trial 492 finished with value: 0.9673756652108502 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.1576417900368438, 'learning_rate': 0.00012548801354653525, 'batch_size': 32, 'weight_decay': 4.24601210814888e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/300 - Train Loss: 0.0425, Val Loss: 0.0813\n",
      "Early stopping at epoch 61\n",
      "Macro F1 Score: 0.9674, Macro Precision: 0.9676, Macro Recall: 0.9672\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.93      0.93      0.93        61\n",
      "           2       0.98      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 494\n",
      "Training with F1=16, F2=32, D=2, dropout=0.14202873466023944, LR=9.006971083534207e-05, BS=32, WD=5.538448567849281e-05\n",
      "Epoch 1/300 - Train Loss: 0.3014, Val Loss: 0.1127\n",
      "Epoch 2/300 - Train Loss: 0.1160, Val Loss: 0.0895\n",
      "Epoch 3/300 - Train Loss: 0.1028, Val Loss: 0.0744\n",
      "Epoch 4/300 - Train Loss: 0.0936, Val Loss: 0.0764\n",
      "Epoch 5/300 - Train Loss: 0.0923, Val Loss: 0.0760\n",
      "Epoch 6/300 - Train Loss: 0.0896, Val Loss: 0.0731\n",
      "Epoch 7/300 - Train Loss: 0.0890, Val Loss: 0.0830\n",
      "Epoch 8/300 - Train Loss: 0.0904, Val Loss: 0.0711\n",
      "Epoch 9/300 - Train Loss: 0.0842, Val Loss: 0.0710\n",
      "Epoch 10/300 - Train Loss: 0.0856, Val Loss: 0.0720\n",
      "Epoch 11/300 - Train Loss: 0.0820, Val Loss: 0.0855\n",
      "Epoch 12/300 - Train Loss: 0.0819, Val Loss: 0.0764\n",
      "Epoch 13/300 - Train Loss: 0.0809, Val Loss: 0.0719\n",
      "Epoch 14/300 - Train Loss: 0.0832, Val Loss: 0.0692\n",
      "Epoch 15/300 - Train Loss: 0.0800, Val Loss: 0.0769\n",
      "Epoch 16/300 - Train Loss: 0.0799, Val Loss: 0.0715\n",
      "Epoch 17/300 - Train Loss: 0.0787, Val Loss: 0.0687\n",
      "Epoch 18/300 - Train Loss: 0.0776, Val Loss: 0.0683\n",
      "Epoch 19/300 - Train Loss: 0.0774, Val Loss: 0.0756\n",
      "Epoch 20/300 - Train Loss: 0.0773, Val Loss: 0.0727\n",
      "Epoch 21/300 - Train Loss: 0.0761, Val Loss: 0.0701\n",
      "Epoch 22/300 - Train Loss: 0.0766, Val Loss: 0.0649\n",
      "Epoch 23/300 - Train Loss: 0.0765, Val Loss: 0.0696\n",
      "Epoch 24/300 - Train Loss: 0.0749, Val Loss: 0.0734\n",
      "Epoch 25/300 - Train Loss: 0.0742, Val Loss: 0.0773\n",
      "Epoch 26/300 - Train Loss: 0.0749, Val Loss: 0.0703\n",
      "Epoch 27/300 - Train Loss: 0.0738, Val Loss: 0.0695\n",
      "Epoch 28/300 - Train Loss: 0.0732, Val Loss: 0.0657\n",
      "Epoch 29/300 - Train Loss: 0.0736, Val Loss: 0.0675\n",
      "Epoch 30/300 - Train Loss: 0.0711, Val Loss: 0.0694\n",
      "Epoch 31/300 - Train Loss: 0.0711, Val Loss: 0.0659\n",
      "Epoch 32/300 - Train Loss: 0.0723, Val Loss: 0.0650\n",
      "Epoch 33/300 - Train Loss: 0.0725, Val Loss: 0.0665\n",
      "Epoch 34/300 - Train Loss: 0.0707, Val Loss: 0.0669\n",
      "Epoch 35/300 - Train Loss: 0.0703, Val Loss: 0.0658\n",
      "Epoch 36/300 - Train Loss: 0.0703, Val Loss: 0.0645\n",
      "Epoch 37/300 - Train Loss: 0.0711, Val Loss: 0.0667\n",
      "Epoch 38/300 - Train Loss: 0.0695, Val Loss: 0.0707\n",
      "Epoch 39/300 - Train Loss: 0.0684, Val Loss: 0.0666\n",
      "Epoch 40/300 - Train Loss: 0.0687, Val Loss: 0.0681\n",
      "Epoch 41/300 - Train Loss: 0.0695, Val Loss: 0.0690\n",
      "Epoch 42/300 - Train Loss: 0.0692, Val Loss: 0.0675\n",
      "Epoch 43/300 - Train Loss: 0.0676, Val Loss: 0.0659\n",
      "Epoch 44/300 - Train Loss: 0.0679, Val Loss: 0.0696\n",
      "Epoch 45/300 - Train Loss: 0.0670, Val Loss: 0.0693\n",
      "Epoch 46/300 - Train Loss: 0.0667, Val Loss: 0.0715\n",
      "Epoch 47/300 - Train Loss: 0.0685, Val Loss: 0.0697\n",
      "Epoch 48/300 - Train Loss: 0.0665, Val Loss: 0.0755\n",
      "Epoch 49/300 - Train Loss: 0.0656, Val Loss: 0.0660\n",
      "Epoch 50/300 - Train Loss: 0.0642, Val Loss: 0.0678\n",
      "Epoch 51/300 - Train Loss: 0.0661, Val Loss: 0.0656\n",
      "Epoch 52/300 - Train Loss: 0.0654, Val Loss: 0.0658\n",
      "Epoch 53/300 - Train Loss: 0.0642, Val Loss: 0.0694\n",
      "Epoch 54/300 - Train Loss: 0.0624, Val Loss: 0.0732\n",
      "Epoch 55/300 - Train Loss: 0.0614, Val Loss: 0.0702\n",
      "Epoch 56/300 - Train Loss: 0.0616, Val Loss: 0.0704\n",
      "Epoch 57/300 - Train Loss: 0.0641, Val Loss: 0.0750\n",
      "Epoch 58/300 - Train Loss: 0.0642, Val Loss: 0.0681\n",
      "Epoch 59/300 - Train Loss: 0.0613, Val Loss: 0.0639\n",
      "Epoch 60/300 - Train Loss: 0.0624, Val Loss: 0.0641\n",
      "Epoch 61/300 - Train Loss: 0.0618, Val Loss: 0.0672\n",
      "Epoch 62/300 - Train Loss: 0.0614, Val Loss: 0.0709\n",
      "Epoch 63/300 - Train Loss: 0.0611, Val Loss: 0.0703\n",
      "Epoch 64/300 - Train Loss: 0.0633, Val Loss: 0.0624\n",
      "Epoch 65/300 - Train Loss: 0.0601, Val Loss: 0.0714\n",
      "Epoch 66/300 - Train Loss: 0.0600, Val Loss: 0.0669\n",
      "Epoch 67/300 - Train Loss: 0.0616, Val Loss: 0.0653\n",
      "Epoch 68/300 - Train Loss: 0.0583, Val Loss: 0.0666\n",
      "Epoch 69/300 - Train Loss: 0.0602, Val Loss: 0.0675\n",
      "Epoch 70/300 - Train Loss: 0.0573, Val Loss: 0.0698\n",
      "Epoch 71/300 - Train Loss: 0.0597, Val Loss: 0.0626\n",
      "Epoch 72/300 - Train Loss: 0.0575, Val Loss: 0.0682\n",
      "Epoch 73/300 - Train Loss: 0.0574, Val Loss: 0.0677\n",
      "Epoch 74/300 - Train Loss: 0.0574, Val Loss: 0.0662\n",
      "Epoch 75/300 - Train Loss: 0.0604, Val Loss: 0.0787\n",
      "Epoch 76/300 - Train Loss: 0.0580, Val Loss: 0.0658\n",
      "Epoch 77/300 - Train Loss: 0.0575, Val Loss: 0.0653\n",
      "Epoch 78/300 - Train Loss: 0.0572, Val Loss: 0.0662\n",
      "Epoch 79/300 - Train Loss: 0.0590, Val Loss: 0.0684\n",
      "Epoch 80/300 - Train Loss: 0.0568, Val Loss: 0.0650\n",
      "Epoch 81/300 - Train Loss: 0.0565, Val Loss: 0.0645\n",
      "Epoch 82/300 - Train Loss: 0.0551, Val Loss: 0.0682\n",
      "Epoch 83/300 - Train Loss: 0.0569, Val Loss: 0.0662\n",
      "Epoch 84/300 - Train Loss: 0.0546, Val Loss: 0.0636\n",
      "Epoch 85/300 - Train Loss: 0.0544, Val Loss: 0.0673\n",
      "Epoch 86/300 - Train Loss: 0.0570, Val Loss: 0.0652\n",
      "Epoch 87/300 - Train Loss: 0.0549, Val Loss: 0.0692\n",
      "Epoch 88/300 - Train Loss: 0.0563, Val Loss: 0.0656\n",
      "Epoch 89/300 - Train Loss: 0.0547, Val Loss: 0.0651\n",
      "Epoch 90/300 - Train Loss: 0.0566, Val Loss: 0.0733\n",
      "Epoch 91/300 - Train Loss: 0.0542, Val Loss: 0.0685\n",
      "Epoch 92/300 - Train Loss: 0.0528, Val Loss: 0.0679\n",
      "Epoch 93/300 - Train Loss: 0.0520, Val Loss: 0.0728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:21:12,228] Trial 493 finished with value: 0.9685374149659863 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.14202873466023944, 'learning_rate': 9.006971083534207e-05, 'batch_size': 32, 'weight_decay': 5.538448567849281e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94/300 - Train Loss: 0.0536, Val Loss: 0.0719\n",
      "Early stopping at epoch 94\n",
      "Macro F1 Score: 0.9685, Macro Precision: 0.9605, Macro Recall: 0.9773\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 495\n",
      "Training with F1=32, F2=8, D=2, dropout=0.16621536610050594, LR=3.516486043702181e-05, BS=32, WD=2.551500258082141e-05\n",
      "Epoch 1/300 - Train Loss: 0.5879, Val Loss: 0.2857\n",
      "Epoch 2/300 - Train Loss: 0.2536, Val Loss: 0.1959\n",
      "Epoch 3/300 - Train Loss: 0.1796, Val Loss: 0.1366\n",
      "Epoch 4/300 - Train Loss: 0.1438, Val Loss: 0.1066\n",
      "Epoch 5/300 - Train Loss: 0.1246, Val Loss: 0.0969\n",
      "Epoch 6/300 - Train Loss: 0.1159, Val Loss: 0.0961\n",
      "Epoch 7/300 - Train Loss: 0.1109, Val Loss: 0.0856\n",
      "Epoch 8/300 - Train Loss: 0.1073, Val Loss: 0.0832\n",
      "Epoch 9/300 - Train Loss: 0.1032, Val Loss: 0.0831\n",
      "Epoch 10/300 - Train Loss: 0.1007, Val Loss: 0.0854\n",
      "Epoch 11/300 - Train Loss: 0.1003, Val Loss: 0.0866\n",
      "Epoch 12/300 - Train Loss: 0.0998, Val Loss: 0.0801\n",
      "Epoch 13/300 - Train Loss: 0.0955, Val Loss: 0.0814\n",
      "Epoch 14/300 - Train Loss: 0.0955, Val Loss: 0.0737\n",
      "Epoch 15/300 - Train Loss: 0.0952, Val Loss: 0.0781\n",
      "Epoch 16/300 - Train Loss: 0.0942, Val Loss: 0.0823\n",
      "Epoch 17/300 - Train Loss: 0.0927, Val Loss: 0.0763\n",
      "Epoch 18/300 - Train Loss: 0.0903, Val Loss: 0.0730\n",
      "Epoch 19/300 - Train Loss: 0.0905, Val Loss: 0.0736\n",
      "Epoch 20/300 - Train Loss: 0.0894, Val Loss: 0.0753\n",
      "Epoch 21/300 - Train Loss: 0.0893, Val Loss: 0.0765\n",
      "Epoch 22/300 - Train Loss: 0.0893, Val Loss: 0.0717\n",
      "Epoch 23/300 - Train Loss: 0.0884, Val Loss: 0.0727\n",
      "Epoch 24/300 - Train Loss: 0.0869, Val Loss: 0.0710\n",
      "Epoch 25/300 - Train Loss: 0.0866, Val Loss: 0.0706\n",
      "Epoch 26/300 - Train Loss: 0.0884, Val Loss: 0.0759\n",
      "Epoch 27/300 - Train Loss: 0.0867, Val Loss: 0.0718\n",
      "Epoch 28/300 - Train Loss: 0.0860, Val Loss: 0.0713\n",
      "Epoch 29/300 - Train Loss: 0.0871, Val Loss: 0.0749\n",
      "Epoch 30/300 - Train Loss: 0.0877, Val Loss: 0.0777\n",
      "Epoch 31/300 - Train Loss: 0.0826, Val Loss: 0.0716\n",
      "Epoch 32/300 - Train Loss: 0.0860, Val Loss: 0.0769\n",
      "Epoch 33/300 - Train Loss: 0.0823, Val Loss: 0.0732\n",
      "Epoch 34/300 - Train Loss: 0.0826, Val Loss: 0.0727\n",
      "Epoch 35/300 - Train Loss: 0.0836, Val Loss: 0.0723\n",
      "Epoch 36/300 - Train Loss: 0.0848, Val Loss: 0.0713\n",
      "Epoch 37/300 - Train Loss: 0.0849, Val Loss: 0.0712\n",
      "Epoch 38/300 - Train Loss: 0.0863, Val Loss: 0.0769\n",
      "Epoch 39/300 - Train Loss: 0.0837, Val Loss: 0.0696\n",
      "Epoch 40/300 - Train Loss: 0.0830, Val Loss: 0.0712\n",
      "Epoch 41/300 - Train Loss: 0.0825, Val Loss: 0.0765\n",
      "Epoch 42/300 - Train Loss: 0.0817, Val Loss: 0.0780\n",
      "Epoch 43/300 - Train Loss: 0.0834, Val Loss: 0.0702\n",
      "Epoch 44/300 - Train Loss: 0.0803, Val Loss: 0.0705\n",
      "Epoch 45/300 - Train Loss: 0.0817, Val Loss: 0.0706\n",
      "Epoch 46/300 - Train Loss: 0.0810, Val Loss: 0.0688\n",
      "Epoch 47/300 - Train Loss: 0.0825, Val Loss: 0.0696\n",
      "Epoch 48/300 - Train Loss: 0.0818, Val Loss: 0.0675\n",
      "Epoch 49/300 - Train Loss: 0.0813, Val Loss: 0.0710\n",
      "Epoch 50/300 - Train Loss: 0.0805, Val Loss: 0.0705\n",
      "Epoch 51/300 - Train Loss: 0.0801, Val Loss: 0.0685\n",
      "Epoch 52/300 - Train Loss: 0.0809, Val Loss: 0.0727\n",
      "Epoch 53/300 - Train Loss: 0.0794, Val Loss: 0.0716\n",
      "Epoch 54/300 - Train Loss: 0.0808, Val Loss: 0.0742\n",
      "Epoch 55/300 - Train Loss: 0.0801, Val Loss: 0.0709\n",
      "Epoch 56/300 - Train Loss: 0.0780, Val Loss: 0.0693\n",
      "Epoch 57/300 - Train Loss: 0.0823, Val Loss: 0.0706\n",
      "Epoch 58/300 - Train Loss: 0.0776, Val Loss: 0.0695\n",
      "Epoch 59/300 - Train Loss: 0.0800, Val Loss: 0.0731\n",
      "Epoch 60/300 - Train Loss: 0.0795, Val Loss: 0.0689\n",
      "Epoch 61/300 - Train Loss: 0.0776, Val Loss: 0.0776\n",
      "Epoch 62/300 - Train Loss: 0.0799, Val Loss: 0.0682\n",
      "Epoch 63/300 - Train Loss: 0.0787, Val Loss: 0.0691\n",
      "Epoch 64/300 - Train Loss: 0.0778, Val Loss: 0.0682\n",
      "Epoch 65/300 - Train Loss: 0.0793, Val Loss: 0.0668\n",
      "Epoch 66/300 - Train Loss: 0.0792, Val Loss: 0.0732\n",
      "Epoch 67/300 - Train Loss: 0.0776, Val Loss: 0.0750\n",
      "Epoch 68/300 - Train Loss: 0.0803, Val Loss: 0.0697\n",
      "Epoch 69/300 - Train Loss: 0.0761, Val Loss: 0.0731\n",
      "Epoch 70/300 - Train Loss: 0.0780, Val Loss: 0.0684\n",
      "Epoch 71/300 - Train Loss: 0.0785, Val Loss: 0.0764\n",
      "Epoch 72/300 - Train Loss: 0.0775, Val Loss: 0.0749\n",
      "Epoch 73/300 - Train Loss: 0.0780, Val Loss: 0.0707\n",
      "Epoch 74/300 - Train Loss: 0.0745, Val Loss: 0.0778\n",
      "Epoch 75/300 - Train Loss: 0.0762, Val Loss: 0.0717\n",
      "Epoch 76/300 - Train Loss: 0.0784, Val Loss: 0.0739\n",
      "Epoch 77/300 - Train Loss: 0.0783, Val Loss: 0.0718\n",
      "Epoch 78/300 - Train Loss: 0.0751, Val Loss: 0.0689\n",
      "Epoch 79/300 - Train Loss: 0.0773, Val Loss: 0.0697\n",
      "Epoch 80/300 - Train Loss: 0.0789, Val Loss: 0.0673\n",
      "Epoch 81/300 - Train Loss: 0.0773, Val Loss: 0.0680\n",
      "Epoch 82/300 - Train Loss: 0.0763, Val Loss: 0.0691\n",
      "Epoch 83/300 - Train Loss: 0.0758, Val Loss: 0.0679\n",
      "Epoch 84/300 - Train Loss: 0.0761, Val Loss: 0.0674\n",
      "Epoch 85/300 - Train Loss: 0.0757, Val Loss: 0.0705\n",
      "Epoch 86/300 - Train Loss: 0.0743, Val Loss: 0.0660\n",
      "Epoch 87/300 - Train Loss: 0.0755, Val Loss: 0.0695\n",
      "Epoch 88/300 - Train Loss: 0.0780, Val Loss: 0.0737\n",
      "Epoch 89/300 - Train Loss: 0.0765, Val Loss: 0.0693\n",
      "Epoch 90/300 - Train Loss: 0.0757, Val Loss: 0.0712\n",
      "Epoch 91/300 - Train Loss: 0.0741, Val Loss: 0.0675\n",
      "Epoch 92/300 - Train Loss: 0.0751, Val Loss: 0.0681\n",
      "Epoch 93/300 - Train Loss: 0.0733, Val Loss: 0.0695\n",
      "Epoch 94/300 - Train Loss: 0.0763, Val Loss: 0.0692\n",
      "Epoch 95/300 - Train Loss: 0.0757, Val Loss: 0.0718\n",
      "Epoch 96/300 - Train Loss: 0.0759, Val Loss: 0.0653\n",
      "Epoch 97/300 - Train Loss: 0.0738, Val Loss: 0.0669\n",
      "Epoch 98/300 - Train Loss: 0.0738, Val Loss: 0.0662\n",
      "Epoch 99/300 - Train Loss: 0.0759, Val Loss: 0.0686\n",
      "Epoch 100/300 - Train Loss: 0.0758, Val Loss: 0.0698\n",
      "Epoch 101/300 - Train Loss: 0.0748, Val Loss: 0.0734\n",
      "Epoch 102/300 - Train Loss: 0.0733, Val Loss: 0.0727\n",
      "Epoch 103/300 - Train Loss: 0.0756, Val Loss: 0.0692\n",
      "Epoch 104/300 - Train Loss: 0.0741, Val Loss: 0.0692\n",
      "Epoch 105/300 - Train Loss: 0.0737, Val Loss: 0.0690\n",
      "Epoch 106/300 - Train Loss: 0.0735, Val Loss: 0.0695\n",
      "Epoch 107/300 - Train Loss: 0.0739, Val Loss: 0.0686\n",
      "Epoch 108/300 - Train Loss: 0.0748, Val Loss: 0.0686\n",
      "Epoch 109/300 - Train Loss: 0.0740, Val Loss: 0.0733\n",
      "Epoch 110/300 - Train Loss: 0.0727, Val Loss: 0.0683\n",
      "Epoch 111/300 - Train Loss: 0.0739, Val Loss: 0.0683\n",
      "Epoch 112/300 - Train Loss: 0.0743, Val Loss: 0.0667\n",
      "Epoch 113/300 - Train Loss: 0.0735, Val Loss: 0.0681\n",
      "Epoch 114/300 - Train Loss: 0.0726, Val Loss: 0.0724\n",
      "Epoch 115/300 - Train Loss: 0.0725, Val Loss: 0.0726\n",
      "Epoch 116/300 - Train Loss: 0.0717, Val Loss: 0.0748\n",
      "Epoch 117/300 - Train Loss: 0.0748, Val Loss: 0.0662\n",
      "Epoch 118/300 - Train Loss: 0.0733, Val Loss: 0.0674\n",
      "Epoch 119/300 - Train Loss: 0.0729, Val Loss: 0.0658\n",
      "Epoch 120/300 - Train Loss: 0.0717, Val Loss: 0.0681\n",
      "Epoch 121/300 - Train Loss: 0.0719, Val Loss: 0.0683\n",
      "Epoch 122/300 - Train Loss: 0.0701, Val Loss: 0.0680\n",
      "Epoch 123/300 - Train Loss: 0.0750, Val Loss: 0.0702\n",
      "Epoch 124/300 - Train Loss: 0.0708, Val Loss: 0.0660\n",
      "Epoch 125/300 - Train Loss: 0.0731, Val Loss: 0.0672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:26:28,041] Trial 494 finished with value: 0.9693616971084471 and parameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.16621536610050594, 'learning_rate': 3.516486043702181e-05, 'batch_size': 32, 'weight_decay': 2.551500258082141e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 126/300 - Train Loss: 0.0722, Val Loss: 0.0701\n",
      "Early stopping at epoch 126\n",
      "Macro F1 Score: 0.9694, Macro Precision: 0.9571, Macro Recall: 0.9830\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       789\n",
      "           1       0.90      0.98      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 496\n",
      "Training with F1=32, F2=32, D=8, dropout=0.17892132300905975, LR=7.748380559117077e-05, BS=32, WD=3.239312339294278e-05\n",
      "Epoch 1/300 - Train Loss: 0.2504, Val Loss: 0.1051\n",
      "Epoch 2/300 - Train Loss: 0.1113, Val Loss: 0.0892\n",
      "Epoch 3/300 - Train Loss: 0.1009, Val Loss: 0.0811\n",
      "Epoch 4/300 - Train Loss: 0.0937, Val Loss: 0.0801\n",
      "Epoch 5/300 - Train Loss: 0.0892, Val Loss: 0.0737\n",
      "Epoch 6/300 - Train Loss: 0.0876, Val Loss: 0.0804\n",
      "Epoch 7/300 - Train Loss: 0.0836, Val Loss: 0.0779\n",
      "Epoch 8/300 - Train Loss: 0.0844, Val Loss: 0.0678\n",
      "Epoch 9/300 - Train Loss: 0.0815, Val Loss: 0.0771\n",
      "Epoch 10/300 - Train Loss: 0.0814, Val Loss: 0.0733\n",
      "Epoch 11/300 - Train Loss: 0.0792, Val Loss: 0.0763\n",
      "Epoch 12/300 - Train Loss: 0.0782, Val Loss: 0.0683\n",
      "Epoch 13/300 - Train Loss: 0.0778, Val Loss: 0.0702\n",
      "Epoch 14/300 - Train Loss: 0.0771, Val Loss: 0.0789\n",
      "Epoch 15/300 - Train Loss: 0.0768, Val Loss: 0.0655\n",
      "Epoch 16/300 - Train Loss: 0.0745, Val Loss: 0.0747\n",
      "Epoch 17/300 - Train Loss: 0.0732, Val Loss: 0.0715\n",
      "Epoch 18/300 - Train Loss: 0.0721, Val Loss: 0.0679\n",
      "Epoch 19/300 - Train Loss: 0.0730, Val Loss: 0.0689\n",
      "Epoch 20/300 - Train Loss: 0.0717, Val Loss: 0.0695\n",
      "Epoch 21/300 - Train Loss: 0.0713, Val Loss: 0.0688\n",
      "Epoch 22/300 - Train Loss: 0.0683, Val Loss: 0.0693\n",
      "Epoch 23/300 - Train Loss: 0.0694, Val Loss: 0.0800\n",
      "Epoch 24/300 - Train Loss: 0.0685, Val Loss: 0.0711\n",
      "Epoch 25/300 - Train Loss: 0.0669, Val Loss: 0.0698\n",
      "Epoch 26/300 - Train Loss: 0.0668, Val Loss: 0.0683\n",
      "Epoch 27/300 - Train Loss: 0.0664, Val Loss: 0.0735\n",
      "Epoch 28/300 - Train Loss: 0.0664, Val Loss: 0.0724\n",
      "Epoch 29/300 - Train Loss: 0.0634, Val Loss: 0.0642\n",
      "Epoch 30/300 - Train Loss: 0.0643, Val Loss: 0.0705\n",
      "Epoch 31/300 - Train Loss: 0.0634, Val Loss: 0.0719\n",
      "Epoch 32/300 - Train Loss: 0.0630, Val Loss: 0.0657\n",
      "Epoch 33/300 - Train Loss: 0.0612, Val Loss: 0.0887\n",
      "Epoch 34/300 - Train Loss: 0.0628, Val Loss: 0.0783\n",
      "Epoch 35/300 - Train Loss: 0.0607, Val Loss: 0.0713\n",
      "Epoch 36/300 - Train Loss: 0.0603, Val Loss: 0.0664\n",
      "Epoch 37/300 - Train Loss: 0.0597, Val Loss: 0.0733\n",
      "Epoch 38/300 - Train Loss: 0.0573, Val Loss: 0.0686\n",
      "Epoch 39/300 - Train Loss: 0.0596, Val Loss: 0.0793\n",
      "Epoch 40/300 - Train Loss: 0.0596, Val Loss: 0.0809\n",
      "Epoch 41/300 - Train Loss: 0.0577, Val Loss: 0.0655\n",
      "Epoch 42/300 - Train Loss: 0.0578, Val Loss: 0.0707\n",
      "Epoch 43/300 - Train Loss: 0.0563, Val Loss: 0.0660\n",
      "Epoch 44/300 - Train Loss: 0.0560, Val Loss: 0.0703\n",
      "Epoch 45/300 - Train Loss: 0.0563, Val Loss: 0.0695\n",
      "Epoch 46/300 - Train Loss: 0.0550, Val Loss: 0.0686\n",
      "Epoch 47/300 - Train Loss: 0.0557, Val Loss: 0.0671\n",
      "Epoch 48/300 - Train Loss: 0.0548, Val Loss: 0.0685\n",
      "Epoch 49/300 - Train Loss: 0.0538, Val Loss: 0.0761\n",
      "Epoch 50/300 - Train Loss: 0.0531, Val Loss: 0.0651\n",
      "Epoch 51/300 - Train Loss: 0.0547, Val Loss: 0.0759\n",
      "Epoch 52/300 - Train Loss: 0.0530, Val Loss: 0.0716\n",
      "Epoch 53/300 - Train Loss: 0.0511, Val Loss: 0.0680\n",
      "Epoch 54/300 - Train Loss: 0.0515, Val Loss: 0.0710\n",
      "Epoch 55/300 - Train Loss: 0.0516, Val Loss: 0.0688\n",
      "Epoch 56/300 - Train Loss: 0.0491, Val Loss: 0.0709\n",
      "Epoch 57/300 - Train Loss: 0.0499, Val Loss: 0.0714\n",
      "Epoch 58/300 - Train Loss: 0.0540, Val Loss: 0.0690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:32:52,914] Trial 495 finished with value: 0.9708343642906406 and parameters: {'F1': 32, 'F2': 32, 'D': 8, 'dropout': 0.17892132300905975, 'learning_rate': 7.748380559117077e-05, 'batch_size': 32, 'weight_decay': 3.239312339294278e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/300 - Train Loss: 0.0494, Val Loss: 0.0678\n",
      "Early stopping at epoch 59\n",
      "Macro F1 Score: 0.9708, Macro Precision: 0.9645, Macro Recall: 0.9775\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 497\n",
      "Training with F1=16, F2=16, D=2, dropout=0.1339731334314469, LR=0.0008806478179353543, BS=32, WD=7.911444381682655e-05\n",
      "Epoch 1/300 - Train Loss: 0.1617, Val Loss: 0.0888\n",
      "Epoch 2/300 - Train Loss: 0.0976, Val Loss: 0.0778\n",
      "Epoch 3/300 - Train Loss: 0.0940, Val Loss: 0.0816\n",
      "Epoch 4/300 - Train Loss: 0.0899, Val Loss: 0.0891\n",
      "Epoch 5/300 - Train Loss: 0.0874, Val Loss: 0.0754\n",
      "Epoch 6/300 - Train Loss: 0.0865, Val Loss: 0.0854\n",
      "Epoch 7/300 - Train Loss: 0.0846, Val Loss: 0.0738\n",
      "Epoch 8/300 - Train Loss: 0.0833, Val Loss: 0.0696\n",
      "Epoch 9/300 - Train Loss: 0.0817, Val Loss: 0.0794\n",
      "Epoch 10/300 - Train Loss: 0.0830, Val Loss: 0.0736\n",
      "Epoch 11/300 - Train Loss: 0.0808, Val Loss: 0.0722\n",
      "Epoch 12/300 - Train Loss: 0.0796, Val Loss: 0.0737\n",
      "Epoch 13/300 - Train Loss: 0.0811, Val Loss: 0.0765\n",
      "Epoch 14/300 - Train Loss: 0.0808, Val Loss: 0.0772\n",
      "Epoch 15/300 - Train Loss: 0.0785, Val Loss: 0.0865\n",
      "Epoch 16/300 - Train Loss: 0.0762, Val Loss: 0.0783\n",
      "Epoch 17/300 - Train Loss: 0.0771, Val Loss: 0.0737\n",
      "Epoch 18/300 - Train Loss: 0.0751, Val Loss: 0.0732\n",
      "Epoch 19/300 - Train Loss: 0.0754, Val Loss: 0.0777\n",
      "Epoch 20/300 - Train Loss: 0.0731, Val Loss: 0.0772\n",
      "Epoch 21/300 - Train Loss: 0.0740, Val Loss: 0.0869\n",
      "Epoch 22/300 - Train Loss: 0.0740, Val Loss: 0.0806\n",
      "Epoch 23/300 - Train Loss: 0.0739, Val Loss: 0.0825\n",
      "Epoch 24/300 - Train Loss: 0.0743, Val Loss: 0.0800\n",
      "Epoch 25/300 - Train Loss: 0.0709, Val Loss: 0.0725\n",
      "Epoch 26/300 - Train Loss: 0.0715, Val Loss: 0.0796\n",
      "Epoch 27/300 - Train Loss: 0.0709, Val Loss: 0.0690\n",
      "Epoch 28/300 - Train Loss: 0.0715, Val Loss: 0.0685\n",
      "Epoch 29/300 - Train Loss: 0.0707, Val Loss: 0.0767\n",
      "Epoch 30/300 - Train Loss: 0.0707, Val Loss: 0.0825\n",
      "Epoch 31/300 - Train Loss: 0.0680, Val Loss: 0.0691\n",
      "Epoch 32/300 - Train Loss: 0.0688, Val Loss: 0.0849\n",
      "Epoch 33/300 - Train Loss: 0.0712, Val Loss: 0.0849\n",
      "Epoch 34/300 - Train Loss: 0.0686, Val Loss: 0.0765\n",
      "Epoch 35/300 - Train Loss: 0.0673, Val Loss: 0.0714\n",
      "Epoch 36/300 - Train Loss: 0.0675, Val Loss: 0.0820\n",
      "Epoch 37/300 - Train Loss: 0.0671, Val Loss: 0.0756\n",
      "Epoch 38/300 - Train Loss: 0.0684, Val Loss: 0.0735\n",
      "Epoch 39/300 - Train Loss: 0.0658, Val Loss: 0.0766\n",
      "Epoch 40/300 - Train Loss: 0.0654, Val Loss: 0.0716\n",
      "Epoch 41/300 - Train Loss: 0.0642, Val Loss: 0.0779\n",
      "Epoch 42/300 - Train Loss: 0.0643, Val Loss: 0.0772\n",
      "Epoch 43/300 - Train Loss: 0.0664, Val Loss: 0.0744\n",
      "Epoch 44/300 - Train Loss: 0.0669, Val Loss: 0.0746\n",
      "Epoch 45/300 - Train Loss: 0.0651, Val Loss: 0.0820\n",
      "Epoch 46/300 - Train Loss: 0.0622, Val Loss: 0.0883\n",
      "Epoch 47/300 - Train Loss: 0.0652, Val Loss: 0.0768\n",
      "Epoch 48/300 - Train Loss: 0.0659, Val Loss: 0.0880\n",
      "Epoch 49/300 - Train Loss: 0.0628, Val Loss: 0.0778\n",
      "Epoch 50/300 - Train Loss: 0.0634, Val Loss: 0.0728\n",
      "Epoch 51/300 - Train Loss: 0.0598, Val Loss: 0.0820\n",
      "Epoch 52/300 - Train Loss: 0.0613, Val Loss: 0.0714\n",
      "Epoch 53/300 - Train Loss: 0.0622, Val Loss: 0.0700\n",
      "Epoch 54/300 - Train Loss: 0.0630, Val Loss: 0.0824\n",
      "Epoch 55/300 - Train Loss: 0.0660, Val Loss: 0.0896\n",
      "Epoch 56/300 - Train Loss: 0.0635, Val Loss: 0.0848\n",
      "Epoch 57/300 - Train Loss: 0.0620, Val Loss: 0.0755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:34:55,035] Trial 496 finished with value: 0.964819135519522 and parameters: {'F1': 16, 'F2': 16, 'D': 2, 'dropout': 0.1339731334314469, 'learning_rate': 0.0008806478179353543, 'batch_size': 32, 'weight_decay': 7.911444381682655e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/300 - Train Loss: 0.0609, Val Loss: 0.0741\n",
      "Early stopping at epoch 58\n",
      "Macro F1 Score: 0.9648, Macro Precision: 0.9509, Macro Recall: 0.9806\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       789\n",
      "           1       0.88      0.98      0.93        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.95      0.98      0.96      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 498\n",
      "Training with F1=32, F2=32, D=2, dropout=0.15260115554635983, LR=0.00010969378429121506, BS=32, WD=9.95638083661766e-05\n",
      "Epoch 1/300 - Train Loss: 0.2424, Val Loss: 0.0931\n",
      "Epoch 2/300 - Train Loss: 0.1079, Val Loss: 0.0890\n",
      "Epoch 3/300 - Train Loss: 0.0987, Val Loss: 0.0802\n",
      "Epoch 4/300 - Train Loss: 0.0939, Val Loss: 0.0821\n",
      "Epoch 5/300 - Train Loss: 0.0898, Val Loss: 0.0756\n",
      "Epoch 6/300 - Train Loss: 0.0884, Val Loss: 0.0777\n",
      "Epoch 7/300 - Train Loss: 0.0856, Val Loss: 0.0851\n",
      "Epoch 8/300 - Train Loss: 0.0855, Val Loss: 0.0702\n",
      "Epoch 9/300 - Train Loss: 0.0833, Val Loss: 0.0777\n",
      "Epoch 10/300 - Train Loss: 0.0812, Val Loss: 0.0735\n",
      "Epoch 11/300 - Train Loss: 0.0800, Val Loss: 0.0715\n",
      "Epoch 12/300 - Train Loss: 0.0823, Val Loss: 0.0712\n",
      "Epoch 13/300 - Train Loss: 0.0804, Val Loss: 0.0765\n",
      "Epoch 14/300 - Train Loss: 0.0793, Val Loss: 0.0811\n",
      "Epoch 15/300 - Train Loss: 0.0771, Val Loss: 0.0741\n",
      "Epoch 16/300 - Train Loss: 0.0766, Val Loss: 0.0809\n",
      "Epoch 17/300 - Train Loss: 0.0768, Val Loss: 0.0762\n",
      "Epoch 18/300 - Train Loss: 0.0756, Val Loss: 0.0815\n",
      "Epoch 19/300 - Train Loss: 0.0745, Val Loss: 0.0747\n",
      "Epoch 20/300 - Train Loss: 0.0737, Val Loss: 0.0708\n",
      "Epoch 21/300 - Train Loss: 0.0723, Val Loss: 0.0692\n",
      "Epoch 22/300 - Train Loss: 0.0742, Val Loss: 0.0705\n",
      "Epoch 23/300 - Train Loss: 0.0718, Val Loss: 0.0719\n",
      "Epoch 24/300 - Train Loss: 0.0711, Val Loss: 0.0746\n",
      "Epoch 25/300 - Train Loss: 0.0723, Val Loss: 0.0737\n",
      "Epoch 26/300 - Train Loss: 0.0698, Val Loss: 0.0705\n",
      "Epoch 27/300 - Train Loss: 0.0710, Val Loss: 0.0745\n",
      "Epoch 28/300 - Train Loss: 0.0703, Val Loss: 0.0700\n",
      "Epoch 29/300 - Train Loss: 0.0664, Val Loss: 0.0753\n",
      "Epoch 30/300 - Train Loss: 0.0676, Val Loss: 0.0732\n",
      "Epoch 31/300 - Train Loss: 0.0678, Val Loss: 0.0742\n",
      "Epoch 32/300 - Train Loss: 0.0670, Val Loss: 0.0736\n",
      "Epoch 33/300 - Train Loss: 0.0656, Val Loss: 0.0694\n",
      "Epoch 34/300 - Train Loss: 0.0664, Val Loss: 0.0694\n",
      "Epoch 35/300 - Train Loss: 0.0652, Val Loss: 0.0697\n",
      "Epoch 36/300 - Train Loss: 0.0667, Val Loss: 0.0691\n",
      "Epoch 37/300 - Train Loss: 0.0645, Val Loss: 0.0726\n",
      "Epoch 38/300 - Train Loss: 0.0634, Val Loss: 0.0698\n",
      "Epoch 39/300 - Train Loss: 0.0634, Val Loss: 0.0708\n",
      "Epoch 40/300 - Train Loss: 0.0622, Val Loss: 0.0721\n",
      "Epoch 41/300 - Train Loss: 0.0642, Val Loss: 0.0735\n",
      "Epoch 42/300 - Train Loss: 0.0609, Val Loss: 0.0764\n",
      "Epoch 43/300 - Train Loss: 0.0626, Val Loss: 0.0725\n",
      "Epoch 44/300 - Train Loss: 0.0606, Val Loss: 0.0722\n",
      "Epoch 45/300 - Train Loss: 0.0597, Val Loss: 0.0679\n",
      "Epoch 46/300 - Train Loss: 0.0622, Val Loss: 0.0726\n",
      "Epoch 47/300 - Train Loss: 0.0590, Val Loss: 0.0758\n",
      "Epoch 48/300 - Train Loss: 0.0614, Val Loss: 0.0675\n",
      "Epoch 49/300 - Train Loss: 0.0624, Val Loss: 0.0679\n",
      "Epoch 50/300 - Train Loss: 0.0585, Val Loss: 0.0729\n",
      "Epoch 51/300 - Train Loss: 0.0582, Val Loss: 0.0690\n",
      "Epoch 52/300 - Train Loss: 0.0585, Val Loss: 0.0706\n",
      "Epoch 53/300 - Train Loss: 0.0573, Val Loss: 0.0713\n",
      "Epoch 54/300 - Train Loss: 0.0556, Val Loss: 0.0738\n",
      "Epoch 55/300 - Train Loss: 0.0605, Val Loss: 0.0787\n",
      "Epoch 56/300 - Train Loss: 0.0551, Val Loss: 0.0708\n",
      "Epoch 57/300 - Train Loss: 0.0562, Val Loss: 0.0704\n",
      "Epoch 58/300 - Train Loss: 0.0542, Val Loss: 0.0733\n",
      "Epoch 59/300 - Train Loss: 0.0551, Val Loss: 0.0708\n",
      "Epoch 60/300 - Train Loss: 0.0527, Val Loss: 0.0715\n",
      "Epoch 61/300 - Train Loss: 0.0560, Val Loss: 0.0723\n",
      "Epoch 62/300 - Train Loss: 0.0527, Val Loss: 0.0721\n",
      "Epoch 63/300 - Train Loss: 0.0533, Val Loss: 0.0703\n",
      "Epoch 64/300 - Train Loss: 0.0527, Val Loss: 0.0800\n",
      "Epoch 65/300 - Train Loss: 0.0528, Val Loss: 0.0712\n",
      "Epoch 66/300 - Train Loss: 0.0511, Val Loss: 0.0722\n",
      "Epoch 67/300 - Train Loss: 0.0532, Val Loss: 0.0753\n",
      "Epoch 68/300 - Train Loss: 0.0501, Val Loss: 0.0692\n",
      "Epoch 69/300 - Train Loss: 0.0532, Val Loss: 0.0714\n",
      "Epoch 70/300 - Train Loss: 0.0513, Val Loss: 0.0710\n",
      "Epoch 71/300 - Train Loss: 0.0505, Val Loss: 0.0705\n",
      "Epoch 72/300 - Train Loss: 0.0505, Val Loss: 0.0696\n",
      "Epoch 73/300 - Train Loss: 0.0516, Val Loss: 0.0798\n",
      "Epoch 74/300 - Train Loss: 0.0516, Val Loss: 0.0742\n",
      "Epoch 75/300 - Train Loss: 0.0499, Val Loss: 0.0734\n",
      "Epoch 76/300 - Train Loss: 0.0495, Val Loss: 0.0740\n",
      "Epoch 77/300 - Train Loss: 0.0474, Val Loss: 0.0736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:38:29,663] Trial 497 finished with value: 0.9709540969483809 and parameters: {'F1': 32, 'F2': 32, 'D': 2, 'dropout': 0.15260115554635983, 'learning_rate': 0.00010969378429121506, 'batch_size': 32, 'weight_decay': 9.95638083661766e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/300 - Train Loss: 0.0510, Val Loss: 0.0709\n",
      "Early stopping at epoch 78\n",
      "Macro F1 Score: 0.9710, Macro Precision: 0.9692, Macro Recall: 0.9728\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99       789\n",
      "           1       0.94      0.95      0.94        61\n",
      "           2       0.99      0.98      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.97      0.97      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 499\n",
      "Training with F1=32, F2=8, D=8, dropout=0.23497913386635555, LR=0.00022079295462288558, BS=32, WD=5.1806684276364865e-05\n",
      "Epoch 1/300 - Train Loss: 0.2290, Val Loss: 0.0969\n",
      "Epoch 2/300 - Train Loss: 0.1069, Val Loss: 0.0764\n",
      "Epoch 3/300 - Train Loss: 0.0970, Val Loss: 0.0708\n",
      "Epoch 4/300 - Train Loss: 0.0919, Val Loss: 0.0943\n",
      "Epoch 5/300 - Train Loss: 0.0886, Val Loss: 0.0731\n",
      "Epoch 6/300 - Train Loss: 0.0861, Val Loss: 0.0762\n",
      "Epoch 7/300 - Train Loss: 0.0856, Val Loss: 0.0677\n",
      "Epoch 8/300 - Train Loss: 0.0841, Val Loss: 0.0688\n",
      "Epoch 9/300 - Train Loss: 0.0847, Val Loss: 0.0763\n",
      "Epoch 10/300 - Train Loss: 0.0819, Val Loss: 0.0694\n",
      "Epoch 11/300 - Train Loss: 0.0824, Val Loss: 0.0684\n",
      "Epoch 12/300 - Train Loss: 0.0799, Val Loss: 0.0705\n",
      "Epoch 13/300 - Train Loss: 0.0829, Val Loss: 0.0691\n",
      "Epoch 14/300 - Train Loss: 0.0795, Val Loss: 0.0686\n",
      "Epoch 15/300 - Train Loss: 0.0792, Val Loss: 0.0659\n",
      "Epoch 16/300 - Train Loss: 0.0777, Val Loss: 0.0658\n",
      "Epoch 17/300 - Train Loss: 0.0770, Val Loss: 0.0673\n",
      "Epoch 18/300 - Train Loss: 0.0787, Val Loss: 0.0677\n",
      "Epoch 19/300 - Train Loss: 0.0774, Val Loss: 0.0694\n",
      "Epoch 20/300 - Train Loss: 0.0778, Val Loss: 0.0765\n",
      "Epoch 21/300 - Train Loss: 0.0770, Val Loss: 0.0680\n",
      "Epoch 22/300 - Train Loss: 0.0766, Val Loss: 0.0834\n",
      "Epoch 23/300 - Train Loss: 0.0756, Val Loss: 0.0655\n",
      "Epoch 24/300 - Train Loss: 0.0756, Val Loss: 0.0650\n",
      "Epoch 25/300 - Train Loss: 0.0739, Val Loss: 0.0660\n",
      "Epoch 26/300 - Train Loss: 0.0758, Val Loss: 0.0679\n",
      "Epoch 27/300 - Train Loss: 0.0750, Val Loss: 0.0776\n",
      "Epoch 28/300 - Train Loss: 0.0714, Val Loss: 0.0754\n",
      "Epoch 29/300 - Train Loss: 0.0724, Val Loss: 0.0648\n",
      "Epoch 30/300 - Train Loss: 0.0736, Val Loss: 0.0652\n",
      "Epoch 31/300 - Train Loss: 0.0728, Val Loss: 0.0673\n",
      "Epoch 32/300 - Train Loss: 0.0728, Val Loss: 0.0684\n",
      "Epoch 33/300 - Train Loss: 0.0755, Val Loss: 0.0687\n",
      "Epoch 34/300 - Train Loss: 0.0699, Val Loss: 0.0728\n",
      "Epoch 35/300 - Train Loss: 0.0718, Val Loss: 0.0659\n",
      "Epoch 36/300 - Train Loss: 0.0712, Val Loss: 0.0667\n",
      "Epoch 37/300 - Train Loss: 0.0706, Val Loss: 0.0634\n",
      "Epoch 38/300 - Train Loss: 0.0708, Val Loss: 0.0704\n",
      "Epoch 39/300 - Train Loss: 0.0700, Val Loss: 0.0685\n",
      "Epoch 40/300 - Train Loss: 0.0718, Val Loss: 0.0658\n",
      "Epoch 41/300 - Train Loss: 0.0697, Val Loss: 0.0632\n",
      "Epoch 42/300 - Train Loss: 0.0693, Val Loss: 0.0664\n",
      "Epoch 43/300 - Train Loss: 0.0708, Val Loss: 0.0689\n",
      "Epoch 44/300 - Train Loss: 0.0681, Val Loss: 0.0674\n",
      "Epoch 45/300 - Train Loss: 0.0706, Val Loss: 0.0666\n",
      "Epoch 46/300 - Train Loss: 0.0692, Val Loss: 0.0661\n",
      "Epoch 47/300 - Train Loss: 0.0682, Val Loss: 0.0648\n",
      "Epoch 48/300 - Train Loss: 0.0696, Val Loss: 0.0677\n",
      "Epoch 49/300 - Train Loss: 0.0678, Val Loss: 0.0658\n",
      "Epoch 50/300 - Train Loss: 0.0665, Val Loss: 0.0706\n",
      "Epoch 51/300 - Train Loss: 0.0670, Val Loss: 0.0702\n",
      "Epoch 52/300 - Train Loss: 0.0655, Val Loss: 0.0680\n",
      "Epoch 53/300 - Train Loss: 0.0685, Val Loss: 0.0712\n",
      "Epoch 54/300 - Train Loss: 0.0678, Val Loss: 0.0661\n",
      "Epoch 55/300 - Train Loss: 0.0681, Val Loss: 0.0808\n",
      "Epoch 56/300 - Train Loss: 0.0673, Val Loss: 0.0657\n",
      "Epoch 57/300 - Train Loss: 0.0662, Val Loss: 0.0705\n",
      "Epoch 58/300 - Train Loss: 0.0672, Val Loss: 0.0661\n",
      "Epoch 59/300 - Train Loss: 0.0667, Val Loss: 0.0650\n",
      "Epoch 60/300 - Train Loss: 0.0668, Val Loss: 0.0707\n",
      "Epoch 61/300 - Train Loss: 0.0665, Val Loss: 0.0686\n",
      "Epoch 62/300 - Train Loss: 0.0672, Val Loss: 0.0727\n",
      "Epoch 63/300 - Train Loss: 0.0645, Val Loss: 0.0657\n",
      "Epoch 64/300 - Train Loss: 0.0642, Val Loss: 0.0666\n",
      "Epoch 65/300 - Train Loss: 0.0659, Val Loss: 0.0665\n",
      "Epoch 66/300 - Train Loss: 0.0648, Val Loss: 0.0699\n",
      "Epoch 67/300 - Train Loss: 0.0647, Val Loss: 0.0650\n",
      "Epoch 68/300 - Train Loss: 0.0647, Val Loss: 0.0654\n",
      "Epoch 69/300 - Train Loss: 0.0642, Val Loss: 0.0643\n",
      "Epoch 70/300 - Train Loss: 0.0635, Val Loss: 0.0678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:45:16,918] Trial 498 finished with value: 0.9698358847635905 and parameters: {'F1': 32, 'F2': 8, 'D': 8, 'dropout': 0.23497913386635555, 'learning_rate': 0.00022079295462288558, 'batch_size': 32, 'weight_decay': 5.1806684276364865e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/300 - Train Loss: 0.0642, Val Loss: 0.0644\n",
      "Early stopping at epoch 71\n",
      "Macro F1 Score: 0.9698, Macro Precision: 0.9640, Macro Recall: 0.9761\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.92      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "\n",
      "Trial 500\n",
      "Training with F1=16, F2=32, D=2, dropout=0.12127149797542267, LR=7.019918163943999e-05, BS=32, WD=3.7133046010277204e-05\n",
      "Epoch 1/300 - Train Loss: 0.3539, Val Loss: 0.1482\n",
      "Epoch 2/300 - Train Loss: 0.1423, Val Loss: 0.1023\n",
      "Epoch 3/300 - Train Loss: 0.1170, Val Loss: 0.0882\n",
      "Epoch 4/300 - Train Loss: 0.1076, Val Loss: 0.0931\n",
      "Epoch 5/300 - Train Loss: 0.0997, Val Loss: 0.0776\n",
      "Epoch 6/300 - Train Loss: 0.0967, Val Loss: 0.0855\n",
      "Epoch 7/300 - Train Loss: 0.0957, Val Loss: 0.0761\n",
      "Epoch 8/300 - Train Loss: 0.0917, Val Loss: 0.0756\n",
      "Epoch 9/300 - Train Loss: 0.0897, Val Loss: 0.0751\n",
      "Epoch 10/300 - Train Loss: 0.0896, Val Loss: 0.0777\n",
      "Epoch 11/300 - Train Loss: 0.0878, Val Loss: 0.0719\n",
      "Epoch 12/300 - Train Loss: 0.0876, Val Loss: 0.0758\n",
      "Epoch 13/300 - Train Loss: 0.0846, Val Loss: 0.0731\n",
      "Epoch 14/300 - Train Loss: 0.0847, Val Loss: 0.0756\n",
      "Epoch 15/300 - Train Loss: 0.0838, Val Loss: 0.0732\n",
      "Epoch 16/300 - Train Loss: 0.0817, Val Loss: 0.0720\n",
      "Epoch 17/300 - Train Loss: 0.0828, Val Loss: 0.0741\n",
      "Epoch 18/300 - Train Loss: 0.0826, Val Loss: 0.0731\n",
      "Epoch 19/300 - Train Loss: 0.0793, Val Loss: 0.0722\n",
      "Epoch 20/300 - Train Loss: 0.0804, Val Loss: 0.0711\n",
      "Epoch 21/300 - Train Loss: 0.0811, Val Loss: 0.0736\n",
      "Epoch 22/300 - Train Loss: 0.0796, Val Loss: 0.0683\n",
      "Epoch 23/300 - Train Loss: 0.0789, Val Loss: 0.0703\n",
      "Epoch 24/300 - Train Loss: 0.0789, Val Loss: 0.0732\n",
      "Epoch 25/300 - Train Loss: 0.0773, Val Loss: 0.0756\n",
      "Epoch 26/300 - Train Loss: 0.0761, Val Loss: 0.0732\n",
      "Epoch 27/300 - Train Loss: 0.0758, Val Loss: 0.0688\n",
      "Epoch 28/300 - Train Loss: 0.0760, Val Loss: 0.0712\n",
      "Epoch 29/300 - Train Loss: 0.0754, Val Loss: 0.0690\n",
      "Epoch 30/300 - Train Loss: 0.0747, Val Loss: 0.0705\n",
      "Epoch 31/300 - Train Loss: 0.0741, Val Loss: 0.0685\n",
      "Epoch 32/300 - Train Loss: 0.0741, Val Loss: 0.0690\n",
      "Epoch 33/300 - Train Loss: 0.0736, Val Loss: 0.0709\n",
      "Epoch 34/300 - Train Loss: 0.0739, Val Loss: 0.0702\n",
      "Epoch 35/300 - Train Loss: 0.0744, Val Loss: 0.0732\n",
      "Epoch 36/300 - Train Loss: 0.0722, Val Loss: 0.0768\n",
      "Epoch 37/300 - Train Loss: 0.0732, Val Loss: 0.0690\n",
      "Epoch 38/300 - Train Loss: 0.0732, Val Loss: 0.0697\n",
      "Epoch 39/300 - Train Loss: 0.0731, Val Loss: 0.0683\n",
      "Epoch 40/300 - Train Loss: 0.0713, Val Loss: 0.0691\n",
      "Epoch 41/300 - Train Loss: 0.0729, Val Loss: 0.0689\n",
      "Epoch 42/300 - Train Loss: 0.0681, Val Loss: 0.0700\n",
      "Epoch 43/300 - Train Loss: 0.0706, Val Loss: 0.0725\n",
      "Epoch 44/300 - Train Loss: 0.0691, Val Loss: 0.0707\n",
      "Epoch 45/300 - Train Loss: 0.0692, Val Loss: 0.0714\n",
      "Epoch 46/300 - Train Loss: 0.0691, Val Loss: 0.0749\n",
      "Epoch 47/300 - Train Loss: 0.0699, Val Loss: 0.0725\n",
      "Epoch 48/300 - Train Loss: 0.0694, Val Loss: 0.0696\n",
      "Epoch 49/300 - Train Loss: 0.0728, Val Loss: 0.0702\n",
      "Epoch 50/300 - Train Loss: 0.0695, Val Loss: 0.0724\n",
      "Epoch 51/300 - Train Loss: 0.0670, Val Loss: 0.0705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_524047/1455158977.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
      "[I 2025-05-02 05:47:07,994] Trial 499 finished with value: 0.9670490334339995 and parameters: {'F1': 16, 'F2': 32, 'D': 2, 'dropout': 0.12127149797542267, 'learning_rate': 7.019918163943999e-05, 'batch_size': 32, 'weight_decay': 3.7133046010277204e-05}. Best is trial 355 with value: 0.9823538998172442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/300 - Train Loss: 0.0695, Val Loss: 0.0720\n",
      "Early stopping at epoch 52\n",
      "Macro F1 Score: 0.9670, Macro Precision: 0.9593, Macro Recall: 0.9756\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       789\n",
      "           1       0.91      0.97      0.94        61\n",
      "           2       0.99      0.97      0.98       593\n",
      "\n",
      "    accuracy                           0.98      1443\n",
      "   macro avg       0.96      0.98      0.97      1443\n",
      "weighted avg       0.98      0.98      0.98      1443\n",
      "\n",
      "*** New best model found with F1: 0.9474 ***\n",
      "*** New best model found with F1: 0.9667 ***\n",
      "*** New best model found with F1: 0.9680 ***\n",
      "*** New best model found with F1: 0.9694 ***\n",
      "*** New best model found with F1: 0.9700 ***\n",
      "*** New best model found with F1: 0.9720 ***\n",
      "*** New best model found with F1: 0.9743 ***\n",
      "*** New best model found with F1: 0.9761 ***\n",
      "*** New best model found with F1: 0.9790 ***\n",
      "*** New best model found with F1: 0.9804 ***\n",
      "*** New best model found with F1: 0.9824 ***\n",
      "\n",
      "All results saved to 'hyperparameter_results_optuna.csv'\n",
      "\n",
      "Best F1 Score: 0.9824\n",
      "Best Hyperparameters: {'F1': 32, 'F2': 8, 'D': 2, 'dropout': 0.1604465584506531, 'learning_rate': 0.0007497847670531389, 'batch_size': 32, 'weight_decay': 0.00021416718031719688}\n",
      "\n",
      "Study statistics: \n",
      "  Number of finished trials: 500\n",
      "  Best trial: 355\n",
      "Optimization history plot saved to 'optuna_optimization_history.html'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import random\n",
    "from sklearn.metrics import classification_report\n",
    "import optuna\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from eegnet import EEGNet\n",
    "\n",
    "# ------------------ 1. Reproducibility ------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ------------------ 2. Device ------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------------------ 3. Dataset ------------------\n",
    "full_dataset = TensorDataset(train_eeg_data, train_labels)\n",
    "total_size = len(full_dataset)\n",
    "val_size = int(0.05 * total_size)\n",
    "train_size = total_size - val_size\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=generator)\n",
    "\n",
    "# ------------------ 4. Training Function ------------------\n",
    "def train_and_evaluate(F1, F2, D, dropout, learning_rate, batch_size, weight_decay):\n",
    "    # Initialize the model with the current parameters\n",
    "    model = EEGNet(F1=F1, F2=F2, D=D, dropout_rate=dropout, num_classes=3).to(device)\n",
    "    \n",
    "    # Initialize optimizer with weight decay\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # DataLoaders with current batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 300\n",
    "    patience = 30  # Early stopping patience\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_eeg, batch_labels in train_loader:\n",
    "            batch_eeg, batch_labels = batch_eeg.to(device), batch_labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_eeg)\n",
    "            loss = criterion(outputs, batch_labels)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_eeg, batch_labels in val_loader:\n",
    "                batch_eeg, batch_labels = batch_eeg.to(device), batch_labels.to(device)\n",
    "                outputs = model(batch_eeg)\n",
    "                loss = criterion(outputs, batch_labels)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            counter = 0\n",
    "            # Save best model based on validation loss\n",
    "            torch.save(model.state_dict(), \"current_best_model.pth\")\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Load the best model before final evaluation\n",
    "    model.load_state_dict(torch.load(\"current_best_model.pth\"))\n",
    "    \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_eeg, batch_labels in val_loader:\n",
    "            batch_eeg, batch_labels = batch_eeg.to(device), batch_labels.to(device)\n",
    "            outputs = model(batch_eeg)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(batch_labels.cpu().numpy())\n",
    "    \n",
    "    # Metrics - using macro average instead of weighted\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    print(f\"Macro F1 Score: {f1:.4f}, Macro Precision: {precision:.4f}, Macro Recall: {recall:.4f}\")\n",
    "    report = classification_report(all_labels, all_preds)\n",
    "    print(\"\\nClassification Report:\\n\", report)\n",
    "    \n",
    "    return {\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'val_loss': best_val_loss\n",
    "    }\n",
    "\n",
    "# ------------------ 5. Optuna Study ------------------\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to optimize - focusing on ranges appropriate for 2 channels\n",
    "    F1 = trial.suggest_categorical('F1', [4, 8, 16, 32])\n",
    "    F2 = trial.suggest_categorical('F2', [8, 16, 32])\n",
    "    D = trial.suggest_categorical('D', [2, 4,8])\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.7)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64, 128, 256])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-2, log=True)\n",
    "    \n",
    "    print(f\"\\nTrial {trial.number + 1}\")\n",
    "    print(f\"Training with F1={F1}, F2={F2}, D={D}, dropout={dropout}, LR={learning_rate}, BS={batch_size}, WD={weight_decay}\")\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    metrics = train_and_evaluate(F1, F2, D, dropout, learning_rate, batch_size, weight_decay)\n",
    "    \n",
    "    # Store all results\n",
    "    trial.set_user_attr('precision', metrics['precision'])\n",
    "    trial.set_user_attr('recall', metrics['recall'])\n",
    "    trial.set_user_attr('val_loss', metrics['val_loss'])\n",
    "    \n",
    "    return metrics['f1_score']  # Optimize for F1 score\n",
    "\n",
    "# ------------------ 6. Run Optimization ------------------\n",
    "best_f1 = 0\n",
    "best_params = {}\n",
    "results = []\n",
    "save_every_n = 5  # Save intermediate results every n trials\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=500)  # Adjust number of trials as needed\n",
    "\n",
    "# ------------------ 7. Save Results ------------------\n",
    "# Extract and save all trials' results\n",
    "for trial in study.trials:\n",
    "    trial_params = trial.params\n",
    "    results.append({\n",
    "        'F1': trial_params.get('F1'),\n",
    "        'F2': trial_params.get('F2'),\n",
    "        'D': trial_params.get('D'),\n",
    "        'dropout': trial_params.get('dropout'),\n",
    "        'learning_rate': trial_params.get('learning_rate'),\n",
    "        'batch_size': trial_params.get('batch_size'),\n",
    "        'weight_decay': trial_params.get('weight_decay'),\n",
    "        'f1_score': trial.value,\n",
    "        'precision': trial.user_attrs.get('precision'),\n",
    "        'recall': trial.user_attrs.get('recall'),\n",
    "        'val_loss': trial.user_attrs.get('val_loss')\n",
    "    })\n",
    "    \n",
    "    # Save model with best F1 score\n",
    "    if trial.value > best_f1:\n",
    "        best_f1 = trial.value\n",
    "        best_params = trial_params\n",
    "        print(f\"*** New best model found with F1: {best_f1:.4f} ***\")\n",
    "\n",
    "# Save all results to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"hyperparameter_results_optuna.csv\", index=False)\n",
    "print(\"\\nAll results saved to 'hyperparameter_results_optuna.csv'\")\n",
    "\n",
    "# Print best parameters\n",
    "print(f\"\\nBest F1 Score: {best_f1:.4f}\")\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "\n",
    "# You can get more insights from the study\n",
    "print(\"\\nStudy statistics: \")\n",
    "print(f\"  Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"  Best trial: {study.best_trial.number}\")\n",
    "\n",
    "# Plot optimization history\n",
    "try:\n",
    "    from optuna.visualization import plot_optimization_history\n",
    "    fig = plot_optimization_history(study)\n",
    "    fig.write_html(\"optuna_optimization_history.html\")\n",
    "    print(\"Optimization history plot saved to 'optuna_optimization_history.html'\")\n",
    "except ImportError:\n",
    "    print(\"Optuna visualization requires plotly. Install with: pip install plotly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260dcc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba8f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e6ec3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
